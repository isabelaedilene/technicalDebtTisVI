file path,line #,comment,satd
scikit-learn/conftest.py,1,Even if empty this file is useful so that when running from the root folder,not
scikit-learn/conftest.py,2,./sklearn is added to sys.path by pytest. See,not
scikit-learn/conftest.py,3,https://docs.pytest.org/en/latest/pythonpath.html for more details.  For,not
scikit-learn/conftest.py,4,"example, this allows to build extensions in place and run pytest",not
scikit-learn/conftest.py,5,doc/modules/clustering.rst and use sklearn from the local folder rather than,not
scikit-learn/conftest.py,6,the one from site-packages.,not
scikit-learn/conftest.py,36,FeatureHasher is not compatible with PyPy,not
scikit-learn/conftest.py,45,Skip tests which require internet if the flag is provided,not
scikit-learn/conftest.py,53,numpy changed the str/repr formatting of numpy arrays in 1.14. We want to,not
scikit-learn/conftest.py,54,run doctests only for numpy >= 1.14.,not
scikit-learn/conftest.py,90,declare our custom markers to avoid PytestUnknownMarkWarning,not
scikit-learn/conftest.py,102,TODO: Remove when modules are deprecated in 0.24,SATD
scikit-learn/conftest.py,103,Configures pytest to ignore deprecated modules.,not
scikit-learn/setup.py,1,! /usr/bin/env python,not
scikit-learn/setup.py,2,,not
scikit-learn/setup.py,3,Copyright (C) 2007-2009 Cournapeau David <cournape@gmail.com>,not
scikit-learn/setup.py,4,2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/setup.py,5,License: 3-clause BSD,not
scikit-learn/setup.py,18,Python 2 compat: just to be able to declare that Python >=3.6 is needed.,not
scikit-learn/setup.py,21,This is a bit (!) hackish: we are setting a global variable so that the,SATD
scikit-learn/setup.py,22,main sklearn __init__ can detect if it is being loaded by the setup,not
scikit-learn/setup.py,23,"routine, to avoid attempting to load components that aren't built yet:",not
scikit-learn/setup.py,24,the numpy distutils extensions that are used by scikit-learn to,not
scikit-learn/setup.py,25,recursively build the compiled extensions in sub-packages is based on the,not
scikit-learn/setup.py,26,Python import machinery.,not
scikit-learn/setup.py,45,We can actually import a restricted version of sklearn that,not
scikit-learn/setup.py,46,does not need the compiled code,not
scikit-learn/setup.py,61,Optional setuptools features,not
scikit-learn/setup.py,62,"We need to import setuptools early, if we want setuptools features,",not
scikit-learn/setup.py,63,as it monkey-patches the 'setup' function,not
scikit-learn/setup.py,64,"For some commands, use setuptools",not
scikit-learn/setup.py,75,the package can run out of an .egg file,not
scikit-learn/setup.py,88,Custom clean command to remove build artifacts,not
scikit-learn/setup.py,95,Remove c files if we are not within a sdist package,not
scikit-learn/setup.py,120,custom build_ext command to set OpenMP compile flags depending on os and,not
scikit-learn/setup.py,121,compiler,not
scikit-learn/setup.py,122,build_ext has to be imported after setuptools,not
scikit-learn/setup.py,124,noqa,not
scikit-learn/setup.py,142,Numpy should not be a dependency just to be able to introspect,not
scikit-learn/setup.py,143,that python 3.6 is required.,not
scikit-learn/setup.py,147,Optional wheelhouse-uploader features,not
scikit-learn/setup.py,148,To automate release of binary packages for scikit-learn we need a tool,not
scikit-learn/setup.py,149,to download the packages generated by travis and appveyor workers (with,not
scikit-learn/setup.py,150,version number matching the current release) and upload them all at once,not
scikit-learn/setup.py,151,to PyPI at release time.,not
scikit-learn/setup.py,152,The URL of the artifact repositories are configured in the setup.cfg file.,not
scikit-learn/setup.py,170,Avoid non-useful msg:,not
scikit-learn/setup.py,171,"""Ignoring attempt to set 'name' (from ... """,not
scikit-learn/setup.py,177,Cython is required by config.add_subpackage for templated extensions,not
scikit-learn/setup.py,178,that need the tempita sub-submodule. So check that we have the correct,not
scikit-learn/setup.py,179,version of Cython so as to be able to raise a more informative error,not
scikit-learn/setup.py,180,message from the start if it's not the case.,not
scikit-learn/setup.py,274,"For these actions, NumPy is not required",not
scikit-learn/setup.py,275,,not
scikit-learn/setup.py,276,They are required to succeed without Numpy for example when,not
scikit-learn/setup.py,277,pip is used to install Scikit-learn when Numpy is not yet present in,not
scikit-learn/setup.py,278,the system.,not
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,7,"To use this experimental feature, we need to explicitly ask for it:",not
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,8,noqa,not
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,107,loss='auto' does not work with get_equivalent_estimator(),not
scikit-learn/benchmarks/bench_hist_gradient_boosting.py,111,regression,not
scikit-learn/benchmarks/bench_covertype.py,44,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/benchmarks/bench_covertype.py,45,Arnaud Joly <arnaud.v.joly@gmail.com>,not
scikit-learn/benchmarks/bench_covertype.py,46,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_covertype.py,64,Memoize the data extraction and memory map the resulting,not
scikit-learn/benchmarks/bench_covertype.py,65,train / test splits in readonly mode,not
scikit-learn/benchmarks/bench_covertype.py,73,,not
scikit-learn/benchmarks/bench_covertype.py,74,Load dataset,not
scikit-learn/benchmarks/bench_covertype.py,81,"Create train-test split (as [Joachims, 2006])",not
scikit-learn/benchmarks/bench_covertype.py,89,Standardize first 10 features (the numerical ones),not
scikit-learn/benchmarks/bench_random_projections.py,39,number of microseconds in a second,not
scikit-learn/benchmarks/bench_random_projections.py,49,start time,not
scikit-learn/benchmarks/bench_random_projections.py,53,stop time,not
scikit-learn/benchmarks/bench_random_projections.py,56,start time,not
scikit-learn/benchmarks/bench_random_projections.py,60,stop time,not
scikit-learn/benchmarks/bench_random_projections.py,66,Make some random data with uniformly located non zero entries with,not
scikit-learn/benchmarks/bench_random_projections.py,67,Gaussian distributed values,not
scikit-learn/benchmarks/bench_random_projections.py,86,,not
scikit-learn/benchmarks/bench_random_projections.py,87,Option parser,not
scikit-learn/benchmarks/bench_random_projections.py,88,,not
scikit-learn/benchmarks/bench_random_projections.py,146,,not
scikit-learn/benchmarks/bench_random_projections.py,147,Generate dataset,not
scikit-learn/benchmarks/bench_random_projections.py,148,,not
scikit-learn/benchmarks/bench_random_projections.py,166,,not
scikit-learn/benchmarks/bench_random_projections.py,167,Set transformer input,not
scikit-learn/benchmarks/bench_random_projections.py,168,,not
scikit-learn/benchmarks/bench_random_projections.py,171,,not
scikit-learn/benchmarks/bench_random_projections.py,172,Set GaussianRandomProjection input,not
scikit-learn/benchmarks/bench_random_projections.py,180,,not
scikit-learn/benchmarks/bench_random_projections.py,181,Set SparseRandomProjection input,not
scikit-learn/benchmarks/bench_random_projections.py,192,,not
scikit-learn/benchmarks/bench_random_projections.py,193,Perform benchmark,not
scikit-learn/benchmarks/bench_random_projections.py,194,,not
scikit-learn/benchmarks/bench_random_projections.py,221,,not
scikit-learn/benchmarks/bench_random_projections.py,222,Print results,not
scikit-learn/benchmarks/bench_random_projections.py,223,,not
scikit-learn/benchmarks/bench_plot_fastkmeans.py,39,let's prepare the data in small chunks,not
scikit-learn/benchmarks/bench_plot_fastkmeans.py,93,register the 3d projection,not
scikit-learn/benchmarks/bench_tree.py,21,to store the results,not
scikit-learn/benchmarks/bench_tree.py,25,number of microseconds in a second,not
scikit-learn/benchmarks/bench_tree.py,35,start time,not
scikit-learn/benchmarks/bench_tree.py,40,stop time,not
scikit-learn/benchmarks/bench_tree.py,53,start time,not
scikit-learn/benchmarks/bench_tree.py,58,stop time,not
scikit-learn/benchmarks/bench_lasso.py,39,Normalize data,not
scikit-learn/benchmarks/bench_lasso.py,64,regularization parameter,not
scikit-learn/benchmarks/bench_multilabel_metrics.py,1,!/usr/bin/env python,not
scikit-learn/benchmarks/bench_saga.py,78,Makes cpu cache even for all fit calls,not
scikit-learn/benchmarks/bench_saga.py,90,Lightning predict_proba is not implemented for n_classes > 2,not
scikit-learn/benchmarks/bench_tsne_mnist.py,8,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_tsne_mnist.py,47,Normalize features,not
scikit-learn/benchmarks/bench_tsne_mnist.py,100,Put TSNE in methods,not
scikit-learn/benchmarks/bench_tsne_mnist.py,124,PCA preprocessing is done elsewhere in the benchmark script,not
scikit-learn/benchmarks/bench_tsne_mnist.py,125,TODO find a way to report the number of iterations,SATD
scikit-learn/benchmarks/bench_isotonic.py,33,Triggers O(n^2) complexity on the original implementation.,not
scikit-learn/benchmarks/bench_isotonic.py,89,"If we're not plotting, dump the timing to stdout",not
scikit-learn/benchmarks/bench_plot_svd.py,57,register the 3d projection,not
scikit-learn/benchmarks/bench_plot_svd.py,71,plot the actual surface,not
scikit-learn/benchmarks/bench_plot_svd.py,74,dummy point plot to stick the legend to since surface plot do not,not
scikit-learn/benchmarks/bench_plot_svd.py,75,support legends (yet?),not
scikit-learn/benchmarks/bench_plot_omp_lars.py,34,dataset_kwargs = {,not
scikit-learn/benchmarks/bench_plot_omp_lars.py,35,"'n_train_samples': n_samples,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,36,"'n_test_samples': 2,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,37,"'n_features': n_features,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,38,"'n_informative': n_informative,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,39,"'effective_rank': min(n_samples, n_features) / 10,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,40,"#'effective_rank': None,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,41,"'bias': 0.0,",not
scikit-learn/benchmarks/bench_plot_omp_lars.py,42,},not
scikit-learn/benchmarks/bench_plot_omp_lars.py,59,precomputed Gram matrix,not
scikit-learn/benchmarks/bench_lof.py,30,to control the random selection of anomalies in SA,not
scikit-learn/benchmarks/bench_lof.py,32,"datasets available: ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']",not
scikit-learn/benchmarks/bench_lof.py,37,loading and vectorization,not
scikit-learn/benchmarks/bench_lof.py,49,we remove data with label 4,not
scikit-learn/benchmarks/bench_lof.py,50,normal data are then those of class 1,not
scikit-learn/benchmarks/bench_lof.py,60,normal data are those with attribute 2,not
scikit-learn/benchmarks/bench_lof.py,61,abnormal those with attribute 4,not
scikit-learn/benchmarks/bench_lof.py,93,"the lower, the more normal",not
scikit-learn/benchmarks/bench_plot_incremental_pca.py,87,Compare runtimes and error for fixed batch size,not
scikit-learn/benchmarks/bench_plot_incremental_pca.py,119,Create flat baselines to compare the variation over batch size,not
scikit-learn/benchmarks/bench_plot_incremental_pca.py,140,limit dataset to 5000 people (don't care who they are!),not
scikit-learn/benchmarks/bench_feature_expansions.py,23,CSR,not
scikit-learn/benchmarks/bench_feature_expansions.py,27,Dense,not
scikit-learn/benchmarks/bench_feature_expansions.py,32,densely dashdotdotted,not
scikit-learn/benchmarks/bench_feature_expansions.py,33,solid,not
scikit-learn/benchmarks/bench_plot_nmf.py,4,Authors: Tom Dupre la Tour (benchmark),not
scikit-learn/benchmarks/bench_plot_nmf.py,5,Chih-Jen Linn (original projected gradient NMF implementation),not
scikit-learn/benchmarks/bench_plot_nmf.py,6,"Anthony Di Franco (projected gradient, Python and NumPy port)",not
scikit-learn/benchmarks/bench_plot_nmf.py,7,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_plot_nmf.py,33,,not
scikit-learn/benchmarks/bench_plot_nmf.py,34,Start of _PGNMF,not
scikit-learn/benchmarks/bench_plot_nmf.py,35,,not
scikit-learn/benchmarks/bench_plot_nmf.py,36,This class implements a projected gradient solver for the NMF.,not
scikit-learn/benchmarks/bench_plot_nmf.py,37,"The projected gradient solver was removed from scikit-learn in version 0.19,",not
scikit-learn/benchmarks/bench_plot_nmf.py,38,and a simplified copy is used here for comparison purpose only.,not
scikit-learn/benchmarks/bench_plot_nmf.py,39,"It is not tested, and it may change or disappear without notice.",not
scikit-learn/benchmarks/bench_plot_nmf.py,103,values justified in the paper (alpha is renamed gamma),not
scikit-learn/benchmarks/bench_plot_nmf.py,112,The following multiplication with a boolean array is more than twice,not
scikit-learn/benchmarks/bench_plot_nmf.py,113,as fast as indexing into grad.,not
scikit-learn/benchmarks/bench_plot_nmf.py,120,Gradient step.,not
scikit-learn/benchmarks/bench_plot_nmf.py,122,Projection step.,not
scikit-learn/benchmarks/bench_plot_nmf.py,159,"max(0.001, tol) to force alternating minimizations of W and H",not
scikit-learn/benchmarks/bench_plot_nmf.py,164,stopping condition as discussed in paper,not
scikit-learn/benchmarks/bench_plot_nmf.py,171,update W,not
scikit-learn/benchmarks/bench_plot_nmf.py,179,update H,not
scikit-learn/benchmarks/bench_plot_nmf.py,185,fix up negative zeros,not
scikit-learn/benchmarks/bench_plot_nmf.py,251,"check W and H, or initialize them",not
scikit-learn/benchmarks/bench_plot_nmf.py,262,fit_transform,not
scikit-learn/benchmarks/bench_plot_nmf.py,266,transform,not
scikit-learn/benchmarks/bench_plot_nmf.py,280,,not
scikit-learn/benchmarks/bench_plot_nmf.py,281,End of _PGNMF,not
scikit-learn/benchmarks/bench_plot_nmf.py,282,,not
scikit-learn/benchmarks/bench_plot_nmf.py,313,use joblib to cache the results.,not
scikit-learn/benchmarks/bench_plot_nmf.py,314,X_shape is specified in arguments for avoiding hashing X,not
scikit-learn/benchmarks/bench_plot_nmf.py,356,"print(""loss: %.6f, time: %.3f sec"" % (this_loss, duration))",not
scikit-learn/benchmarks/bench_plot_nmf.py,361,Use a panda dataframe to organize the results,not
scikit-learn/benchmarks/bench_plot_nmf.py,366,plot the results,not
scikit-learn/benchmarks/bench_plot_nmf.py,404,"first benchmark on 20 newsgroup dataset: sparse, shape(11314, 39116)",not
scikit-learn/benchmarks/bench_plot_nmf.py,413,"second benchmark on Olivetti faces dataset: dense, shape(400, 4096)",not
scikit-learn/benchmarks/bench_sgd_regression.py,1,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/benchmarks/bench_sgd_regression.py,2,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_sgd_regression.py,51,Shuffle data,not
scikit-learn/benchmarks/bench_sgd_regression.py,112,Plot results,not
scikit-learn/benchmarks/bench_plot_neighbors.py,39,------------------------------------------------------------,not
scikit-learn/benchmarks/bench_plot_neighbors.py,40,varying N,not
scikit-learn/benchmarks/bench_plot_neighbors.py,62,------------------------------------------------------------,not
scikit-learn/benchmarks/bench_plot_neighbors.py,63,varying D,not
scikit-learn/benchmarks/bench_plot_neighbors.py,85,------------------------------------------------------------,not
scikit-learn/benchmarks/bench_plot_neighbors.py,86,varying k,not
scikit-learn/benchmarks/bench_isolation_forest.py,47,Set this to true for plotting score histograms for each dataset:,not
scikit-learn/benchmarks/bench_isolation_forest.py,50,"datasets available = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']",not
scikit-learn/benchmarks/bench_isolation_forest.py,53,Loop over all datasets for fitting and scoring the estimator:,not
scikit-learn/benchmarks/bench_isolation_forest.py,56,Loading and vectorizing the data:,not
scikit-learn/benchmarks/bench_isolation_forest.py,70,we remove data with label 4,not
scikit-learn/benchmarks/bench_isolation_forest.py,71,normal data are then those of class 1,not
scikit-learn/benchmarks/bench_isolation_forest.py,82,normal data are those with attribute 2,not
scikit-learn/benchmarks/bench_isolation_forest.py,83,abnormal those with attribute 4,not
scikit-learn/benchmarks/bench_isolation_forest.py,128,"the lower, the more abnormal",not
scikit-learn/benchmarks/bench_isolation_forest.py,141,Show ROC Curves,not
scikit-learn/benchmarks/bench_isolation_forest.py,147,Print AUC score and train/test time:,not
scikit-learn/benchmarks/bench_plot_lasso_path.py,35,"'effective_rank': None,",not
scikit-learn/benchmarks/bench_plot_lasso_path.py,46,precomputed Gram matrix,not
scikit-learn/benchmarks/bench_plot_lasso_path.py,84,register the 3d projection,not
scikit-learn/benchmarks/bench_plot_lasso_path.py,101,plot the actual surface,not
scikit-learn/benchmarks/bench_plot_lasso_path.py,104,dummy point plot to stick the legend to since surface plot do not,not
scikit-learn/benchmarks/bench_plot_lasso_path.py,105,support legends (yet?),not
scikit-learn/benchmarks/bench_plot_lasso_path.py,106,"ax.plot([1], [1], [1], color=c, label=label)",not
scikit-learn/benchmarks/bench_plot_lasso_path.py,113,ax.legend(),not
scikit-learn/benchmarks/bench_sparsify.py,61,sparsify input,not
scikit-learn/benchmarks/bench_sparsify.py,66,sparsify coef,not
scikit-learn/benchmarks/bench_sparsify.py,70,add noise,not
scikit-learn/benchmarks/bench_sparsify.py,73,Split data in train set and test set,not
scikit-learn/benchmarks/bench_sparsify.py,79,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,19,number of microseconds in a second,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,26,start time,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,30,stop time,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,35,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,36,Option parser,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,37,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,62,"op.add_option(""--random-seed"",",not
scikit-learn/benchmarks/bench_sample_without_replacement.py,63,"dest=""random_seed"", default=13, type=int,",not
scikit-learn/benchmarks/bench_sample_without_replacement.py,64,"help=""Seed used by the random number generators."")",not
scikit-learn/benchmarks/bench_sample_without_replacement.py,77,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,78,List sampling algorithm,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,79,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,80,We assume that sampling algorithm has the following signature:,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,81,"sample(n_population, n_sample)",not
scikit-learn/benchmarks/bench_sample_without_replacement.py,82,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,85,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,86,Set Python core input,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,91,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,92,Set custom automatic method selection,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,98,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,99,Set custom tracking based method,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,107,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,108,Set custom reservoir based method,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,116,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,117,Set custom reservoir based method,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,125,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,126,Numpy permutation based,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,131,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,132,Remove unspecified algorithm,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,137,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,138,Perform benchmark,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,139,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,166,Print results,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,167,,not
scikit-learn/benchmarks/bench_sample_without_replacement.py,195,Sort legend labels,not
scikit-learn/benchmarks/bench_mnist.py,29,Author: Issam H. Laradji,not
scikit-learn/benchmarks/bench_mnist.py,30,Arnaud Joly <arnaud.v.joly@gmail.com>,not
scikit-learn/benchmarks/bench_mnist.py,31,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_mnist.py,54,Memoize the data extraction and memory map the resulting,not
scikit-learn/benchmarks/bench_mnist.py,55,train / test splits in readonly mode,not
scikit-learn/benchmarks/bench_mnist.py,63,,not
scikit-learn/benchmarks/bench_mnist.py,64,Load dataset,not
scikit-learn/benchmarks/bench_mnist.py,70,Normalize features,not
scikit-learn/benchmarks/bench_mnist.py,73,"Create train-test split (as [Joachims, 2006])",not
scikit-learn/benchmarks/bench_plot_parallel_pairwise.py,1,Author: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/benchmarks/bench_plot_parallel_pairwise.py,2,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_glmnet.py,25,alpha = 0.01,not
scikit-learn/benchmarks/bench_glmnet.py,35,start time,not
scikit-learn/benchmarks/bench_glmnet.py,39,stop time,not
scikit-learn/benchmarks/bench_glmnet.py,50,Delayed import of matplotlib.pyplot,not
scikit-learn/benchmarks/bench_glmnet.py,89,now do a benchmark where the number of points is fixed,not
scikit-learn/benchmarks/bench_glmnet.py,90,and the variable is the number of features,not
scikit-learn/benchmarks/bench_hist_gradient_boosting_higgsboson.py,12,"To use this experimental feature, we need to explicitly ask for it:",not
scikit-learn/benchmarks/bench_hist_gradient_boosting_higgsboson.py,13,noqa,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,65,Author: Giorgio Patrini,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,93,"If this is enabled, tests are much slower and will crash with the large data",not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,96,TODO: compute approximate spectral norms with the power method as in,SATD
scikit-learn/benchmarks/bench_plot_randomized_svd.py,97,Estimating the largest eigenvalues by the power and Lanczos methods with,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,98,"a random start, Jacek Kuczynski and Henryk Wozniakowski, SIAM Journal on",not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,99,"Matrix Analysis and Applications, 13 (4): 1094-1122, 1992.",not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,100,"This approximation is a very fast estimate of the spectral norm, but depends",not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,101,on starting random vectors.,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,103,"Determine when to switch to batch computation for matrix norms,",not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,104,in case the reconstructed (dense) matrix is too large,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,107,The following datasets can be downloaded manually from:,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,108,CIFAR 10: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,109,SVHN: http://ufldl.stanford.edu/housenumbers/train_32x32.mat,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,251,There is a different convention for l here,not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,270,"s = sp.linalg.norm(A, ord=2)  # slow",not
scikit-learn/benchmarks/bench_plot_randomized_svd.py,281,"if the input is not too big, just call scipy",not
scikit-learn/benchmarks/bench_20newsgroups.py,29,,not
scikit-learn/benchmarks/bench_20newsgroups.py,30,Data,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,1,Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,2,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,3,,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,4,License: BSD 3 clause,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,24,compute logistic loss,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,34,We use joblib to cache individual fits. Note that we do not pass the dataset,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,35,"as argument as the hashing would be too slow, so we assume that the dataset",SATD
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,36,never changes.,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,150,consider the binary classification problem 'CCAT' vs the rest,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,155,parameters,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,160,max_iter range,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,199,compute the same step_size than in LR-sag,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,215,"We keep only 200 features, to have a dense dataset,",not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,216,"and compare to lightning SAG, which seems incorrect in the sparse case.",not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,224,Split training and testing. Switch train and test subset compared to,not
scikit-learn/benchmarks/bench_rcv1_logreg_convergence.py,225,"LYRL2004 split, to have a larger training dataset.",not
scikit-learn/doc/conf.py,1,-*- coding: utf-8 -*-,not
scikit-learn/doc/conf.py,2,,not
scikit-learn/doc/conf.py,3,"scikit-learn documentation build configuration file, created by",not
scikit-learn/doc/conf.py,4,sphinx-quickstart on Fri Jan  8 09:13:42 2010.,not
scikit-learn/doc/conf.py,5,,not
scikit-learn/doc/conf.py,6,This file is execfile()d with the current directory set to its containing,not
scikit-learn/doc/conf.py,7,dir.,not
scikit-learn/doc/conf.py,8,,not
scikit-learn/doc/conf.py,9,Note that not all possible configuration values are present in this,not
scikit-learn/doc/conf.py,10,autogenerated file.,not
scikit-learn/doc/conf.py,11,,not
scikit-learn/doc/conf.py,12,All configuration values have a default; values that are commented out,not
scikit-learn/doc/conf.py,13,serve to show the default.,not
scikit-learn/doc/conf.py,22,If extensions (or modules to document with autodoc) are in another,not
scikit-learn/doc/conf.py,23,"directory, add these directories to sys.path here. If the directory",not
scikit-learn/doc/conf.py,24,"is relative to the documentation root, use os.path.abspath to make it",not
scikit-learn/doc/conf.py,25,"absolute, like shown here.",not
scikit-learn/doc/conf.py,31,-- General configuration ---------------------------------------------------,not
scikit-learn/doc/conf.py,33,"Add any Sphinx extension module names here, as strings. They can be",not
scikit-learn/doc/conf.py,34,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.,not
scikit-learn/doc/conf.py,45,this is needed for some reason...,not
scikit-learn/doc/conf.py,46,see https://github.com/numpy/numpydoc/issues/69,not
scikit-learn/doc/conf.py,50,"For maths, use mathjax by default and svg if NO_MATHJAX env variable is set",not
scikit-learn/doc/conf.py,51,(useful for viewing the doc offline),not
scikit-learn/doc/conf.py,66,"Add any paths that contain templates here, relative to this directory.",not
scikit-learn/doc/conf.py,69,generate autosummary even if no references,not
scikit-learn/doc/conf.py,72,The suffix of source filenames.,not
scikit-learn/doc/conf.py,75,The encoding of source files.,not
scikit-learn/doc/conf.py,76,source_encoding = 'utf-8',not
scikit-learn/doc/conf.py,78,The master toctree document.,not
scikit-learn/doc/conf.py,81,General information about the project.,not
scikit-learn/doc/conf.py,85,"The version info for the project you're documenting, acts as replacement for",not
scikit-learn/doc/conf.py,86,"|version| and |release|, also used in various other places throughout the",not
scikit-learn/doc/conf.py,87,built documents.,not
scikit-learn/doc/conf.py,88,,not
scikit-learn/doc/conf.py,89,The short X.Y version.,not
scikit-learn/doc/conf.py,93,"The full version, including alpha/beta/rc tags.",not
scikit-learn/doc/conf.py,94,Removes post from release name,not
scikit-learn/doc/conf.py,100,The language for content autogenerated by Sphinx. Refer to documentation,not
scikit-learn/doc/conf.py,101,for a list of supported languages.,not
scikit-learn/doc/conf.py,102,language = None,not
scikit-learn/doc/conf.py,104,"There are two options for replacing |today|: either, you set today to some",not
scikit-learn/doc/conf.py,105,"non-false value, then it is used:",not
scikit-learn/doc/conf.py,106,today = '',not
scikit-learn/doc/conf.py,107,"Else, today_fmt is used as the format for a strftime call.",not
scikit-learn/doc/conf.py,108,"today_fmt = '%B %d, %Y'",not
scikit-learn/doc/conf.py,110,"List of patterns, relative to source directory, that match files and",not
scikit-learn/doc/conf.py,111,directories to ignore when looking for source files.,not
scikit-learn/doc/conf.py,114,The reST default role (used for this markup: `text`) to use for all,not
scikit-learn/doc/conf.py,115,documents.,not
scikit-learn/doc/conf.py,118,"If true, '()' will be appended to :func: etc. cross-reference text.",not
scikit-learn/doc/conf.py,121,"If true, the current module name will be prepended to all description",not
scikit-learn/doc/conf.py,122,unit titles (such as .. function::).,not
scikit-learn/doc/conf.py,123,add_module_names = True,not
scikit-learn/doc/conf.py,125,"If true, sectionauthor and moduleauthor directives will be shown in the",not
scikit-learn/doc/conf.py,126,output. They are ignored by default.,not
scikit-learn/doc/conf.py,127,show_authors = False,not
scikit-learn/doc/conf.py,129,The name of the Pygments (syntax highlighting) style to use.,not
scikit-learn/doc/conf.py,132,A list of ignored prefixes for module index sorting.,not
scikit-learn/doc/conf.py,133,modindex_common_prefix = [],not
scikit-learn/doc/conf.py,136,-- Options for HTML output -------------------------------------------------,not
scikit-learn/doc/conf.py,138,The theme to use for HTML and HTML Help pages.  Major themes that come with,not
scikit-learn/doc/conf.py,139,Sphinx are currently 'default' and 'sphinxdoc'.,not
scikit-learn/doc/conf.py,142,Theme options are theme-specific and customize the look and feel of a theme,not
scikit-learn/doc/conf.py,143,"further.  For a list of options available for each theme, see the",not
scikit-learn/doc/conf.py,144,documentation.,not
scikit-learn/doc/conf.py,148,"Add any paths that contain custom themes here, relative to this directory.",not
scikit-learn/doc/conf.py,152,"The name for this set of Sphinx documents.  If None, it defaults to",not
scikit-learn/doc/conf.py,153,"""<project> v<release> documentation"".",not
scikit-learn/doc/conf.py,154,html_title = None,not
scikit-learn/doc/conf.py,156,A shorter title for the navigation bar.  Default is the same as html_title.,not
scikit-learn/doc/conf.py,159,The name of an image file (relative to this directory) to place at the top,not
scikit-learn/doc/conf.py,160,of the sidebar.,not
scikit-learn/doc/conf.py,163,The name of an image file (within the static path) to use as favicon of the,not
scikit-learn/doc/conf.py,164,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32,not
scikit-learn/doc/conf.py,165,pixels large.,not
scikit-learn/doc/conf.py,168,"Add any paths that contain custom static files (such as style sheets) here,",not
scikit-learn/doc/conf.py,169,"relative to this directory. They are copied after the builtin static files,",not
scikit-learn/doc/conf.py,170,"so a file named ""default.css"" will overwrite the builtin ""default.css"".",not
scikit-learn/doc/conf.py,173,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",not
scikit-learn/doc/conf.py,174,using the given strftime format.,not
scikit-learn/doc/conf.py,175,"html_last_updated_fmt = '%b %d, %Y'",not
scikit-learn/doc/conf.py,177,"Custom sidebar templates, maps document names to template names.",not
scikit-learn/doc/conf.py,178,html_sidebars = {},not
scikit-learn/doc/conf.py,180,"Additional templates that should be rendered to pages, maps page names to",not
scikit-learn/doc/conf.py,181,template names.,not
scikit-learn/doc/conf.py,184,redirects to index,not
scikit-learn/doc/conf.py,186,"If false, no module index is generated.",not
scikit-learn/doc/conf.py,189,"If false, no index is generated.",not
scikit-learn/doc/conf.py,192,"If true, the index is split into individual pages for each letter.",not
scikit-learn/doc/conf.py,193,html_split_index = False,not
scikit-learn/doc/conf.py,195,"If true, links to the reST sources are added to the pages.",not
scikit-learn/doc/conf.py,196,html_show_sourcelink = True,not
scikit-learn/doc/conf.py,198,"If true, an OpenSearch description file will be output, and all pages will",not
scikit-learn/doc/conf.py,199,contain a <link> tag referring to it.  The value of this option must be the,not
scikit-learn/doc/conf.py,200,base URL from which the finished HTML is served.,not
scikit-learn/doc/conf.py,201,html_use_opensearch = '',not
scikit-learn/doc/conf.py,203,"If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").",not
scikit-learn/doc/conf.py,204,html_file_suffix = '',not
scikit-learn/doc/conf.py,206,Output file base name for HTML help builder.,not
scikit-learn/doc/conf.py,209,"If true, the reST sources are included in the HTML build as _sources/name.",not
scikit-learn/doc/conf.py,212,Adds variables into templates,not
scikit-learn/doc/conf.py,214,finds latest release highlights and places it into HTML context for,not
scikit-learn/doc/conf.py,215,index.html,not
scikit-learn/doc/conf.py,217,Finds the highlight with the latest version number,not
scikit-learn/doc/conf.py,224,get version from higlight name assuming highlights have the form,not
scikit-learn/doc/conf.py,225,plot_release_highlights_0_22_0,not
scikit-learn/doc/conf.py,229,-- Options for LaTeX output ------------------------------------------------,not
scikit-learn/doc/conf.py,231,The paper size ('letterpaper' or 'a4paper').,not
scikit-learn/doc/conf.py,232,"'papersize': 'letterpaper',",not
scikit-learn/doc/conf.py,234,"The font size ('10pt', '11pt' or '12pt').",not
scikit-learn/doc/conf.py,235,"'pointsize': '10pt',",not
scikit-learn/doc/conf.py,237,Additional stuff for the LaTeX preamble.,not
scikit-learn/doc/conf.py,244,Grouping the document tree into LaTeX files. List of tuples,not
scikit-learn/doc/conf.py,245,"(source start file, target name, title, author, documentclass",not
scikit-learn/doc/conf.py,246,[howto/manual]).,not
scikit-learn/doc/conf.py,250,The name of an image file (relative to this directory) to place at the top of,not
scikit-learn/doc/conf.py,251,the title page.,not
scikit-learn/doc/conf.py,254,Documents to append as an appendix to all manuals.,not
scikit-learn/doc/conf.py,255,latex_appendices = [],not
scikit-learn/doc/conf.py,257,"If false, no module index is generated.",not
scikit-learn/doc/conf.py,262,intersphinx configuration,not
scikit-learn/doc/conf.py,303,Forces Release Highlights to the top,not
scikit-learn/doc/conf.py,338,avoid generating too many cross links,not
scikit-learn/doc/conf.py,344,The following dictionary contains the information used to create the,not
scikit-learn/doc/conf.py,345,thumbnails for the front page of the scikit-learn home page.,not
scikit-learn/doc/conf.py,346,key: first image in set,not
scikit-learn/doc/conf.py,347,"values: (number of plot in set, height of thumbnail)",not
scikit-learn/doc/conf.py,351,enable experimental module so that experimental estimators can be,not
scikit-learn/doc/conf.py,352,discovered properly by sphinx,not
scikit-learn/doc/conf.py,353,noqa,not
scikit-learn/doc/conf.py,354,noqa,not
scikit-learn/doc/conf.py,375,searchindex only exist when generating html,not
scikit-learn/doc/conf.py,392,Config for sphinx_issues,not
scikit-learn/doc/conf.py,394,we use the issues path for PRs since the issues URL will forward,not
scikit-learn/doc/conf.py,399,to hide/show the prompt in code examples:,not
scikit-learn/doc/conf.py,404,The following is used by sphinx.ext.linkcode to provide links to github,not
scikit-learn/doc/conftest.py,24,skip the test in rcv1.rst if the dataset is not already loaded,not
scikit-learn/doc/conftest.py,48,noqa,not
scikit-learn/doc/conftest.py,55,noqa,not
scikit-learn/doc/conftest.py,62,noqa,not
scikit-learn/doc/conftest.py,66,ignore deprecation warnings from scipy.misc.face,not
scikit-learn/doc/tutorial/machine_learning_map/svg2imagemap.py,1,!/usr/local/bin/python,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1,module pyparsing.py,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3,Copyright (c) 2003-2016  Paul T. McGuire,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5,"Permission is hereby granted, free of charge, to any person obtaining",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,6,a copy of this software and associated documentation files (the,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,7,"""Software""), to deal in the Software without restriction, including",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,8,"without limitation the rights to use, copy, modify, merge, publish,",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,9,"distribute, sublicense, and/or sell copies of the Software, and to",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,10,"permit persons to whom the Software is furnished to do so, subject to",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,11,the following conditions:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,12,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,13,The above copyright notice and this permission notice shall be,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,14,included in all copies or substantial portions of the Software.,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,15,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,16,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,17,"EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,18,"MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,19,IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,20,"CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,21,"TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,22,SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,23,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,24,flake8: noqa,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,94,"~ sys.stderr.write( ""testing pyparsing module, version %s, %s\n"" % (__version__,__versionTime__ ) )",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,125,"build list of single arg builtins, that can be used as parse actions",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,141,"If this works, then _ustr(obj) has the same behaviour as str(obj), so",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,142,it won't break any existing code.,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,146,Else encode it,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,152,"build list of single arg builtins, tolerant of Python version, that can be used as parse actions",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,166,ampersand must be replaced first,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,185,"Performance tuning: we construct a *lot* of these, so keep this",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,186,constructor as small and fast as possible,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,271,~ class ReparseException(ParseBaseException):,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,272,"~ """"""Experimental class - parse actions can raise this exception to cause",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,273,~ pyparsing to reparse the input string:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,274,"~ - with a modified input string, and/or",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,275,~ - with a modified start location,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,276,"~ Set the values of the ReparseException in the constructor, and raise the",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,277,~ exception in a parse action to cause pyparsing to use the new string/location.,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,278,~ Setting the values as None causes no change to be made.,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,279,"~ """"""",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,280,"~ def __init_( self, newstring, restartLoc ):",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,281,~ self.newParseText = newstring,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,282,~ self.reparseLoc = restartLoc,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,348,"Performance tuning: we construct a *lot* of these, so keep this",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,349,constructor as small and fast as possible,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,372,"will always return a str, but use _ustr for consistency",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,416,convert int to slice,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,421,get removed indices,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,424,fixup indices in token dictionary,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,578,fixup indices in token dictionary,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,659,useful for merging many ParseResults using sum() builtin,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,662,this may raise a TypeError - so be it,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,754,collapse out indents if formatting is not desired,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,788,"individual token, see if there is a name for it",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,917,add support for pickle protocol,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,994,Only works on Python 3.x - nonlocal is toxic to Python 2 installs,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,995,~ 'decorator to trim function calls to match the arity of the target',not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,996,"~ def _trim_arity(func, maxargs=3):",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,997,~ if func in singleArgBuiltins:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,998,"~ return lambda s,l,t: func(t)",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,999,~ limit = 0,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1000,~ foundArity = False,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1001,~ def wrapper(*args):,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1002,"~ nonlocal limit,foundArity",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1003,~ while 1:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1004,~ try:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1005,~ ret = func(*args[limit:]),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1006,~ foundArity = True,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1007,~ return ret,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1008,~ except TypeError:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1009,~ if limit == maxargs or foundArity:,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1010,~ raise,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1011,~ limit += 1,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1012,~ continue,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1013,~ return wrapper,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1015,this version is Python 2.x-3.x cross-compatible,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1032,synthesize what would be returned by traceback.extract_stack at the call to,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1033,"user's parse action 'func', so that we don't incur call penalty at parse time",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1036,"IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1037,"THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1048,re-raise TypeErrors if they did not come from our arity testing,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1064,copy func name to wrapper for sensible debug output,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1119,"~ self.name = ""<unknown>""  # don't define self.name, let subclasses try/except upcall",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1126,used when checking for left-recursion,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1131,used to optimize exception handling for subclasses that don't advance parse index,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1133,used to mark results names as modal (report only last) or cumulative (list all),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1134,custom debug actions,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1136,used to avoid redundant calls to preParse,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1341,~ @profile,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1343,and doActions ),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1346,"~ print (""Match"",self,""at loc"",loc,""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) ))",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1360,"~ print (""Exception raised:"", err)",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1394,"~ print ""Exception raised in user parse action:"", err",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1408,"~ print (""Matched"",self,""->"",retTokens.asList())",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1508,argument cache for optimizing repeated calls when backtracking through recursive expressions,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1509,this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1513,this method gets repeatedly called during backtracking with the same arguments -,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1514,we can cache these arguments and save ourselves the trouble of re-parsing the contained expression,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1526,"cache a copy of the exception, without the traceback",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1611,~ self.saveAsList = True,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1626,"catch and re-raise exception from here, clears out pyparsing internal stack trace",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1699,"catch and re-raise exception from here, clears out pyparsing internal stack trace",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1721,"force preservation of <TAB>s, to minimize unwanted transformation of string, and to",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1722,keep string locs straight between transformString and scanString,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1742,"catch and re-raise exception from here, clears out pyparsing internal stack trace",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,1769,"catch and re-raise exception from here, clears out pyparsing internal stack trace",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2163,"catch and re-raise exception from here, clears out pyparsing internal stack trace",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2400,Performance tuning: this routine gets called a *lot*,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2401,"if this is a single character match string  and the first character matches,",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2402,"short-circuit as quickly as possible, and avoid calling startswith",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2403,~ @profile,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2490,Preserve the defining literal.,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2839,remove white space from quote chars - wont work anyway,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2911,strip off quotes,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2915,replace escaped whitespace,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2926,replace escaped characters,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,2930,replace escaped quotes,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3038,~ self.leaveWhitespace(),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3163,see if entire string up to here is just whitespace and ignoreables,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3242,"if sequence of strings provided, wrap with Literal",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3298,"collapse nested And's of the form And( And( And( a,b), c), d) to And( a,b,c,d )",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3299,but only if there are no parse actions or resultsNames on the nested And's,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3300,(likewise for Or's and MatchFirst's),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3371,"pass False as last arg to _parse for first element, since we already",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3372,pre-parsed the string as part of our And pre-parsing,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3398,"And( [ self, other ] )",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3455,"save match among all matches, to retry longest to shortest",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3479,"Or( [ self, other ] )",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3536,"only got here if no expression matched, raise exception for match that made it the furthest",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3547,"MatchFirst( [ self, other ] )",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3662,"add any unmatched Optionals, in case they have default values defined",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3804,~ self.leaveWhitespace(),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3805,"do NOT use self.leaveWhitespace(), don't want to propagate to exprs",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3839,must be at least one (but first see if we are the stopOn sentinel;,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,3840,"if so, fail)",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4078,break if failOn expression matches,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4083,advance past ignore expressions,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4093,"no match, advance loc in string",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4096,"matched skipto expr, done",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4100,"ran off the end of the input string without matching skipto expr, fail",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4103,build up return values,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4175,stubbed out for now - creates awful memory and perf issues,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4204,", savelist )",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4226,"suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4324,ParseResults(i),not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4420,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4421,global helpers,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4422,,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4501,flatten t tokens,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4539,~  escape these chars: ^-],not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4605,"~ print (strs,""->"", ""|"".join( [ _escapeRegexChars(sym) for sym in symbols] ))",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4616,"last resort, just use MatchFirst",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,4722,convenience constants for positional expressions,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5095,try to avoid LR with this extra test,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5326,~ FollowedBy(blockStatementExpr) +,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5344,"it's easy to get these comment structures wrong - they're very common, so may as well make them available",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5371,some other useful expressions - using lower-case class name since we are really using this as a namespace,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5541,streamlining this expression makes the docs nicer-looking,not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5662,"demo runTests method, including embedded comments in test string",not
scikit-learn/doc/tutorial/machine_learning_map/pyparsing.py,5696,"any int or real number, returned as float",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,1,!/usr/local/bin/python,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,16,ParserElement.enablePackrat(),not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,40,"~ raise ParseException( instring, loc, self.errmsg )",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,62,"note that almost all these fields are optional,",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,63,and this can match almost anything. We rely on Pythons built-in,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,64,float() function to clear out invalid values - loosely matching like this,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,65,speeds up parsing quite a lot,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,77,same as FP constant but don't allow a - sign,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,87,comma or whitespace can separate values all over the place in SVG,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,101,commands,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,112,rx,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,113,ry,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,119,"rx, ry",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,120,rotation,not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,121,"large-arc-flag, sweep-flag",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,122,"(x,y)",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,133,"curve = Group(Command(""C"") + Arguments(coordinatePairTripleSequence))",not
scikit-learn/doc/tutorial/machine_learning_map/parse_path.py,143,~ number.debug = True,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,9,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,10,License: Simplified BSD,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,22,The training data folder must be passed as first argument,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,26,Split the dataset in training and test set:,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,31,TASK: Build a vectorizer that splits strings into sequence of 1 to 3,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,32,characters instead of word tokens,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,36,TASK: Build a vectorizer / classifier pipeline using the previous analyzer,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,37,the pipeline instance should stored in a variable named clf,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,43,TASK: Fit the pipeline on the training set,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,46,TASK: Predict the outcome on the testing set in a variable named y_predicted,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,49,Print the classification report,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,53,Plot the confusion matrix,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,57,import matlotlib.pyplot as plt,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,58,"plt.matshow(cm, cmap=plt.cm.jet)",not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,59,plt.show(),not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py,61,Predict the result on some short new sentences:,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,11,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,12,License: Simplified BSD,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,25,"NOTE: we put the following in a 'if __name__ == ""__main__""' protected",not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,26,block to be able to use a multi-core grid search that also works under,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,27,"Windows, see: http://docs.python.org/library/multiprocessing.html#windows",not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,28,The multiprocessing module is used as the backend of joblib.Parallel,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,29,that is used when n_jobs != 1 in GridSearchCV,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,31,the training data folder must be passed as first argument,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,36,split the dataset in training and test set:,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,40,TASK: Build a vectorizer / classifier pipeline that filters out tokens,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,41,that are too rare or too frequent,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,47,TASK: Build a grid search to find out whether unigrams or bigrams are,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,48,more useful.,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,49,Fit the pipeline on the training set using grid search for the parameters,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,56,TASK: print the mean and std for each candidate along with the parameter,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,57,settings for all the candidates explored by grid search.,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,65,TASK: Predict the outcome on the testing set and store it in a variable,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,66,named y_predicted,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,69,Print the classification report,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,73,Print and plot the confusion matrix,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,77,import matplotlib.pyplot as plt,not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,78,plt.matshow(cm),not
scikit-learn/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,79,plt.show(),not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,9,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,10,License: Simplified BSD,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,22,The training data folder must be passed as first argument,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,26,Split the dataset in training and test set:,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,31,TASK: Build a vectorizer that splits strings into sequence of 1 to 3,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,32,characters instead of word tokens,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,34,TASK: Build a vectorizer / classifier pipeline using the previous analyzer,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,35,the pipeline instance should stored in a variable named clf,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,37,TASK: Fit the pipeline on the training set,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,39,TASK: Predict the outcome on the testing set in a variable named y_predicted,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,41,Print the classification report,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,45,Plot the confusion matrix,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,49,import matplotlib.pyplot as plt,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,50,"plt.matshow(cm, cmap=plt.cm.jet)",not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,51,plt.show(),not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py,53,Predict the result on some short new sentences:,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,11,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,12,License: Simplified BSD,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,25,"NOTE: we put the following in a 'if __name__ == ""__main__""' protected",not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,26,block to be able to use a multi-core grid search that also works under,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,27,"Windows, see: http://docs.python.org/library/multiprocessing.html#windows",not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,28,The multiprocessing module is used as the backend of joblib.Parallel,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,29,that is used when n_jobs != 1 in GridSearchCV,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,31,the training data folder must be passed as first argument,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,36,split the dataset in training and test set:,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,40,TASK: Build a vectorizer / classifier pipeline that filters out tokens,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,41,that are too rare or too frequent,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,43,TASK: Build a grid search to find out whether unigrams or bigrams are,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,44,more useful.,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,45,Fit the pipeline on the training set using grid search for the parameters,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,47,TASK: print the cross-validated scores for the each parameters set,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,48,explored by the grid search,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,50,TASK: Predict the outcome on the testing set and store it in a variable,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,51,named y_predicted,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,53,Print the classification report,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,57,Print and plot the confusion matrix,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,61,import matplotlib.pyplot as plt,not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,62,plt.matshow(cm),not
scikit-learn/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py,63,plt.show(),not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,2,simple python script to collect text paragraphs from various languages on the,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,3,same topic namely the Wikipedia encyclopedia itself,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,15,noqa: E501,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,25,noqa: E501,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,26,"u'zh': u'http://zh.wikipedia.org/wiki/Wikipedia',",not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,53,change the User Agent to avoid being blocked by Wikipedia,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,54,downloading a couple of articles should not be considered abusive,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,59,decode the payload explicitly as UTF-8 since lxml is confused for some,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,60,reason,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,69,skip paragraphs that are too short - probably too noisy and not,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,70,representative of the actual language,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,79,split the paragraph into fake smaller paragraphs to make the,SATD
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,80,problem harder e.g. more similar to tweets,not
scikit-learn/doc/tutorial/text_analytics/data/languages/fetch_data.py,82,FIXME: whitespace tokenizing does not work on chinese and japanese,SATD
scikit-learn/doc/sphinxext/github_link.py,44,Python 2 only,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,48,process 'py' domain first for python classes,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,57,resolve :term:,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,61,replace literal nodes with inline nodes,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,70,"next, do the standard domain",not
scikit-learn/doc/sphinxext/custom_references_resolver.py,83,the domain doesn't yet support the new interface,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,84,we have to manually collect possible references (SLOW),not
scikit-learn/doc/sphinxext/custom_references_resolver.py,93,no results considered to be <code>,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,99,"Override ""any"" class with the actual role type to get the styling",not
scikit-learn/doc/sphinxext/custom_references_resolver.py,100,approximately correct.,not
scikit-learn/doc/sphinxext/custom_references_resolver.py,114,Support sphinx 1.6.*,not
scikit-learn/doc/sphinxext/sphinx_issues.py,1,-*- coding: utf-8 -*-,not
scikit-learn/doc/sphinxext/sphinx_issues.py,98,External repo,not
scikit-learn/doc/sphinxext/sphinx_issues.py,195,Format template for issues URI,not
scikit-learn/doc/sphinxext/sphinx_issues.py,196,e.g. 'https://github.com/sloria/marshmallow/issues/{issue},not
scikit-learn/doc/sphinxext/sphinx_issues.py,198,Format template for PR URI,not
scikit-learn/doc/sphinxext/sphinx_issues.py,199,e.g. 'https://github.com/sloria/marshmallow/pull/{issue},not
scikit-learn/doc/sphinxext/sphinx_issues.py,201,Format template for commit URI,not
scikit-learn/doc/sphinxext/sphinx_issues.py,202,e.g. 'https://github.com/sloria/marshmallow/commits/{commit},not
scikit-learn/doc/sphinxext/sphinx_issues.py,204,"Shortcut for Github, e.g. 'sloria/marshmallow'",not
scikit-learn/doc/sphinxext/sphinx_issues.py,206,Format template for user profile URI,not
scikit-learn/doc/sphinxext/sphinx_issues.py,207,e.g. 'https://github.com/{user}',not
scikit-learn/build_tools/generate_authors_table.py,41,get members of scikit-learn core-dev on GitHub,not
scikit-learn/build_tools/generate_authors_table.py,44,30 per page,not
scikit-learn/build_tools/generate_authors_table.py,49,get members of scikit-learn on GitHub,not
scikit-learn/build_tools/generate_authors_table.py,51,30 per page,not
scikit-learn/build_tools/generate_authors_table.py,57,keep only the logins,not
scikit-learn/build_tools/generate_authors_table.py,61,add missing contributors with GitHub accounts,not
scikit-learn/build_tools/generate_authors_table.py,63,add missing contributors without GitHub accounts,not
scikit-learn/build_tools/generate_authors_table.py,65,remove CI bots,not
scikit-learn/build_tools/generate_authors_table.py,70,"remove duplicate, and get the difference of the two sets",not
scikit-learn/build_tools/generate_authors_table.py,75,get profiles from GitHub,not
scikit-learn/build_tools/generate_authors_table.py,79,sort by last name,not
scikit-learn/build_tools/generate_authors_table.py,97,fix missing names,not
scikit-learn/build_tools/circle/list_versions.py,1,!/usr/bin/env python3,not
scikit-learn/build_tools/circle/list_versions.py,3,List all available versions of the documentation,not
scikit-learn/build_tools/circle/list_versions.py,20,https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size,not
scikit-learn/build_tools/circle/list_versions.py,51,noqa,not
scikit-learn/build_tools/circle/list_versions.py,52,noqa,not
scikit-learn/build_tools/circle/list_versions.py,56,"Gather data for each version directory, including symlinks",not
scikit-learn/build_tools/circle/list_versions.py,74,Symlinks should have same data as target,not
scikit-learn/build_tools/circle/list_versions.py,79,"Output in order: dev, stable, decreasing other version",not
scikit-learn/build_tools/circle/list_versions.py,86,symlink came first,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,18,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,19,Maria Telenczuk <https://github.com/maikia>,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,20,Katrina Ni <https://github.com/nilichen>,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,21,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,22,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,31,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,32,Load the data,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,33,-------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,34,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,35,First we need to load the data.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,40,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,41,Data preprocessing,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,42,-------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,43,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,44,"Next, we will split our dataset to use 90% for training and leave the rest",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,45,for testing. We will also set the regression model parameters. You can play,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,46,with these parameters to see how the results change.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,47,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,48,n_estimators : the number of boosting stages that will be performed.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,49,"Later, we will plot deviance against boosting iterations.",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,50,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,51,max_depth : limits the number of nodes in the tree.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,52,The best value depends on the interaction of the input variables.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,53,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,54,min_samples_split : the minimum number of samples required to split an,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,55,internal node.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,56,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,57,learning_rate : how much the contribution of each tree will shrink.,SATD
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,58,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,59,loss : loss function to optimize. The least squares function is  used in this,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,60,"case however, there are many other options (see",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,61,:class:`~sklearn.ensemble.GradientBoostingRegressor` ).,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,72,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,73,Fit regression model,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,74,-------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,75,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,76,Now we will initiate the gradient boosting regressors and fit it with our,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,77,training data. Let's also look and the mean squared error on the test data.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,85,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,86,Plot training deviance,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,87,-------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,88,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,89,"Finally, we will visualize the results. To do that we will first compute the",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,90,test set deviance and then plot it against boosting iterations.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,109,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,110,Plot feature importance,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,111,-------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,112,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,113,"Careful, impurity-based feature importances can be misleading for",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,114,"high cardinality features (many unique values). As an alternative,",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,115,the permutation importances of ``reg`` can be computed on a,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,116,held out test set. See :ref:`permutation_importance` for more details.,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,117,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,118,"For this example, the impurity-based and permutation methods identify the",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,119,same 2 strongly predictive features but not in the same order. The third most,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,120,"predictive feature, ""bp"", is also the same for the 2 methods. The remaining",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,121,features are less predictive and the error bars of the permutation plot,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regression.py,122,show that they overlap with 0.,not
scikit-learn/examples/ensemble/plot_voting_probas.py,45,predict class probabilities for all classifiers,not
scikit-learn/examples/ensemble/plot_voting_probas.py,48,get class probabilities for the first sample in the dataset,not
scikit-learn/examples/ensemble/plot_voting_probas.py,53,plotting,not
scikit-learn/examples/ensemble/plot_voting_probas.py,55,number of groups,not
scikit-learn/examples/ensemble/plot_voting_probas.py,56,group positions,not
scikit-learn/examples/ensemble/plot_voting_probas.py,57,bar width,not
scikit-learn/examples/ensemble/plot_voting_probas.py,61,bars for classifier 1-3,not
scikit-learn/examples/ensemble/plot_voting_probas.py,67,bars for VotingClassifier,not
scikit-learn/examples/ensemble/plot_voting_probas.py,73,plot annotations,not
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,21,Number of cores to use to perform parallel fitting of the forest model,not
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,24,Load the faces dataset,not
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,28,Limit to 5 classes,not
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,32,Build a forest and compute the pixel importances,not
scikit-learn/examples/ensemble/plot_forest_importances_faces.py,45,Plot pixel importances,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,35,make a synthetic dataset,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,38,use RandomTreesEmbedding to transform data,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,42,Visualize result after dimensionality reduction using truncated SVD,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,46,Learn a Naive Bayes classifier on the transformed data,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,51,Learn an ExtraTreesClassifier for comparison,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,56,scatter plot of original and reduced data,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,72,"Plot the decision in original space. For that, we will assign a color",not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,73,"to each point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,79,transform grid using RandomTreesEmbedding,not
scikit-learn/examples/ensemble/plot_random_forest_embedding.py,92,transform grid using ExtraTreesClassifier,not
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,21,noqa,not
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,33,positive correlation with y,not
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,34,negative correlation with y,not
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,44,Without any constraint,not
scikit-learn/examples/ensemble/plot_monotonic_constraints.py,52,With positive and negative constraints,not
scikit-learn/examples/ensemble/plot_voting_decision_regions.py,38,Loading some example data,not
scikit-learn/examples/ensemble/plot_voting_decision_regions.py,43,Training classifiers,not
scikit-learn/examples/ensemble/plot_voting_decision_regions.py,56,Plotting decision regions,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,28,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,29,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,30,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,41,Generate data (adapted from G. Ridgeway's gbm example),not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,57,Fit classifier with out-of-bag estimates,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,89,Estimate best n_estimator using cross-validation,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,92,Compute best n_estimator for test data,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,95,negative cumulative sum of oob improvements,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,98,min loss according to OOB,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,101,min loss according to test (normalize such that first loss is 0),not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,105,min loss according to cv (normalize such that first loss is 0),not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,109,color brew for the three curves,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,114,plot curves and vertical lines for best iterations,not
scikit-learn/examples/ensemble/plot_gradient_boosting_oob.py,122,add three vertical lines to xticks,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,21,Author: Noel Dawe <noel.dawe@gmail.com>,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,22,,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,23,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,33,Construct dataset,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,43,Create and fit an AdaBoosted decision tree,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,56,Plot the decision boundaries,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,68,Plot the training points,not
scikit-learn/examples/ensemble/plot_adaboost_twoclass.py,82,Plot the two-class decision scores,not
scikit-learn/examples/ensemble/plot_bias_variance.py,66,Author: Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/examples/ensemble/plot_bias_variance.py,67,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_bias_variance.py,75,Settings,not
scikit-learn/examples/ensemble/plot_bias_variance.py,76,Number of iterations for computing expectations,not
scikit-learn/examples/ensemble/plot_bias_variance.py,77,Size of the training set,not
scikit-learn/examples/ensemble/plot_bias_variance.py,78,Size of the test set,not
scikit-learn/examples/ensemble/plot_bias_variance.py,79,Standard deviation of the noise,not
scikit-learn/examples/ensemble/plot_bias_variance.py,82,Change this for exploring the bias-variance decomposition of other,not
scikit-learn/examples/ensemble/plot_bias_variance.py,83,"estimators. This should work well for estimators with high variance (e.g.,",SATD
scikit-learn/examples/ensemble/plot_bias_variance.py,84,"decision trees or KNN), but poorly for estimators with low variance (e.g.,",not
scikit-learn/examples/ensemble/plot_bias_variance.py,85,linear models).,not
scikit-learn/examples/ensemble/plot_bias_variance.py,92,Generate data,not
scikit-learn/examples/ensemble/plot_bias_variance.py,128,Loop over estimators to compare,not
scikit-learn/examples/ensemble/plot_bias_variance.py,130,Compute predictions,not
scikit-learn/examples/ensemble/plot_bias_variance.py,137,Bias^2 + Variance + Noise decomposition of the mean squared error,not
scikit-learn/examples/ensemble/plot_bias_variance.py,157,Plot figures,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,17,Author: Noel Dawe <noel.dawe@gmail.com>,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,18,,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,19,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,21,importing necessary libraries,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,27,Create the dataset,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,32,Fit regression model,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,41,Predict,not
scikit-learn/examples/ensemble/plot_adaboost_regression.py,45,Plot the results,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,20,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,21,Maria Telenczuk    <https://github.com/maikia>,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,22,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,25,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,26,Download the dataset,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,27,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,28,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,29,We will use `Ames Housing`_ dataset which was first compiled by Dean De Cock,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,30,and became better known after it was used in Kaggle challenge. It is a set,SATD
scikit-learn/examples/ensemble/plot_stack_predictors.py,31,"of 1460 residential homes in Ames, Iowa, each described by 80 features. We",not
scikit-learn/examples/ensemble/plot_stack_predictors.py,32,will use it to predict the final logarithmic price of the houses. In this,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,33,example we will use only 20 most interesting features chosen using,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,34,GradientBoostingRegressor() and limit number of entries (here we won't go,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,35,into the details on how to select the most interesting features).,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,36,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,37,The Ames housing dataset is not shipped with scikit-learn and therefore we,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,38,will fetch it from `OpenML`_.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,39,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,40,.. _`Ames Housing`: http://jse.amstat.org/v19n3/decock.pdf,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,41,.. _`OpenML`: https://www.openml.org/d/42165,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,71,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,72,Make pipeline to preprocess the data,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,73,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,74,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,75,Before we can use Ames dataset we still need to do some preprocessing.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,76,"First, the dataset has many missing values. To impute them, we will exchange",not
scikit-learn/examples/ensemble/plot_stack_predictors.py,77,categorical missing values with the new category 'missing' while the,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,78,numerical missing values with the 'mean' of the column. We will also encode,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,79,the categories with either :class:`sklearn.preprocessing.OneHotEncoder,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,80,<sklearn.preprocessing.OneHotEncoder>` or,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,81,:class:`sklearn.preprocessing.OrdinalEncoder,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,82,<sklearn.preprocessing.OrdinalEncoder>` depending for which type of model we,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,83,will use them (linear or non-linear model). To falicitate this preprocessing,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,84,we will make two pipelines.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,85,You can skip this section if your data is ready to use and does,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,86,not need preprocessing,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,104,noqa,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,126,transformation to use for non-linear estimators,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,132,transformation to use for linear estimators,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,139,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,140,Stack of predictors on a single data set,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,141,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,142,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,143,It is sometimes tedious to find the model which will best perform on a given,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,144,dataset. Stacking provide an alternative by combining the outputs of several,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,145,"learners, without the need to choose a model specifically. The performance of",not
scikit-learn/examples/ensemble/plot_stack_predictors.py,146,stacking is usually close to the best model and sometimes it can outperform,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,147,the prediction performance of each individual model.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,148,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,149,"Here, we combine 3 learners (linear and non-linear) and use a ridge regressor",not
scikit-learn/examples/ensemble/plot_stack_predictors.py,150,to combine their outputs together.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,151,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,152,Note: although we will make new pipelines with the processors which we wrote,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,153,"in the previous section for the 3 learners, the final estimator RidgeCV()",not
scikit-learn/examples/ensemble/plot_stack_predictors.py,154,does not need preprocessing of the data as it will be fed with the already,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,155,preprocessed output from the 3 learners.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,158,noqa,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,184,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,185,Measure and plot the results,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,186,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,187,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,188,Now we can use Ames Housing dataset to make the predictions. We check the,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,189,performance of each individual predictor as well as of the stack of the,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,190,regressors.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,191,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,192,The function ``plot_regression_results`` is used to plot the predicted and,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,193,true targets.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,253,,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,254,The stacked regressor will combine the strengths of the different regressors.,not
scikit-learn/examples/ensemble/plot_stack_predictors.py,255,"However, we also see that training the stacked regressor is much more",not
scikit-learn/examples/ensemble/plot_stack_predictors.py,256,computationally expensive.,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,24,Author: Tim Head <betatim@gmail.com>,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,25,,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,26,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,46,It is important to train the ensemble of trees on a different subset,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,47,of the training data than the linear regression model to avoid,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,48,"overfitting, in particular if the total number of leaves is",not
scikit-learn/examples/ensemble/plot_feature_transformation.py,49,similar to the number of training samples,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,53,Unsupervised transformation based on totally random trees,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,63,Supervised transformation based on random forests,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,74,Supervised transformation based on gradient boosted trees,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,86,The gradient boosted model by itself,not
scikit-learn/examples/ensemble/plot_feature_transformation.py,90,The random forest model by itself,not
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,23,"Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,",SATD
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,24,Noel Dawe <noel.dawe@gmail.com>,not
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,25,,not
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,26,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_adaboost_hastie_10_2.py,38,A learning rate of 1. may not be optimal for both SAMME and SAMME.R,not
scikit-learn/examples/ensemble/plot_forest_iris.py,54,Parameters,not
scikit-learn/examples/ensemble/plot_forest_iris.py,58,fine step width for decision surface contours,not
scikit-learn/examples/ensemble/plot_forest_iris.py,59,step widths for coarse classifier guesses,not
scikit-learn/examples/ensemble/plot_forest_iris.py,60,fix the seed on each iteration,not
scikit-learn/examples/ensemble/plot_forest_iris.py,62,Load data,not
scikit-learn/examples/ensemble/plot_forest_iris.py,75,We only take the two corresponding features,not
scikit-learn/examples/ensemble/plot_forest_iris.py,79,Shuffle,not
scikit-learn/examples/ensemble/plot_forest_iris.py,86,Standardize,not
scikit-learn/examples/ensemble/plot_forest_iris.py,91,Train,not
scikit-learn/examples/ensemble/plot_forest_iris.py,95,Create a title for each column and the console by using str() and,not
scikit-learn/examples/ensemble/plot_forest_iris.py,96,slicing away useless parts of the string,not
scikit-learn/examples/ensemble/plot_forest_iris.py,109,Add a title at the top of each column,not
scikit-learn/examples/ensemble/plot_forest_iris.py,112,Now plot the decision boundary using a fine mesh as input to a,not
scikit-learn/examples/ensemble/plot_forest_iris.py,113,filled contour plot,not
scikit-learn/examples/ensemble/plot_forest_iris.py,119,Plot either a single DecisionTreeClassifier or alpha blend the,not
scikit-learn/examples/ensemble/plot_forest_iris.py,120,decision surfaces of the ensemble of classifiers,not
scikit-learn/examples/ensemble/plot_forest_iris.py,126,Choose alpha blend level with respect to the number,not
scikit-learn/examples/ensemble/plot_forest_iris.py,127,of estimators,not
scikit-learn/examples/ensemble/plot_forest_iris.py,128,that are in use (noting that AdaBoost can use fewer estimators,not
scikit-learn/examples/ensemble/plot_forest_iris.py,129,than its maximum if it achieves a good enough fit early on),not
scikit-learn/examples/ensemble/plot_forest_iris.py,136,Build a coarser grid to plot a set of ensemble classifications,not
scikit-learn/examples/ensemble/plot_forest_iris.py,137,to show how these are different to what we see in the decision,not
scikit-learn/examples/ensemble/plot_forest_iris.py,138,surfaces. These points are regularly space and do not have a,not
scikit-learn/examples/ensemble/plot_forest_iris.py,139,black outline,not
scikit-learn/examples/ensemble/plot_forest_iris.py,150,"Plot the training points, these are clustered together and have a",not
scikit-learn/examples/ensemble/plot_forest_iris.py,151,black outline,not
scikit-learn/examples/ensemble/plot_forest_iris.py,155,move on to the next plot in sequence,SATD
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,22,----------------------------------------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,23,First the noiseless case,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,27,Observations,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,35,"Mesh the input space for evaluations of the real function, the prediction and",not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,36,its MSE,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,49,Make the prediction on the meshed x-axis,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,55,Make the prediction on the meshed x-axis,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,61,Make the prediction on the meshed x-axis,not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,64,"Plot the function, the prediction and the 95% confidence interval based on",not
scikit-learn/examples/ensemble/plot_gradient_boosting_quantile.py,65,the MSE,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,28,Author: Kian Ho <hui.kian.ho@gmail.com>,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,29,Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,30,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,31,,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,32,License: BSD 3 Clause,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,38,Generate a binary classification dataset.,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,43,NOTE: Setting the `warm_start` construction parameter to `True` disables,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,44,support for parallelized ensembles but is necessary for tracking the OOB,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,45,error trajectory during training.,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,61,"Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.",not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,64,Range of `n_estimators` values to explore.,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,73,Record the OOB error for each `n_estimators=i` setting.,not
scikit-learn/examples/ensemble/plot_ensemble_oob.py,77,"Generate the ""OOB error rate"" vs. ""n_estimators"" plot.",not
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,25,Author: Tim Head <betatim@gmail.com>,not
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,26,,not
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,27,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,36,Create a random dataset,not
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,55,Predict on new data,not
scikit-learn/examples/ensemble/plot_random_forest_regression_multioutput.py,59,Plot the results,not
scikit-learn/examples/ensemble/plot_isolation_forest.py,33,Generate train data,not
scikit-learn/examples/ensemble/plot_isolation_forest.py,36,Generate some regular novel observations,not
scikit-learn/examples/ensemble/plot_isolation_forest.py,39,Generate some abnormal novel observations,not
scikit-learn/examples/ensemble/plot_isolation_forest.py,42,fit the model,not
scikit-learn/examples/ensemble/plot_isolation_forest.py,49,"plot the line, the samples, and the nearest vectors to the plane",not
scikit-learn/examples/ensemble/plot_voting_regressor.py,35,,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,36,Training classifiers,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,37,--------------------------------,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,38,,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,39,"First, we will load the diabetes dataset and initiate a gradient boosting",not
scikit-learn/examples/ensemble/plot_voting_regressor.py,40,"regressor, a random forest regressor and a linear regression. Next, we will",not
scikit-learn/examples/ensemble/plot_voting_regressor.py,41,use the 3 regressors to build the voting regressor:,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,45,Train classifiers,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,57,,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,58,Making predictions,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,59,--------------------------------,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,60,,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,61,Now we will use each of the regressors to make the 20 first predictions.,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,70,,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,71,Plot the results,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,72,--------------------------------,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,73,,not
scikit-learn/examples/ensemble/plot_voting_regressor.py,74,"Finally, we will visualize the 20 predictions. The red stars show the average",not
scikit-learn/examples/ensemble/plot_voting_regressor.py,75,prediction made by :class:`~ensemble.VotingRegressor`.,not
scikit-learn/examples/ensemble/plot_forest_importances.py,27,Build a classification task using 3 informative features,not
scikit-learn/examples/ensemble/plot_forest_importances.py,37,Build a forest and compute the impurity-based feature importances,not
scikit-learn/examples/ensemble/plot_forest_importances.py,47,Print the feature ranking,not
scikit-learn/examples/ensemble/plot_forest_importances.py,53,Plot the impurity-based feature importances of the forest,not
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,28,Author: Noel Dawe <noel.dawe@gmail.com>,not
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,29,,not
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,30,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,75,"Boosting might terminate early, but the following arrays are always",not
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,76,n_estimators long. We crop them to the actual number of trees here:,not
scikit-learn/examples/ensemble/plot_adaboost_multiclass.py,116,prevent overlapping y-axis labels,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,34,Authors: Vighnesh Birodkar <vighneshbirodkar@nyu.edu>,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,35,Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,36,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,67,We specify that if the scores don't improve by atleast 0.01 for the last,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,68,"10 stages, stop fitting additional stages",not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,94,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,95,Compare scores with and without early stopping,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,96,----------------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,132,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,133,Compare fit times with and without early stopping,not
scikit-learn/examples/ensemble/plot_gradient_boosting_early_stopping.py,134,-------------------------------------------------,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,24,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,25,,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,26,License: BSD 3 clause,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,38,"map labels from {-1, 1} to {0, 1}",not
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,65,compute test set deviance,not
scikit-learn/examples/ensemble/plot_gradient_boosting_regularization.py,69,"clf.loss_ assumes that y_test[i] in {0, 1}",not
scikit-learn/examples/exercises/plot_cv_diabetes.py,40,plot error lines showing +/- std. errors of the scores,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,46,alpha=0.2 controls the translucency of the fill color,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,54,,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,55,Bonus: how much can you trust the selection of alpha?,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,57,To answer this question we use the LassoCV object that sets its alpha,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,58,parameter automatically from the data by internal cross-validation (i.e. it,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,59,performs cross-validation on the training data it receives).,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,60,We use external cross-validation to see how much the automatically obtained,not
scikit-learn/examples/exercises/plot_cv_diabetes.py,61,alphas differ across different cross-validation folds.,not
scikit-learn/examples/exercises/plot_cv_digits.py,31,Do the plotting,not
scikit-learn/examples/exercises/plot_iris_exercise.py,37,fit the model,not
scikit-learn/examples/exercises/plot_iris_exercise.py,47,Circle out the test data,not
scikit-learn/examples/exercises/plot_iris_exercise.py,60,Put the result into a color plot,not
scikit-learn/examples/datasets/plot_iris_dataset.py,1,!/usr/bin/python,not
scikit-learn/examples/datasets/plot_iris_dataset.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/datasets/plot_iris_dataset.py,22,Code source: Gaël Varoquaux,not
scikit-learn/examples/datasets/plot_iris_dataset.py,23,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/datasets/plot_iris_dataset.py,24,License: BSD 3 clause,not
scikit-learn/examples/datasets/plot_iris_dataset.py,31,import some data to play with,not
scikit-learn/examples/datasets/plot_iris_dataset.py,33,we only take the first two features.,not
scikit-learn/examples/datasets/plot_iris_dataset.py,42,Plot the training points,not
scikit-learn/examples/datasets/plot_iris_dataset.py,53,To getter a better understanding of interaction of the dimensions,SATD
scikit-learn/examples/datasets/plot_iris_dataset.py,54,plot the first three PCA dimensions,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,45,red,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,46,blue,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,47,purple,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,48,yellow,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,49,orange,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,50,green,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,51,brown,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,54,Use same random seed for multiple calls to make_multilabel_classification to,not
scikit-learn/examples/datasets/plot_random_multilabel_dataset.py,55,ensure same distributions,not
scikit-learn/examples/datasets/plot_digits_last_image.py,1,!/usr/bin/python,not
scikit-learn/examples/datasets/plot_digits_last_image.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/datasets/plot_digits_last_image.py,21,Code source: Gaël Varoquaux,not
scikit-learn/examples/datasets/plot_digits_last_image.py,22,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/datasets/plot_digits_last_image.py,23,License: BSD 3 clause,not
scikit-learn/examples/datasets/plot_digits_last_image.py,29,Load the digits dataset,not
scikit-learn/examples/datasets/plot_digits_last_image.py,32,Display the first digit,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,32,,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,33,Random Forest Feature Importance on Breast Cancer Data,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,34,------------------------------------------------------,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,35,"First, we train a random forest on the breast cancer dataset and evaluate",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,36,its accuracy on a test set:,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,45,,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,46,"Next, we plot the tree based feature importance and the permutation",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,47,importance. The permutation importance plot shows that permuting a feature,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,48,"drops the accuracy by at most `0.012`, which would suggest that none of the",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,49,features are important. This is in contradiction with the high test accuracy,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,50,computed above: some feature must be important. The permutation importance,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,51,is calculated on the training set to show how much the model relies on each,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,52,feature during training.,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,71,,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,72,Handling Multicollinear Features,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,73,--------------------------------,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,74,"When features are collinear, permutating one feature will have little",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,75,effect on the models performance because it can get the same information,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,76,from a correlated feature. One way to handle multicollinear features is by,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,77,"performing hierarchical clustering on the Spearman rank-order correlations,",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,78,"picking a threshold, and keeping a single feature from each cluster. First,",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,79,we plot a heatmap of the correlated features:,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,95,,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,96,"Next, we manually pick a threshold by visual inspection of the dendrogram",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,97,to group our features into clusters and choose a feature from each cluster to,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,98,"keep, select those features from our dataset, and train a new random forest.",not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,99,The test accuracy of the new random forest did not change much compared to,not
scikit-learn/examples/inspection/plot_permutation_importance_multicollinear.py,100,the random forest trained on the complete dataset.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,39,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,40,The dataset: wages,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,41,------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,42,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,43,We fetch the data from `OpenML <http://openml.org/>`_.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,44,Note that setting the parameter `as_frame` to True will retrieve the data,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,45,as a pandas dataframe.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,51,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,52,"Then, we identify features `X` and targets `y`: the column WAGE is our",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,53,"target variable (i.e., the variable which we want to predict).",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,54,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,58,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,59,Note that the dataset contains categorical and numerical variables.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,60,We will need to take this into account when preprocessing the dataset,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,61,thereafter.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,65,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,66,Our target for prediction: the wage.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,67,Wages are described as floating-point number in dollars per hour.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,71,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,72,We split the sample into a train and a test dataset.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,73,Only the train dataset will be used in the following exploratory analysis.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,74,This is a way to emulate a real situation where predictions are performed on,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,75,"an unknown target, and we don't want our analysis and decisions to be biased",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,76,by our knowledge of the test data.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,84,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,85,"First, let's get some insights by looking at the variable distributions and",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,86,at the pairwise relationships between them. Only numerical,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,87,"variables will be used. In the following plot, each dot represents a sample.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,88,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,89,.. _marginal_dependencies:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,95,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,96,Looking closely at the WAGE distribution reveals that it has a,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,97,"long tail. For this reason, we should take its logarithm",SATD
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,98,to turn it approximately into a normal distribution (linear models such,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,99,as ridge or lasso work best for a normal distribution of error).,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,100,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,101,The WAGE is increasing when EDUCATION is increasing.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,102,Note that the dependence between WAGE and EDUCATION,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,103,"represented here is a marginal dependence, i.e., it describes the behavior",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,104,of a specific variable without keeping the others fixed.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,105,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,106,"Also, the EXPERIENCE and AGE are strongly linearly correlated.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,107,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,108,.. _the-pipeline:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,109,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,110,The machine-learning pipeline,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,111,-----------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,112,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,113,"To design our machine-learning pipeline, we first manually",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,114,check the type of data that we are dealing with:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,118,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,119,"As seen previously, the dataset contains columns with different data types",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,120,and we need to apply a specific preprocessing for each data types.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,121,In particular categorical variables cannot be included in linear model if not,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,122,"coded as integers first. In addition, to avoid categorical features to be",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,123,"treated as ordered values, we need to one-hot-encode them.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,124,Our pre-processor will,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,125,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,126,"- one-hot encode (i.e., generate a column by category) the categorical",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,127,columns;,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,128,- as a first approach (we will see after how the normalisation of numerical,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,129,"values will affect our discussion), keep numerical values as they are.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,143,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,144,To describe the dataset as a linear model we use a ridge regressor,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,145,with a very small regularization and to model the logarithm of the WAGE.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,161,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,162,Processing the dataset,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,163,----------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,164,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,165,"First, we fit the model.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,169,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,170,Then we check the performance of the computed model plotting its predictions,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,171,"on the test set and computing,",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,172,"for example, the median absolute error of the model.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,193,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,194,The model learnt is far from being a good model making accurate predictions:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,195,"this is obvious when looking at the plot above, where good predictions",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,196,should lie on the red line.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,197,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,198,"In the following section, we will interpret the coefficients of the model.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,199,"While we do so, we should keep in mind that any conclusion we draw is",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,200,"about the model that we build, rather than about the true (real-world)",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,201,generative process of the data.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,202,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,203,Interpreting coefficients: scale matters,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,204,---------------------------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,205,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,206,"First of all, we can take a look to the values of the coefficients of the",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,207,regressor we have fitted.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,222,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,223,"The AGE coefficient is expressed in ""dollars/hour per living years"" while the",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,224,"EDUCATION one is expressed in ""dollars/hour per years of education"". This",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,225,representation of the coefficients has the benefit of making clear the,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,226,practical predictions of the model: an increase of :math:`1` year in AGE,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,227,"means a decrease of :math:`0.030867` dollars/hour, while an increase of",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,228,:math:`1` year in EDUCATION means an increase of :math:`0.054699`,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,229,"dollars/hour. On the other hand, categorical variables (as UNION or SEX) are",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,230,adimensional numbers taking either the value 0 or 1. Their coefficients,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,231,"are expressed in dollars/hour. Then, we cannot compare the magnitude of",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,232,"different coefficients since the features have different natural scales, and",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,233,"hence value ranges, because of their different unit of measure. This is more",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,234,visible if we plot the coefficients.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,241,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,242,"Indeed, from the plot above the most important factor in determining WAGE",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,243,appears to be the,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,244,"variable UNION, even if our intuition might tell us that variables",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,245,like EXPERIENCE should have more impact.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,246,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,247,Looking at the coefficient plot to gauge feature importance can be,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,248,"misleading as some of them vary on a small scale, while others, like AGE,",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,249,"varies a lot more, several decades.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,250,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,251,This is visible if we compare the standard deviations of different,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,252,features.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,263,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,264,Multiplying the coefficients by the standard deviation of the related,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,265,feature would reduce all the coefficients to the same unit of measure.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,266,As we will see :ref:`after<scaling_num>` this is equivalent to normalize,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,267,"numerical variables to their standard deviation,",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,268,as :math:`y = \sum{coef_i \times X_i} =,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,269,\sum{(coef_i \times std_i) \times (X_i / std_i)}`.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,270,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,271,"In that way, we emphasize that the",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,272,"greater the variance of a feature, the larger the weight of the corresponding",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,273,"coefficient on the output, all else being equal.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,285,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,286,"Now that the coefficients have been scaled, we can safely compare them.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,287,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,288,.. warning::,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,289,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,290,Why does the plot above suggest that an increase in age leads to a,SATD
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,291,decrease in wage? Why the :ref:`initial pairplot,SATD
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,292,<marginal_dependencies>` is telling the opposite?,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,293,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,294,The plot above tells us about dependencies between a specific feature and,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,295,"the target when all other features remain constant, i.e., **conditional",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,296,dependencies**. An increase of the AGE will induce a decrease,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,297,"of the WAGE when all other features remain constant. On the contrary, an",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,298,increase of the EXPERIENCE will induce an increase of the WAGE when all,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,299,other features remain constant.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,300,"Also, AGE, EXPERIENCE and EDUCATION are the three variables that most",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,301,influence the model.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,302,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,303,Checking the variability of the coefficients,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,304,--------------------------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,305,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,306,We can check the coefficient variability through cross-validation:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,307,it is a form of data perturbation (related to,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,308,`resampling <https://en.wikipedia.org/wiki/Resampling_(statistics)>`_).,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,309,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,310,If coefficients vary significantly when changing the input dataset,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,311,"their robustness is not guaranteed, and they should probably be interpreted",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,312,with caution.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,335,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,336,The problem of correlated variables,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,337,-----------------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,338,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,339,The AGE and EXPERIENCE coefficients are affected by strong variability which,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,340,might be due to the collinearity between the 2 features: as AGE and,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,341,"EXPERIENCE vary together in the data, their effect is difficult to tease",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,342,apart.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,343,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,344,To verify this interpretation we plot the variability of the AGE and,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,345,EXPERIENCE coefficient.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,346,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,347,.. _covariation:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,358,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,359,Two regions are populated: when the EXPERIENCE coefficient is,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,360,positive the AGE one is negative and viceversa.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,361,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,362,To go further we remove one of the 2 features and check what is the impact,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,363,on the model stability.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,386,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,387,The estimation of the EXPERIENCE coefficient is now less variable and,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,388,remain important for all models trained during cross-validation.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,389,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,390,.. _scaling_num:,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,391,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,392,Preprocessing numerical variables,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,393,---------------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,394,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,395,"As said above (see "":ref:`the-pipeline`""), we could also choose to scale",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,396,numerical values before training the model.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,397,This can be useful to apply a similar amount regularization to all of them,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,398,in the Ridge.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,399,The preprocessor is redefined in order to subtract the mean and scale,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,400,variables to unit variance.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,410,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,411,The model will stay unchanged.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,424,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,425,"Again, we check the performance of the computed",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,426,"model using, for example, the median absolute error of the model and the R",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,427,squared coefficient.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,447,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,448,"For the coefficient analysis, scaling is not needed this time.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,459,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,460,We now inspect the coefficients across several cross-validation folds.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,478,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,479,The result is quite similar to the non-normalized case.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,480,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,481,Linear models with regularization,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,482,---------------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,483,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,484,"In machine-learning practice, Ridge Regression is more often used with",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,485,non-negligible regularization.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,486,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,487,"Above, we limited this regularization to a very little amount.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,488,Regularization improves the conditioning of the problem and reduces the,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,489,variance of the estimates. RidgeCV applies cross validation in order to,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,490,determine which value of the regularization parameter (`alpha`) is best,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,491,suited for prediction.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,506,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,507,First we check which value of :math:`\alpha` has been selected.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,511,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,512,Then we check the quality of the predictions.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,533,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,534,The ability to reproduce the data of the regularized model is similar to,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,535,the one of the non-regularized model.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,546,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,547,The coefficients are significantly different.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,548,AGE and EXPERIENCE coefficients are both positive but they now have less,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,549,influence on the prediction.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,550,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,551,The regularization reduces the influence of correlated,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,552,variables on the model because the weight is shared between the two,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,553,"predictive variables, so neither alone would have strong weights.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,554,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,555,"On the other hand, the weights obtained with regularization are more",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,556,stable  (see the :ref:`ridge_regression` User Guide section). This,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,557,"increased stability is visible from the plot, obtained from data",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,558,"perturbations, in a cross validation. This plot can  be compared with",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,559,the :ref:`previous one<covariation>`.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,581,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,582,Linear models with sparse coefficients,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,583,--------------------------------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,584,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,585,"Another possibility to take into account correlated variables in the dataset,",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,586,is to estimate sparse coefficients. In some way we already did it manually,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,587,when we dropped the AGE column in a previous Ridge estimation.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,588,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,589,Lasso models (see the :ref:`lasso` User Guide section) estimates sparse,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,590,coefficients. LassoCV applies cross validation in order to,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,591,determine which value of the regularization parameter (`alpha`) is best,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,592,suited for the model estimation.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,607,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,608,First we verify which value of :math:`\alpha` has been selected.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,612,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,613,Then we check the quality of the predictions.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,634,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,635,"For our dataset, again the model is not very predictive.",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,646,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,647,A Lasso model identifies the correlation between,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,648,AGE and EXPERIENCE and suppresses one of them for the sake of the prediction.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,649,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,650,It is important to keep in mind that the coefficients that have been,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,651,dropped may still be related to the outcome by themselves: the model,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,652,chose to suppress them because they bring little or no additional,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,653,"information on top of the other features. Additionnaly, this selection",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,654,"is unstable for correlated features, and should be interpreted with",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,655,caution.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,656,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,657,Lessons learned,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,658,---------------,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,659,,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,660,* Coefficients must be scaled to the same unit of measure to retrieve,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,661,feature importance. Scaling them with the standard-deviation of the,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,662,feature is a useful proxy.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,663,* Coefficients in multivariate linear models represent the dependency,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,664,"between a given feature and the target, **conditional** on the other",not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,665,features.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,666,* Correlated features induce instabilities in the coefficients of linear,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,667,models and their effects cannot be well teased apart.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,668,* Different linear models respond differently to feature correlation and,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,669,coefficients could significantly vary from one another.,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,670,* Inspecting coefficients across the folds of a cross-validation loop,not
scikit-learn/examples/inspection/plot_linear_model_coefficient_interpretation.py,671,gives an idea of their stability.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,40,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,41,Data Loading and Feature Engineering,not
scikit-learn/examples/inspection/plot_permutation_importance.py,42,------------------------------------,not
scikit-learn/examples/inspection/plot_permutation_importance.py,43,Let's use pandas to load a copy of the titanic dataset. The following shows,not
scikit-learn/examples/inspection/plot_permutation_importance.py,44,how to apply separate preprocessing on numerical and categorical features.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,45,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,46,We further include two random variables that are not correlated in any way,not
scikit-learn/examples/inspection/plot_permutation_importance.py,47,with the target variable (``survived``):,not
scikit-learn/examples/inspection/plot_permutation_importance.py,48,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,49,- ``random_num`` is a high cardinality numerical variable (as many unique,not
scikit-learn/examples/inspection/plot_permutation_importance.py,50,values as records).,not
scikit-learn/examples/inspection/plot_permutation_importance.py,51,- ``random_cat`` is a low cardinality categorical variable (3 possible,not
scikit-learn/examples/inspection/plot_permutation_importance.py,52,values).,not
scikit-learn/examples/inspection/plot_permutation_importance.py,84,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,85,Accuracy of the Model,not
scikit-learn/examples/inspection/plot_permutation_importance.py,86,---------------------,not
scikit-learn/examples/inspection/plot_permutation_importance.py,87,"Prior to inspecting the feature importances, it is important to check that",not
scikit-learn/examples/inspection/plot_permutation_importance.py,88,the model predictive performance is high enough. Indeed there would be little,not
scikit-learn/examples/inspection/plot_permutation_importance.py,89,interest of inspecting the important features of a non-predictive model.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,90,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,91,Here one can observe that the train accuracy is very high (the forest model,not
scikit-learn/examples/inspection/plot_permutation_importance.py,92,has enough capacity to completely memorize the training set) but it can still,not
scikit-learn/examples/inspection/plot_permutation_importance.py,93,generalize well enough to the test set thanks to the built-in bagging of,not
scikit-learn/examples/inspection/plot_permutation_importance.py,94,random forests.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,95,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,96,It might be possible to trade some accuracy on the training set for a,not
scikit-learn/examples/inspection/plot_permutation_importance.py,97,slightly better accuracy on the test set by limiting the capacity of the,SATD
scikit-learn/examples/inspection/plot_permutation_importance.py,98,trees (for instance by setting ``min_samples_leaf=5`` or,not
scikit-learn/examples/inspection/plot_permutation_importance.py,99,``min_samples_leaf=10``) so as to limit overfitting while not introducing too,not
scikit-learn/examples/inspection/plot_permutation_importance.py,100,much underfitting.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,101,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,102,However let's keep our high capacity random forest model for now so as to,not
scikit-learn/examples/inspection/plot_permutation_importance.py,103,illustrate some pitfalls with feature importance on variables with many,not
scikit-learn/examples/inspection/plot_permutation_importance.py,104,unique values.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,109,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,110,Tree's Feature Importance from Mean Decrease in Impurity (MDI),not
scikit-learn/examples/inspection/plot_permutation_importance.py,111,--------------------------------------------------------------,not
scikit-learn/examples/inspection/plot_permutation_importance.py,112,The impurity-based feature importance ranks the numerical features to be the,not
scikit-learn/examples/inspection/plot_permutation_importance.py,113,"most important features. As a result, the non-predictive ``random_num``",not
scikit-learn/examples/inspection/plot_permutation_importance.py,114,variable is ranked the most important!,not
scikit-learn/examples/inspection/plot_permutation_importance.py,115,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,116,This problem stems from two limitations of impurity-based feature,not
scikit-learn/examples/inspection/plot_permutation_importance.py,117,importances:,not
scikit-learn/examples/inspection/plot_permutation_importance.py,118,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,119,- impurity-based importances are biased towards high cardinality features;,not
scikit-learn/examples/inspection/plot_permutation_importance.py,120,- impurity-based importances are computed on training set statistics and,not
scikit-learn/examples/inspection/plot_permutation_importance.py,121,therefore do not reflect the ability of feature to be useful to make,not
scikit-learn/examples/inspection/plot_permutation_importance.py,122,predictions that generalize to the test set (when the model has enough,not
scikit-learn/examples/inspection/plot_permutation_importance.py,123,capacity).,not
scikit-learn/examples/inspection/plot_permutation_importance.py,144,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,145,"As an alternative, the permutation importances of ``rf`` are computed on a",not
scikit-learn/examples/inspection/plot_permutation_importance.py,146,"held out test set. This shows that the low cardinality categorical feature,",not
scikit-learn/examples/inspection/plot_permutation_importance.py,147,``sex`` is the most important feature.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,148,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,149,Also note that both random features have very low importances (close to 0) as,not
scikit-learn/examples/inspection/plot_permutation_importance.py,150,expected.,not
scikit-learn/examples/inspection/plot_permutation_importance.py,162,,not
scikit-learn/examples/inspection/plot_permutation_importance.py,163,It is also possible to compute the permutation importances on the training,not
scikit-learn/examples/inspection/plot_permutation_importance.py,164,set. This reveals that ``random_num`` gets a significantly higher importance,not
scikit-learn/examples/inspection/plot_permutation_importance.py,165,ranking than when computed on the test set. The difference between those two,not
scikit-learn/examples/inspection/plot_permutation_importance.py,166,plots is a confirmation that the RF model has enough capacity to use that,not
scikit-learn/examples/inspection/plot_permutation_importance.py,167,random numerical feature to overfit. You can further confirm this by,not
scikit-learn/examples/inspection/plot_permutation_importance.py,168,re-running this example with constrained RF with min_samples_leaf=10.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,43,noqa,not
scikit-learn/examples/inspection/plot_partial_dependence.py,49,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,50,California Housing data preprocessing,not
scikit-learn/examples/inspection/plot_partial_dependence.py,51,-------------------------------------,not
scikit-learn/examples/inspection/plot_partial_dependence.py,52,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,53,Center target to avoid gradient boosting init bias: gradient boosting,not
scikit-learn/examples/inspection/plot_partial_dependence.py,54,with the 'recursion' method does not account for the initial estimator,not
scikit-learn/examples/inspection/plot_partial_dependence.py,55,"(here the average target, by default)",not
scikit-learn/examples/inspection/plot_partial_dependence.py,66,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,67,Partial Dependence computation for multi-layer perceptron,not
scikit-learn/examples/inspection/plot_partial_dependence.py,68,---------------------------------------------------------,not
scikit-learn/examples/inspection/plot_partial_dependence.py,69,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,70,Let's fit a MLPRegressor and compute single-variable partial dependence,not
scikit-learn/examples/inspection/plot_partial_dependence.py,71,plots,not
scikit-learn/examples/inspection/plot_partial_dependence.py,83,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,84,We configured a pipeline to scale the numerical input features and tuned the,not
scikit-learn/examples/inspection/plot_partial_dependence.py,85,neural network size and learning rate to get a reasonable compromise between,not
scikit-learn/examples/inspection/plot_partial_dependence.py,86,training time and predictive performance on a test set.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,87,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,88,"Importantly, this tabular dataset has very different dynamic ranges for its",not
scikit-learn/examples/inspection/plot_partial_dependence.py,89,features. Neural networks tend to be very sensitive to features with varying,not
scikit-learn/examples/inspection/plot_partial_dependence.py,90,scales and forgetting to preprocess the numeric feature would lead to a very,not
scikit-learn/examples/inspection/plot_partial_dependence.py,91,poor model.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,92,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,93,It would be possible to get even higher predictive performance with a larger,not
scikit-learn/examples/inspection/plot_partial_dependence.py,94,neural network but the training would also be significantly more expensive.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,95,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,96,Note that it is important to check that the model is accurate enough on a,not
scikit-learn/examples/inspection/plot_partial_dependence.py,97,test set before plotting the partial dependence since there would be little,not
scikit-learn/examples/inspection/plot_partial_dependence.py,98,use in explaining the impact of a given feature on the prediction function of,not
scikit-learn/examples/inspection/plot_partial_dependence.py,99,a poor model.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,100,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,101,Let's now compute the partial dependence plots for this neural network using,not
scikit-learn/examples/inspection/plot_partial_dependence.py,102,the model-agnostic (brute-force) method:,not
scikit-learn/examples/inspection/plot_partial_dependence.py,106,"We don't compute the 2-way PDP (5, 1) here, because it is a lot slower",not
scikit-learn/examples/inspection/plot_partial_dependence.py,107,with the brute method.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,117,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,118,Partial Dependence computation for Gradient Boosting,not
scikit-learn/examples/inspection/plot_partial_dependence.py,119,----------------------------------------------------,not
scikit-learn/examples/inspection/plot_partial_dependence.py,120,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,121,Let's now fit a GradientBoostingRegressor and compute the partial dependence,not
scikit-learn/examples/inspection/plot_partial_dependence.py,122,plots either or one or two variables at a time.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,131,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,132,"Here, we used the default hyperparameters for the gradient boosting model",not
scikit-learn/examples/inspection/plot_partial_dependence.py,133,without any preprocessing as tree-based models are naturally robust to,not
scikit-learn/examples/inspection/plot_partial_dependence.py,134,monotonic transformations of numerical features.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,135,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,136,"Note that on this tabular dataset, Gradient Boosting Machines are both",not
scikit-learn/examples/inspection/plot_partial_dependence.py,137,significantly faster to train and more accurate than neural networks. It is,not
scikit-learn/examples/inspection/plot_partial_dependence.py,138,also significantly cheaper to tune their hyperparameters (the default tend to,not
scikit-learn/examples/inspection/plot_partial_dependence.py,139,work well while this is not often the case for neural networks).,not
scikit-learn/examples/inspection/plot_partial_dependence.py,140,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,141,"Finally, as we will see next, computing partial dependence plots tree-based",not
scikit-learn/examples/inspection/plot_partial_dependence.py,142,models is also orders of magnitude faster making it cheap to compute partial,not
scikit-learn/examples/inspection/plot_partial_dependence.py,143,dependence plots for pairs of interacting features:,not
scikit-learn/examples/inspection/plot_partial_dependence.py,158,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,159,Analysis of the plots,not
scikit-learn/examples/inspection/plot_partial_dependence.py,160,---------------------,not
scikit-learn/examples/inspection/plot_partial_dependence.py,161,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,162,We can clearly see that the median house price shows a linear relationship,not
scikit-learn/examples/inspection/plot_partial_dependence.py,163,with the median income (top left) and that the house price drops when the,not
scikit-learn/examples/inspection/plot_partial_dependence.py,164,average occupants per household increases (top middle).,not
scikit-learn/examples/inspection/plot_partial_dependence.py,165,The top right plot shows that the house age in a district does not have,not
scikit-learn/examples/inspection/plot_partial_dependence.py,166,a strong influence on the (median) house price; so does the average rooms,not
scikit-learn/examples/inspection/plot_partial_dependence.py,167,per household.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,168,The tick marks on the x-axis represent the deciles of the feature values,not
scikit-learn/examples/inspection/plot_partial_dependence.py,169,in the training data.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,170,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,171,We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much,not
scikit-learn/examples/inspection/plot_partial_dependence.py,172,smoother predictions than,not
scikit-learn/examples/inspection/plot_partial_dependence.py,173,:class:`~sklearn.ensemble.HistGradientBoostingRegressor`. For the plots to be,not
scikit-learn/examples/inspection/plot_partial_dependence.py,174,"comparable, it is necessary to subtract the average value of the target",not
scikit-learn/examples/inspection/plot_partial_dependence.py,175,"``y``: The 'recursion' method, used by default for",not
scikit-learn/examples/inspection/plot_partial_dependence.py,176,":class:`~sklearn.ensemble.HistGradientBoostingRegressor`, does not account",not
scikit-learn/examples/inspection/plot_partial_dependence.py,177,for the initial predictor (in our case the average target). Setting the,not
scikit-learn/examples/inspection/plot_partial_dependence.py,178,target average to 0 avoids this bias.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,179,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,180,Partial dependence plots with two target features enable us to visualize,not
scikit-learn/examples/inspection/plot_partial_dependence.py,181,interactions among them. The two-way partial dependence plot shows the,not
scikit-learn/examples/inspection/plot_partial_dependence.py,182,dependence of median house price on joint values of house age and average,not
scikit-learn/examples/inspection/plot_partial_dependence.py,183,occupants per household. We can clearly see an interaction between the,not
scikit-learn/examples/inspection/plot_partial_dependence.py,184,"two features: for an average occupancy greater than two, the house price is",not
scikit-learn/examples/inspection/plot_partial_dependence.py,185,"nearly independent of the house age, whereas for values less than two there",not
scikit-learn/examples/inspection/plot_partial_dependence.py,186,is a strong dependence on age.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,188,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,189,3D interaction plots,not
scikit-learn/examples/inspection/plot_partial_dependence.py,190,--------------------,not
scikit-learn/examples/inspection/plot_partial_dependence.py,191,,not
scikit-learn/examples/inspection/plot_partial_dependence.py,192,"Let's make the same partial dependence plot for the 2 features interaction,",not
scikit-learn/examples/inspection/plot_partial_dependence.py,193,this time in 3 dimensions.,not
scikit-learn/examples/inspection/plot_partial_dependence.py,208,pretty init view,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,26,,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,27,Generate sample data,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,35,,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,36,Compute clustering with Means,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,43,,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,44,Compute clustering with MiniBatchKMeans,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,52,,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,53,Plot result,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,59,We want to have the same colors for the same cluster from the,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,60,MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,61,closest one.,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,70,KMeans,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,85,MiniBatchKMeans,not
scikit-learn/examples/cluster/plot_mini_batch_kmeans.py,100,Initialise the different array to all False,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,40,============,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,41,Generate datasets. We choose the size big enough to see the scalability,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,42,"of the algorithms, but not too big to avoid too long running times",not
scikit-learn/examples/cluster/plot_cluster_comparison.py,43,============,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,51,Anisotropicly distributed data,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,58,blobs with varied variances,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,63,============,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,64,Set up cluster parameters,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,65,============,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,95,update parameters with dataset-specific values,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,101,normalize dataset for easier parameter selection,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,104,estimate bandwidth for mean shift,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,107,connectivity matrix for structured Ward,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,110,make connectivity symmetric,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,113,============,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,114,Create cluster objects,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,115,============,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,153,catch warnings related to kneighbors_graph,not
scikit-learn/examples/cluster/plot_cluster_comparison.py,182,add black color for outliers (if any),not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,24,"Authors: Gael Varoquaux, Nelle Varoquaux",not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,25,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,34,Generate sample data,not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,46,Create a graph capturing local connectivity. Larger number of neighbors,not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,47,will give more homogeneous clusters to the cost of computation,not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,48,time. A very large number of neighbors gives more evenly distributed,not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,49,"cluster sizes, but may not impose the local manifold structure of",not
scikit-learn/examples/cluster/plot_agglomerative_clustering.py,50,the data,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,38,,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,39,Generate datasets. We choose the size big enough to see the scalability,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,40,"of the algorithms, but not too big to avoid too long running times",not
scikit-learn/examples/cluster/plot_linkage_comparison.py,49,Anisotropicly distributed data,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,56,blobs with varied variances,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,61,,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,62,Run the clustering and plot,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,64,Set up cluster parameters,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,83,update parameters with dataset-specific values,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,89,normalize dataset for easier parameter selection,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,92,============,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,93,Create cluster objects,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,94,============,not
scikit-learn/examples/cluster/plot_linkage_comparison.py,114,catch warnings related to kneighbors_graph,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,11,"Author : Vincent Michel, 2010",not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,12,"Alexandre Gramfort, 2011",not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,13,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,32,these were introduced in skimage-0.14,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,38,,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,39,Generate data,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,42,Resize it to 20% of the original size to speed up the processing,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,43,Applying a Gaussian filter for smoothing prior to down-scaling,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,44,reduces aliasing artifacts.,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,51,,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,52,Define the structure A of the data. Pixels connected to their neighbors.,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,55,,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,56,Compute clustering,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,59,number of regions,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,68,,not
scikit-learn/examples/cluster/plot_coin_ward_segmentation.py,69,Plot the results on an image,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,35,Author: Gael Varoquaux,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,36,License: BSD 3-Clause or CC-0,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,46,Generate waveform data,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,61,Make the noise sparse,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,76,Plot the ground-truth labelling,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,91,Plot the distances,not
scikit-learn/examples/cluster/plot_agglomerative_clustering_metrics.py,115,Plot clustering results,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,1,"Authors: Mathew Kallada, Andreas Mueller",not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,2,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,20,Create linkage matrix and then plot the dendrogram,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,22,create the counts of samples under each node,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,29,leaf node,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,37,Plot the corresponding dendrogram,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,44,setting distance_threshold=0 ensures we compute the full tree.,not
scikit-learn/examples/cluster/plot_agglomerative_dendrogram.py,49,plot the top three levels of the dendrogram,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,16,Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,17,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,18,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,32,Generate centers for the blobs so that it forms a 10 X 10 grid.,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,39,Generate blobs to do a comparison between MiniBatchKMeans and Birch.,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,42,Use all colors that matplotlib provides by default.,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,48,Compute clustering with Birch with and without the final clustering step,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,49,and plot.,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,61,Plot result,not
scikit-learn/examples/cluster/plot_birch_vs_minibatchkmeans.py,80,Compute clustering with MiniBatchKMeans.,not
scikit-learn/examples/cluster/plot_dbscan.py,1,-*- coding: utf-8 -*-,not
scikit-learn/examples/cluster/plot_dbscan.py,20,,not
scikit-learn/examples/cluster/plot_dbscan.py,21,Generate sample data,not
scikit-learn/examples/cluster/plot_dbscan.py,28,,not
scikit-learn/examples/cluster/plot_dbscan.py,29,Compute DBSCAN,not
scikit-learn/examples/cluster/plot_dbscan.py,35,"Number of clusters in labels, ignoring noise if present.",not
scikit-learn/examples/cluster/plot_dbscan.py,51,,not
scikit-learn/examples/cluster/plot_dbscan.py,52,Plot result,not
scikit-learn/examples/cluster/plot_dbscan.py,55,Black removed and is used for noise instead.,not
scikit-learn/examples/cluster/plot_dbscan.py,61,Black used for noise.,not
scikit-learn/examples/cluster/plot_affinity_propagation.py,17,,not
scikit-learn/examples/cluster/plot_affinity_propagation.py,18,Generate sample data,not
scikit-learn/examples/cluster/plot_affinity_propagation.py,23,,not
scikit-learn/examples/cluster/plot_affinity_propagation.py,24,Compute Affinity Propagation,not
scikit-learn/examples/cluster/plot_affinity_propagation.py,42,,not
scikit-learn/examples/cluster/plot_affinity_propagation.py,43,Plot result,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,29,Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,30,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,31,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,54,,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,55,4 circles,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,58,We use a mask that limits to the foreground: the problem that we are,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,59,"interested in here is not separating the objects from the background,",not
scikit-learn/examples/cluster/plot_segmentation_toy.py,60,but separating them one from the other.,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,66,Convert the image into a graph with the value of the gradient on the,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,67,edges.,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,70,Take a decreasing function of the gradient: we take it weakly,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,71,dependent from the gradient the segmentation is close to a voronoi,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,74,"Force the solver to be arpack, since amg is numerically",not
scikit-learn/examples/cluster/plot_segmentation_toy.py,75,unstable on this example,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,83,,not
scikit-learn/examples/cluster/plot_segmentation_toy.py,84,2 circles,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,21,Authors: Chirag Nagpal,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,22,Christos Aridas,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,67,Generate some training data from clustering,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,74,Train a clustering algorithm on the training data and get the cluster labels,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,85,Generate new samples and plot them along with the original dataset,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,96,Declare the inductive learning model that it will be used to,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,97,predict cluster membership for unknown instances,not
scikit-learn/examples/cluster/plot_inductive_clustering.py,108,Plotting decision regions,not
scikit-learn/examples/cluster/plot_color_quantization.py,1,-*- coding: utf-8 -*-,not
scikit-learn/examples/cluster/plot_color_quantization.py,21,Authors: Robert Layton <robertlayton@gmail.com>,not
scikit-learn/examples/cluster/plot_color_quantization.py,22,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/cluster/plot_color_quantization.py,23,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/examples/cluster/plot_color_quantization.py,24,,not
scikit-learn/examples/cluster/plot_color_quantization.py,25,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_color_quantization.py,38,Load the Summer Palace photo,not
scikit-learn/examples/cluster/plot_color_quantization.py,41,Convert to floats instead of the default 8 bits integer coding. Dividing by,not
scikit-learn/examples/cluster/plot_color_quantization.py,42,255 is important so that plt.imshow behaves works well on float data (need to,not
scikit-learn/examples/cluster/plot_color_quantization.py,43,be in the range [0-1]),not
scikit-learn/examples/cluster/plot_color_quantization.py,46,Load Image and transform to a 2D numpy array.,not
scikit-learn/examples/cluster/plot_color_quantization.py,57,Get labels for all points,not
scikit-learn/examples/cluster/plot_color_quantization.py,84,"Display all results, alongside original image",not
scikit-learn/examples/cluster/plot_kmeans_digits.py,79,"in this case the seeding of the centers is deterministic, hence we run the",not
scikit-learn/examples/cluster/plot_kmeans_digits.py,80,kmeans algorithm only once with n_init=1,not
scikit-learn/examples/cluster/plot_kmeans_digits.py,87,,not
scikit-learn/examples/cluster/plot_kmeans_digits.py,88,Visualize the results on PCA-reduced data,not
scikit-learn/examples/cluster/plot_kmeans_digits.py,94,Step size of the mesh. Decrease to increase the quality of the VQ.,not
scikit-learn/examples/cluster/plot_kmeans_digits.py,95,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/cluster/plot_kmeans_digits.py,97,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/cluster/plot_kmeans_digits.py,102,Obtain labels for each point in mesh. Use last trained model.,not
scikit-learn/examples/cluster/plot_kmeans_digits.py,105,Put the result into a color plot,not
scikit-learn/examples/cluster/plot_kmeans_digits.py,115,Plot the centroids as a white X,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,42,Generating the sample data from make_blobs,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,43,This particular setting has one distinct cluster and 3 clusters placed close,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,44,together.,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,51,For reproducibility,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,56,Create a subplot with 1 row and 2 columns,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,60,The 1st subplot is the silhouette plot,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,61,"The silhouette coefficient can range from -1, 1 but in this example all",not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,62,"lie within [-0.1, 1]",not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,64,The (n_clusters+1)*10 is for inserting blank space between silhouette,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,65,"plots of individual clusters, to demarcate them clearly.",not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,68,Initialize the clusterer with n_clusters value and a random generator,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,69,seed of 10 for reproducibility.,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,73,The silhouette_score gives the average value for all the samples.,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,74,This gives a perspective into the density and separation of the formed,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,75,clusters,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,80,Compute the silhouette scores for each sample,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,85,Aggregate the silhouette scores for samples belonging to,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,86,"cluster i, and sort them",not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,100,Label the silhouette plots with their cluster numbers at the middle,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,103,Compute the new y_lower for next plot,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,104,10 for the 0 samples,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,110,The vertical line for average silhouette score of all the values,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,113,Clear the yaxis labels / ticks,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,116,2nd Plot showing the actual clusters formed,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,121,Labeling the clusters,not
scikit-learn/examples/cluster/plot_kmeans_silhouette_analysis.py,123,Draw white circles at cluster centers,not
scikit-learn/examples/cluster/plot_digits_agglomeration.py,1,!/usr/bin/python,not
scikit-learn/examples/cluster/plot_digits_agglomeration.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/cluster/plot_digits_agglomeration.py,14,Code source: Gaël Varoquaux,not
scikit-learn/examples/cluster/plot_digits_agglomeration.py,15,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/cluster/plot_digits_agglomeration.py,16,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,22,"Authors : Vincent Michel, 2010",not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,23,"Alexandre Gramfort, 2010",not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,24,"Gael Varoquaux, 2010",not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,25,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,36,,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,37,Generate data (swiss roll dataset),not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,41,Make it thinner,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,44,,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,45,Compute clustering,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,54,,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,55,Plot result,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,66,,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,67,Define the structure A of the data. Here a 10 nearest neighbors,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,71,,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,72,Compute clustering,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,82,,not
scikit-learn/examples/cluster/plot_ward_structured_vs_unstructured.py,83,Plot result,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,26,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,27,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,40,Number of run (with randomly generated dataset) for each strategy so as,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,41,to be able to compute an estimate of the standard deviation,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,44,k-means models can do several random inits so as to be able to trade,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,45,CPU time for convergence robustness,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,48,Datasets generation parameters,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,70,Part 1: Quantitative evaluation of various init methods,not
scikit-learn/examples/cluster/plot_kmeans_stability_low_dim_dense.py,102,Part 2: Qualitative visual inspection of the convergence,not
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,25,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,26,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,68,2 independent random clusterings with equal cluster number,not
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,96,Random labeling with varying n_clusters against ground class labels,not
scikit-learn/examples/cluster/plot_adjusted_for_chance_measures.py,97,with fixed number of clusters,not
scikit-learn/examples/cluster/plot_digits_linkage.py,21,Authors: Gael Varoquaux,not
scikit-learn/examples/cluster/plot_digits_linkage.py,22,License: BSD 3 clause (C) INRIA 2014,not
scikit-learn/examples/cluster/plot_digits_linkage.py,39,Having a larger dataset shows more clearly the behavior of the,not
scikit-learn/examples/cluster/plot_digits_linkage.py,40,"methods, but we multiply the size of the dataset only by 2, as the",not
scikit-learn/examples/cluster/plot_digits_linkage.py,41,cost of the hierarchical clustering methods are strongly,not
scikit-learn/examples/cluster/plot_digits_linkage.py,42,super-linear in n_samples,not
scikit-learn/examples/cluster/plot_digits_linkage.py,55,----------------------------------------------------------------------,not
scikit-learn/examples/cluster/plot_digits_linkage.py,56,Visualize the clustering,not
scikit-learn/examples/cluster/plot_digits_linkage.py,74,----------------------------------------------------------------------,not
scikit-learn/examples/cluster/plot_digits_linkage.py,75,2D embedding of the digits dataset,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,16,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,17,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,37,,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,38,Generate data,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,40,image size,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,51,smooth data,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,59,add noise,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,61,,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,62,Compute the coefs of a Bayesian Ridge with GridSearch,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,63,cross-validation generator for model selection,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,68,Ward agglomeration followed by BayesianRidge,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,73,Select the optimal number of parcels with grid search,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,75,set the best parameters,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,80,Anova univariate feature selection followed by BayesianRidge,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,81,caching function,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,84,Select the optimal percentage of features with grid search,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,86,set the best parameters,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,91,,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,92,Inverse the transformation to plot the results on an image,not
scikit-learn/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py,107,"Attempt to remove the temporary cachedir, but don't worry if it fails",not
scikit-learn/examples/cluster/plot_dict_face_patches.py,36,,not
scikit-learn/examples/cluster/plot_dict_face_patches.py,37,Learn the dictionary of images,not
scikit-learn/examples/cluster/plot_dict_face_patches.py,47,The online learning part: cycle over the whole dataset 6 times,not
scikit-learn/examples/cluster/plot_dict_face_patches.py,69,,not
scikit-learn/examples/cluster/plot_dict_face_patches.py,70,Plot the results,not
scikit-learn/examples/cluster/plot_mean_shift.py,19,,not
scikit-learn/examples/cluster/plot_mean_shift.py,20,Generate sample data,not
scikit-learn/examples/cluster/plot_mean_shift.py,24,,not
scikit-learn/examples/cluster/plot_mean_shift.py,25,Compute clustering with MeanShift,not
scikit-learn/examples/cluster/plot_mean_shift.py,27,The following bandwidth can be automatically detected using,not
scikit-learn/examples/cluster/plot_mean_shift.py,40,,not
scikit-learn/examples/cluster/plot_mean_shift.py,41,Plot result,not
scikit-learn/examples/cluster/plot_cluster_iris.py,1,!/usr/bin/python,not
scikit-learn/examples/cluster/plot_cluster_iris.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/cluster/plot_cluster_iris.py,22,Code source: Gaël Varoquaux,not
scikit-learn/examples/cluster/plot_cluster_iris.py,23,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/cluster/plot_cluster_iris.py,24,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_cluster_iris.py,28,"Though the following import is not directly being used, it is required",not
scikit-learn/examples/cluster/plot_cluster_iris.py,29,for 3D projection to work,not
scikit-learn/examples/cluster/plot_cluster_iris.py,67,Plot the ground truth,not
scikit-learn/examples/cluster/plot_cluster_iris.py,79,Reorder the labels to have colors matching the cluster results,not
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,14,Author: Phil Roth <mr.phil.roth@gmail.com>,not
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,15,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,29,Incorrect number of clusters,not
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,36,Anisotropicly distributed data,not
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,45,Different variance,not
scikit-learn/examples/cluster/plot_kmeans_assumptions.py,55,Unevenly sized blobs,not
scikit-learn/examples/cluster/plot_face_compress.py,1,!/usr/bin/python,not
scikit-learn/examples/cluster/plot_face_compress.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/cluster/plot_face_compress.py,17,Code source: Gaël Varoquaux,not
scikit-learn/examples/cluster/plot_face_compress.py,18,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/cluster/plot_face_compress.py,19,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_face_compress.py,28,SciPy >= 0.16 have face in misc,not
scikit-learn/examples/cluster/plot_face_compress.py,37,"We need an (n_sample, n_feature) array",not
scikit-learn/examples/cluster/plot_face_compress.py,43,create an array from labels and values,not
scikit-learn/examples/cluster/plot_face_compress.py,50,original face,not
scikit-learn/examples/cluster/plot_face_compress.py,54,compressed face,not
scikit-learn/examples/cluster/plot_face_compress.py,58,equal bins face,not
scikit-learn/examples/cluster/plot_face_compress.py,61,mean,not
scikit-learn/examples/cluster/plot_face_compress.py,67,histogram,not
scikit-learn/examples/cluster/plot_optics.py,15,Authors: Shane Grigsby <refuge@rocktalus.com>,not
scikit-learn/examples/cluster/plot_optics.py,16,Adrin Jalali <adrin.jalali@gmail.com>,not
scikit-learn/examples/cluster/plot_optics.py,17,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_optics.py,25,Generate sample data,not
scikit-learn/examples/cluster/plot_optics.py,40,Run the fit,not
scikit-learn/examples/cluster/plot_optics.py,61,Reachability plot,not
scikit-learn/examples/cluster/plot_optics.py,73,OPTICS,not
scikit-learn/examples/cluster/plot_optics.py,81,DBSCAN at 0.5,not
scikit-learn/examples/cluster/plot_optics.py,89,DBSCAN at 2.,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,22,"Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung",not
scikit-learn/examples/cluster/plot_coin_segmentation.py,23,License: BSD 3 clause,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,38,these were introduced in skimage-0.14,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,44,load the coins as a numpy array,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,47,Resize it to 20% of the original size to speed up the processing,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,48,Applying a Gaussian filter for smoothing prior to down-scaling,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,49,reduces aliasing artifacts.,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,54,Convert the image into a graph with the value of the gradient on the,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,55,edges.,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,58,Take a decreasing function of the gradient: an exponential,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,59,"The smaller beta is, the more independent the segmentation is of the",not
scikit-learn/examples/cluster/plot_coin_segmentation.py,60,"actual image. For beta=1, the segmentation is close to a voronoi",not
scikit-learn/examples/cluster/plot_coin_segmentation.py,65,Apply spectral clustering (this step goes much faster if you have pyamg,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,66,installed),not
scikit-learn/examples/cluster/plot_coin_segmentation.py,69,,not
scikit-learn/examples/cluster/plot_coin_segmentation.py,70,Visualize the resulting regions,not
scikit-learn/examples/tree/plot_iris_dtc.py,25,Parameters,not
scikit-learn/examples/tree/plot_iris_dtc.py,30,Load data,not
scikit-learn/examples/tree/plot_iris_dtc.py,35,We only take the two corresponding features,not
scikit-learn/examples/tree/plot_iris_dtc.py,39,Train,not
scikit-learn/examples/tree/plot_iris_dtc.py,42,Plot the decision boundary,not
scikit-learn/examples/tree/plot_iris_dtc.py,58,Plot the training points,not
scikit-learn/examples/tree/plot_tree_regression.py,18,Import the necessary modules and libraries,not
scikit-learn/examples/tree/plot_tree_regression.py,23,Create a random dataset,not
scikit-learn/examples/tree/plot_tree_regression.py,29,Fit regression model,not
scikit-learn/examples/tree/plot_tree_regression.py,35,Predict,not
scikit-learn/examples/tree/plot_tree_regression.py,40,Plot the results,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,26,,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,27,Total impurity of leaves vs effective alphas of pruned tree,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,28,---------------------------------------------------------------,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,29,"Minimal cost complexity pruning recursively finds the node with the ""weakest",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,30,"link"". The weakest link is characterized by an effective alpha, where the",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,31,nodes with the smallest effective alpha are pruned first. To get an idea of,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,32,"what values of ``ccp_alpha`` could be appropriate, scikit-learn provides",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,33,:func:`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,34,effective alphas and the corresponding total leaf impurities at each step of,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,35,"the pruning process. As alpha increases, more of the tree is pruned, which",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,36,increases the total impurity of its leaves.,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,44,,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,45,"In the following plot, the maximum effective alpha value is removed, because",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,46,it is the trivial tree with only one node.,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,53,,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,54,"Next, we train a decision tree using the effective alphas. The last value",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,55,"in ``ccp_alphas`` is the alpha value that prunes the whole tree,",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,56,"leaving the tree, ``clfs[-1]``, with one node.",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,65,,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,66,"For the remainder of this example, we remove the last element in",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,67,"``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,68,node. Here we show that the number of nodes and tree depth decreases as alpha,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,69,increases.,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,86,,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,87,Accuracy vs alpha for training and testing sets,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,88,----------------------------------------------------,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,89,When ``ccp_alpha`` is set to zero and keeping the other default parameters,not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,90,"of :class:`DecisionTreeClassifier`, the tree overfits, leading to",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,91,"a 100% training accuracy and 88% testing accuracy. As alpha increases, more",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,92,"of the tree is pruned, thus creating a decision tree that generalizes better.",not
scikit-learn/examples/tree/plot_cost_complexity_pruning.py,93,"In this example, setting ``ccp_alpha=0.015`` maximizes the testing accuracy.",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,32,The decision estimator has an attribute called tree_  which stores the entire,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,33,tree structure and allows access to low level attributes. The binary tree,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,34,tree_ is represented as a number of parallel arrays. The i-th element of each,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,35,array holds information about the node `i`. Node 0 is the tree's root. NOTE:,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,36,"Some of the arrays only apply to either leaves or split nodes, resp. In this",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,37,case the values of nodes of the other type are arbitrary!,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,38,,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,39,"Among those arrays, we have:",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,40,"- left_child, id of the left child of the node",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,41,"- right_child, id of the right child of the node",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,42,"- feature, feature used for splitting the node",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,43,"- threshold, threshold value at the node",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,44,,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,46,"Using those arrays, we can parse the tree structure:",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,55,The tree structure can be traversed to compute various properties such,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,56,as the depth of each node and whether or not it is a leaf.,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,59,seed is the root node id and its parent depth,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,64,If we have a test node,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,89,First let's retrieve the decision path of each sample. The decision_path,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,90,method allows to retrieve the node indicator functions. A non zero element of,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,91,"indicator matrix at the position (i, j) indicates that the sample i goes",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,92,through the node j.,not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,96,"Similarly, we can also have the leaves ids reached by each sample.",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,100,"Now, it's possible to get the tests that were used to predict a sample or",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,101,"a group of samples. First, let's make it for the sample.",not
scikit-learn/examples/tree/plot_unveil_tree_structure.py,125,"For a group of samples, we have the following common node.",not
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,23,Create a random dataset,not
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,29,Fit regression model,not
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,37,Predict,not
scikit-learn/examples/tree/plot_tree_regression_multioutput.py,43,Plot the results,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,1,-*- coding: utf-8 -*-,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,19,Author: Tom Dupré la Tour,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,20,License: BSD 3 clause,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,36,construct the datasets,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,69,transform the dataset with KBinsDiscretizer,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,77,horizontal stripes,not
scikit-learn/examples/preprocessing/plot_discretization_strategies.py,80,vertical stripes,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,36,Author: Eric Chang <ericchang2017@u.northwestern.edu>,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,37,Nicolas Hug <contact@nicolas-hug.com>,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,38,License: BSD 3 clause,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,58,n_quantiles is set to the training set size rather than the default value,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,59,to avoid a warning being raised by this example,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,65,lognormal distribution,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,68,chi-squared distribution,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,72,weibull distribution,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,76,gaussian distribution,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,80,uniform distribution,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,83,bimodal distribution,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,89,create plots,not
scikit-learn/examples/preprocessing/plot_map_data_to_normal.py,114,perform power transforms and quantile transform,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,1,!/usr/bin/env python,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,45,Author:  Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,46,Guillaume Lemaitre <g.lemaitre58@gmail.com>,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,47,Thomas Unterthiner,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,48,License: BSD 3 clause,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,72,Take only 2 features to make visualization easier,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,73,Feature of 0 has a long tail distribution.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,74,Feature 5 has a few but very large outliers.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,102,scale the output between 0 and 1 for the colorbar,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,105,plasma does not exist in matplotlib < 1.5,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,112,define the axis for the first plot,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,126,define the axis for the zoomed-in plot,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,138,define the axis for the colorbar,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,157,The scatter plot,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,161,Removing the top and the right spine for aesthetics,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,162,make nice axis layout,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,170,Histogram for axis X1 (feature 5),not
scikit-learn/examples/preprocessing/plot_all_scaling.py,176,Histogram for axis X0 (feature 0),not
scikit-learn/examples/preprocessing/plot_all_scaling.py,182,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,183,Two plots will be shown for each scaler/normalizer/transformer. The left,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,184,figure will show a scatter plot of the full data set while the right figure,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,185,"will exclude the extreme values considering only 99 % of the data set,",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,186,"excluding marginal outliers. In addition, the marginal distributions for each",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,187,feature will be shown on the side of the scatter plot.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,199,zoom-in,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,219,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,220,.. _results:,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,221,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,222,Original data,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,223,-------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,224,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,225,"Each transformation is plotted showing two transformed features, with the",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,226,"left plot showing the entire dataset, and the right zoomed-in to show the",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,227,dataset without the marginal outliers. A large majority of the samples are,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,228,"compacted to a specific range, [0, 10] for the median income and [0, 6] for",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,229,the number of households. Note that there are some marginal outliers (some,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,230,"blocks have more than 1200 households). Therefore, a specific pre-processing",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,231,"can be very beneficial depending of the application. In the following, we",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,232,present some insights and behaviors of those pre-processing methods in the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,233,presence of marginal outliers.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,237,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,238,StandardScaler,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,239,--------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,240,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,241,``StandardScaler`` removes the mean and scales the data to unit variance.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,242,"However, the outliers have an influence when computing the empirical mean and",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,243,standard deviation which shrink the range of the feature values as shown in,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,244,the left figure below. Note in particular that because the outliers on each,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,245,"feature have different magnitudes, the spread of the transformed data on",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,246,"each feature is very different: most of the data lie in the [-2, 4] range for",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,247,the transformed median income feature while the same data is squeezed in the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,248,"smaller [-0.2, 0.2] range for the transformed number of households.",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,249,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,250,``StandardScaler`` therefore cannot guarantee balanced feature scales in the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,251,presence of outliers.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,255,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,256,MinMaxScaler,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,257,------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,258,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,259,``MinMaxScaler`` rescales the data set such that all feature values are in,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,260,"the range [0, 1] as shown in the right panel below. However, this scaling",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,261,"compress all inliers in the narrow range [0, 0.005] for the transformed",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,262,number of households.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,263,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,264,"As ``StandardScaler``, ``MinMaxScaler`` is very sensitive to the presence of",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,265,outliers.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,269,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,270,MaxAbsScaler,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,271,------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,272,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,273,``MaxAbsScaler`` differs from the previous scaler such that the absolute,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,274,"values are mapped in the range [0, 1]. On positive only data, this scaler",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,275,behaves similarly to ``MinMaxScaler`` and therefore also suffers from the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,276,presence of large outliers.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,280,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,281,RobustScaler,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,282,------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,283,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,284,"Unlike the previous scalers, the centering and scaling statistics of this",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,285,scaler are based on percentiles and are therefore not influenced by a few,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,286,"number of very large marginal outliers. Consequently, the resulting range of",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,287,"the transformed feature values is larger than for the previous scalers and,",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,288,"more importantly, are approximately similar: for both features most of the",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,289,"transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,290,Note that the outliers themselves are still present in the transformed data.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,291,"If a separate outlier clipping is desirable, a non-linear transformation is",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,292,required (see below).,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,296,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,297,PowerTransformer,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,298,----------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,299,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,300,``PowerTransformer`` applies a power transformation to each feature to make,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,301,"the data more Gaussian-like. Currently, ``PowerTransformer`` implements the",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,302,Yeo-Johnson and Box-Cox transforms. The power transform finds the optimal,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,303,scaling factor to stabilize variance and mimimize skewness through maximum,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,304,"likelihood estimation. By default, ``PowerTransformer`` also applies",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,305,"zero-mean, unit variance normalization to the transformed output. Note that",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,306,Box-Cox can only be applied to strictly positive data. Income and number of,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,307,"households happen to be strictly positive, but if negative values are present",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,308,the Yeo-Johnson transformed is to be preferred.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,313,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,314,QuantileTransformer (Gaussian output),not
scikit-learn/examples/preprocessing/plot_all_scaling.py,315,-------------------------------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,316,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,317,``QuantileTransformer`` has an additional ``output_distribution`` parameter,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,318,allowing to match a Gaussian distribution instead of a uniform distribution.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,319,Note that this non-parametetric transformer introduces saturation artifacts,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,320,for extreme values.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,324,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,325,QuantileTransformer (uniform output),not
scikit-learn/examples/preprocessing/plot_all_scaling.py,326,------------------------------------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,327,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,328,``QuantileTransformer`` applies a non-linear transformation such that the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,329,probability density function of each feature will be mapped to a uniform,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,330,"distribution. In this case, all the data will be mapped in the range [0, 1],",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,331,even the outliers which cannot be distinguished anymore from the inliers.,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,332,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,333,"As ``RobustScaler``, ``QuantileTransformer`` is robust to outliers in the",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,334,sense that adding or removing outliers in the training set will yield,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,335,approximately the same transformation on held out data. But contrary to,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,336,"``RobustScaler``, ``QuantileTransformer`` will also automatically collapse",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,337,any outlier by setting them to the a priori defined range boundaries (0 and,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,338,1).,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,342,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,343,Normalizer,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,344,----------,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,345,,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,346,"The ``Normalizer`` rescales the vector for each sample to have unit norm,",not
scikit-learn/examples/preprocessing/plot_all_scaling.py,347,independently of the distribution of the samples. It can be seen on both,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,348,figures below where all samples are mapped onto the unit circle. In our,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,349,example the two selected features have only positive values; therefore the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,350,transformed data only lie in the positive quadrant. This would not be the,not
scikit-learn/examples/preprocessing/plot_all_scaling.py,351,case if some original features had a mix of positive and negative values.,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,1,!/usr/bin/python,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,54,Code source: Tyler Lanigan <tylerlanigan@gmail.com>,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,55,Sebastian Raschka <mail@sebastianraschka.com>,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,57,License: BSD 3 clause,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,65,Make a train/test split using 30% test size,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,70,Fit to data and predict using pipelined GNB and PCA.,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,75,"Fit to data and predict using pipelined scaling, GNB and PCA.",not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,80,Show prediction accuracies in scaled and unscaled data.,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,87,Extract PCA from pipeline,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,91,Show first principal components,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,95,Use PCA without and with scale on X_train data for visualization.,not
scikit-learn/examples/preprocessing/plot_scaling_importance.py,100,visualize standardized vs. untouched dataset with PCA performed,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,1,!/usr/bin/python,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,31,Code source: Tom Dupré la Tour,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,32,Adapted from plot_classifier_comparison by Gaël Varoquaux and Andreas Müller,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,33,,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,34,License: BSD 3 clause,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,53,step size in the mesh,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,64,"list of (estimator, param_grid), where param_grid is used in GridSearchCV",not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,108,iterate over datasets,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,112,"preprocess dataset, split into training and test part",not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,117,create the grid for background colors,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,123,plot the dataset first,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,127,plot the training points,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,130,and testing points,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,138,iterate over classifiers,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,149,"plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,150,"point in the mesh [x_min, x_max]*[y_min, y_max].",not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,156,put the result into a color plot,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,160,plot the training points,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,163,and testing points,not
scikit-learn/examples/preprocessing/plot_discretization_classification.py,180,Add suptitles above the figure,not
scikit-learn/examples/preprocessing/plot_discretization.py,1,-*- coding: utf-8 -*-,not
scikit-learn/examples/preprocessing/plot_discretization.py,32,Author: Andreas Müller,not
scikit-learn/examples/preprocessing/plot_discretization.py,33,Hanmin Qin <qinhanmin2005@sina.com>,not
scikit-learn/examples/preprocessing/plot_discretization.py,34,License: BSD 3 clause,not
scikit-learn/examples/preprocessing/plot_discretization.py,45,construct the dataset,not
scikit-learn/examples/preprocessing/plot_discretization.py,51,transform the dataset with KBinsDiscretizer,not
scikit-learn/examples/preprocessing/plot_discretization.py,55,predict with original dataset,not
scikit-learn/examples/preprocessing/plot_discretization.py,70,predict with transformed dataset,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,1,!/usr/bin/python,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,30,Author: Jaques Grobler <jaques.grobler@inria.fr>,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,31,License: BSD 3 clause,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,45,Next line to silence pyflakes.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,48,Variables for manifold learning.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,52,Create our sphere.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,57,Sever the poles from the sphere.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,64,Plot our dataset.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,75,Perform Locally Linear Embedding Manifold learning,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,94,Perform Isomap Manifold learning.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,108,Perform Multi-dimensional scaling.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,122,Perform Spectral Embedding.,not
scikit-learn/examples/manifold/plot_manifold_sphere.py,137,Perform t-distributed stochastic neighbor embedding.,not
scikit-learn/examples/manifold/plot_mds.py,12,Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,not
scikit-learn/examples/manifold/plot_mds.py,13,License: BSD,not
scikit-learn/examples/manifold/plot_mds.py,30,Center the data,not
scikit-learn/examples/manifold/plot_mds.py,35,Add noise to the similarities,not
scikit-learn/examples/manifold/plot_mds.py,50,Rescale the data,not
scikit-learn/examples/manifold/plot_mds.py,54,Rotate the data,not
scikit-learn/examples/manifold/plot_mds.py,74,Plot the edges,not
scikit-learn/examples/manifold/plot_mds.py,76,"a sequence of (*line0*, *line1*, *line2*), where::",not
scikit-learn/examples/manifold/plot_mds.py,77,"linen = (x0, y0), (x1, y1), ... (xm, ym)",not
scikit-learn/examples/manifold/plot_lle_digits.py,25,Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/examples/manifold/plot_lle_digits.py,26,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/manifold/plot_lle_digits.py,27,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/examples/manifold/plot_lle_digits.py,28,Gael Varoquaux,not
scikit-learn/examples/manifold/plot_lle_digits.py,29,License: BSD 3 clause (C) INRIA 2011,not
scikit-learn/examples/manifold/plot_lle_digits.py,46,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,47,Scale and visualize the embedding vectors,not
scikit-learn/examples/manifold/plot_lle_digits.py,60,only print thumbnails with matplotlib > 1.0,not
scikit-learn/examples/manifold/plot_lle_digits.py,61,just something big,not
scikit-learn/examples/manifold/plot_lle_digits.py,65,don't show points that are too close,not
scikit-learn/examples/manifold/plot_lle_digits.py,77,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,78,Plot images of the digits,not
scikit-learn/examples/manifold/plot_lle_digits.py,93,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,94,Random 2D projection using a random unitary matrix,not
scikit-learn/examples/manifold/plot_lle_digits.py,101,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,102,Projection on to the first 2 principal components,not
scikit-learn/examples/manifold/plot_lle_digits.py,111,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,112,Projection on to the first 2 linear discriminant components,not
scikit-learn/examples/manifold/plot_lle_digits.py,116,Make X invertible,not
scikit-learn/examples/manifold/plot_lle_digits.py,125,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,126,Isomap projection of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,136,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,137,Locally linear embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,149,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,150,Modified Locally linear embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,162,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,163,HLLE embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,175,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,176,LTSA embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,187,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,188,MDS  embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,198,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,199,Random Trees embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,212,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,213,Spectral embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,224,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,225,t-SNE embedding of the digits dataset,not
scikit-learn/examples/manifold/plot_lle_digits.py,235,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_lle_digits.py,236,NCA projection of the digits dataset,not
scikit-learn/examples/manifold/plot_compare_methods.py,22,Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>,not
scikit-learn/examples/manifold/plot_compare_methods.py,36,Next line to silence pyflakes. This import is needed.,not
scikit-learn/examples/manifold/plot_compare_methods.py,44,Create figure,not
scikit-learn/examples/manifold/plot_compare_methods.py,49,Add 3d scatter plot,not
scikit-learn/examples/manifold/plot_compare_methods.py,54,Set-up manifold methods,not
scikit-learn/examples/manifold/plot_compare_methods.py,70,Plot results,not
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,26,Author: Narine Kokhlikyan <narine@slice.com>,not
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,27,License: BSD,not
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,71,Another example using s-curve,not
scikit-learn/examples/manifold/plot_t_sne_perplexity.py,96,Another example using a 2D uniform grid,not
scikit-learn/examples/manifold/plot_swissroll.py,10,Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>,not
scikit-learn/examples/manifold/plot_swissroll.py,11,License: BSD 3 clause (C) INRIA 2011,not
scikit-learn/examples/manifold/plot_swissroll.py,17,This import is needed to modify the way figure behaves,not
scikit-learn/examples/manifold/plot_swissroll.py,21,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_swissroll.py,22,Locally linear embedding of the swiss roll,not
scikit-learn/examples/manifold/plot_swissroll.py,32,----------------------------------------------------------------------,not
scikit-learn/examples/manifold/plot_swissroll.py,33,Plot result,not
scikit-learn/examples/model_selection/plot_learning_curve.py,109,Plot learning curve,not
scikit-learn/examples/model_selection/plot_learning_curve.py,123,Plot n_samples vs fit_times,not
scikit-learn/examples/model_selection/plot_learning_curve.py,132,Plot fit_time vs score,not
scikit-learn/examples/model_selection/plot_learning_curve.py,149,Cross validation with 100 iterations to get smoother mean test and train,not
scikit-learn/examples/model_selection/plot_learning_curve.py,150,"score curves, each time with 20% data randomly selected as a validation set.",not
scikit-learn/examples/model_selection/plot_learning_curve.py,158,SVC is more expensive so we do a lower number of CV iterations:,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,45,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,46,Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,47,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,48,License: BSD 3 clause,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,62,Display progress logs on stdout,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,67,,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,68,Load some categories from the training set,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,73,Uncomment the following to do the analysis on all the categories,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,74,categories = None,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,84,,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,85,Define a pipeline combining a text feature extractor with a simple,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,86,classifier,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,93,uncommenting more parameters will give better exploring power but will,SATD
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,94,increase processing time in a combinatorial way,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,97,"'vect__max_features': (None, 5000, 10000, 50000),",not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,98,unigrams or bigrams,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,99,"'tfidf__use_idf': (True, False),",not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,100,"'tfidf__norm': ('l1', 'l2'),",not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,104,"'clf__max_iter': (10, 50, 80),",not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,108,multiprocessing requires the fork to happen in a __main__ protected,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,109,block,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,111,find the best parameters for both the feature extraction and the,not
scikit-learn/examples/model_selection/grid_search_text_feature_extraction.py,112,classifier,not
scikit-learn/examples/model_selection/plot_cv_indices.py,25,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,26,Visualize our data,not
scikit-learn/examples/model_selection/plot_cv_indices.py,27,------------------,not
scikit-learn/examples/model_selection/plot_cv_indices.py,28,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,29,"First, we must understand the structure of our data. It has 100 randomly",not
scikit-learn/examples/model_selection/plot_cv_indices.py,30,"generated input datapoints, 3 classes split unevenly across datapoints,",not
scikit-learn/examples/model_selection/plot_cv_indices.py,31,"and 10 ""groups"" split evenly across datapoints.",not
scikit-learn/examples/model_selection/plot_cv_indices.py,32,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,33,"As we'll see, some cross-validation objects do specific things with",not
scikit-learn/examples/model_selection/plot_cv_indices.py,34,"labeled data, others behave differently with grouped data, and others",not
scikit-learn/examples/model_selection/plot_cv_indices.py,35,do not use this information.,not
scikit-learn/examples/model_selection/plot_cv_indices.py,36,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,37,"To begin, we'll visualize our data.",not
scikit-learn/examples/model_selection/plot_cv_indices.py,39,Generate the class/group data,not
scikit-learn/examples/model_selection/plot_cv_indices.py,47,Evenly spaced groups repeated once,not
scikit-learn/examples/model_selection/plot_cv_indices.py,52,Visualize dataset groups,not
scikit-learn/examples/model_selection/plot_cv_indices.py,64,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,65,Define a function to visualize cross-validation behavior,not
scikit-learn/examples/model_selection/plot_cv_indices.py,66,--------------------------------------------------------,not
scikit-learn/examples/model_selection/plot_cv_indices.py,67,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,68,We'll define a function that lets us visualize the behavior of each,not
scikit-learn/examples/model_selection/plot_cv_indices.py,69,cross-validation object. We'll perform 4 splits of the data. On each,SATD
scikit-learn/examples/model_selection/plot_cv_indices.py,70,"split, we'll visualize the indices chosen for the training set",SATD
scikit-learn/examples/model_selection/plot_cv_indices.py,71,(in blue) and the test set (in red).,not
scikit-learn/examples/model_selection/plot_cv_indices.py,77,Generate the training/testing visualizations for each CV split,not
scikit-learn/examples/model_selection/plot_cv_indices.py,79,Fill in indices with the training/test groups,not
scikit-learn/examples/model_selection/plot_cv_indices.py,84,Visualize the results,not
scikit-learn/examples/model_selection/plot_cv_indices.py,89,Plot the data classes and groups at the end,not
scikit-learn/examples/model_selection/plot_cv_indices.py,96,Formatting,not
scikit-learn/examples/model_selection/plot_cv_indices.py,105,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,106,Let's see how it looks for the :class:`~sklearn.model_selection.KFold`,not
scikit-learn/examples/model_selection/plot_cv_indices.py,107,cross-validation object:,not
scikit-learn/examples/model_selection/plot_cv_indices.py,113,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,114,"As you can see, by default the KFold cross-validation iterator does not",not
scikit-learn/examples/model_selection/plot_cv_indices.py,115,take either datapoint class or group into consideration. We can change this,not
scikit-learn/examples/model_selection/plot_cv_indices.py,116,by using the ``StratifiedKFold`` like so.,not
scikit-learn/examples/model_selection/plot_cv_indices.py,122,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,123,"In this case, the cross-validation retained the same ratio of classes across",not
scikit-learn/examples/model_selection/plot_cv_indices.py,124,each CV split. Next we'll visualize this behavior for a number of CV,SATD
scikit-learn/examples/model_selection/plot_cv_indices.py,125,iterators.,not
scikit-learn/examples/model_selection/plot_cv_indices.py,126,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,127,Visualize cross-validation indices for many CV objects,not
scikit-learn/examples/model_selection/plot_cv_indices.py,128,------------------------------------------------------,not
scikit-learn/examples/model_selection/plot_cv_indices.py,129,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,130,Let's visually compare the cross validation behavior for many,not
scikit-learn/examples/model_selection/plot_cv_indices.py,131,scikit-learn cross-validation objects. Below we will loop through several,not
scikit-learn/examples/model_selection/plot_cv_indices.py,132,"common cross-validation objects, visualizing the behavior of each.",not
scikit-learn/examples/model_selection/plot_cv_indices.py,133,,not
scikit-learn/examples/model_selection/plot_cv_indices.py,134,Note how some use the group/class information while others do not.,not
scikit-learn/examples/model_selection/plot_cv_indices.py,147,Make the legend fit,not
scikit-learn/examples/model_selection/plot_roc.py,49,Import some data to play with,not
scikit-learn/examples/model_selection/plot_roc.py,54,Binarize the output,not
scikit-learn/examples/model_selection/plot_roc.py,58,Add noisy features to make the problem harder,not
scikit-learn/examples/model_selection/plot_roc.py,63,shuffle and split training and test sets,not
scikit-learn/examples/model_selection/plot_roc.py,67,Learn to predict each class against the other,not
scikit-learn/examples/model_selection/plot_roc.py,72,Compute ROC curve and ROC area for each class,not
scikit-learn/examples/model_selection/plot_roc.py,80,Compute micro-average ROC curve and ROC area,not
scikit-learn/examples/model_selection/plot_roc.py,85,,not
scikit-learn/examples/model_selection/plot_roc.py,86,Plot of a ROC curve for a specific class,not
scikit-learn/examples/model_selection/plot_roc.py,101,,not
scikit-learn/examples/model_selection/plot_roc.py,102,Plot ROC curves for the multilabel problem,not
scikit-learn/examples/model_selection/plot_roc.py,103,..........................................,not
scikit-learn/examples/model_selection/plot_roc.py,104,Compute macro-average ROC curve and ROC area,not
scikit-learn/examples/model_selection/plot_roc.py,106,First aggregate all false positive rates,not
scikit-learn/examples/model_selection/plot_roc.py,109,Then interpolate all ROC curves at this points,not
scikit-learn/examples/model_selection/plot_roc.py,114,Finally average it and compute AUC,not
scikit-learn/examples/model_selection/plot_roc.py,121,Plot all ROC curves,not
scikit-learn/examples/model_selection/plot_roc.py,149,,not
scikit-learn/examples/model_selection/plot_roc.py,150,Area under ROC for the multiclass problem,not
scikit-learn/examples/model_selection/plot_roc.py,151,.........................................,not
scikit-learn/examples/model_selection/plot_roc.py,152,The :func:`sklearn.metrics.roc_auc_score` function can be used for,not
scikit-learn/examples/model_selection/plot_roc.py,153,multi-class classification. The multi-class One-vs-One scheme compares every,not
scikit-learn/examples/model_selection/plot_roc.py,154,"unique pairwise combination of classes. In this section, we calculate the AUC",not
scikit-learn/examples/model_selection/plot_roc.py,155,"using the OvR and OvO schemes. We report a macro average, and a",not
scikit-learn/examples/model_selection/plot_roc.py,156,prevalence-weighted average.,not
scikit-learn/examples/model_selection/plot_grid_search_refit_callable.py,19,Author: Wenhao Zhang <wenhaoz@ucla.edu>,not
scikit-learn/examples/model_selection/plot_cv_predict.py,19,cross_val_predict returns an array of the same size as `y` where each entry,not
scikit-learn/examples/model_selection/plot_cv_predict.py,20,is a prediction obtained by cross validation:,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,42,,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,43,Data IO and generation,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,45,Import some data to play with,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,52,Add noisy features,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,56,,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,57,Classification and ROC analysis,not
scikit-learn/examples/model_selection/plot_roc_crossval.py,59,Run classifier with cross-validation and plot ROC curves,not
scikit-learn/examples/model_selection/plot_grid_search_digits.py,26,Loading the Digits dataset,not
scikit-learn/examples/model_selection/plot_grid_search_digits.py,29,"To apply an classifier on this data, we need to flatten the image, to",not
scikit-learn/examples/model_selection/plot_grid_search_digits.py,30,"turn the data in a (samples, feature) matrix:",not
scikit-learn/examples/model_selection/plot_grid_search_digits.py,35,Split the dataset in two equal parts,SATD
scikit-learn/examples/model_selection/plot_grid_search_digits.py,39,Set the parameters by cross-validation,not
scikit-learn/examples/model_selection/plot_grid_search_digits.py,77,Note the problem is too easy: the hyperparameter plateau is too flat and the,not
scikit-learn/examples/model_selection/plot_grid_search_digits.py,78,output model is the same for precision and recall with ties in quality.,not
scikit-learn/examples/model_selection/plot_underfitting_overfitting.py,56,Evaluate the models using crossvalidation,not
scikit-learn/examples/model_selection/plot_randomized_search.py,33,get some data,not
scikit-learn/examples/model_selection/plot_randomized_search.py,36,build a classifier,not
scikit-learn/examples/model_selection/plot_randomized_search.py,41,Utility function to report best scores,SATD
scikit-learn/examples/model_selection/plot_randomized_search.py,54,specify parameters and distributions to sample from,not
scikit-learn/examples/model_selection/plot_randomized_search.py,59,run randomized search,not
scikit-learn/examples/model_selection/plot_randomized_search.py,70,use a full grid over all parameters,not
scikit-learn/examples/model_selection/plot_randomized_search.py,75,run grid search,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,18,Author: Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,19,License: BSD,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,32,,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,33,Running ``GridSearchCV`` using multiple evaluation metrics,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,34,----------------------------------------------------------,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,35,,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,39,The scorers can be either be one of the predefined metric strings or a scorer,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,40,"callable, like the one returned by make_scorer",not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,43,"Setting refit='AUC', refits an estimator on the whole dataset with the",not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,44,parameter setting that has the best cross-validated AUC score.,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,45,That estimator is made available at ``gs.best_estimator_`` along with,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,46,"parameters like ``gs.best_score_``, ``gs.best_params_`` and",not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,47,``gs.best_index_``,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,54,,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,55,Plotting the result,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,56,-------------------,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,69,Get the regular numpy array from the MaskedArray,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,86,Plot a dotted vertical line at the best score for that scorer marked by x,not
scikit-learn/examples/model_selection/plot_multi_metric_evaluation.py,90,Annotate the best score for that scorer,not
scikit-learn/examples/model_selection/plot_precision_recall.py,93,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,94,In binary classification settings,not
scikit-learn/examples/model_selection/plot_precision_recall.py,95,--------------------------------------------------------,not
scikit-learn/examples/model_selection/plot_precision_recall.py,96,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,97,Create simple data,not
scikit-learn/examples/model_selection/plot_precision_recall.py,98,..................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,99,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,100,Try to differentiate the two first classes of the iris data,not
scikit-learn/examples/model_selection/plot_precision_recall.py,109,Add noisy features,not
scikit-learn/examples/model_selection/plot_precision_recall.py,114,"Limit to the two first classes, and split into training and test",not
scikit-learn/examples/model_selection/plot_precision_recall.py,119,Create a simple classifier,not
scikit-learn/examples/model_selection/plot_precision_recall.py,124,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,125,Compute the average precision score,not
scikit-learn/examples/model_selection/plot_precision_recall.py,126,...................................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,133,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,134,Plot the Precision-Recall curve,not
scikit-learn/examples/model_selection/plot_precision_recall.py,135,................................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,144,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,145,In multi-label settings,not
scikit-learn/examples/model_selection/plot_precision_recall.py,146,------------------------,not
scikit-learn/examples/model_selection/plot_precision_recall.py,147,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,148,"Create multi-label data, fit, and predict",not
scikit-learn/examples/model_selection/plot_precision_recall.py,149,...........................................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,150,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,151,"We create a multi-label dataset, to illustrate the precision-recall in",not
scikit-learn/examples/model_selection/plot_precision_recall.py,152,multi-label settings,not
scikit-learn/examples/model_selection/plot_precision_recall.py,156,Use label_binarize to be multi-label like settings,not
scikit-learn/examples/model_selection/plot_precision_recall.py,160,Split into training and test,not
scikit-learn/examples/model_selection/plot_precision_recall.py,164,We use OneVsRestClassifier for multi-label prediction,not
scikit-learn/examples/model_selection/plot_precision_recall.py,167,Run classifier,not
scikit-learn/examples/model_selection/plot_precision_recall.py,173,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,174,The average precision score in multi-label settings,not
scikit-learn/examples/model_selection/plot_precision_recall.py,175,....................................................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,179,For each class,not
scikit-learn/examples/model_selection/plot_precision_recall.py,188,"A ""micro-average"": quantifying score on all classes jointly",not
scikit-learn/examples/model_selection/plot_precision_recall.py,196,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,197,Plot the micro-averaged Precision-Recall curve,not
scikit-learn/examples/model_selection/plot_precision_recall.py,198,...............................................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,199,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,212,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,213,Plot Precision-Recall curve for each class and iso-f1 curves,not
scikit-learn/examples/model_selection/plot_precision_recall.py,214,.............................................................,not
scikit-learn/examples/model_selection/plot_precision_recall.py,215,,not
scikit-learn/examples/model_selection/plot_precision_recall.py,217,setup plot details,not
scikit-learn/examples/model_selection/plot_confusion_matrix.py,36,import some data to play with,not
scikit-learn/examples/model_selection/plot_confusion_matrix.py,42,Split the data into a training set and a test set,not
scikit-learn/examples/model_selection/plot_confusion_matrix.py,45,"Run classifier, using a model that is too regularized (C too low) to see",not
scikit-learn/examples/model_selection/plot_confusion_matrix.py,46,the impact on the results,not
scikit-learn/examples/model_selection/plot_confusion_matrix.py,51,Plot non-normalized confusion matrix,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,54,Number of random trials,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,57,Load the dataset,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,62,Set up possible values of parameters to optimize over,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,66,"We will use a Support Vector Classifier with ""rbf"" kernel",not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,69,Arrays to store scores,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,73,Loop for each trial,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,76,"Choose cross-validation techniques for the inner and outer loops,",not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,77,independently of the dataset.,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,78,"E.g ""GroupKFold"", ""LeaveOneOut"", ""LeaveOneGroupOut"", etc.",not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,82,Non_nested parameter search and scoring,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,87,Nested CV with parameter optimization,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,96,Plot scores on each trial for nested and non-nested CV,not
scikit-learn/examples/model_selection/plot_nested_cross_validation_iris.py,108,Plot bar chart of the difference.,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,16,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,17,License: BSD 3 clause,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,22,,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,23,Generate sample data,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,27,only the top 10 features are impacting the model,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,31,Split train and test data,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,35,,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,36,Compute train and test errors,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,51,Estimate the coef_ on full data with optimal regularization parameter,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,55,,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,56,Plot results functions,not
scikit-learn/examples/model_selection/plot_train_error_vs_test_error.py,69,Show estimated coef_ vs true coef,not
scikit-learn/examples/svm/plot_custom_kernel.py,16,import some data to play with,not
scikit-learn/examples/svm/plot_custom_kernel.py,18,we only take the first two features. We could,not
scikit-learn/examples/svm/plot_custom_kernel.py,19,avoid this ugly slicing by using a two-dim dataset,SATD
scikit-learn/examples/svm/plot_custom_kernel.py,35,step size in the mesh,not
scikit-learn/examples/svm/plot_custom_kernel.py,37,we create an instance of SVM and fit out data.,not
scikit-learn/examples/svm/plot_custom_kernel.py,41,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/svm/plot_custom_kernel.py,42,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/svm/plot_custom_kernel.py,48,Put the result into a color plot,not
scikit-learn/examples/svm/plot_custom_kernel.py,52,Plot also the training points,not
scikit-learn/examples/svm/plot_svm_anova.py,23,,not
scikit-learn/examples/svm/plot_svm_anova.py,24,Import some data to play with,not
scikit-learn/examples/svm/plot_svm_anova.py,26,Add non-informative features,not
scikit-learn/examples/svm/plot_svm_anova.py,30,,not
scikit-learn/examples/svm/plot_svm_anova.py,31,"Create a feature-selection transform, a scaler and an instance of SVM that we",not
scikit-learn/examples/svm/plot_svm_anova.py,32,combine together to have an full-blown estimator,not
scikit-learn/examples/svm/plot_svm_anova.py,37,,not
scikit-learn/examples/svm/plot_svm_anova.py,38,Plot the cross-validation score as a function of percentile of features,not
scikit-learn/examples/svm/plot_separating_hyperplane.py,18,we create 40 separable points,not
scikit-learn/examples/svm/plot_separating_hyperplane.py,21,"fit the model, don't regularize for illustration purposes",not
scikit-learn/examples/svm/plot_separating_hyperplane.py,27,plot the decision function,not
scikit-learn/examples/svm/plot_separating_hyperplane.py,32,create grid to evaluate model,not
scikit-learn/examples/svm/plot_separating_hyperplane.py,39,plot decision boundary and margins,not
scikit-learn/examples/svm/plot_separating_hyperplane.py,42,plot support vectors,not
scikit-learn/examples/svm/plot_rbf_parameters.py,85,Utility function to move the midpoint of a colormap to be around,not
scikit-learn/examples/svm/plot_rbf_parameters.py,86,the values of interest.,not
scikit-learn/examples/svm/plot_rbf_parameters.py,98,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,99,Load and prepare data set,not
scikit-learn/examples/svm/plot_rbf_parameters.py,100,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,101,dataset for grid search,not
scikit-learn/examples/svm/plot_rbf_parameters.py,107,Dataset for decision function visualization: we only keep the first two,not
scikit-learn/examples/svm/plot_rbf_parameters.py,108,features in X and sub-sample the dataset to keep only 2 classes and,not
scikit-learn/examples/svm/plot_rbf_parameters.py,109,make it a binary classification problem.,not
scikit-learn/examples/svm/plot_rbf_parameters.py,116,It is usually a good idea to scale the data for SVM training.,not
scikit-learn/examples/svm/plot_rbf_parameters.py,117,"We are cheating a bit in this example in scaling all of the data,",not
scikit-learn/examples/svm/plot_rbf_parameters.py,118,instead of fitting the transformation on the training set and,not
scikit-learn/examples/svm/plot_rbf_parameters.py,119,just applying it on the test set.,not
scikit-learn/examples/svm/plot_rbf_parameters.py,125,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,126,Train classifiers,not
scikit-learn/examples/svm/plot_rbf_parameters.py,127,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,128,"For an initial search, a logarithmic grid with basis",not
scikit-learn/examples/svm/plot_rbf_parameters.py,129,"10 is often helpful. Using a basis of 2, a finer",not
scikit-learn/examples/svm/plot_rbf_parameters.py,130,tuning can be achieved but at a much higher cost.,not
scikit-learn/examples/svm/plot_rbf_parameters.py,142,Now we need to fit a classifier for all parameters in the 2d version,not
scikit-learn/examples/svm/plot_rbf_parameters.py,143,(we use a smaller set of parameters here because it takes a while to train),not
scikit-learn/examples/svm/plot_rbf_parameters.py,154,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,155,Visualization,not
scikit-learn/examples/svm/plot_rbf_parameters.py,156,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,157,draw visualization of parameter effects,not
scikit-learn/examples/svm/plot_rbf_parameters.py,162,evaluate decision function in a grid,not
scikit-learn/examples/svm/plot_rbf_parameters.py,166,visualize decision function for these parameters,not
scikit-learn/examples/svm/plot_rbf_parameters.py,171,visualize parameter's effect on decision function,not
scikit-learn/examples/svm/plot_rbf_parameters.py,182,Draw heatmap of the validation accuracy as a function of gamma and C,not
scikit-learn/examples/svm/plot_rbf_parameters.py,183,,not
scikit-learn/examples/svm/plot_rbf_parameters.py,184,The score are encoded as colors with the hot colormap which varies from dark,not
scikit-learn/examples/svm/plot_rbf_parameters.py,185,red to bright yellow. As the most interesting scores are all located in the,not
scikit-learn/examples/svm/plot_rbf_parameters.py,186,0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so,not
scikit-learn/examples/svm/plot_rbf_parameters.py,187,as to make it easier to visualize the small variations of score values in the,not
scikit-learn/examples/svm/plot_rbf_parameters.py,188,interesting range while not brutally collapsing all the low score values to,not
scikit-learn/examples/svm/plot_rbf_parameters.py,189,the same color.,not
scikit-learn/examples/svm/plot_svm_scale_c.py,82,Author: Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/examples/svm/plot_svm_scale_c.py,83,Jaques Grobler <jaques.grobler@inria.fr>,not
scikit-learn/examples/svm/plot_svm_scale_c.py,84,License: BSD 3 clause,not
scikit-learn/examples/svm/plot_svm_scale_c.py,98,set up dataset,not
scikit-learn/examples/svm/plot_svm_scale_c.py,102,l1 data (only 5 informative features),not
scikit-learn/examples/svm/plot_svm_scale_c.py,107,"l2 data: non sparse, but less features",not
scikit-learn/examples/svm/plot_svm_scale_c.py,122,set up the plot for each regressor,not
scikit-learn/examples/svm/plot_svm_scale_c.py,127,"To get nice curve, we need a large number of iterations to",SATD
scikit-learn/examples/svm/plot_svm_scale_c.py,128,reduce the variance,not
scikit-learn/examples/svm/plot_svm_scale_c.py,143,scale the C's,not
scikit-learn/examples/svm/plot_svm_nonlinear.py,24,fit the model,not
scikit-learn/examples/svm/plot_svm_nonlinear.py,28,plot the decision function for each datapoint on the grid,not
scikit-learn/examples/svm/plot_iris_svc.py,80,import some data to play with,not
scikit-learn/examples/svm/plot_iris_svc.py,82,Take the first two features. We could avoid this by using a two-dim dataset,not
scikit-learn/examples/svm/plot_iris_svc.py,86,we create an instance of SVM and fit out data. We do not scale our,not
scikit-learn/examples/svm/plot_iris_svc.py,87,data since we want to plot the support vectors,not
scikit-learn/examples/svm/plot_iris_svc.py,88,SVM regularization parameter,not
scikit-learn/examples/svm/plot_iris_svc.py,95,title for the plots,not
scikit-learn/examples/svm/plot_iris_svc.py,101,Set-up 2x2 grid for plotting.,not
scikit-learn/examples/svm/plot_svm_kernels.py,1,!/usr/bin/python,not
scikit-learn/examples/svm/plot_svm_kernels.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/svm/plot_svm_kernels.py,18,Code source: Gaël Varoquaux,not
scikit-learn/examples/svm/plot_svm_kernels.py,19,License: BSD 3 clause,not
scikit-learn/examples/svm/plot_svm_kernels.py,26,Our dataset and targets,not
scikit-learn/examples/svm/plot_svm_kernels.py,36,--,not
scikit-learn/examples/svm/plot_svm_kernels.py,46,figure number,not
scikit-learn/examples/svm/plot_svm_kernels.py,49,fit the model,not
scikit-learn/examples/svm/plot_svm_kernels.py,54,"plot the line, the points, and the nearest vectors to the plane",not
scikit-learn/examples/svm/plot_svm_kernels.py,72,Put the result into a color plot,not
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,21,"""hinge"" is the standard SVM loss",not
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,23,obtain the support vectors through the decision function,not
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,25,we can also calculate the decision function manually,not
scikit-learn/examples/svm/plot_linearsvc_support_vectors.py,26,"decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]",not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,34,we create two clusters of random points,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,44,fit the model and get the separating hyperplane,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,48,fit the model and get the separating hyperplane using weighted classes,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,52,plot the samples,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,55,plot the decision functions for both classifiers,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,60,create grid to evaluate model,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,66,get the separating hyperplane,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,69,plot decision boundary and margins,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,72,get the separating hyperplane for weighted classes,not
scikit-learn/examples/svm/plot_separating_hyperplane_unbalanced.py,75,plot decision boundary and margins for weighted classes,not
scikit-learn/examples/svm/plot_svm_regression.py,15,,not
scikit-learn/examples/svm/plot_svm_regression.py,16,Generate sample data,not
scikit-learn/examples/svm/plot_svm_regression.py,20,,not
scikit-learn/examples/svm/plot_svm_regression.py,21,Add noise to targets,not
scikit-learn/examples/svm/plot_svm_regression.py,24,,not
scikit-learn/examples/svm/plot_svm_regression.py,25,Fit regression model,not
scikit-learn/examples/svm/plot_svm_regression.py,31,,not
scikit-learn/examples/svm/plot_svm_regression.py,32,Look at the results,not
scikit-learn/examples/svm/plot_svm_tie_breaking.py,18,"Code source: Andreas Mueller, Adrin Jalali",not
scikit-learn/examples/svm/plot_svm_tie_breaking.py,19,License: BSD 3 clause,not
scikit-learn/examples/svm/plot_svm_margin.py,1,!/usr/bin/python,not
scikit-learn/examples/svm/plot_svm_margin.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/svm/plot_svm_margin.py,21,Code source: Gaël Varoquaux,not
scikit-learn/examples/svm/plot_svm_margin.py,22,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/svm/plot_svm_margin.py,23,License: BSD 3 clause,not
scikit-learn/examples/svm/plot_svm_margin.py,29,we create 40 separable points,not
scikit-learn/examples/svm/plot_svm_margin.py,34,figure number,not
scikit-learn/examples/svm/plot_svm_margin.py,37,fit the model,not
scikit-learn/examples/svm/plot_svm_margin.py,43,get the separating hyperplane,not
scikit-learn/examples/svm/plot_svm_margin.py,49,plot the parallels to the separating hyperplane that pass through the,not
scikit-learn/examples/svm/plot_svm_margin.py,50,support vectors (margin away from hyperplane in direction,not
scikit-learn/examples/svm/plot_svm_margin.py,51,perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in,not
scikit-learn/examples/svm/plot_svm_margin.py,52,2-d.,not
scikit-learn/examples/svm/plot_svm_margin.py,57,"plot the line, the points, and the nearest vectors to the plane",not
scikit-learn/examples/svm/plot_svm_margin.py,78,Put the result into a color plot,not
scikit-learn/examples/svm/plot_oneclass.py,20,Generate train data,not
scikit-learn/examples/svm/plot_oneclass.py,23,Generate some regular novel observations,not
scikit-learn/examples/svm/plot_oneclass.py,26,Generate some abnormal novel observations,not
scikit-learn/examples/svm/plot_oneclass.py,29,fit the model,not
scikit-learn/examples/svm/plot_oneclass.py,39,"plot the line, the points, and the nearest vectors to the plane",not
scikit-learn/examples/svm/plot_weighted_samples.py,23,plot the decision function,not
scikit-learn/examples/svm/plot_weighted_samples.py,29,"plot the line, the points, and the nearest vectors to the plane",not
scikit-learn/examples/svm/plot_weighted_samples.py,38,we create 20 points,not
scikit-learn/examples/svm/plot_weighted_samples.py,44,and bigger weights to some outliers,not
scikit-learn/examples/svm/plot_weighted_samples.py,48,"for reference, first fit without sample weights",not
scikit-learn/examples/svm/plot_weighted_samples.py,50,fit the model,not
scikit-learn/examples/mixture/plot_gmm_selection.py,29,Number of samples per component,not
scikit-learn/examples/mixture/plot_gmm_selection.py,32,"Generate random sample, two components",not
scikit-learn/examples/mixture/plot_gmm_selection.py,44,Fit a Gaussian mixture with EM,not
scikit-learn/examples/mixture/plot_gmm_selection.py,59,Plot the BIC scores,not
scikit-learn/examples/mixture/plot_gmm_selection.py,76,Plot the winner,not
scikit-learn/examples/mixture/plot_gmm_selection.py,86,Plot an ellipse to show the Gaussian component,not
scikit-learn/examples/mixture/plot_gmm_selection.py,88,convert to degrees,not
scikit-learn/examples/mixture/plot_gmm_pdf.py,18,"generate random sample, two components",not
scikit-learn/examples/mixture/plot_gmm_pdf.py,21,"generate spherical data centered on (20, 20)",not
scikit-learn/examples/mixture/plot_gmm_pdf.py,24,generate zero centered stretched Gaussian data,not
scikit-learn/examples/mixture/plot_gmm_pdf.py,28,concatenate the two datasets into the final training set,not
scikit-learn/examples/mixture/plot_gmm_pdf.py,31,fit a Gaussian Mixture Model with two components,not
scikit-learn/examples/mixture/plot_gmm_pdf.py,35,display predicted scores by the model as a contour plot,not
scikit-learn/examples/mixture/plot_concentration_prior.py,30,Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/examples/mixture/plot_concentration_prior.py,31,License: BSD 3 clause,not
scikit-learn/examples/mixture/plot_concentration_prior.py,48,Ellipse needs degrees,not
scikit-learn/examples/mixture/plot_concentration_prior.py,50,eigenvector normalization,not
scikit-learn/examples/mixture/plot_concentration_prior.py,87,Parameters of the dataset,not
scikit-learn/examples/mixture/plot_concentration_prior.py,99,mean_precision_prior= 0.8 to minimize the influence of the prior,not
scikit-learn/examples/mixture/plot_concentration_prior.py,114,Generate data,not
scikit-learn/examples/mixture/plot_concentration_prior.py,122,Plot results in two different figures,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,29,"Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux",not
scikit-learn/examples/mixture/plot_gmm_covariances.py,30,Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,31,License: BSD 3 clause,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,60,convert to degrees,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,71,Break up the dataset into non-overlapping training (75%) and testing,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,72,(25%) sets.,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,74,Only take the first fold.,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,85,Try GMMs using different types of covariances.,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,98,"Since we have class labels for the training data, we can",not
scikit-learn/examples/mixture/plot_gmm_covariances.py,99,initialize the GMM parameters in a supervised manner.,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,103,Train the other parameters using the EM algorithm.,not
scikit-learn/examples/mixture/plot_gmm_covariances.py,113,Plot the test data with crosses,not
scikit-learn/examples/mixture/plot_gmm_sin.py,64,as the DP will not use every component it has access to,not
scikit-learn/examples/mixture/plot_gmm_sin.py,65,"unless it needs it, we shouldn't plot the redundant",not
scikit-learn/examples/mixture/plot_gmm_sin.py,66,components.,not
scikit-learn/examples/mixture/plot_gmm_sin.py,71,Plot an ellipse to show the Gaussian component,not
scikit-learn/examples/mixture/plot_gmm_sin.py,73,convert to degrees,not
scikit-learn/examples/mixture/plot_gmm_sin.py,89,as the DP will not use every component it has access to,not
scikit-learn/examples/mixture/plot_gmm_sin.py,90,"unless it needs it, we shouldn't plot the redundant",not
scikit-learn/examples/mixture/plot_gmm_sin.py,91,components.,not
scikit-learn/examples/mixture/plot_gmm_sin.py,103,Parameters,not
scikit-learn/examples/mixture/plot_gmm_sin.py,106,Generate random sample following a sine curve,not
scikit-learn/examples/mixture/plot_gmm_sin.py,120,Fit a Gaussian mixture with EM using ten components,not
scikit-learn/examples/mixture/plot_gmm.py,46,as the DP will not use every component it has access to,not
scikit-learn/examples/mixture/plot_gmm.py,47,"unless it needs it, we shouldn't plot the redundant",not
scikit-learn/examples/mixture/plot_gmm.py,48,components.,not
scikit-learn/examples/mixture/plot_gmm.py,53,Plot an ellipse to show the Gaussian component,not
scikit-learn/examples/mixture/plot_gmm.py,55,convert to degrees,not
scikit-learn/examples/mixture/plot_gmm.py,68,Number of samples per component,not
scikit-learn/examples/mixture/plot_gmm.py,71,"Generate random sample, two components",not
scikit-learn/examples/mixture/plot_gmm.py,77,Fit a Gaussian mixture with EM using five components,not
scikit-learn/examples/mixture/plot_gmm.py,82,Fit a Dirichlet process Gaussian mixture using five components,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,23,Authors: Clay Woolam <clay@woolam.org>,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,24,License: BSD,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,76,compute the entropies of transduced label distributions,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,80,select up to 5 digit examples that the classifier is most uncertain about,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,85,keep track of indices that we get labels for,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,88,"for more than 5 iterations, visualize the gain only on the first 5",not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,96,"for more than 5 iterations, visualize the gain only on the first 5",not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits_active_learning.py,104,"labeling 5 points, remote from labeled set",not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,15,Authors: Clay Woolam <clay@woolam.org>,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,16,License: BSD,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,31,step size in the mesh,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,38,we create an instance of SVM and fit out data. We do not scale our,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,39,data since we want to plot the support vectors,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,45,create a mesh to plot in,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,51,title for the plots,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,60,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,61,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,65,Put the result into a color plot,not
scikit-learn/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,70,Plot also the training points,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,19,Authors: Clay Woolam <clay@woolam.org>,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,20,License: BSD,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,48,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,49,Shuffle everything around,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,53,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,54,Learn with LabelSpreading,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,70,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,71,Calculate uncertainty values for each transduced distribution,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,74,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,75,Pick the top 10 most uncertain labels,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,78,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_digits.py,79,Plot,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,14,Authors: Clay Woolam <clay@woolam.org>,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,15,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,16,License: BSD,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,23,generate ring with inner box,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,31,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,32,Learn with LabelSpreading,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,36,,not
scikit-learn/examples/semi_supervised/plot_label_propagation_structure.py,37,Plot output labels,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,35,Author: Adam Kleczewski,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,36,License: BSD 3 clause,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,49,Load a multi-label dataset from https://www.openml.org/d/40597,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,55,Fit an independent logistic regression model for each class using the,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,56,OneVsRestClassifier wrapper.,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,63,Fit an ensemble of logistic regression classifier chains and take the,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,64,take the average prediction of all the chains.,not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,99,"Plot the Jaccard similarity scores for the independent model, each of the",not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,100,"chains, and the ensemble (note that the vertical axis on this plot does",not
scikit-learn/examples/multioutput/plot_classifier_chain_yeast.py,101,not begin at 0).,not
scikit-learn/examples/linear_model/plot_omp.py,20,generate the data,not
scikit-learn/examples/linear_model/plot_omp.py,22,y = Xw,not
scikit-learn/examples/linear_model/plot_omp.py,23,|x|_0 = n_nonzero_coefs,not
scikit-learn/examples/linear_model/plot_omp.py,33,distort the clean signal,not
scikit-learn/examples/linear_model/plot_omp.py,36,plot the sparse signal,not
scikit-learn/examples/linear_model/plot_omp.py,43,plot the noise-free reconstruction,not
scikit-learn/examples/linear_model/plot_omp.py,53,plot the noisy reconstruction,not
scikit-learn/examples/linear_model/plot_omp.py,62,plot the noisy reconstruction with number of non-zeros set by CV,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,29,Author: Yoshihiro Uchida <nimbus1after2a1sun7shower@gmail.com>,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,40,,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,41,Generate sinusoidal data with noise,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,49,,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,50,Fit by cubic polynomial,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,55,,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,56,Plot the true and predicted curves with log marginal likelihood (L),not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,60,Bayesian ridge regression with different initial value pairs,not
scikit-learn/examples/linear_model/plot_bayesian_ridge_curvefit.py,62,Default values,not
scikit-learn/examples/linear_model/plot_robust_fit.py,46,Make sure that it X is 2D,not
scikit-learn/examples/linear_model/plot_theilsen.py,37,Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>,not
scikit-learn/examples/linear_model/plot_theilsen.py,38,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_theilsen.py,54,,not
scikit-learn/examples/linear_model/plot_theilsen.py,55,Outliers only in the y direction,not
scikit-learn/examples/linear_model/plot_theilsen.py,59,"Linear model y = 3*x + N(2, 0.1**2)",not
scikit-learn/examples/linear_model/plot_theilsen.py,65,10% outliers,not
scikit-learn/examples/linear_model/plot_theilsen.py,83,,not
scikit-learn/examples/linear_model/plot_theilsen.py,84,Outliers in the X direction,not
scikit-learn/examples/linear_model/plot_theilsen.py,87,"Linear model y = 3*x + N(2, 0.1**2)",not
scikit-learn/examples/linear_model/plot_theilsen.py,91,10% outliers,not
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,15,Authors: Manoj Kumar mks542@nyu.edu,not
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,16,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,26,Generate toy data.,not
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,31,Add four strong outliers to the dataset.,not
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,42,Fit the huber regressor over a series of epsilon values.,not
scikit-learn/examples/linear_model/plot_huber_vs_ridge.py,53,Fit a ridge regressor to compare it to huber regressor.,not
scikit-learn/examples/linear_model/plot_ridge_path.py,29,Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>,not
scikit-learn/examples/linear_model/plot_ridge_path.py,30,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_ridge_path.py,38,X is the 10x10 Hilbert matrix,not
scikit-learn/examples/linear_model/plot_ridge_path.py,42,,not
scikit-learn/examples/linear_model/plot_ridge_path.py,43,Compute paths,not
scikit-learn/examples/linear_model/plot_ridge_path.py,54,,not
scikit-learn/examples/linear_model/plot_ridge_path.py,55,Display results,not
scikit-learn/examples/linear_model/plot_ridge_path.py,61,reverse axis,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,46,"Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort",not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,47,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,57,This is to avoid division by zero while doing np.log10,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,63,add some bad features,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,65,normalize data as done by Lars to allow for comparison,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,68,,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,69,LassoLarsIC: least angle regression with BIC/AIC criterion,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,99,,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,100,LassoCV: coordinate descent,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,102,Compute paths,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,108,Display results,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,126,,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,127,LassoLarsCV: least angle regression,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,129,Compute paths,not
scikit-learn/examples/linear_model/plot_lasso_model_selection.py,135,Display results,not
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,13,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,14,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,27,Standardize data (easier to set the l1_ratio parameter),not
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,29,Compute paths,not
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,31,the smaller it is the longer is the path,not
scikit-learn/examples/linear_model/plot_lasso_coordinate_descent_path.py,47,Display results,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,1,!/usr/bin/env python,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,26,Author: Mathieu Blondel,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,27,Jake Vanderplas,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,28,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,43,generate points used to plot,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,46,generate points and keep a subset of them,not
scikit-learn/examples/linear_model/plot_polynomial_interpolation.py,53,create matrix versions of these arrays,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,38,Authors: Tom Dupre la Tour,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,39,,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,40,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,60,Load data from http://openml.org/d/554,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,63,take only two classes for binary classification,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,89,Define the estimators to compare,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,101,Load the dataset,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,120,Transform the results in a pandas dataframe for easy plotting,not
scikit-learn/examples/linear_model/plot_sgd_early_stopping.py,127,"Define what to plot (x_axis, y_axis)",not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,31,Author: Arthur Mensch <arthur.mensch@m4x.org>,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,32,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,34,Turn down for faster convergence,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,38,Load data from https://www.openml.org/d/554,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,54,Turn up tolerance for faster convergence,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_mnist.py,61,print('Best C % .4f' % clf.C_),not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,11,Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,12,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,19,make 3-class dataset for classification,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,29,print the training scores,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,32,create a mesh to plot in,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,33,step size in the mesh,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,39,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,40,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,42,Put the result into a color plot,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,49,Plot also the training points,not
scikit-learn/examples/linear_model/plot_logistic_multinomial.py,56,Plot the three one-against-all classifiers,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,18,,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,19,Generate some sparse data to play with,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,25,Decreasing coef w. alternated signs for visualization,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,28,sparsify coef,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,31,Add noise,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,34,Split data in train set and test set,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,39,,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,40,Lasso,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,51,,not
scikit-learn/examples/linear_model/plot_lasso_and_elasticnet.py,52,ElasticNet,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,18,import some data to play with,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,21,we only take the first two features. We could,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,22,avoid this ugly slicing by using a two-dim dataset,SATD
scikit-learn/examples/linear_model/plot_sgd_iris.py,27,shuffle,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,34,standardize,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,39,step size in the mesh,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,43,create a mesh to plot in,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,49,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/linear_model/plot_sgd_iris.py,50,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/linear_model/plot_sgd_iris.py,52,Put the result into a color plot,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,57,Plot also the training points,not
scikit-learn/examples/linear_model/plot_sgd_iris.py,65,Plot the three one-against-all classifiers,not
scikit-learn/examples/linear_model/plot_ard.py,33,,not
scikit-learn/examples/linear_model/plot_ard.py,34,Generating simulated data with Gaussian weights,not
scikit-learn/examples/linear_model/plot_ard.py,36,Parameters of the example,not
scikit-learn/examples/linear_model/plot_ard.py,39,Create Gaussian data,not
scikit-learn/examples/linear_model/plot_ard.py,41,Create weights with a precision lambda_ of 4.,not
scikit-learn/examples/linear_model/plot_ard.py,44,Only keep 10 weights of interest,not
scikit-learn/examples/linear_model/plot_ard.py,48,Create noise with a precision alpha of 50.,not
scikit-learn/examples/linear_model/plot_ard.py,51,Create the target,not
scikit-learn/examples/linear_model/plot_ard.py,54,,not
scikit-learn/examples/linear_model/plot_ard.py,55,Fit the ARD Regression,not
scikit-learn/examples/linear_model/plot_ard.py,62,,not
scikit-learn/examples/linear_model/plot_ard.py,63,"Plot the true weights, the estimated weights, the histogram of the",not
scikit-learn/examples/linear_model/plot_ard.py,64,"weights, and predictions with standard deviations",not
scikit-learn/examples/linear_model/plot_ard.py,92,Plotting some predictions for polynomial regression,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,39,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,40,Roman Yurchak <rth.yurchak@gmail.com>,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,41,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,42,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,48,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,49,The French Motor Third-Party Liability Claims dataset,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,50,-----------------------------------------------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,51,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,52,Let's load the motor claim dataset from OpenML:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,53,https://www.openml.org/d/41214,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,61,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,62,The number of claims (``ClaimNb``) is a positive integer that can be modeled,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,63,as a Poisson distribution. It is then assumed to be the number of discrete,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,64,"events occurring with a constant rate in a given time interval (``Exposure``,",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,65,in units of years).,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,66,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,67,Here we want to model the frequency ``y = ClaimNb / Exposure`` conditionally,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,68,"on ``X`` via a (scaled) Poisson distribution, and use ``Exposure`` as",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,69,``sample_weight``.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,88,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,89,The remaining columns can be used to predict the frequency of claim events.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,90,Those columns are very heterogeneous with a mix of categorical and numeric,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,91,"variables with different scales, possibly very unevenly distributed.",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,92,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,93,In order to fit linear models with those predictors it is therefore,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,94,necessary to perform standard feature transformations as follows:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,122,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,123,A constant prediction baseline,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,124,------------------------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,125,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,126,It is worth noting that more than 93% of policyholders have zero claims. If,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,127,"we were to convert this problem into a binary classification task, it would",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,128,"be significantly imbalanced, and even a simplistic model that would only",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,129,predict mean can achieve an accuracy of 93%.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,130,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,131,"To evaluate the pertinence of the used metrics, we will consider as a",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,132,"baseline a ""dummy"" estimator that constantly predicts the mean frequency of",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,133,the training sample.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,148,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,149,Let's compute the performance of this constant prediction baseline with 3,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,150,different regression metrics:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,168,"Ignore non-positive predictions, as they are invalid for",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,169,the Poisson deviance.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,186,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,187,(Generalized) linear models,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,188,---------------------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,189,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,190,We start by modeling the target variable with the (l2 penalized) least,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,191,"squares linear regression model, more comonly known as Ridge regression. We",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,192,"use a low penalization `alpha`, as we expect such a linear model to under-fit",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,193,on such a large dataset.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,204,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,205,The Poisson deviance cannot be computed on non-positive values predicted by,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,206,the model. For models that do return a few non-positive predictions (e.g.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,207,":class:`~sklearn.linear_model.Ridge`) we ignore the corresponding samples,",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,208,meaning that the obtained Poisson deviance is approximate. An alternative,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,209,approach could be to use :class:`~sklearn.compose.TransformedTargetRegressor`,SATD
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,210,meta-estimator to map ``y_pred`` to a strictly positive domain.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,215,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,216,Next we fit the Poisson regressor on the target variable. We set the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,217,regularization strength ``alpha`` to approximately 1e-6 over number of,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,218,samples (i.e. `1e-12`) in order to mimic the Ridge regressor whose L2 penalty,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,219,term scales differently with the number of samples.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,220,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,221,Since the Poisson regressor internally models the log of the expected target,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,222,"value instead of the expected value directly (log vs identity link function),",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,223,the relationship between X and y is not exactly linear anymore. Therefore the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,224,Poisson regressor is called a Generalized Linear Model (GLM) rather than a,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,225,vanilla linear model as is the case for Ridge regression.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,241,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,242,Gradient Boosting Regression Trees for Poisson regression,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,243,---------------------------------------------------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,244,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,245,"Finally, we will consider a non-linear model, namely Gradient Boosting",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,246,Regression Trees. Tree-based models do not require the categorical data to be,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,247,"one-hot encoded: instead, we can encode each category label with an arbitrary",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,248,integer using :class:`~sklearn.preprocessing.OrdinalEncoder`. With this,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,249,"encoding, the trees will treat the categorical features as ordered features,",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,250,which might not be always a desired behavior. However this effect is limited,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,251,for deep enough trees which are able to recover the categorical nature of the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,252,features. The main advantage of the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,253,:class:`~sklearn.preprocessing.OrdinalEncoder` over the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,254,:class:`~sklearn.preprocessing.OneHotEncoder` is that it will make training,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,255,faster.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,256,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,257,Gradient Boosting also gives the possibility to fit the trees with a Poisson,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,258,loss (with an implicit log-link function) instead of the default,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,259,least-squares loss. Here we only fit trees with the Poisson loss to keep this,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,260,example concise.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,262,noqa,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,288,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,289,"Like the Poisson GLM above, the gradient boosted trees model minimizes",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,290,"the Poisson deviance. However, because of a higher predictive power,",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,291,it reaches lower values of Poisson deviance.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,292,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,293,Evaluating models with a single train / test split is prone to random,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,294,"fluctuations. If computing resources allow, it should be verified that",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,295,cross-validated performance metrics would lead to similar conclusions.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,296,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,297,The qualitative difference between these models can also be visualized by,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,298,comparing the histogram of observed target values with that of predicted,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,299,values:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,328,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,329,The experimental data presents a long tail distribution for ``y``. In all,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,330,"models, we predict the expected frequency of a random variable, so we will",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,331,have necessarily fewer extreme values than for the observed realizations of,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,332,that random variable. This explains that the mode of the histograms of model,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,333,predictions doesn't necessarily correspond to the smallest value.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,334,"Additionally, the normal distribution used in ``Ridge`` has a constant",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,335,"variance, while for the Poisson distribution used in ``PoissonRegressor`` and",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,336,"``HistGradientBoostingRegressor``, the variance is proportional to the",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,337,predicted expected value.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,338,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,339,"Thus, among the considered estimators, ``PoissonRegressor`` and",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,340,``HistGradientBoostingRegressor`` are a-priori better suited for modeling the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,341,long tail distribution of the non-negative data as compared to the ``Ridge``,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,342,model which makes a wrong assumption on the distribution of the target,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,343,variable.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,344,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,345,The ``HistGradientBoostingRegressor`` estimator has the most flexibility and,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,346,is able to predict higher expected values.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,347,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,348,Note that we could have used the least squares loss for the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,349,``HistGradientBoostingRegressor`` model. This would wrongly assume a normal,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,350,"distributed response variable as does the `Ridge` model, and possibly",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,351,also lead to slightly negative predictions. However the gradient boosted,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,352,trees would still perform relatively well and in particular better than,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,353,``PoissonRegressor`` thanks to the flexibility of the trees combined with the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,354,large number of training samples.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,355,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,356,Evaluation of the calibration of predictions,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,357,--------------------------------------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,358,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,359,To ensure that estimators yield reasonable predictions for different,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,360,"policyholder types, we can bin test samples according to ``y_pred`` returned",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,361,"by each model. Then for each bin, we compare the mean predicted ``y_pred``,",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,362,with the mean observed target:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,423,Name of the model after the estimator used in the last step of the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,424,pipeline.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,440,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,441,The dummy regression model predicts a constant frequency. This model does not,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,442,attribute the same tied rank to all samples but is none-the-less globally,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,443,well calibrated (to estimate the mean frequency of the entire population).,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,444,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,445,The ``Ridge`` regression model can predict very low expected frequencies that,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,446,do not match the data. It can therefore severly under-estimate the risk for,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,447,some policyholders.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,448,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,449,``PoissonRegressor`` and ``HistGradientBoostingRegressor`` show better,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,450,"consistency between predicted and observed targets, especially for low",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,451,predicted target values.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,452,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,453,The sum of all predictions also confirms the calibration issue of the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,454,``Ridge`` model: it under-estimates by more than 3% the total number of,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,455,claims in the test set while the other three models can approximately recover,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,456,the total number of claims of the test portfolio.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,457,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,458,Evaluation of the ranking power,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,459,-------------------------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,460,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,461,"For some business applications, we are interested in the ability of the model",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,462,"to rank the riskiest from the safest policyholders, irrespective of the",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,463,"absolute value of the prediction. In this case, the model evaluation would",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,464,cast the problem as a ranking problem rather than a regression problem.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,465,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,466,"To compare the 3 models from this perspective, one can plot the cumulative",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,467,proportion of claims vs the cumulative proportion of exposure for the test,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,468,"samples order by the model predictions, from safest to riskiest according to",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,469,each model.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,470,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,471,This plot is called a Lorenz curve and can be summarized by the Gini index:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,480,order samples by increasing predicted risk:,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,502,Oracle model: y_pred == y_test,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,510,Random Baseline,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,520,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,521,"As expected, the dummy regressor is unable to correctly rank the samples and",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,522,therefore performs the worst on this plot.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,523,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,524,The tree-based model is significantly better at ranking policyholders by risk,SATD
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,525,while the two linear models perform similarly.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,526,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,527,All three models are significantly better than chance but also very far from,SATD
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,528,making perfect predictions.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,529,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,530,This last point is expected due to the nature of the problem: the occurrence,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,531,of accidents is mostly dominated by circumstantial causes that are not,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,532,captured in the columns of the dataset and can indeed be considered as purely,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,533,random.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,534,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,535,The linear models assume no interactions between the input variables which,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,536,likely causes under-fitting. Inserting a polynomial feature extractor,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,537,(:func:`~sklearn.preprocessing.PolynomialFeatures`) indeed increases their,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,538,discrimative power by 2 points of Gini index. In particular it improves the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,539,ability of the models to identify the top 5% riskiest profiles.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,540,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,541,Main takeaways,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,542,--------------,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,543,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,544,- The performance of the models can be evaluated by their ability to yield,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,545,well-calibrated predictions and a good ranking.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,546,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,547,- The calibration of the model can be assessed by plotting the mean observed,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,548,value vs the mean predicted value on groups of test samples binned by,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,549,predicted risk.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,550,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,551,- The least squares loss (along with the implicit use of the identity link,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,552,function) of the Ridge regression model seems to cause this model to be,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,553,"badly calibrated. In particular, it tends to underestimate the risk and can",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,554,even predict invalid negative frequencies.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,555,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,556,- Using the Poisson loss with a log-link can correct these problems and lead,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,557,to a well-calibrated linear model.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,558,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,559,- The Gini index reflects the ability of a model to rank predictions,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,560,"irrespective of their absolute values, and therefore only assess their",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,561,ranking power.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,562,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,563,"- Despite the improvement in calibration, the ranking power of both linear",not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,564,models are comparable and well below the ranking power of the Gradient,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,565,Boosting Regression Trees.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,566,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,567,- The Poisson deviance computed as an evaluation metric reflects both the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,568,calibration and the ranking power of the model. It also makes a linear,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,569,assumption on the ideal relationship between the expected value and the,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,570,variance of the response variable. For the sake of conciseness we did not,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,571,check whether this assumption holds.,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,572,,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,573,- Traditional regression metrics such as Mean Squared Error and Mean Absolute,not
scikit-learn/examples/linear_model/plot_poisson_regression_non_normal_loss.py,574,Error are hard to meaningfully interpret on count values with many zeros.,not
scikit-learn/examples/linear_model/plot_ridge_coeffs.py,40,Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>,not
scikit-learn/examples/linear_model/plot_ridge_coeffs.py,61,Train the model with different regularisation strengths,not
scikit-learn/examples/linear_model/plot_ridge_coeffs.py,68,Display results,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,33,,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,34,Generating simulated data with Gaussian weights,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,37,Create Gaussian data,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,38,Create weights with a precision lambda_ of 4.,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,41,Only keep 10 weights of interest,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,45,Create noise with a precision alpha of 50.,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,48,Create the target,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,51,,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,52,Fit the Bayesian Ridge Regression and an OLS for comparison,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,59,,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,60,"Plot true weights, estimated weights, histogram of the weights, and",not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,61,predictions with standard deviations,not
scikit-learn/examples/linear_model/plot_bayesian_ridge.py,90,Plotting some predictions for polynomial regression,not
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,15,we create 20 points,not
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,20,and assign a bigger weight to the last 10 samples,not
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,23,plot the weighted data points,not
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,29,fit the unweighted model,not
scikit-learn/examples/linear_model/plot_sgd_weighted_samples.py,36,fit the weighted model,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,33,Author: Arthur Mensch,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,39,We use SAGA solver,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,42,Turn down for faster run time,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,63,Add initial chance-level values for plotting purpose,not
scikit-learn/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py,70,Small number of epochs for fast runtime,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,42,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,43,Roman Yurchak <rth.yurchak@gmail.com>,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,44,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,45,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,74,freMTPL2freq dataset from https://www.openml.org/d/41214,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,79,freMTPL2sev dataset from https://www.openml.org/d/41215,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,82,sum ClaimAmount over identical IDs,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,88,unquote string fields,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,113,aggregate observed and predicted variables by feature level,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,148,Use default scorer if it exists,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,166,Score the model consisting of the product of frequency and,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,167,severity models.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,194,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,195,"Loading datasets, basic feature extraction and target definitions",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,196,-----------------------------------------------------------------,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,197,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,198,"We construct the freMTPL2 dataset by joining the freMTPL2freq table,",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,199,"containing the number of claims (``ClaimNb``), with the freMTPL2sev table,",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,200,containing the claim amount (``ClaimAmount``) for the same policy ids,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,201,(``IDpol``).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,205,"Note: filter out claims with zero amount, as the severity model",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,206,requires strictly positive target values.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,209,Correct for unreasonable observations (that might be data error),not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,210,and a few exceptionally large claim amounts,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,235,"Insurances companies are interested in modeling the Pure Premium, that is",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,236,the expected total claim amount per unit of exposure for each policyholder,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,237,in their portfolio:,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,240,This can be indirectly approximated by a 2-step modeling: the product of the,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,241,Frequency times the average claim amount per claim:,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,248,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,249,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,250,Frequency model -- Poisson distribution,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,251,---------------------------------------,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,252,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,253,The number of claims (``ClaimNb``) is a positive integer (0 included).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,254,"Thus, this target can be modelled by a Poisson distribution.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,255,It is then assumed to be the number of discrete events occuring with a,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,256,"constant rate in a given time interval (``Exposure``, in units of years).",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,257,"Here we model the frequency ``y = ClaimNb / Exposure``, which is still a",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,258,"(scaled) Poisson distribution, and use ``Exposure`` as `sample_weight`.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,262,The parameters of the model are estimated by minimizing the Poisson deviance,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,263,on the training set via a quasi-Newton solver: l-BFGS. Some of the features,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,264,"are collinear, we use a weak penalization to avoid numerical issues.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,281,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,282,"We can visually compare observed and predicted values, aggregated by the",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,283,"drivers age (``DrivAge``), vehicle age (``VehAge``) and the insurance",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,284,bonus/malus (``BonusMalus``).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,337,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,338,"According to the observed data, the frequency of accidents is higher for",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,339,"drivers younger than 30 years old, and is positively correlated with the",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,340,`BonusMalus` variable. Our model is able to mostly correctly model this,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,341,behaviour.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,342,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,343,Severity Model -  Gamma distribution,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,344,------------------------------------,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,345,The mean claim amount or severity (`AvgClaimAmount`) can be empirically,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,346,shown to follow approximately a Gamma distribution. We fit a GLM model for,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,347,the severity with the same features as the frequency model.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,348,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,349,Note:,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,350,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,351,- We filter out ``ClaimAmount == 0`` as the Gamma distribution has support,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,352,"on :math:`(0, \infty)`, not :math:`[0, \infty)`.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,353,- We use ``ClaimNb`` as `sample_weight` to account for policies that contain,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,354,more than one claim.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,379,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,380,"Here, the scores for the test data call for caution as they are",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,381,significantly worse than for the training data indicating an overfit despite,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,382,the strong regularization.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,383,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,384,Note that the resulting model is the average claim amount per claim. As,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,385,"such, it is conditional on having at least one claim, and cannot be used to",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,386,predict the average claim amount per policy in general.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,396,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,397,"We can visually compare observed and predicted values, aggregated for",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,398,the drivers age (``DrivAge``).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,426,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,427,"Overall, the drivers age (``DrivAge``) has a weak impact on the claim",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,428,"severity, both in observed and predicted data.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,429,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,430,Pure Premium Modeling via a Product Model vs single TweedieRegressor,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,431,--------------------------------------------------------------------,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,432,"As mentioned in the introduction, the total claim amount per unit of",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,433,exposure can be modeled as the product of the prediction of the,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,434,frequency model by the prediction of the severity model.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,435,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,436,"Alternatively, one can directly model the total loss with a unique",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,437,Compound Poisson Gamma generalized linear model (with a log link function).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,438,"This model is a special case of the Tweedie GLM with a ""power"" parameter",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,439,":math:`p \in (1, 2)`. Here, we fix apriori the `power` parameter of the",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,440,Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,441,would select this value via grid-search by minimizing the negative,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,442,"log-likelihood of the Tweedie model, but unfortunately the current",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,443,implementation does not allow for this (yet).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,444,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,445,We will compare the performance of both approaches.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,446,"To quantify the performance of both models, one can compute",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,447,the mean deviance of the train and test data assuming a Compound,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,448,Poisson-Gamma distribution of the total claim amount. This is equivalent to,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,449,a Tweedie distribution with a `power` parameter between 1 and 2.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,450,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,451,The :func:`sklearn.metrics.mean_tweedie_deviance` depends on a `power`,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,452,"parameter. As we do not know the true value of the `power` parameter, we here",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,453,"compute the mean deviances for a grid of possible values, and compare the",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,454,"models side by side, i.e. we compare them at identical values of `power`.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,455,"Ideally, we hope that one model will be consistently better than the other,",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,456,regardless of `power`.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,494,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,495,"In this example, both modeling approaches yield comparable performance",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,496,"metrics. For implementation reasons, the percentage of explained variance",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,497,:math:`D^2` is not available for the product model.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,498,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,499,We can additionally validate these models by comparing observed and,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,500,"predicted total claim amount over the test and train subsets. We see that,",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,501,"on average, both model tend to underestimate the total claim (but this",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,502,behavior depends on the amount of regularization).,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,525,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,526,"Finally, we can compare the two models using a plot of cumulated claims: for",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,527,"each model, the policyholders are ranked from safest to riskiest and the",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,528,fraction of observed total cumulated claims is plotted on the y axis. This,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,529,plot is often called the ordered Lorenz curve of the model.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,530,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,531,The Gini coefficient (based on the area under the curve) can be used as a,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,532,model selection metric to quantify the ability of the model to rank,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,533,policyholders. Note that this metric does not reflect the ability of the,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,534,models to make accurate predictions in terms of absolute value of total,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,535,claim amounts but only in terms of relative amounts as a ranking metric.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,536,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,537,Both models are able to rank policyholders by risky-ness significantly,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,538,better than chance although they are also both far from perfect due to the,SATD
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,539,natural difficulty of the prediction problem from few features.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,540,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,541,Note that the Gini index only characterize the ranking performance of the,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,542,model but not its calibration: any monotonic transformation of the,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,543,predictions leaves the Gini index of the model unchanged.,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,544,,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,545,Finally one should highlight that the Compound Poisson Gamma model that,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,546,is directly fit on the pure premium is operationally simpler to develop and,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,547,maintain as it consists in a single scikit-learn estimator instead of a,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,548,"pair of models, each with its own set of hyperparameters.",not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,555,order samples by increasing predicted risk:,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,578,Oracle model: y_pred == y_test,not
scikit-learn/examples/linear_model/plot_tweedie_regression_insurance_claims.py,586,Random baseline,not
scikit-learn/examples/linear_model/plot_lasso_lars.py,1,!/usr/bin/env python,not
scikit-learn/examples/linear_model/plot_lasso_lars.py,15,Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/examples/linear_model/plot_lasso_lars.py,16,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/linear_model/plot_lasso_lars.py,17,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_ols_3d.py,1,!/usr/bin/python,not
scikit-learn/examples/linear_model/plot_ols_3d.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/linear_model/plot_ols_3d.py,18,Code source: Gaël Varoquaux,not
scikit-learn/examples/linear_model/plot_ols_3d.py,19,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/linear_model/plot_ols_3d.py,20,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_ols_3d.py,40,,not
scikit-learn/examples/linear_model/plot_ols_3d.py,41,Plot the figure,not
scikit-learn/examples/linear_model/plot_ols_3d.py,62,Generate the three different figures from different views,not
scikit-learn/examples/linear_model/plot_logistic.py,1,!/usr/bin/python,not
scikit-learn/examples/linear_model/plot_logistic.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/linear_model/plot_logistic.py,18,Code source: Gael Varoquaux,not
scikit-learn/examples/linear_model/plot_logistic.py,19,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_logistic.py,27,General a toy dataset:s it's just a straight line with some Gaussian noise:,not
scikit-learn/examples/linear_model/plot_logistic.py,38,Fit the classifier,not
scikit-learn/examples/linear_model/plot_logistic.py,42,and plot the result,not
scikit-learn/examples/linear_model/plot_sgd_comparison.py,10,Author: Rob Zinkov <rob at zinkov dot com>,not
scikit-learn/examples/linear_model/plot_sgd_comparison.py,11,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,1,!/usr/bin/python,not
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,26,Code source: Gaël Varoquaux,not
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,27,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/linear_model/plot_ols_ridge_variance.py,28,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,1,!/usr/bin/python,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,17,Code source: Gaël Varoquaux,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,18,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,19,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,26,import some data to play with,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,28,we only take the first two features.,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,33,Create an instance of Logistic Regression Classifier and fit the data.,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,36,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/linear_model/plot_iris_logistic.py,37,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/linear_model/plot_iris_logistic.py,40,step size in the mesh,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,44,Put the result into a color plot,not
scikit-learn/examples/linear_model/plot_iris_logistic.py,49,Plot also the training points,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,19,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,20,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,21,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,22,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,35,classify small against large digits,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,38,L1 weight in the Elastic-Net regularization,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,42,Set regularization parameter,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,44,turn down tolerance for short training time,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,57,coef_l1_LR contains zeros due to the,not
scikit-learn/examples/linear_model/plot_logistic_l1_l2_sparsity.py,58,L1 sparsity inducing norm,not
scikit-learn/examples/linear_model/plot_logistic_path.py,1,!/usr/bin/env python,not
scikit-learn/examples/linear_model/plot_logistic_path.py,30,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/linear_model/plot_logistic_path.py,31,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_logistic_path.py,48,Normalize X to speed-up convergence,not
scikit-learn/examples/linear_model/plot_logistic_path.py,50,,not
scikit-learn/examples/linear_model/plot_logistic_path.py,51,Demo path functions,not
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,20,,not
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,21,The two Lasso implementations on Dense data,not
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,42,,not
scikit-learn/examples/linear_model/plot_lasso_dense_vs_sparse_data.py,43,The two Lasso implementations on Sparse data,not
scikit-learn/examples/linear_model/plot_ols.py,1,!/usr/bin/python,not
scikit-learn/examples/linear_model/plot_ols.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/linear_model/plot_ols.py,22,Code source: Jaques Grobler,not
scikit-learn/examples/linear_model/plot_ols.py,23,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_ols.py,31,Load the diabetes dataset,not
scikit-learn/examples/linear_model/plot_ols.py,34,Use only one feature,not
scikit-learn/examples/linear_model/plot_ols.py,37,Split the data into training/testing sets,not
scikit-learn/examples/linear_model/plot_ols.py,41,Split the targets into training/testing sets,not
scikit-learn/examples/linear_model/plot_ols.py,45,Create linear regression object,not
scikit-learn/examples/linear_model/plot_ols.py,48,Train the model using the training sets,not
scikit-learn/examples/linear_model/plot_ols.py,51,Make predictions using the testing set,not
scikit-learn/examples/linear_model/plot_ols.py,54,The coefficients,not
scikit-learn/examples/linear_model/plot_ols.py,56,The mean squared error,not
scikit-learn/examples/linear_model/plot_ols.py,59,The coefficient of determination: 1 is perfect prediction,not
scikit-learn/examples/linear_model/plot_ols.py,63,Plot outputs,not
scikit-learn/examples/linear_model/plot_sgd_separating_hyperplane.py,17,we create 50 separable points,not
scikit-learn/examples/linear_model/plot_sgd_separating_hyperplane.py,20,fit the model,not
scikit-learn/examples/linear_model/plot_sgd_separating_hyperplane.py,25,"plot the line, the points, and the nearest vectors to the plane",not
scikit-learn/examples/linear_model/plot_ransac.py,24,Add outlier data,not
scikit-learn/examples/linear_model/plot_ransac.py,29,Fit line using all data,not
scikit-learn/examples/linear_model/plot_ransac.py,33,Robustly fit linear model with RANSAC algorithm,not
scikit-learn/examples/linear_model/plot_ransac.py,39,Predict data of estimated models,not
scikit-learn/examples/linear_model/plot_ransac.py,44,Compare estimated coefficients,not
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,1,!/usr/bin/env python,not
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,18,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,19,License: BSD 3 clause,not
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,28,Generate some 2D coefficients with sine waves with random frequency and phase,not
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,42,,not
scikit-learn/examples/linear_model/plot_multi_task_lasso_support.py,43,Plot support and time series,not
scikit-learn/examples/classification/plot_digits_classification.py,15,Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>,not
scikit-learn/examples/classification/plot_digits_classification.py,16,License: BSD 3 clause,not
scikit-learn/examples/classification/plot_digits_classification.py,18,Standard scientific Python imports,not
scikit-learn/examples/classification/plot_digits_classification.py,21,"Import datasets, classifiers and performance metrics",not
scikit-learn/examples/classification/plot_digits_classification.py,25,The digits dataset,not
scikit-learn/examples/classification/plot_digits_classification.py,28,"The data that we are interested in is made of 8x8 images of digits, let's",not
scikit-learn/examples/classification/plot_digits_classification.py,29,"have a look at the first 4 images, stored in the `images` attribute of the",not
scikit-learn/examples/classification/plot_digits_classification.py,30,"dataset.  If we were working from image files, we could load them using",not
scikit-learn/examples/classification/plot_digits_classification.py,31,matplotlib.pyplot.imread.  Note that each image must have the same size. For these,not
scikit-learn/examples/classification/plot_digits_classification.py,32,"images, we know which digit they represent: it is given in the 'target' of",not
scikit-learn/examples/classification/plot_digits_classification.py,33,the dataset.,not
scikit-learn/examples/classification/plot_digits_classification.py,41,"To apply a classifier on this data, we need to flatten the image, to",not
scikit-learn/examples/classification/plot_digits_classification.py,42,"turn the data in a (samples, feature) matrix:",not
scikit-learn/examples/classification/plot_digits_classification.py,46,Create a classifier: a support vector classifier,not
scikit-learn/examples/classification/plot_digits_classification.py,49,Split data into train and test subsets,not
scikit-learn/examples/classification/plot_digits_classification.py,53,We learn the digits on the first half of the digits,not
scikit-learn/examples/classification/plot_digits_classification.py,56,Now predict the value of the digit on the second half:,not
scikit-learn/examples/classification/plot_classification_probability.py,20,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/classification/plot_classification_probability.py,21,License: BSD 3 clause,not
scikit-learn/examples/classification/plot_classification_probability.py,34,we only take the first two features for visualization,not
scikit-learn/examples/classification/plot_classification_probability.py,40,for GPC,not
scikit-learn/examples/classification/plot_classification_probability.py,42,Create different classifiers.,not
scikit-learn/examples/classification/plot_classification_probability.py,78,View probabilities:,not
scikit-learn/examples/classification/plot_classifier_comparison.py,1,!/usr/bin/python,not
scikit-learn/examples/classification/plot_classifier_comparison.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/classification/plot_classifier_comparison.py,26,Code source: Gaël Varoquaux,not
scikit-learn/examples/classification/plot_classifier_comparison.py,27,Andreas Müller,not
scikit-learn/examples/classification/plot_classifier_comparison.py,28,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/classification/plot_classifier_comparison.py,29,License: BSD 3 clause,not
scikit-learn/examples/classification/plot_classifier_comparison.py,47,step size in the mesh,not
scikit-learn/examples/classification/plot_classifier_comparison.py,78,iterate over datasets,not
scikit-learn/examples/classification/plot_classifier_comparison.py,80,"preprocess dataset, split into training and test part",not
scikit-learn/examples/classification/plot_classifier_comparison.py,91,just plot the dataset first,not
scikit-learn/examples/classification/plot_classifier_comparison.py,97,Plot the training points,not
scikit-learn/examples/classification/plot_classifier_comparison.py,100,Plot the testing points,not
scikit-learn/examples/classification/plot_classifier_comparison.py,109,iterate over classifiers,not
scikit-learn/examples/classification/plot_classifier_comparison.py,115,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/classification/plot_classifier_comparison.py,116,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/classification/plot_classifier_comparison.py,122,Put the result into a color plot,not
scikit-learn/examples/classification/plot_classifier_comparison.py,126,Plot the training points,not
scikit-learn/examples/classification/plot_classifier_comparison.py,129,Plot the testing points,not
scikit-learn/examples/classification/plot_lda_qda.py,23,,not
scikit-learn/examples/classification/plot_lda_qda.py,24,Colormap,not
scikit-learn/examples/classification/plot_lda_qda.py,33,,not
scikit-learn/examples/classification/plot_lda_qda.py,34,Generate datasets,not
scikit-learn/examples/classification/plot_lda_qda.py,57,,not
scikit-learn/examples/classification/plot_lda_qda.py,58,Plot functions,not
scikit-learn/examples/classification/plot_lda_qda.py,69,True Positive,not
scikit-learn/examples/classification/plot_lda_qda.py,75,class 0: dots,not
scikit-learn/examples/classification/plot_lda_qda.py,78,dark red,not
scikit-learn/examples/classification/plot_lda_qda.py,80,class 1: dots,not
scikit-learn/examples/classification/plot_lda_qda.py,83,dark blue,not
scikit-learn/examples/classification/plot_lda_qda.py,85,class 0 and 1 : areas,not
scikit-learn/examples/classification/plot_lda_qda.py,97,means,not
scikit-learn/examples/classification/plot_lda_qda.py,110,convert to degrees,not
scikit-learn/examples/classification/plot_lda_qda.py,111,filled Gaussian at 2 standard deviation,not
scikit-learn/examples/classification/plot_lda_qda.py,136,Linear Discriminant Analysis,not
scikit-learn/examples/classification/plot_lda_qda.py,143,Quadratic Discriminant Analysis,not
scikit-learn/examples/classification/plot_lda.py,15,samples for training,not
scikit-learn/examples/classification/plot_lda.py,16,samples for testing,not
scikit-learn/examples/classification/plot_lda.py,17,how often to repeat classification,not
scikit-learn/examples/classification/plot_lda.py,18,maximum number of features,not
scikit-learn/examples/classification/plot_lda.py,19,step size for the calculation,not
scikit-learn/examples/classification/plot_lda.py,33,add non-discriminative features,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,12,License: BSD 3 clause,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,23,,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,24,Original points,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,25,---------------,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,26,"First we create a data set of 9 samples from 3 classes, and plot the points",not
scikit-learn/examples/neighbors/plot_nca_illustration.py,27,"in the original space. For this example, we focus on the classification of",not
scikit-learn/examples/neighbors/plot_nca_illustration.py,28,point no. 3. The thickness of a link between point no. 3 and another point,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,29,is proportional to their distance.,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,44,so that boundaries are displayed correctly as circles,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,53,compute exponentiated distances (use the log-sum-exp trick to,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,54,avoid numerical instabilities,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,74,,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,75,Learning an embedding,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,76,---------------------,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,77,We use :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis` to learn an,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,78,embedding and plot the points after the transformation. We then take the,not
scikit-learn/examples/neighbors/plot_nca_illustration.py,79,embedding and find the nearest neighbors.,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,21,load the data,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,24,project the 64-dimensional data to a lower dimension,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,28,use grid search cross-validation to optimize the bandwidth,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,35,use the best estimator to compute the kernel density estimate,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,38,sample 44 new points from the data,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,42,turn data into a 4x11 grid,not
scikit-learn/examples/neighbors/plot_digits_kde_sampling.py,46,plot real digits and resampled digits,not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,37,Generate normal (not abnormal) training observations,not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,40,Generate new normal (not abnormal) observations,not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,43,Generate some abnormal novel observations,not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,46,fit the model for novelty detection (novelty=True),not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,49,"DO NOT use predict, decision_function and score_samples on X_train as this",not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,50,"would give wrong results but only on new unseen data (not used in X_train),",not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,51,"e.g. X_test, X_outliers or the meshgrid",not
scikit-learn/examples/neighbors/plot_lof_novelty_detection.py,57,"plot the learned frontier, the points, and the nearest vectors to the plane",not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,30,License: BSD 3 clause,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,48,Load Digits dataset,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,51,Split into train/test,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,59,Reduce dimension to 2 with PCA,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,63,Reduce dimension to 2 with LinearDiscriminantAnalysis,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,67,Reduce dimension to 2 with NeighborhoodComponentAnalysis,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,72,Use a nearest neighbor classifier to evaluate the methods,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,75,Make a list of the methods to be compared,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,78,plt.figure(),not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,81,"plt.subplot(1, 3, i + 1, aspect=1)",not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,83,Fit the method's model,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,86,Fit a nearest neighbor classifier on the embedded training set,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,89,Compute the nearest neighbor accuracy on the embedded test set,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,92,Embed the data set in 2 dimensions using the fitted model,not
scikit-learn/examples/neighbors/plot_nca_dim_reduction.py,95,Plot the projected points and show the evaluation score,not
scikit-learn/examples/neighbors/plot_nca_classification.py,17,License: BSD 3 clause,not
scikit-learn/examples/neighbors/plot_nca_classification.py,37,we only take two features. We could avoid this ugly,not
scikit-learn/examples/neighbors/plot_nca_classification.py,38,slicing by using a two-dim dataset,not
scikit-learn/examples/neighbors/plot_nca_classification.py,44,step size in the mesh,not
scikit-learn/examples/neighbors/plot_nca_classification.py,46,Create color maps,not
scikit-learn/examples/neighbors/plot_nca_classification.py,71,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/neighbors/plot_nca_classification.py,72,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/neighbors/plot_nca_classification.py,75,Put the result into a color plot,not
scikit-learn/examples/neighbors/plot_nca_classification.py,80,Plot also the training and testing points,not
scikit-learn/examples/neighbors/plot_regression.py,13,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/neighbors/plot_regression.py,14,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/examples/neighbors/plot_regression.py,15,,not
scikit-learn/examples/neighbors/plot_regression.py,16,License: BSD 3 clause (C) INRIA,not
scikit-learn/examples/neighbors/plot_regression.py,19,,not
scikit-learn/examples/neighbors/plot_regression.py,20,Generate sample data,not
scikit-learn/examples/neighbors/plot_regression.py,30,Add noise to targets,not
scikit-learn/examples/neighbors/plot_regression.py,33,,not
scikit-learn/examples/neighbors/plot_regression.py,34,Fit regression model,not
scikit-learn/examples/neighbors/plot_kde_1d.py,29,Author: Jake Vanderplas <jakevdp@cs.washington.edu>,not
scikit-learn/examples/neighbors/plot_kde_1d.py,30,,not
scikit-learn/examples/neighbors/plot_kde_1d.py,38,`normed` is being deprecated in favor of `density` in histograms,not
scikit-learn/examples/neighbors/plot_kde_1d.py,44,----------------------------------------------------------------------,not
scikit-learn/examples/neighbors/plot_kde_1d.py,45,Plot the progression of histograms to kernels,not
scikit-learn/examples/neighbors/plot_kde_1d.py,56,histogram 1,not
scikit-learn/examples/neighbors/plot_kde_1d.py,60,histogram 2,not
scikit-learn/examples/neighbors/plot_kde_1d.py,64,tophat KDE,not
scikit-learn/examples/neighbors/plot_kde_1d.py,70,Gaussian KDE,not
scikit-learn/examples/neighbors/plot_kde_1d.py,87,----------------------------------------------------------------------,not
scikit-learn/examples/neighbors/plot_kde_1d.py,88,Plot all available kernels,not
scikit-learn/examples/neighbors/plot_kde_1d.py,122,----------------------------------------------------------------------,not
scikit-learn/examples/neighbors/plot_kde_1d.py,123,Plot a 1D density example,not
scikit-learn/examples/neighbors/plot_classification.py,18,import some data to play with,not
scikit-learn/examples/neighbors/plot_classification.py,21,we only take the first two features. We could avoid this ugly,not
scikit-learn/examples/neighbors/plot_classification.py,22,slicing by using a two-dim dataset,not
scikit-learn/examples/neighbors/plot_classification.py,26,step size in the mesh,not
scikit-learn/examples/neighbors/plot_classification.py,28,Create color maps,not
scikit-learn/examples/neighbors/plot_classification.py,33,we create an instance of Neighbours Classifier and fit the data.,not
scikit-learn/examples/neighbors/plot_classification.py,37,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/neighbors/plot_classification.py,38,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/neighbors/plot_classification.py,45,Put the result into a color plot,not
scikit-learn/examples/neighbors/plot_classification.py,50,Plot also the training points,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,44,Author: Tom Dupre la Tour,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,45,,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,46,License: BSD 3 clause,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,91,see more metric in the manual,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,92,https://github.com/nmslib/nmslib/tree/master/manual,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,109,"For compatibility reasons, as each sample is considered as its own",not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,110,"neighbor, one extra neighbor will be computed.",not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,160,"For compatibility reasons, as each sample is considered as its own",not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,161,"neighbor, one extra neighbor will be computed.",not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,225,TSNE requires a certain number of neighbors which depends on the,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,226,perplexity parameter.,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,227,Add one since we include each sample as its own neighbor.,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,255,init the plot,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,273,print the duration report,not
scikit-learn/examples/neighbors/approximate_nearest_neighbors.py,278,plot TSNE embedding which should be very similar across methods,not
scikit-learn/examples/neighbors/plot_species_kde.py,38,Author: Jake Vanderplas <jakevdp@cs.washington.edu>,not
scikit-learn/examples/neighbors/plot_species_kde.py,39,,not
scikit-learn/examples/neighbors/plot_species_kde.py,40,License: BSD 3 clause,not
scikit-learn/examples/neighbors/plot_species_kde.py,47,"if basemap is available, we'll use it.",not
scikit-learn/examples/neighbors/plot_species_kde.py,48,"otherwise, we'll improvise later...",not
scikit-learn/examples/neighbors/plot_species_kde.py,69,"x,y coordinates for corner cells",not
scikit-learn/examples/neighbors/plot_species_kde.py,75,x coordinates of the grid cells,not
scikit-learn/examples/neighbors/plot_species_kde.py,77,y coordinates of the grid cells,not
scikit-learn/examples/neighbors/plot_species_kde.py,83,Get matrices/arrays of species IDs and locations,not
scikit-learn/examples/neighbors/plot_species_kde.py,91,Convert lat/long to radians,not
scikit-learn/examples/neighbors/plot_species_kde.py,93,Set up the data grid for the contour plot,not
scikit-learn/examples/neighbors/plot_species_kde.py,103,Plot map of South America with distributions of each species,not
scikit-learn/examples/neighbors/plot_species_kde.py,110,construct a kernel density estimate of the distribution,not
scikit-learn/examples/neighbors/plot_species_kde.py,116,evaluate only on the land: -9999 indicates ocean,not
scikit-learn/examples/neighbors/plot_species_kde.py,121,plot contours of the density,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,19,import some data to play with,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,21,we only take the first two features. We could avoid this ugly,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,22,slicing by using a two-dim dataset,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,26,step size in the mesh,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,28,Create color maps,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,33,we create an instance of Neighbours Classifier and fit the data.,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,38,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,39,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,46,Put the result into a color plot,not
scikit-learn/examples/neighbors/plot_nearest_centroid.py,51,Plot also the training points,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,19,Author: Tom Dupre la Tour,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,20,,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,21,License: BSD 3 clause,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,35,The transformer computes the nearest neighbors graph using the maximum number,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,36,of neighbors necessary in the grid search. The classifier model filters the,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,37,nearest neighbors graph as required by its own n_neighbors parameter.,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,42,Note that we give `memory` a directory to cache the graph computation,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,43,that will be used several times when tuning the hyperparameters of the,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,44,classifier.,not
scikit-learn/examples/neighbors/plot_caching_nearest_neighbors.py,54,Plot the results of the grid search.,not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,34,Generate train data,not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,38,Generate some outliers,not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,46,fit the model for outlier detection (default),not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,48,use fit_predict to compute the predicted labels of the training samples,not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,49,"(when LOF is used for outlier detection, the estimator has no predict,",not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,50,decision_function and score_samples methods).,not
scikit-learn/examples/neighbors/plot_lof_outlier_detection.py,57,plot circles with radius proportional to the outlier scores,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,32,"Authors: Alexandre Gramfort, Gael Varoquaux",not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,33,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,40,,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,41,Generate sample data,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,46,Mix data,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,47,Mixing matrix,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,49,Generate observations,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,55,Estimate the sources,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,60,,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,61,Plot results,not
scikit-learn/examples/decomposition/plot_ica_vs_pca.py,71,Trick to get legend to work,not
scikit-learn/examples/decomposition/plot_kernel_pca.py,11,Authors: Mathieu Blondel,not
scikit-learn/examples/decomposition/plot_kernel_pca.py,12,Andreas Mueller,not
scikit-learn/examples/decomposition/plot_kernel_pca.py,13,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_kernel_pca.py,31,Plot results,not
scikit-learn/examples/decomposition/plot_kernel_pca.py,48,projection on the first principal component (in the phi space),not
scikit-learn/examples/decomposition/plot_pca_vs_lda.py,39,Percentage of variance explained for each components,not
scikit-learn/examples/decomposition/plot_image_denoising.py,47,SciPy >= 0.16 have face in misc,not
scikit-learn/examples/decomposition/plot_image_denoising.py,53,Convert from uint8 representation with values between 0 and 255 to,not
scikit-learn/examples/decomposition/plot_image_denoising.py,54,a floating point representation with values between 0 and 1.,not
scikit-learn/examples/decomposition/plot_image_denoising.py,57,downsample for higher speed,not
scikit-learn/examples/decomposition/plot_image_denoising.py,62,Distort the right half of the image,not
scikit-learn/examples/decomposition/plot_image_denoising.py,67,Extract all reference patches from the left half of the image,not
scikit-learn/examples/decomposition/plot_image_denoising.py,77,,not
scikit-learn/examples/decomposition/plot_image_denoising.py,78,Learn the dictionary from reference patches,not
scikit-learn/examples/decomposition/plot_image_denoising.py,100,,not
scikit-learn/examples/decomposition/plot_image_denoising.py,101,Display the distorted image,not
scikit-learn/examples/decomposition/plot_image_denoising.py,125,,not
scikit-learn/examples/decomposition/plot_image_denoising.py,126,Extract noisy patches and reconstruct them using the dictionary,not
scikit-learn/examples/decomposition/plot_incremental_pca.py,23,Authors: Kyle Kastner,not
scikit-learn/examples/decomposition/plot_incremental_pca.py,24,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,27,Authors: Alexandre Gramfort,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,28,Denis A. Engemann,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,29,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,42,,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,43,Create the data,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,51,Adding homoscedastic noise,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,54,Adding heteroscedastic noise,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,58,,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,59,Fit the models,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,61,options for n_components,not
scikit-learn/examples/decomposition/plot_pca_vs_fa_model_selection.py,114,compare with other covariance estimators,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,14,"Authors: Vlad Niculae, Alexandre Gramfort",not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,15,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,27,Display progress logs on stdout,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,35,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,36,Load faces data,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,41,global centering,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,44,local centering,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,63,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,64,"List of the different estimators, whether to center and transpose the",not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,65,"problem, and whether the transformer uses the clustering API.",not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,103,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,104,Plot a sample of the input data,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,108,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,109,Do the estimation and plot it,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,125,Plot an image representing the pixelwise variance provided by the,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,126,"estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,",not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,127,"via the PCA decomposition, also provides a scalar noise_variance_",not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,128,(the mean of pixelwise variance) that cannot be displayed as an image,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,129,so we skip it.,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,131,Skip the Eigenfaces case,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,140,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,141,Various positivity constraints applied to dictionary learning.,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,172,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,173,Plot a sample of the input data,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,178,,not
scikit-learn/examples/decomposition/plot_faces_decomposition.py,179,Do the estimation and plot it,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,47,subsampling factor,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,51,Compute a wavelet dictionary,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,58,Generate a signal,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,64,List the different sparse coding methods in the following format:,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,65,"(title, transform_algorithm, transform_alpha,",not
scikit-learn/examples/decomposition/plot_sparse_coding.py,66,"transform_n_nozero_coefs, color)",not
scikit-learn/examples/decomposition/plot_sparse_coding.py,70,Avoid FutureWarning about default value change when numpy >= 1.14,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,79,Do a wavelet approximation,not
scikit-learn/examples/decomposition/plot_sparse_coding.py,91,Soft thresholding debiasing,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,24,,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,25,Generate sample data,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,30,Signal 1 : sinusoidal signal,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,31,Signal 2 : square signal,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,32,Signal 3: saw tooth signal,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,35,Add noise,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,37,Standardize data,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,38,Mix data,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,39,Mixing matrix,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,40,Generate observations,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,42,Compute ICA,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,44,Reconstruct signals,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,45,Get estimated mixing matrix,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,47,We can `prove` that the ICA model applies by reverting the unmixing.,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,50,"For comparison, compute PCA",not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,52,Reconstruct signals based on orthogonal components,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,54,,not
scikit-learn/examples/decomposition/plot_ica_blind_source_separation.py,55,Plot results,not
scikit-learn/examples/decomposition/plot_pca_iris.py,1,!/usr/bin/python,not
scikit-learn/examples/decomposition/plot_pca_iris.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/decomposition/plot_pca_iris.py,18,Code source: Gaël Varoquaux,not
scikit-learn/examples/decomposition/plot_pca_iris.py,19,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_pca_iris.py,51,Reorder the labels to have colors matching the cluster results,not
scikit-learn/examples/decomposition/plot_pca_3d.py,1,!/usr/bin/python,not
scikit-learn/examples/decomposition/plot_pca_3d.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/decomposition/plot_pca_3d.py,16,Authors: Gael Varoquaux,not
scikit-learn/examples/decomposition/plot_pca_3d.py,17,Jaques Grobler,not
scikit-learn/examples/decomposition/plot_pca_3d.py,18,Kevin Hughes,not
scikit-learn/examples/decomposition/plot_pca_3d.py,19,License: BSD 3 clause,not
scikit-learn/examples/decomposition/plot_pca_3d.py,29,,not
scikit-learn/examples/decomposition/plot_pca_3d.py,30,Create the data,not
scikit-learn/examples/decomposition/plot_pca_3d.py,58,,not
scikit-learn/examples/decomposition/plot_pca_3d.py,59,Plot the figures,not
scikit-learn/examples/decomposition/plot_pca_3d.py,68,"Using SciPy's SVD, this would be:",not
scikit-learn/examples/decomposition/plot_pca_3d.py,69,"_, pca_score, Vt = scipy.linalg.svd(Y, full_matrices=False)",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,22,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,23,New plotting API,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,24,----------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,25,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,26,A new plotting API is available for creating visualizations. This new API,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,27,allows for quickly adjusting the visuals of a plot without involving any,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,28,recomputation. It is also possible to add different plots to the same,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,29,"figure. The following example illustrates :class:`~metrics.plot_roc_curve`,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,30,but other plots utilities are supported like,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,31,":class:`~inspection.plot_partial_dependence`,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,32,":class:`~metrics.plot_precision_recall_curve`, and",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,33,:class:`~metrics.plot_confusion_matrix`. Read more about this new API in the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,34,:ref:`User Guide <visualizations>`.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,57,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,58,Stacking Classifier and Regressor,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,59,---------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,60,:class:`~ensemble.StackingClassifier` and,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,61,:class:`~ensemble.StackingRegressor`,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,62,allow you to have a stack of estimators with a final classifier or,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,63,a regressor.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,64,Stacked generalization consists in stacking the output of individual,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,65,estimators and use a classifier to compute the final prediction. Stacking,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,66,allows to use the strength of each individual estimator by using their output,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,67,as input of a final estimator.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,68,Base estimators are fitted on the full ``X`` while,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,69,the final estimator is trained using cross-validated predictions of the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,70,base estimators using ``cross_val_predict``.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,71,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,72,Read more in the :ref:`User Guide <stacking>`.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,96,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,97,Permutation-based feature importance,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,98,------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,99,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,100,The :func:`inspection.permutation_importance` can be used to get an,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,101,"estimate of the importance of each feature, for any fitted estimator:",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,120,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,121,Native support for missing values for gradient boosting,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,122,-------------------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,123,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,124,The :class:`ensemble.HistGradientBoostingClassifier`,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,125,and :class:`ensemble.HistGradientBoostingRegressor` now have native,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,126,support for missing values (NaNs). This means that there is no need for,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,127,imputing data when training or predicting.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,129,noqa,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,139,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,140,Precomputed sparse nearest neighbors graph,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,141,------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,142,Most estimators based on nearest neighbors graphs now accept precomputed,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,143,"sparse graphs as input, to reuse the same graph for multiple estimator fits.",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,144,"To use this feature in a pipeline, one can use the `memory` parameter, along",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,145,"with one of the two new transformers,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,146,:class:`neighbors.KNeighborsTransformer` and,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,147,:class:`neighbors.RadiusNeighborsTransformer`. The precomputation,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,148,can also be performed by custom estimators to use alternative,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,149,"implementations, such as approximate nearest neighbors methods.",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,150,See more details in the :ref:`User Guide <neighbors_transformer>`.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,166,We can decrease the number of neighbors and the graph will not be,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,167,recomputed.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,171,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,172,KNN Based Imputation,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,173,------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,174,We now support imputation for completing missing values using k-Nearest,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,175,Neighbors.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,176,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,177,Each sample's missing values are imputed using the mean value from,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,178,``n_neighbors`` nearest neighbors found in the training set. Two samples are,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,179,close if the features that neither is missing are close.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,180,"By default, a euclidean distance metric",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,181,"that supports missing values,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,182,":func:`~metrics.nan_euclidean_distances`, is used to find the nearest",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,183,neighbors.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,184,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,185,Read more in the :ref:`User Guide <knnimpute>`.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,194,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,195,Tree pruning,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,196,------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,197,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,198,It is now possible to prune most tree-based estimators once the trees are,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,199,built. The pruning is based on minimal cost-complexity. Read more in the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,200,:ref:`User Guide <minimal_cost_complexity_pruning>` for details.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,212,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,213,Retrieve dataframes from OpenML,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,214,-------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,215,:func:`datasets.fetch_openml` can now return pandas dataframe and thus,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,216,properly handle datasets with heterogeneous data:,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,223,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,224,Checking scikit-learn compatibility of an estimator,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,225,---------------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,226,Developers can check the compatibility of their scikit-learn compatible,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,227,estimators using :func:`~utils.estimator_checks.check_estimator`. For,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,228,"instance, the ``check_estimator(LinearSVC())`` passes.",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,229,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,230,We now provide a ``pytest`` specific decorator which allows ``pytest``,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,231,to run all checks independently and report the checks that are failing.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,232,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,233,..note::,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,234,"This entry was slightly updated in version 0.24, where passing classes",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,235,isn't supported anymore: pass instances instead.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,246,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,247,ROC AUC now supports multiclass classification,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,248,----------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,249,The :func:`roc_auc_score` function can also be used in multi-class,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,250,classification. Two averaging strategies are currently supported: the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,251,"one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,252,the one-vs-rest algorithm computes the average of the ROC AUC scores for each,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,253,"class against all other classes. In both cases, the multiclass ROC AUC scores",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,254,are computed from the probability estimates that a sample belongs to a,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,255,particular class according to the model. The OvO and OvR algorithms support,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,256,weighting uniformly (``average='macro'``) and weighting by the prevalence,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,257,(``average='weighted'``).,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,258,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_22_0.py,259,Read more in the :ref:`User Guide <roc_metrics>`.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,1,flake8: noqa,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,23,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,24,"Generalized Linear Models, and Poisson loss for gradient boosting",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,25,-----------------------------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,26,Long-awaited Generalized Linear Models with non-normal loss functions are now,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,27,"available. In particular, three new regressors were implemented:",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,28,":class:`~sklearn.linear_model.PoissonRegressor`,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,29,":class:`~sklearn.linear_model.GammaRegressor`, and",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,30,:class:`~sklearn.linear_model.TweedieRegressor`. The Poisson regressor can be,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,31,"used to model positive integer counts, or relative frequencies. Read more in",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,32,"the :ref:`User Guide <Generalized_linear_regression>`. Additionally,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,33,:class:`~sklearn.ensemble.HistGradientBoostingRegressor` supports a new,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,34,'poisson' loss as well.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,39,noqa,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,45,"positive integer target correlated with X[:, 5] with many zeros:",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,55,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,56,Rich visual representation of estimators,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,57,-----------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,58,Estimators can now be visualized in notebooks by enabling the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,59,`display='diagram'` option. This is particularly useful to summarise the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,60,"structure of pipelines and other composite estimators, with interactivity to",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,61,provide detail.  Click on the example image below to expand Pipeline,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,62,elements.  See :ref:`visualizing_composite_estimators` for how you can use,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,63,this feature.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,85,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,86,Scalability and stability improvements to KMeans,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,87,------------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,88,"The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,89,"is now significantly faster and more stable. In addition, the Elkan algorithm",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,90,is now compatible with sparse matrices. The estimator uses OpenMP based,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,91,"parallelism instead of relying on joblib, so the `n_jobs` parameter has no",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,92,"effect anymore. For more details on how to control the number of threads,",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,93,please refer to our :ref:`parallelism` notes.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,108,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,109,Improvements to the histogram-based Gradient Boosting estimators,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,110,----------------------------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,111,Various improvements were made to,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,112,:class:`~sklearn.ensemble.HistGradientBoostingClassifier` and,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,113,:class:`~sklearn.ensemble.HistGradientBoostingRegressor`. On top of the,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,114,"Poisson loss mentionned above, these estimators now support :ref:`sample",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,115,"weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,116,early-stopping is enabled by default when the number of samples exceeds 10k.,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,117,"Finally, users can now define :ref:`monotonic constraints",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,118,<monotonic_cst_gbdt>` to constrain the predictions based on the variations of,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,119,"specific features. In the following example, we construct a target that is",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,120,"generally positively correlated with the first feature, with some noise.",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,121,Applying monotoinc constraints allows the prediction to capture the global,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,122,"effect of the first feature, instead of fitting the noise.",not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,127,noqa,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,149,,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,150,Sample-weight support for Lasso and ElasticNet,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,151,----------------------------------------------,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,152,The two linear regressors :class:`~sklearn.linear_model.Lasso` and,not
scikit-learn/examples/release_highlights/plot_release_highlights_0_23_0.py,153,:class:`~sklearn.linear_model.ElasticNet` now support sample weights.,not
scikit-learn/examples/text/plot_document_clustering.py,52,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/examples/text/plot_document_clustering.py,53,Lars Buitinck,not
scikit-learn/examples/text/plot_document_clustering.py,54,License: BSD 3 clause,not
scikit-learn/examples/text/plot_document_clustering.py,74,Display progress logs on stdout,not
scikit-learn/examples/text/plot_document_clustering.py,78,parse commandline arguments,not
scikit-learn/examples/text/plot_document_clustering.py,107,work-around for Jupyter notebook and IPython console,SATD
scikit-learn/examples/text/plot_document_clustering.py,115,,not
scikit-learn/examples/text/plot_document_clustering.py,116,Load some categories from the training set,not
scikit-learn/examples/text/plot_document_clustering.py,123,Uncomment the following to do the analysis on all the categories,not
scikit-learn/examples/text/plot_document_clustering.py,124,categories = None,not
scikit-learn/examples/text/plot_document_clustering.py,144,Perform an IDF normalization on the output of HashingVectorizer,not
scikit-learn/examples/text/plot_document_clustering.py,166,"Vectorizer results are normalized, which makes KMeans behave as",not
scikit-learn/examples/text/plot_document_clustering.py,167,spherical k-means for better results. Since LSA/SVD results are,SATD
scikit-learn/examples/text/plot_document_clustering.py,168,"not normalized, we have to redo the normalization.",not
scikit-learn/examples/text/plot_document_clustering.py,184,,not
scikit-learn/examples/text/plot_document_clustering.py,185,Do the actual clustering,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,16,Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,17,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,18,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,19,Lars Buitinck,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,20,License: BSD 3 clause,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,47,Display progress logs on stdout,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,84,work-around for Jupyter notebook and IPython console,SATD
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,96,,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,97,Load data from the training set,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,98,------------------------------------,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,99,Let's load data from the newsgroups dataset which comprises around 18000,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,100,newsgroups posts on 20 topics split in two subsets: one for training (or,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,101,development) and the other one for testing (or for performance evaluation).,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,129,order of labels in `target_names` can be different from `categories`,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,147,split a training set and a test set,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,173,mapping from integer feature name to original token string,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,187,keep selected feature names,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,202,,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,203,Benchmark classifiers,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,204,------------------------------------,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,205,We train and test the datasets with 15 different classification models,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,206,and get performance results for each model.,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,264,Train Liblinear model,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,268,Train SGD model,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,272,Train SGD with Elastic Net penalty,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,278,Train NearestCentroid without threshold,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,283,Train sparse Naive Bayes classifiers,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,292,"The smaller C, the stronger the regularization.",not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,293,"The more regularization, the more sparsity.",not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,300,,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,301,Add plots,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,302,------------------------------------,not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,303,"The bar plot indicates the accuracy, training time (normalized) and test time",not
scikit-learn/examples/text/plot_document_classification_20newsgroups.py,304,(normalized) of each classifier.,not
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,18,Author: Lars Buitinck,not
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,19,License: BSD 3 clause,not
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,62,Uncomment the following line to use a larger set (11k+ documents),not
scikit-learn/examples/text/plot_hashing_vs_dict_vectorizer.py,63,categories = None,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,27,,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,28,Dataset based latent variables model,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,31,2 latents vars:,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,49,,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,50,Canonical (symmetric) PLS,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,52,Transform data,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,53,~~~~~~~~~~~~~~,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,59,Scatter plot of scores,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,60,~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,61,1) On diagonal plot X vs Y scores on each components,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,89,2) Off diagonal plot components 1 vs 2 for X and Y,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,117,,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,118,"PLS regression, with multivariate response, a.k.a. PLS2",not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,125,each Yj = 1*X1 + 2*X2 + noize,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,132,compare pls2.coef_ with B,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,137,"PLS regression, with univariate response, a.k.a. PLS1",not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,145,note that the number of components exceeds 1 (the dimension of y),not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,149,,not
scikit-learn/examples/cross_decomposition/plot_compare_cross_decomposition.py,150,CCA (PLS mode B with symmetric deflation),not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,45,"To use this experimental feature, we need to explicitly ask for it:",not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,46,noqa,not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,62,~2k samples is enough for the purpose of the example.,not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,63,Remove the following two lines for a slower run with different error bars.,not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,68,"Estimate the score on the entire dataset, with no missing values",not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,78,Add a single missing value to each row,not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,85,Estimate the score after imputation (mean and median strategies),not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,97,Estimate the score after iterative imputation of the missing values,not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,98,with different estimators,not
scikit-learn/examples/impute/plot_iterative_imputer_variants_comparison.py,122,plot california housing results,not
scikit-learn/examples/impute/plot_missing_values.py,32,Authors: Maria Telenczuk  <https://github.com/maikia>,not
scikit-learn/examples/impute/plot_missing_values.py,33,License: BSD 3 clause,not
scikit-learn/examples/impute/plot_missing_values.py,35,,not
scikit-learn/examples/impute/plot_missing_values.py,36,Download the data and make missing values sets,not
scikit-learn/examples/impute/plot_missing_values.py,37,,not
scikit-learn/examples/impute/plot_missing_values.py,38,,not
scikit-learn/examples/impute/plot_missing_values.py,39,First we download the two datasets. Diabetes dataset is shipped with,not
scikit-learn/examples/impute/plot_missing_values.py,40,"scikit-learn. It has 442 entries, each with 10 features. California Housing",not
scikit-learn/examples/impute/plot_missing_values.py,41,dataset is much larger with 20640 entries and 8 features. It needs to be,not
scikit-learn/examples/impute/plot_missing_values.py,42,downloaded. We will only use the first 400 entries for the sake of speeding,not
scikit-learn/examples/impute/plot_missing_values.py,43,up the calculations but feel free to use the whole dataset.,not
scikit-learn/examples/impute/plot_missing_values.py,44,,not
scikit-learn/examples/impute/plot_missing_values.py,63,Add missing values in 75% of the lines,not
scikit-learn/examples/impute/plot_missing_values.py,86,,not
scikit-learn/examples/impute/plot_missing_values.py,87,Impute the missing data and score,not
scikit-learn/examples/impute/plot_missing_values.py,88,,not
scikit-learn/examples/impute/plot_missing_values.py,89,Now we will write a function which will score the results on the differently,not
scikit-learn/examples/impute/plot_missing_values.py,90,imputed data. Let's look at each imputer separately:,not
scikit-learn/examples/impute/plot_missing_values.py,91,,not
scikit-learn/examples/impute/plot_missing_values.py,97,"To use the experimental IterativeImputer, we need to explicitly ask for it:",not
scikit-learn/examples/impute/plot_missing_values.py,98,noqa,not
scikit-learn/examples/impute/plot_missing_values.py,107,,not
scikit-learn/examples/impute/plot_missing_values.py,108,Missing information,not
scikit-learn/examples/impute/plot_missing_values.py,109,-------------------,not
scikit-learn/examples/impute/plot_missing_values.py,110,"In addition to imputing the missing values, the imputers have an",not
scikit-learn/examples/impute/plot_missing_values.py,111,"`add_indicator` parameter that marks the values that were missing, which",not
scikit-learn/examples/impute/plot_missing_values.py,112,might carry some information.,not
scikit-learn/examples/impute/plot_missing_values.py,113,,not
scikit-learn/examples/impute/plot_missing_values.py,135,,not
scikit-learn/examples/impute/plot_missing_values.py,136,Estimate the score,not
scikit-learn/examples/impute/plot_missing_values.py,137,------------------,not
scikit-learn/examples/impute/plot_missing_values.py,138,"First, we want to estimate the score on the original data:",not
scikit-learn/examples/impute/plot_missing_values.py,139,,not
scikit-learn/examples/impute/plot_missing_values.py,154,,not
scikit-learn/examples/impute/plot_missing_values.py,155,Replace missing values by 0,not
scikit-learn/examples/impute/plot_missing_values.py,156,---------------------------,not
scikit-learn/examples/impute/plot_missing_values.py,157,,not
scikit-learn/examples/impute/plot_missing_values.py,158,Now we will estimate the score on the data where the missing values are,not
scikit-learn/examples/impute/plot_missing_values.py,159,replaced by 0:,not
scikit-learn/examples/impute/plot_missing_values.py,160,,not
scikit-learn/examples/impute/plot_missing_values.py,177,,not
scikit-learn/examples/impute/plot_missing_values.py,178,kNN-imputation of the missing values,not
scikit-learn/examples/impute/plot_missing_values.py,179,------------------------------------,not
scikit-learn/examples/impute/plot_missing_values.py,180,,not
scikit-learn/examples/impute/plot_missing_values.py,181,:class:`sklearn.impute.KNNImputer` imputes missing values using the weighted,not
scikit-learn/examples/impute/plot_missing_values.py,182,or unweighted mean of the desired number of nearest neighbors.,not
scikit-learn/examples/impute/plot_missing_values.py,196,,not
scikit-learn/examples/impute/plot_missing_values.py,197,Impute missing values with mean,not
scikit-learn/examples/impute/plot_missing_values.py,198,-------------------------------,not
scikit-learn/examples/impute/plot_missing_values.py,199,,not
scikit-learn/examples/impute/plot_missing_values.py,214,,not
scikit-learn/examples/impute/plot_missing_values.py,215,Iterative imputation of the missing values,not
scikit-learn/examples/impute/plot_missing_values.py,216,------------------------------------------,not
scikit-learn/examples/impute/plot_missing_values.py,217,,not
scikit-learn/examples/impute/plot_missing_values.py,218,Another option is the :class:`sklearn.impute.IterativeImputer`. This uses,not
scikit-learn/examples/impute/plot_missing_values.py,219,"round-robin linear regression, modeling each feature with missing values as a",not
scikit-learn/examples/impute/plot_missing_values.py,220,"function of other features, in turn.",not
scikit-learn/examples/impute/plot_missing_values.py,221,The version implemented assumes Gaussian (output) variables. If your features,not
scikit-learn/examples/impute/plot_missing_values.py,222,"are obviously non-normal, consider transforming them to look more normal",not
scikit-learn/examples/impute/plot_missing_values.py,223,to potentially improve performance.,not
scikit-learn/examples/impute/plot_missing_values.py,224,,not
scikit-learn/examples/impute/plot_missing_values.py,244,,not
scikit-learn/examples/impute/plot_missing_values.py,245,Plot the results,not
scikit-learn/examples/impute/plot_missing_values.py,246,,not
scikit-learn/examples/impute/plot_missing_values.py,247,,not
scikit-learn/examples/impute/plot_missing_values.py,248,Finally we are going to visualize the score:,not
scikit-learn/examples/impute/plot_missing_values.py,249,,not
scikit-learn/examples/impute/plot_missing_values.py,259,plot diabetes results,not
scikit-learn/examples/impute/plot_missing_values.py,274,plot california dataset results,not
scikit-learn/examples/impute/plot_missing_values.py,288,"You can also try different techniques. For instance, the median is a more",not
scikit-learn/examples/impute/plot_missing_values.py,289,robust estimator for data with high magnitude variables which could dominate,not
scikit-learn/examples/impute/plot_missing_values.py,290,results (otherwise known as a 'long tail').,not
scikit-learn/examples/compose/plot_column_transformer.py,26,Author: Matt Terry <matt.terry@gmail.com>,not
scikit-learn/examples/compose/plot_column_transformer.py,27,,not
scikit-learn/examples/compose/plot_column_transformer.py,28,License: BSD 3 clause,not
scikit-learn/examples/compose/plot_column_transformer.py,65,construct object dtype array with two columns,not
scikit-learn/examples/compose/plot_column_transformer.py,66,first column = 'subject' and second column = 'body',not
scikit-learn/examples/compose/plot_column_transformer.py,84,Extract the subject & body,not
scikit-learn/examples/compose/plot_column_transformer.py,87,Use ColumnTransformer to combine the features from subject and body,not
scikit-learn/examples/compose/plot_column_transformer.py,90,Pulling features from the post's subject line (first column),not
scikit-learn/examples/compose/plot_column_transformer.py,93,Pipeline for standard bag-of-words model for body (second column),not
scikit-learn/examples/compose/plot_column_transformer.py,99,Pipeline for pulling ad hoc features from post's body,not
scikit-learn/examples/compose/plot_column_transformer.py,101,returns a list of dicts,not
scikit-learn/examples/compose/plot_column_transformer.py,102,list of dicts -> feature matrix,not
scikit-learn/examples/compose/plot_column_transformer.py,106,weight components in ColumnTransformer,not
scikit-learn/examples/compose/plot_column_transformer.py,114,Use a SVC classifier on the combined features,not
scikit-learn/examples/compose/plot_column_transformer.py,118,limit the list of categories to make running this example faster.,not
scikit-learn/examples/compose/plot_feature_union.py,18,Author: Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/examples/compose/plot_feature_union.py,19,,not
scikit-learn/examples/compose/plot_feature_union.py,20,License: BSD 3 clause,not
scikit-learn/examples/compose/plot_feature_union.py,33,This dataset is way too high-dimensional. Better do PCA:,not
scikit-learn/examples/compose/plot_feature_union.py,36,"Maybe some original features where good, too?",not
scikit-learn/examples/compose/plot_feature_union.py,39,Build estimator from PCA and Univariate selection:,not
scikit-learn/examples/compose/plot_feature_union.py,43,Use combined features to transform dataset:,not
scikit-learn/examples/compose/plot_feature_union.py,49,"Do grid search over k, n_components and C:",not
scikit-learn/examples/compose/plot_compare_reduction.py,1,!/usr/bin/env python,not
scikit-learn/examples/compose/plot_compare_reduction.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/compose/plot_compare_reduction.py,30,"Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre",not
scikit-learn/examples/compose/plot_compare_reduction.py,45,the reduce_dim stage is populated by the param_grid,not
scikit-learn/examples/compose/plot_compare_reduction.py,71,"scores are in the order of param_grid iteration, which is alphabetical",not
scikit-learn/examples/compose/plot_compare_reduction.py,73,select score for best C,not
scikit-learn/examples/compose/plot_compare_reduction.py,92,,not
scikit-learn/examples/compose/plot_compare_reduction.py,93,Caching transformers within a ``Pipeline``,not
scikit-learn/examples/compose/plot_compare_reduction.py,94,,not
scikit-learn/examples/compose/plot_compare_reduction.py,95,It is sometimes worthwhile storing the state of a specific transformer,not
scikit-learn/examples/compose/plot_compare_reduction.py,96,since it could be used again. Using a pipeline in ``GridSearchCV`` triggers,not
scikit-learn/examples/compose/plot_compare_reduction.py,97,"such situations. Therefore, we use the argument ``memory`` to enable caching.",not
scikit-learn/examples/compose/plot_compare_reduction.py,98,,not
scikit-learn/examples/compose/plot_compare_reduction.py,99,.. warning::,not
scikit-learn/examples/compose/plot_compare_reduction.py,100,"Note that this example is, however, only an illustration since for this",not
scikit-learn/examples/compose/plot_compare_reduction.py,101,specific case fitting PCA is not necessarily slower than loading the,not
scikit-learn/examples/compose/plot_compare_reduction.py,102,"cache. Hence, use the ``memory`` constructor parameter when the fitting",not
scikit-learn/examples/compose/plot_compare_reduction.py,103,of a transformer is costly.,not
scikit-learn/examples/compose/plot_compare_reduction.py,108,Create a temporary folder to store the transformers of the pipeline,not
scikit-learn/examples/compose/plot_compare_reduction.py,115,"This time, a cached pipeline will be used within the grid search",not
scikit-learn/examples/compose/plot_compare_reduction.py,118,Delete the temporary cache before exiting,not
scikit-learn/examples/compose/plot_compare_reduction.py,122,,not
scikit-learn/examples/compose/plot_compare_reduction.py,123,The ``PCA`` fitting is only computed at the evaluation of the first,not
scikit-learn/examples/compose/plot_compare_reduction.py,124,configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The,not
scikit-learn/examples/compose/plot_compare_reduction.py,125,other configurations of ``C`` will trigger the loading of the cached ``PCA``,not
scikit-learn/examples/compose/plot_compare_reduction.py,126,"estimator data, leading to save processing time. Therefore, the use of",not
scikit-learn/examples/compose/plot_compare_reduction.py,127,caching the pipeline using ``memory`` is highly beneficial when fitting,not
scikit-learn/examples/compose/plot_compare_reduction.py,128,a transformer is costly.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,24,Author: Pedro Morales <part.morales@gmail.com>,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,25,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,26,License: BSD 3 clause,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,40,Load data from https://www.openml.org/d/40945,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,43,Alternatively X and y can be obtained directly from the frame attribute:,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,44,"X = titanic.frame.drop('survived', axis=1)",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,45,y = titanic.frame['survived'],not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,47,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,48,Use ``ColumnTransformer`` by selecting column by names,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,49,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,50,We will train our classifier with the following features:,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,51,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,52,Numeric Features:,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,53,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,54,* ``age``: float;,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,55,* ``fare``: float.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,56,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,57,Categorical Features:,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,58,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,59,"* ``embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,60,"* ``sex``: categories encoded as strings ``{'female', 'male'}``;",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,61,"* ``pclass``: ordinal integers ``{1, 2, 3}``.",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,62,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,63,We create the preprocessing pipelines for both numeric and categorical data.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,80,Append classifier to preprocessing pipeline.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,81,Now we have a full prediction pipeline.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,90,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,91,HTML representation of ``Pipeline``,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,92,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,93,When the ``Pipeline`` is printed out in a jupyter notebook an HTML,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,94,representation of the estimator is displayed as follows:,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,99,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,100,Use ``ColumnTransformer`` by selecting column by data types,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,101,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,102,"When dealing with a cleaned dataset, the preprocessing can be automatic by",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,103,using the data types of the column to decide whether to treat a column as a,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,104,numerical or categorical feature.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,105,:func:`sklearn.compose.make_column_selector` gives this possibility.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,106,"First, let's only select a subset of columns to simplify our",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,107,example.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,112,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,113,"Then, we introspect the information regarding each column data type.",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,117,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,118,We can observe that the `embarked` and `sex` columns were tagged as,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,119,"`category` columns when loading the data with ``fetch_openml``. Therefore, we",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,120,can use this information to dispatch the categorical columns to the,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,121,``categorical_transformer`` and the remaining columns to the,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,122,``numerical_transformer``.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,124,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,125,".. note:: In practice, you will have to handle yourself the column data type.",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,126,"If you want some columns to be considered as `category`, you will have to",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,127,"convert them into categorical columns. If you are using pandas, you can",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,128,refer to their documentation regarding `Categorical data,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,129,<https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`_.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,138,Reproduce the identical fit/score process,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,144,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,145,Using the prediction pipeline in a grid search,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,146,,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,147,Grid search can also be performed on the different preprocessing steps,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,148,"defined in the ``ColumnTransformer`` object, together with the classifier's",not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,149,hyperparameters as part of the ``Pipeline``.,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,150,We will search for both the imputer strategy of the numeric preprocessing,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,151,and the regularization parameter of the logistic regression using,not
scikit-learn/examples/compose/plot_column_transformer_mixed_types.py,152,:class:`sklearn.model_selection.GridSearchCV`.,not
scikit-learn/examples/compose/plot_transformed_target.py,1,!/usr/bin/env python,not
scikit-learn/examples/compose/plot_transformed_target.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/compose/plot_transformed_target.py,17,Author: Guillaume Lemaitre <guillaume.lemaitre@inria.fr>,not
scikit-learn/examples/compose/plot_transformed_target.py,18,License: BSD 3 clause,not
scikit-learn/examples/compose/plot_transformed_target.py,28,,not
scikit-learn/examples/compose/plot_transformed_target.py,29,Synthetic example,not
scikit-learn/examples/compose/plot_transformed_target.py,30,,not
scikit-learn/examples/compose/plot_transformed_target.py,39,`normed` is being deprecated in favor of `density` in histograms,not
scikit-learn/examples/compose/plot_transformed_target.py,45,,not
scikit-learn/examples/compose/plot_transformed_target.py,46,A synthetic random regression problem is generated. The targets ``y`` are,not
scikit-learn/examples/compose/plot_transformed_target.py,47,modified by: (i) translating all targets such that all entries are,not
scikit-learn/examples/compose/plot_transformed_target.py,48,non-negative and (ii) applying an exponential function to obtain non-linear,not
scikit-learn/examples/compose/plot_transformed_target.py,49,targets which cannot be fitted using a simple linear model.,not
scikit-learn/examples/compose/plot_transformed_target.py,50,,not
scikit-learn/examples/compose/plot_transformed_target.py,51,"Therefore, a logarithmic (`np.log1p`) and an exponential function",not
scikit-learn/examples/compose/plot_transformed_target.py,52,(`np.expm1`) will be used to transform the targets before training a linear,not
scikit-learn/examples/compose/plot_transformed_target.py,53,regression model and using it for prediction.,not
scikit-learn/examples/compose/plot_transformed_target.py,59,,not
scikit-learn/examples/compose/plot_transformed_target.py,60,The following illustrate the probability density functions of the target,not
scikit-learn/examples/compose/plot_transformed_target.py,61,before and after applying the logarithmic functions.,not
scikit-learn/examples/compose/plot_transformed_target.py,81,,not
scikit-learn/examples/compose/plot_transformed_target.py,82,"At first, a linear model will be applied on the original targets. Due to the",not
scikit-learn/examples/compose/plot_transformed_target.py,83,"non-linearity, the model trained will not be precise during the",not
scikit-learn/examples/compose/plot_transformed_target.py,84,"prediction. Subsequently, a logarithmic function is used to linearize the",not
scikit-learn/examples/compose/plot_transformed_target.py,85,"targets, allowing better prediction even with a similar linear model as",SATD
scikit-learn/examples/compose/plot_transformed_target.py,86,reported by the median absolute error (MAE).,not
scikit-learn/examples/compose/plot_transformed_target.py,123,,not
scikit-learn/examples/compose/plot_transformed_target.py,124,Real-world data set,not
scikit-learn/examples/compose/plot_transformed_target.py,125,,not
scikit-learn/examples/compose/plot_transformed_target.py,127,,not
scikit-learn/examples/compose/plot_transformed_target.py,128,"In a similar manner, the boston housing data set is used to show the impact",not
scikit-learn/examples/compose/plot_transformed_target.py,129,"of transforming the targets before learning a model. In this example, the",not
scikit-learn/examples/compose/plot_transformed_target.py,130,targets to be predicted corresponds to the weighted distances to the five,not
scikit-learn/examples/compose/plot_transformed_target.py,131,Boston employment centers.,not
scikit-learn/examples/compose/plot_transformed_target.py,145,,not
scikit-learn/examples/compose/plot_transformed_target.py,146,A :class:`sklearn.preprocessing.QuantileTransformer` is used such that the,not
scikit-learn/examples/compose/plot_transformed_target.py,147,targets follows a normal distribution before applying a,not
scikit-learn/examples/compose/plot_transformed_target.py,148,:class:`sklearn.linear_model.RidgeCV` model.,not
scikit-learn/examples/compose/plot_transformed_target.py,167,,not
scikit-learn/examples/compose/plot_transformed_target.py,168,"The effect of the transformer is weaker than on the synthetic data. However,",not
scikit-learn/examples/compose/plot_transformed_target.py,169,the transform induces a decrease of the MAE.,not
scikit-learn/examples/compose/plot_digits_pipe.py,1,!/usr/bin/python,not
scikit-learn/examples/compose/plot_digits_pipe.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/compose/plot_digits_pipe.py,18,Code source: Gaël Varoquaux,not
scikit-learn/examples/compose/plot_digits_pipe.py,19,Modified for documentation by Jaques Grobler,not
scikit-learn/examples/compose/plot_digits_pipe.py,20,License: BSD 3 clause,not
scikit-learn/examples/compose/plot_digits_pipe.py,34,Define a pipeline to search for the best combination of PCA truncation,SATD
scikit-learn/examples/compose/plot_digits_pipe.py,35,and classifier regularization.,not
scikit-learn/examples/compose/plot_digits_pipe.py,37,set the tolerance to a large value to make the example faster,not
scikit-learn/examples/compose/plot_digits_pipe.py,43,Parameters of pipelines can be set using ‘__’ separated parameter names:,not
scikit-learn/examples/compose/plot_digits_pipe.py,53,Plot the PCA spectrum,not
scikit-learn/examples/compose/plot_digits_pipe.py,65,"For each number of components, find the best classifier results",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,32,,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,33,"Python package and dataset imports, load dataset",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,34,---------------------------------------------------,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,37,Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,38,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,39,License: BSD 3 clause,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,43,Standard scientific Python imports,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,48,"Import datasets, classifiers and performance metrics",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,54,The digits dataset,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,58,,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,59,Timing and accuracy plots,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,60,--------------------------------------------------,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,61,"To apply an classifier on this data, we need to flatten the image, to",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,62,"turn the data in a (samples, feature) matrix:",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,67,We learn the digits on the first half of the digits,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,72,Now predict the value of the digit on the second half:,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,75,data_test = scaler.transform(data_test),not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,77,Create a classifier: a support vector classifier,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,81,create pipeline from kernel approximation,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,82,and linear svm,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,91,fit and predict using linear and kernel svm:,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,125,plot the results:,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,128,second y axis for timings,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,139,horizontal lines for exact rbf and linear kernels:,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,150,vertical line for dataset dimensionality = 64,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,153,legends and labels,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,168,,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,169,Decision Surfaces of RBF Kernel SVM and Linear SVM,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,170,--------------------------------------------------------,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,171,The second plot visualized the decision surfaces of the RBF kernel SVM and,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,172,the linear SVM with approximate kernel maps.,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,173,The plot shows decision surfaces of the classifiers projected onto,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,174,the first two principal components of the data. This visualization should,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,175,be taken with a grain of salt since it is just an interesting slice through,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,176,the decision surface in 64 dimensions. In particular note that,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,177,a datapoint (represented as a dot) does not necessarily be classified,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,178,"into the region it is lying in, since it will not lie on the plane",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,179,that the first two principal components span.,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,180,The usage of :class:`RBFSampler` and :class:`Nystroem` is described in detail,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,181,in :ref:`kernel_approximation`.,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,183,"visualize the decision surface, projected down to the first",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,184,two principal components of the dataset,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,189,Generate grid along first two principal components,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,191,steps along first component,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,193,steps along second component,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,195,combine,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,199,title for the plots,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,208,predict and plot,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,211,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,212,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,216,Put the result into a color plot,not
scikit-learn/examples/miscellaneous/plot_kernel_approximation.py,221,Plot also the training points,not
scikit-learn/examples/miscellaneous/plot_changed_only_pprint_parameter.py,21,"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,",not
scikit-learn/examples/miscellaneous/plot_changed_only_pprint_parameter.py,22,"intercept_scaling=1, l1_ratio=None, max_iter=100,",not
scikit-learn/examples/miscellaneous/plot_changed_only_pprint_parameter.py,23,"multi_class='auto', n_jobs=None, penalty='l1',",not
scikit-learn/examples/miscellaneous/plot_changed_only_pprint_parameter.py,24,"random_state=None, solver='warn', tol=0.0001, verbose=0,",not
scikit-learn/examples/miscellaneous/plot_changed_only_pprint_parameter.py,25,warm_start=False),not
scikit-learn/examples/miscellaneous/plot_changed_only_pprint_parameter.py,30,LogisticRegression(penalty='l1'),not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,16,Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,17,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,18,License: BSD,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,33,,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,34,Fit IsotonicRegression and LinearRegression models,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,41,x needs to be 2d for LinearRegression,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,43,,not
scikit-learn/examples/miscellaneous/plot_isotonic_regression.py,44,Plot result,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,15,noqa,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,28,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,29,Train models on the diabetes dataset,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,30,================================================,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,31,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,32,"First, we train a decision tree and a multi-layer perceptron on the diabetes",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,33,dataset.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,46,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,47,Plotting partial dependence for two features,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,48,============================================,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,49,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,50,"We plot partial dependence curves for features ""age"" and ""bmi"" (body mass",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,51,"index) for the decision tree. With two features,",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,52,:func:`~sklearn.inspection.plot_partial_dependence` expects to plot two,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,53,curves. Here the plot function place a grid of two plots using the space,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,54,defined by `ax` .,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,59,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,60,The partial depdendence curves can be plotted for the multi-layer perceptron.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,61,"In this case, `line_kw` is passed to",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,62,:func:`~sklearn.inspection.plot_partial_dependence` to change the color of,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,63,the curve.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,69,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,70,Plotting partial dependence of the two models together,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,71,======================================================,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,72,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,73,The `tree_disp` and `mlp_disp`,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,74,:class:`~sklearn.inspection.PartialDependenceDisplay` objects contain all the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,75,computed information needed to recreate the partial dependence curves. This,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,76,means we can easily create additional plots without needing to recompute the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,77,curves.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,78,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,79,"One way to plot the curves is to place them in the same figure, with the",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,80,"curves of each model on each row. First, we create a figure with two axes",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,81,within two rows and one column. The two axes are passed to the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,82,:func:`~sklearn.inspection.PartialDependenceDisplay.plot` functions of,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,83,`tree_disp` and `mlp_disp`. The given axes will be used by the plotting,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,84,function to draw the partial dependence. The resulting plot places the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,85,decision tree partial dependence curves in the first row of the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,86,multi-layer perceptron in the second row.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,94,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,95,"Another way to compare the curves is to plot them on top of each other. Here,",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,96,we create a figure with one row and two columns. The axes are passed into the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,97,":func:`~sklearn.inspection.PartialDependenceDisplay.plot` function as a list,",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,98,which will plot the partial dependence curves of each model on the same axes.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,99,The length of the axes list must be equal to the number of plots drawn.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,101,sphinx_gallery_thumbnail_number = 4,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,109,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,110,`tree_disp.axes_` is a numpy array container the axes used to draw the,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,111,partial dependence plots. This can be passed to `mlp_disp` to have the same,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,112,"affect of drawing the plots on top of each other. Furthermore, the",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,113,"`mlp_disp.figure_` stores the figure, which allows for resizing the figure",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,114,"after calling `plot`. In this case `tree_disp.axes_` has two dimensions, thus",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,115,`plot` will only show the y label and y ticks on the left most plot.,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,125,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,126,Plotting partial dependence for one feature,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,127,===========================================,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,128,,not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,129,"Here, we plot the partial dependence curves for a single feature, ""age"", on",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,130,"the same axes. In this case, `tree_disp.axes_` is passed into the second",not
scikit-learn/examples/miscellaneous/plot_partial_dependence_visualization_api.py,131,plot function.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,29,`normed` is being deprecated in favor of `density` in histograms,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,35,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,36,Theoretical bounds,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,37,==================,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,38,The distortion introduced by a random projection `p` is asserted by,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,39,the fact that `p` is defining an eps-embedding with good probability,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,40,as defined by:,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,41,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,42,.. math::,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,43,(1 - eps) \|u - v\|^2 < \|p(u) - p(v)\|^2 < (1 + eps) \|u - v\|^2,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,44,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,45,"Where u and v are any rows taken from a dataset of shape [n_samples,",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,46,"n_features] and p is a projection by a random Gaussian N(0, 1) matrix",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,47,"with shape [n_components, n_features] (or a sparse Achlioptas matrix).",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,48,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,49,The minimum number of components to guarantees the eps-embedding is,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,50,given by:,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,51,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,52,.. math::,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,53,n\_components >= 4 log(n\_samples) / (eps^2 / 2 - eps^3 / 3),not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,54,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,55,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,56,"The first plot shows that with an increasing number of samples ``n_samples``,",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,57,the minimal number of dimensions ``n_components`` increased logarithmically,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,58,in order to guarantee an ``eps``-embedding.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,60,range of admissible distortions,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,64,range of number of samples (observation) to embed,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,79,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,80,The second plot shows that an increase of the admissible,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,81,distortion ``eps`` allows to reduce drastically the minimal number of,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,82,dimensions ``n_components`` for a given number of samples ``n_samples``,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,84,range of admissible distortions,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,87,range of number of samples (observation) to embed,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,102,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,103,Empirical validation,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,104,====================,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,105,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,106,We validate the above bounds on the 20 newsgroups text document,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,107,(TF-IDF word frequencies) dataset or on the digits dataset:,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,108,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,109,- for the 20 newsgroups dataset some 500 documents with 100k,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,110,features in total are projected using a sparse random matrix to smaller,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,111,euclidean spaces with various values for the target number of dimensions,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,112,``n_components``.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,113,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,114,"- for the digits dataset, some 8x8 gray level pixels data for 500",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,115,handwritten digits pictures are randomly projected to spaces for various,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,116,larger number of dimensions ``n_components``.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,117,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,118,The default dataset is the 20 newsgroups dataset. To run the example on the,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,119,"digits dataset, pass the ``--use-digits-dataset`` command line argument to",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,120,this script.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,127,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,128,"For each value of ``n_components``, we plot:",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,129,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,130,- 2D distribution of sample pairs with pairwise distances in original,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,131,and projected spaces as x and y axis respectively.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,132,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,133,- 1D histogram of the ratio of those distances (projected / original).,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,142,select only non-identical samples pairs,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,183,TODO: compute the expected value of eps and add them to the previous plot,SATD
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,184,as vertical lines / region,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,189,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,190,We can see that for low values of ``n_components`` the distribution is wide,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,191,with many distorted pairs and a skewed distribution (due to the hard,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,192,limit of zero ratio on the left as distances are always positives),not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,193,while for larger values of n_components the distortion is controlled,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,194,and the distances are well preserved by the random projection.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,197,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,198,Remarks,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,199,=======,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,200,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,201,"According to the JL lemma, projecting 500 samples without too much distortion",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,202,"will require at least several thousands dimensions, irrespective of the",not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,203,number of features of the original dataset.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,204,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,205,Hence using random projections on the digits dataset which only has 64,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,206,features in the input space does not make sense: it does not allow,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,207,for dimensionality reduction in this case.,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,208,,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,209,On the twenty newsgroups on the other hand the dimensionality can be,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,210,decreased from 56436 down to 10000 while reasonably preserving,not
scikit-learn/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py,211,pairwise distances.,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,27,Load the faces datasets,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,31,Test on independent people,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,33,Test on a subset of people,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,40,Upper half of the faces,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,42,Lower half of the faces,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,47,Fit estimators,not
scikit-learn/examples/miscellaneous/plot_multioutput_face_completion.py,61,Plot the completed faces,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,18,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,19,Load Data and train model,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,20,-------------------------,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,21,"For this example, we load a blood transfusion service center data set from",not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,22,`OpenML <https://www.openml.org/d/1464>`. This is a binary classification,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,23,problem where the target is whether an individual donated blood. Then the,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,24,data is split into a train and test dataset and a logistic regression is,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,25,fitted wtih the train dataset.,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,38,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,39,Create :class:`ConfusionMatrixDisplay`,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,40,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,41,"With the fitted model, we compute the predictions of the model on the test",not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,42,dataset. These predictions are used to compute the confustion matrix which,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,43,is plotted with the :class:`ConfusionMatrixDisplay`,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,53,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,54,Create :class:`RocCurveDisplay`,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,55,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,56,The roc curve requires either the probabilities or the non-thresholded,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,57,decision values from the estimator. Since the logistic regression provides,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,58,"a decision function, we will use it to plot the roc curve:",not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,66,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,67,Create :class:`PrecisionRecallDisplay`,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,68,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,69,"Similarly, the precision recall curve can be plotted using `y_score` from",not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,70,the prevision sections.,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,78,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,79,Combining the display objects into a single plot,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,80,,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,81,The display objects store the computed values that were passed as arguments.,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,82,This allows for the visualizations to be easliy combined using matplotlib's,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,83,"API. In the following example, we place the displays next to each other in a",not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,84,row.,not
scikit-learn/examples/miscellaneous/plot_display_object_visualization.py,86,sphinx_gallery_thumbnail_number = 4,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,34,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,35,License: BSD 3 clause,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,50,,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,51,Generate sample data,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,55,Add noise to targets,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,60,,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,61,Fit regression model,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,99,,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,100,Look at the results,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,115,Visualize training and prediction time,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,118,Generate sample data,not
scikit-learn/examples/miscellaneous/plot_kernel_ridge_regression.py,149,Visualize learning curves,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,54,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,55,Albert Thomas <albert.thomas@telecom-paristech.fr>,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,56,License: BSD 3 clause,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,74,Example settings,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,80,define outlier/anomaly detection methods to be compared,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,90,Define datasets,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,103,Compare given classifiers under given settings,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,115,Add outliers,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,127,fit the data and tag outliers,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,133,plot the levels lines and the points,not
scikit-learn/examples/miscellaneous/plot_anomaly_comparison.py,134,LOF does not implement predict,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,12,,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,13,Load Data and Train a SVC,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,14,-------------------------,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,15,"First, we load the wine dataset and convert it to a binary classification",not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,16,"problem. Then, we train a support vector classifier on a training dataset.",not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,31,,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,32,Plotting the ROC Curve,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,33,----------------------,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,34,"Next, we plot the ROC curve with a single call to",not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,35,:func:`sklearn.metrics.plot_roc_curve`. The returned `svc_disp` object allows,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,36,us to continue using the already computed ROC curve for the SVC in future,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,37,plots.,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,41,,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,42,Training a Random Forest and Plotting the ROC Curve,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,43,--------------------------------------------------------,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,44,We train a random forest classifier and create a plot comparing it to the SVC,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,45,ROC curve. Notice how `svc_disp` uses,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,46,:func:`~sklearn.metrics.RocCurveDisplay.plot` to plot the SVC ROC curve,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,47,"without recomputing the values of the roc curve itself. Furthermore, we",not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,48,pass `alpha=0.8` to the plot functions to adjust the alpha values of the,not
scikit-learn/examples/miscellaneous/plot_roc_curve_visualization_api.py,49,curves.,not
scikit-learn/examples/miscellaneous/plot_multilabel.py,1,"Authors: Vlad Niculae, Mathieu Blondel",not
scikit-learn/examples/miscellaneous/plot_multilabel.py,2,License: BSD 3 clause,not
scikit-learn/examples/miscellaneous/plot_multilabel.py,45,get the separating hyperplane,not
scikit-learn/examples/miscellaneous/plot_multilabel.py,48,make sure the line is long enough,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,29,"Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve",not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,30,License: BSD,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,43,,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,44,Setting up,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,78,Load Data,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,82,0-1 scaling,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,87,Models we will use,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,94,,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,95,Training,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,97,"Hyper-parameters. These were set by cross-validation,",not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,98,using a GridSearchCV. Here we are not performing cross-validation to,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,99,save time.,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,102,"More components tend to give better prediction performance, but larger",SATD
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,103,fitting time,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,107,Training RBM-Logistic Pipeline,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,110,Training the Logistic regression classifier directly on the pixel,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,115,,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,116,Evaluation,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,126,,not
scikit-learn/examples/neural_networks/plot_rbm_logistic_classification.py,127,Plotting,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,21,Author: Issam H. Laradji,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,22,License: BSD 3 clause,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,33,step size in the mesh,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,60,iterate over datasets,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,62,"preprocess dataset, split into training and test part",not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,71,just plot the dataset first,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,75,Plot the training points,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,77,and testing points,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,85,iterate over classifiers,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,91,"Plot the decision boundary. For that, we will assign a color to each",not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,92,"point in the mesh [x_min, x_max]x[y_min, y_max].",not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,98,Put the result into a color plot,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,102,Plot also the training points,not
scikit-learn/examples/neural_networks/plot_mlp_alpha.py,105,and testing points,not
scikit-learn/examples/neural_networks/plot_mnist_filters.py,35,Load data from https://www.openml.org/d/554,not
scikit-learn/examples/neural_networks/plot_mnist_filters.py,39,"rescale the data, use the traditional train/test split",not
scikit-learn/examples/neural_networks/plot_mnist_filters.py,47,"this example won't converge because of CI's time constraints, so we catch the",not
scikit-learn/examples/neural_networks/plot_mnist_filters.py,48,warning and are ignore it here,not
scikit-learn/examples/neural_networks/plot_mnist_filters.py,58,use global min / max to ensure all weights are shown on the same scale,not
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,27,different learning rate schedules and momentum parameters,not
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,57,"for each dataset, plot learning for each learning strategy",not
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,64,digits is larger but converges fairly quickly,not
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,74,some parameter combinations will not converge as can be seen on the,not
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,75,plots so they are ignored here,not
scikit-learn/examples/neural_networks/plot_mlp_training_curves.py,89,load / generate some toy datasets,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,49,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,50,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,66,Generate sample data,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,69,add noise,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,71,Fit KernelRidge with parameter selection based on 5-fold cross validation,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,88,Predict using kernel ridge,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,94,Predict using gaussian process regressor,not
scikit-learn/examples/gaussian_process/plot_compare_gpr_krr.py,104,Plot results,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,20,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,21,,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,22,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,37,First run,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,56,Second run,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy.py,75,Plot LML landscape,not
scikit-learn/examples/gaussian_process/plot_gpr_on_structured_data.py,148,whether there are 'A's in the sequence,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,60,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,61,,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,62,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,93,aggregate monthly sum to produce average,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,104,Kernel with parameters given in GPML book,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,105,long term smooth rising trend,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,107,seasonal component,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,108,medium term irregularity,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,112,noise terms,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,123,Kernel with optimized parameters,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,124,long term smooth rising trend,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,127,seasonal component,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,128,medium term irregularities,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,132,noise terms,not
scikit-learn/examples/gaussian_process/plot_gpr_co2.py,146,Illustration,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,23,Author: Vincent Dubourg <vincent.dubourg@gmail.com>,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,24,Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,25,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,26,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,41,----------------------------------------------------------------------,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,42,First the noiseless case,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,45,Observations,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,48,"Mesh the input space for evaluations of the real function, the prediction and",not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,49,its MSE,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,52,Instantiate a Gaussian Process model,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,56,Fit to data using Maximum Likelihood Estimation of the parameters,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,59,Make the prediction on the meshed x-axis (ask for MSE as well),not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,62,"Plot the function, the prediction and the 95% confidence interval based on",not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,63,the MSE,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,77,----------------------------------------------------------------------,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,78,now the noisy case,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,82,Observations and noise,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,88,Instantiate a Gaussian Process model,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,92,Fit to data using Maximum Likelihood Estimation of the parameters,not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,95,Make the prediction on the meshed x-axis (ask for MSE as well),not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,98,"Plot the function, the prediction and the 95% confidence interval based on",not
scikit-learn/examples/gaussian_process/plot_gpr_noisy_targets.py,99,the MSE,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,1,!/usr/bin/python,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,2,-*- coding: utf-8 -*-,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,14,Author: Vincent Dubourg <vincent.dubourg@gmail.com>,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,15,Adapted to GaussianProcessClassifier:,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,16,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,17,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,27,A few constants,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,36,Design of experiments,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,46,Observations,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,49,Instantiate and fit Gaussian Process Model,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,55,Evaluate real function and the predicted probability,not
scikit-learn/examples/gaussian_process/plot_gpc_isoprobability.py,66,Plot the probabilistic classification iso-values,not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,19,import some data to play with,not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,21,we only take the first two features.,not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,24,step size in the mesh,not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,31,create a mesh to plot in,not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,40,"Plot the predicted probabilities. For that, we will assign a color to",not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,41,"each point in the mesh [x_min, m_max]x[y_min, y_max].",not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,46,Put the result into a color plot,not
scikit-learn/examples/gaussian_process/plot_gpc_iris.py,50,Plot also the training points,not
scikit-learn/examples/gaussian_process/plot_gpc.py,25,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_gpc.py,26,,not
scikit-learn/examples/gaussian_process/plot_gpc.py,27,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpc.py,38,Generate data,not
scikit-learn/examples/gaussian_process/plot_gpc.py,44,Specify Gaussian Processes with fixed and optimized hyperparameters,not
scikit-learn/examples/gaussian_process/plot_gpc.py,65,Plot posteriors,not
scikit-learn/examples/gaussian_process/plot_gpc.py,82,Plot LML landscape,not
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,14,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,15,,not
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,16,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,31,fit the model,not
scikit-learn/examples/gaussian_process/plot_gpc_xor.py,37,plot the decision function for each datapoint on the grid,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,12,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,13,,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,14,License: BSD 3 clause,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,37,Specify Gaussian Process,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,40,Plot prior,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,54,Generate data and fit GP,not
scikit-learn/examples/gaussian_process/plot_gpr_prior_posterior.py,60,Plot posterior,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,66,generate data,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,70,add some outliers,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,75,fit a Minimum Covariance Determinant (MCD) robust estimator to data,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,78,compare estimators learnt from the full data set with true parameters,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,81,,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,82,Display results,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,86,Show data set,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,95,Show contours of the distance functions,not
scikit-learn/examples/covariance/plot_mahalanobis_distances.py,118,Plot the scores for each point,not
scikit-learn/examples/covariance/plot_sparse_cov.py,52,author: Gael Varoquaux <gael.varoquaux@inria.fr>,not
scikit-learn/examples/covariance/plot_sparse_cov.py,53,License: BSD 3 clause,not
scikit-learn/examples/covariance/plot_sparse_cov.py,54,Copyright: INRIA,not
scikit-learn/examples/covariance/plot_sparse_cov.py,62,,not
scikit-learn/examples/covariance/plot_sparse_cov.py,63,Generate the data,not
scikit-learn/examples/covariance/plot_sparse_cov.py,82,,not
scikit-learn/examples/covariance/plot_sparse_cov.py,83,Estimate the covariance,not
scikit-learn/examples/covariance/plot_sparse_cov.py,94,,not
scikit-learn/examples/covariance/plot_sparse_cov.py,95,Plot the results,not
scikit-learn/examples/covariance/plot_sparse_cov.py,99,plot the covariances,not
scikit-learn/examples/covariance/plot_sparse_cov.py,112,plot the precisions,not
scikit-learn/examples/covariance/plot_sparse_cov.py,129,plot the model selection metric,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,63,example settings,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,72,definition of arrays to store results,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,80,computation,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,86,generate data,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,88,add some outliers,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,96,fit a Minimum Covariance Determinant (MCD) robust estimator to data,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,98,compare raw robust estimates with the true location and covariance,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,102,compare estimators learned from the full data set with true,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,103,parameters,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,108,compare with an empirical covariance learned from a pure data set,not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,109,"(i.e. ""perfect"" mcd)",not
scikit-learn/examples/covariance/plot_robust_vs_empirical_covariance.py,116,Display results,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,55,,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,56,Generate sample data,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,62,Color samples,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,67,,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,68,Compute the likelihood on test data,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,70,spanning a range of possible shrinkage coefficient values,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,75,"under the ground-truth model, which we would not have access to in real",not
scikit-learn/examples/covariance/plot_covariance_estimation.py,76,settings,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,81,,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,82,Compare different approaches to setting the parameter,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,84,GridSearch for an optimal shrinkage coefficient,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,89,Ledoit-Wolf optimal shrinkage coefficient estimate,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,93,OAS coefficient estimate,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,97,,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,98,Plot results,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,103,range shrinkage curve,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,109,adjust view,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,116,LW likelihood,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,119,OAS likelihood,not
scikit-learn/examples/covariance/plot_covariance_estimation.py,122,best CV estimator likelihood,not
scikit-learn/examples/covariance/plot_lw_vs_oas.py,32,,not
scikit-learn/examples/covariance/plot_lw_vs_oas.py,34,simulation covariance matrix (AR(1) process),not
scikit-learn/examples/covariance/plot_lw_vs_oas.py,60,plot MSE,not
scikit-learn/examples/covariance/plot_lw_vs_oas.py,71,plot shrinkage coefficient,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,26,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,27,License: BSD Style.,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,41,Generate data,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,48,Train uncalibrated random forest classifier on whole train and validation,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,49,data and evaluate on test data,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,55,"Train random forest classifier, calibrate on validation data and evaluate",not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,56,on test data,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,65,Plot changes in predicted probabilities via arrows,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,74,Plot perfect predictions,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,79,Plot boundaries of unit simplex,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,82,Annotate points on the simplex,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,112,Add grid,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,132,Illustrate calibrator,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,134,generate grid over 2-simplex,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,147,Plot modifications of calibrator,not
scikit-learn/examples/calibration/plot_calibration_multiclass.py,152,Plot boundaries of unit simplex,not
scikit-learn/examples/calibration/plot_calibration.py,26,Author: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/examples/calibration/plot_calibration.py,27,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/examples/calibration/plot_calibration.py,28,Balazs Kegl <balazs.kegl@gmail.com>,not
scikit-learn/examples/calibration/plot_calibration.py,29,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/calibration/plot_calibration.py,30,License: BSD Style.,not
scikit-learn/examples/calibration/plot_calibration.py,44,use 3 bins for calibration_curve as we have 3 clusters here,not
scikit-learn/examples/calibration/plot_calibration.py,46,Generate 3 blobs with 2 classes where the second blob contains,not
scikit-learn/examples/calibration/plot_calibration.py,47,half positive samples and half negative samples. Probability in this,not
scikit-learn/examples/calibration/plot_calibration.py,48,blob is therefore 0.5.,not
scikit-learn/examples/calibration/plot_calibration.py,57,"split train, test for calibration",not
scikit-learn/examples/calibration/plot_calibration.py,61,Gaussian Naive-Bayes with no calibration,not
scikit-learn/examples/calibration/plot_calibration.py,63,GaussianNB itself does not support sample-weights,not
scikit-learn/examples/calibration/plot_calibration.py,66,Gaussian Naive-Bayes with isotonic calibration,not
scikit-learn/examples/calibration/plot_calibration.py,71,Gaussian Naive-Bayes with sigmoid calibration,not
scikit-learn/examples/calibration/plot_calibration.py,87,,not
scikit-learn/examples/calibration/plot_calibration.py,88,Plot the data and the predicted probabilities,not
scikit-learn/examples/calibration/plot_calibration_curve.py,46,Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/examples/calibration/plot_calibration_curve.py,47,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/calibration/plot_calibration_curve.py,48,License: BSD Style.,not
scikit-learn/examples/calibration/plot_calibration_curve.py,62,Create dataset of classification task with many redundant and few,not
scikit-learn/examples/calibration/plot_calibration_curve.py,63,informative features,not
scikit-learn/examples/calibration/plot_calibration_curve.py,74,Calibrated with isotonic calibration,not
scikit-learn/examples/calibration/plot_calibration_curve.py,77,Calibrated with sigmoid calibration,not
scikit-learn/examples/calibration/plot_calibration_curve.py,80,Logistic regression with no calibration as baseline,not
scikit-learn/examples/calibration/plot_calibration_curve.py,96,use decision function,not
scikit-learn/examples/calibration/plot_calibration_curve.py,128,Plot calibration curve for Gaussian Naive Bayes,not
scikit-learn/examples/calibration/plot_calibration_curve.py,131,Plot calibration curve for Linear SVC,not
scikit-learn/examples/calibration/plot_compare_calibration.py,52,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/examples/calibration/plot_compare_calibration.py,53,License: BSD Style.,not
scikit-learn/examples/calibration/plot_compare_calibration.py,70,Samples used for training the models,not
scikit-learn/examples/calibration/plot_compare_calibration.py,77,Create classifiers,not
scikit-learn/examples/calibration/plot_compare_calibration.py,84,,not
scikit-learn/examples/calibration/plot_compare_calibration.py,85,Plot calibration plots,not
scikit-learn/examples/calibration/plot_compare_calibration.py,99,use decision function,not
scikit-learn/examples/bicluster/plot_spectral_biclustering.py,20,Author: Kemal Eren <kemal@kemaleren.com>,not
scikit-learn/examples/bicluster/plot_spectral_biclustering.py,21,License: BSD 3 clause,not
scikit-learn/examples/bicluster/plot_spectral_biclustering.py,39,shuffle clusters,not
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,56,exclude 'comp.os.ms-windows.misc',not
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,103,"Note: the following is identical to X[rows[:, np.newaxis],",not
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,104,cols].sum() but much faster in scipy <= 0.16,SATD
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,132,categories,not
scikit-learn/examples/bicluster/plot_bicluster_newsgroups.py,139,words,not
scikit-learn/examples/bicluster/plot_spectral_coclustering.py,19,Author: Kemal Eren <kemal@kemaleren.com>,not
scikit-learn/examples/bicluster/plot_spectral_coclustering.py,20,License: BSD 3 clause,not
scikit-learn/examples/bicluster/plot_spectral_coclustering.py,36,shuffle clusters,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,24,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,25,Lars Buitinck,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,26,Chyi-Kwei Yau <chyikwei.yau@gmail.com>,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,27,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,50,Load the 20 newsgroups dataset and vectorize it. We use a few heuristics,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,51,"to filter out useless terms early on: the posts are stripped of headers,",not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,52,"footers and quoted replies, and common English words, words occurring in",not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,53,only one document or in at least 95% of the documents are removed.,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,63,Use tf-idf features for NMF.,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,72,Use tf (raw term count) features for LDA.,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,82,Fit the NMF model,not
scikit-learn/examples/applications/plot_topics_extraction_with_nmf_lda.py,95,Fit the NMF model,not
scikit-learn/examples/applications/plot_stock_market.py,63,Author: Gael Varoquaux gael.varoquaux@normalesup.org,not
scikit-learn/examples/applications/plot_stock_market.py,64,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_stock_market.py,79,,not
scikit-learn/examples/applications/plot_stock_market.py,80,Retrieve the data from Internet,not
scikit-learn/examples/applications/plot_stock_market.py,82,The data is from 2003 - 2008. This is reasonably calm: (not too long ago so,not
scikit-learn/examples/applications/plot_stock_market.py,83,"that we get high-tech firms, and before the 2008 crash). This kind of",not
scikit-learn/examples/applications/plot_stock_market.py,84,historical data can be obtained for from APIs like the quandl.com and,not
scikit-learn/examples/applications/plot_stock_market.py,85,alphavantage.co ones.,not
scikit-learn/examples/applications/plot_stock_market.py,159,The daily variations of the quotes are what carry most information,not
scikit-learn/examples/applications/plot_stock_market.py,163,,not
scikit-learn/examples/applications/plot_stock_market.py,164,Learn a graphical structure from the correlations,not
scikit-learn/examples/applications/plot_stock_market.py,167,standardize the time series: using correlations rather than covariance,not
scikit-learn/examples/applications/plot_stock_market.py,168,is more efficient for structure recovery,not
scikit-learn/examples/applications/plot_stock_market.py,173,,not
scikit-learn/examples/applications/plot_stock_market.py,174,Cluster using affinity propagation,not
scikit-learn/examples/applications/plot_stock_market.py,182,,not
scikit-learn/examples/applications/plot_stock_market.py,183,Find a low-dimension embedding for visualization: find the best position of,not
scikit-learn/examples/applications/plot_stock_market.py,184,the nodes (the stocks) on a 2D plane,not
scikit-learn/examples/applications/plot_stock_market.py,186,We use a dense eigen_solver to achieve reproducibility (arpack is,not
scikit-learn/examples/applications/plot_stock_market.py,187,"initiated with random vectors that we don't control). In addition, we",not
scikit-learn/examples/applications/plot_stock_market.py,188,use a large number of neighbors to capture the large-scale structure.,not
scikit-learn/examples/applications/plot_stock_market.py,194,,not
scikit-learn/examples/applications/plot_stock_market.py,195,Visualization,not
scikit-learn/examples/applications/plot_stock_market.py,201,Display a graph of the partial correlations,not
scikit-learn/examples/applications/plot_stock_market.py,208,Plot the nodes using the coordinates of our embedding,not
scikit-learn/examples/applications/plot_stock_market.py,212,Plot the edges,not
scikit-learn/examples/applications/plot_stock_market.py,214,"a sequence of (*line0*, *line1*, *line2*), where::",not
scikit-learn/examples/applications/plot_stock_market.py,215,"linen = (x0, y0), (x1, y1), ... (xm, ym)",not
scikit-learn/examples/applications/plot_stock_market.py,226,Add a label to each node. The challenge here is that we want to,not
scikit-learn/examples/applications/plot_stock_market.py,227,position the labels to avoid overlap with other labels,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,38,Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,39,Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,40,,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,41,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,52,"if basemap is available, we'll use it.",not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,53,"otherwise, we'll improvise later...",not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,76,"x,y coordinates for corner cells",not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,82,x coordinates of the grid cells,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,84,y coordinates of the grid cells,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,101,choose points associated with the desired species,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,105,determine coverage values for each of the training & testing points,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,124,Load the compressed data,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,127,Set up the data grid,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,130,"The grid in x,y coordinates",not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,133,create a bunch for each species,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,141,background points (grid coordinates) for evaluation,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,148,We'll make use of the fact that coverages[6] has measurements at all,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,149,land points.  This will help us decide between land and water.,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,152,"Fit, predict, and plot for each species.",not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,157,Standardize features,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,162,Fit OneClassSVM,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,168,Plot map of South America,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,187,Predict species distribution using the training data,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,190,We'll predict only for the land points.,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,201,plot contours of the prediction,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,205,scatter training/testing points,not
scikit-learn/examples/applications/plot_species_distribution_modeling.py,216,Compute AUC with regards to background points,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,32,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,33,License: BSD 3 clause,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,51,,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,52,"Where to download the data, if not already on disk",not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,72,,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,73,Loading the redirect files,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,104,compute the transitive closure,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,164,stop after 5M links to make it possible to work in RAM,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,174,print the names of the wikipedia related strongest components of the,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,175,principal singular vector which should be similar to the highest eigenvector,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,202,initial guess,not
scikit-learn/examples/applications/wikipedia_principal_eigenvector.py,208,check convergence: normalized l_inf norm,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,38,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,39,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,48,"Define ""classifiers"" to be used",not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,59,Get data,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,60,two clusters,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,62,Learn a frontier for outlier detection with several classifiers,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,75,Plot the results (= shape of the data points cloud),not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,76,two clusters,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,97,,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,98,Second example,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,99,--------------,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,100,The second example shows the ability of the Minimum Covariance Determinant,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,101,robust estimator of covariance to concentrate on the main mode of the data,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,102,"distribution: the location seems to be well estimated, although the",not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,103,"covariance is hard to estimate due to the banana-shaped distribution. Anyway,",not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,104,we can get rid of some outlying observations. The One-Class SVM is able to,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,105,"capture the real data structure, but the difficulty is to adjust its kernel",not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,106,bandwidth parameter so as to obtain a good compromise between the shape of,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,107,the data scatter matrix and the risk of over-fitting the data.,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,109,Get data,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,110,"""banana""-shaped",not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,112,Learn a frontier for outlier detection with several classifiers,not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,125,Plot the results (= shape of the data points cloud),not
scikit-learn/examples/applications/plot_outlier_detection_wine.py,126,"""banana"" shape",not
scikit-learn/examples/applications/plot_prediction_latency.py,16,Authors: Eustache Diemert <eustache@diemert.fr>,not
scikit-learn/examples/applications/plot_prediction_latency.py,17,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_prediction_latency.py,37,Hack to detect whether we are running by the sphinx builder,SATD
scikit-learn/examples/applications/plot_prediction_latency.py,267,,not
scikit-learn/examples/applications/plot_prediction_latency.py,268,Main code,not
scikit-learn/examples/applications/plot_prediction_latency.py,272,,not
scikit-learn/examples/applications/plot_prediction_latency.py,273,Benchmark bulk/atomic prediction speed for various regressors,not
scikit-learn/examples/applications/plot_prediction_latency.py,296,benchmark n_features influence on prediction speed,not
scikit-learn/examples/applications/plot_prediction_latency.py,304,benchmark throughput,not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,40,Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>,not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,41,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,113,"Generate synthetic images, and projections",not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,120,Reconstruction with L2 (Ridge) penalization,not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,125,Reconstruction with L1 (Lasso) penalization,not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,126,the best value of alpha was determined using cross validation,not
scikit-learn/examples/applications/plot_tomography_l1_reconstruction.py,127,with LassoCV,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,19,Author: Eustache Diemert <eustache@diemert.fr>,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,20,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,37,,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,38,Routines,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,41,Initialize random generator,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,125,,not
scikit-learn/examples/applications/plot_model_complexity_influence.py,126,Main code,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,16,Authors: Eustache Diemert <eustache@diemert.fr>,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,17,@FedericoV <https://github.com/FedericoV/>,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,18,License: BSD 3 clause,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,43,Hack to detect whether we are running by the sphinx builder,SATD
scikit-learn/examples/applications/plot_out_of_core_classification.py,46,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,47,Reuters Dataset related routines,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,48,--------------------------------,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,49,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,50,The dataset used in this example is Reuters-21578 as provided by the UCI ML,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,51,repository. It will be automatically downloaded and uncompressed on first,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,52,run.,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,181,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,182,Main,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,183,----,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,184,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,185,Create the vectorizer and limit the number of features to a reasonable,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,186,maximum,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,192,Iterator over parsed Reuters SGML files.,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,195,"We learn a binary classification between the ""acq"" class and all the others.",not
scikit-learn/examples/applications/plot_out_of_core_classification.py,196,"""acq"" was chosen as it is more or less evenly distributed in the Reuters",not
scikit-learn/examples/applications/plot_out_of_core_classification.py,197,"files. For other datasets, one should take care of creating a test set with",not
scikit-learn/examples/applications/plot_out_of_core_classification.py,198,a realistic portion of positive instances.,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,202,Here are some classifiers that support the `partial_fit` method,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,234,test data statistics,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,237,First we hold out a number of examples to estimate accuracy,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,270,Discard test set,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,272,We will feed the classifier with mini-batches of 1000 documents; this means,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,273,we have at most 1000 docs in memory at any time.  The smaller the document,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,274,"batch, the bigger the relative overhead of the partial fit methods.",not
scikit-learn/examples/applications/plot_out_of_core_classification.py,277,Create the data_stream that parses Reuters SGML files and iterates on,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,278,documents as a stream.,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,282,Main loop : iterate on mini-batches of examples,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,291,update estimator with examples in the current mini-batch,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,294,accumulate test accuracy stats,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,314,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,315,Plot results,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,316,------------,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,317,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,318,The plot represents the learning curve of the classifier: the evolution,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,319,of classification accuracy over the course of the mini-batches. Accuracy is,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,320,"measured on the first 1000 samples, held out as a validation set.",not
scikit-learn/examples/applications/plot_out_of_core_classification.py,321,,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,322,"To limit the memory consumption, we queue examples up to a fixed amount",not
scikit-learn/examples/applications/plot_out_of_core_classification.py,323,before feeding them to the learner.,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,340,Plot accuracy evolution,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,343,Plot accuracy evolution with #examples,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,352,Plot accuracy evolution with runtime,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,359,Plot fitting times,not
scikit-learn/examples/applications/plot_out_of_core_classification.py,395,Plot prediction times,not
scikit-learn/examples/applications/svm_gui.py,19,Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>,SATD
scikit-learn/examples/applications/svm_gui.py,20,,not
scikit-learn/examples/applications/svm_gui.py,21,License: BSD 3 clause,not
scikit-learn/examples/applications/svm_gui.py,29,NavigationToolbar2TkAgg was deprecated in matplotlib 2.2,not
scikit-learn/examples/applications/svm_gui.py,84,Whether or not a model has been fitted,not
scikit-learn/examples/applications/svm_gui.py,133,update decision surface if already fitted.,not
scikit-learn/examples/applications/svm_gui.py,155,support for matplotlib (1.*),not
scikit-learn/examples/applications/plot_face_recognition.py,45,Display progress logs on stdout,not
scikit-learn/examples/applications/plot_face_recognition.py,49,,not
scikit-learn/examples/applications/plot_face_recognition.py,50,"Download the data, if not already on disk and load it as numpy arrays",not
scikit-learn/examples/applications/plot_face_recognition.py,54,introspect the images arrays to find the shapes (for plotting),not
scikit-learn/examples/applications/plot_face_recognition.py,57,for machine learning we use the 2 data directly (as relative pixel,not
scikit-learn/examples/applications/plot_face_recognition.py,58,positions info is ignored by this model),not
scikit-learn/examples/applications/plot_face_recognition.py,62,the label to predict is the id of the person,not
scikit-learn/examples/applications/plot_face_recognition.py,73,,not
scikit-learn/examples/applications/plot_face_recognition.py,74,Split into a training set and a test set using a stratified k fold,not
scikit-learn/examples/applications/plot_face_recognition.py,76,split into a training and testing set,not
scikit-learn/examples/applications/plot_face_recognition.py,81,,not
scikit-learn/examples/applications/plot_face_recognition.py,82,Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled,not
scikit-learn/examples/applications/plot_face_recognition.py,83,dataset): unsupervised feature extraction / dimensionality reduction,not
scikit-learn/examples/applications/plot_face_recognition.py,102,,not
scikit-learn/examples/applications/plot_face_recognition.py,103,Train a SVM classification model,not
scikit-learn/examples/applications/plot_face_recognition.py,118,,not
scikit-learn/examples/applications/plot_face_recognition.py,119,Quantitative evaluation of the model quality on the test set,not
scikit-learn/examples/applications/plot_face_recognition.py,130,,not
scikit-learn/examples/applications/plot_face_recognition.py,131,Qualitative evaluation of the predictions using matplotlib,not
scikit-learn/examples/applications/plot_face_recognition.py,145,plot the result of the prediction on a portion of the test set,not
scikit-learn/examples/applications/plot_face_recognition.py,157,plot the gallery of the most significative eigenfaces,not
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,17,Build a classification task using 3 informative features,not
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,22,Create the RFE object and compute a cross-validated score.,not
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,24,"The ""accuracy"" scoring is proportional to the number of correct",not
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,25,classifications,not
scikit-learn/examples/feature_selection/plot_rfe_with_cross_validation.py,32,Plot number of features VS. cross-validation scores,not
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,21,import some data to play with,not
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,28,ANOVA SVM-C,not
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,29,"1) anova filter, take 3 best ranked features",not
scikit-learn/examples/feature_selection/plot_feature_selection_pipeline.py,31,2) svm,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,34,,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,35,Import some data to play with,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,37,The iris dataset,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,40,Some noisy data not correlated,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,43,Add the noisy data to the informative features,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,46,Split dataset to select feature and evaluate the classifier,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,56,,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,57,Univariate feature selection with F-test for feature scoring,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,58,We use the default selection function to select the four,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,59,most significant features,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,67,,not
scikit-learn/examples/feature_selection/plot_feature_selection.py,68,Compare to the weights of an SVM,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,14,Author:  Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,15,License: BSD 3 clause,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,28,,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,29,Loading a dataset,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,35,Some noisy data not correlated,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,39,Add noisy data to the informative features for make the task harder,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,50,,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,51,View histogram of permutation scores,not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,55,"BUG: vlines(..., linestyle='--') fails on older versions of matplotlib",SATD
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,56,"plt.vlines(score, ylim[0], ylim[1], linestyle='--',",not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,57,"color='g', linewidth=3, label='Classification Score'",not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,58,' (pvalue %s)' % pvalue),not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,59,"plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',",not
scikit-learn/examples/feature_selection/plot_permutation_test_for_classification.py,60,"color='k', linewidth=3, label='Luck')",not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,34,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,35,Load the data,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,36,---------------------------------------------------------,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,37,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,38,"First, let's load the diabetes dataset which is available from within",not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,39,"sklearn. Then, we will look what features are collected for the diabates",not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,40,patients:,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,50,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,51,Find importance of the features,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,52,---------------------------------------------------------,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,53,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,54,To decide on the importance of the features we are going to use LassoCV,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,55,estimator. The features with the highest absolute `coef_` value are,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,56,considered the most important,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,62,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,63,Select from the model features with the higest score,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,64,---------------------------------------------------------,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,65,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,66,Now we want to select the two features which are the most important.,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,67,SelectFromModel() allows for setting the threshold. Only the features with,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,68,"the `coef_` higher than the threshold will remain. Here, we want to set the",not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,69,threshold slightly above the third highest `coef_` calculated by LassoCV(),not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,70,from our data.,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,85,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,86,Plot the two most important features,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,87,---------------------------------------------------------,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,88,,not
scikit-learn/examples/feature_selection/plot_select_from_model_diabetes.py,89,Finally we will plot the selected two features from the data.,not
scikit-learn/examples/feature_selection/plot_rfe_digits.py,21,Load the digits dataset,not
scikit-learn/examples/feature_selection/plot_rfe_digits.py,26,Create the RFE object and rank each pixel,not
scikit-learn/examples/feature_selection/plot_rfe_digits.py,32,Plot pixel ranking,not
scikit-learn/sklearn/naive_bayes.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/naive_bayes.py,9,Author: Vincent Michel <vincent.michel@inria.fr>,not
scikit-learn/sklearn/naive_bayes.py,10,Minor fixes by Fabian Pedregosa,not
scikit-learn/sklearn/naive_bayes.py,11,Amit Aides <amitibo@tx.technion.ac.il>,not
scikit-learn/sklearn/naive_bayes.py,12,Yehuda Finkelstein <yehudaf@tx.technion.ac.il>,not
scikit-learn/sklearn/naive_bayes.py,13,Lars Buitinck,not
scikit-learn/sklearn/naive_bayes.py,14,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/naive_bayes.py,15,(parts based on earlier work by Mathieu Blondel),not
scikit-learn/sklearn/naive_bayes.py,16,,not
scikit-learn/sklearn/naive_bayes.py,17,License: BSD 3 clause,not
scikit-learn/sklearn/naive_bayes.py,57,Note that this is not marked @abstractmethod as long as the,not
scikit-learn/sklearn/naive_bayes.py,58,deprecated public alias sklearn.naive_bayes.BayesNB exists,not
scikit-learn/sklearn/naive_bayes.py,59,(until 0.24) to preserve backward compat for 3rd party projects,not
scikit-learn/sklearn/naive_bayes.py,60,with existing derived classes.,not
scikit-learn/sklearn/naive_bayes.py,99,"normalize by P(x) = P(f_1, ..., f_n)",not
scikit-learn/sklearn/naive_bayes.py,261,Compute (potentially weighted) mean and variance of new datapoints,not
scikit-learn/sklearn/naive_bayes.py,277,"Combine mean of old and new data, taking into consideration",not
scikit-learn/sklearn/naive_bayes.py,278,(weighted) number of observations,not
scikit-learn/sklearn/naive_bayes.py,281,"Combine variance of old and new data, taking into consideration",not
scikit-learn/sklearn/naive_bayes.py,282,(weighted) number of observations. This is achieved by combining,not
scikit-learn/sklearn/naive_bayes.py,283,the sum-of-squared-differences (ssd),not
scikit-learn/sklearn/naive_bayes.py,368,"If the ratio of data variance between dimensions is too small, it",not
scikit-learn/sklearn/naive_bayes.py,369,"will cause numerical errors. To address this, we artificially",not
scikit-learn/sklearn/naive_bayes.py,370,"boost the variance by epsilon, a small fraction of the standard",not
scikit-learn/sklearn/naive_bayes.py,371,deviation of the largest dimension.,not
scikit-learn/sklearn/naive_bayes.py,378,This is the first call to partial_fit:,not
scikit-learn/sklearn/naive_bayes.py,379,initialize various cumulative counters,not
scikit-learn/sklearn/naive_bayes.py,387,Initialise the class prior,not
scikit-learn/sklearn/naive_bayes.py,388,Take into account the priors,not
scikit-learn/sklearn/naive_bayes.py,391,Check that the provide prior match the number of classes,not
scikit-learn/sklearn/naive_bayes.py,395,Check that the sum is 1,not
scikit-learn/sklearn/naive_bayes.py,398,Check that the prior are non-negative,not
scikit-learn/sklearn/naive_bayes.py,403,Initialize the priors to zeros for each class,not
scikit-learn/sklearn/naive_bayes.py,410,Put epsilon back in each time,not
scikit-learn/sklearn/naive_bayes.py,444,Update if only no priors is provided,not
scikit-learn/sklearn/naive_bayes.py,446,"Empirical prior, with sample_weight taken into account",not
scikit-learn/sklearn/naive_bayes.py,491,silence the warning when count is 0 because class was not yet,not
scikit-learn/sklearn/naive_bayes.py,492,observed,not
scikit-learn/sklearn/naive_bayes.py,496,"empirical prior, with sample_weight taken into account",not
scikit-learn/sklearn/naive_bayes.py,556,This is the first call to partial_fit:,not
scikit-learn/sklearn/naive_bayes.py,557,initialize various cumulative counters,not
scikit-learn/sklearn/naive_bayes.py,573,label_binarize() returns arrays with dtype=np.int64.,not
scikit-learn/sklearn/naive_bayes.py,574,We convert it to np.float64 to support sample_weight consistently,not
scikit-learn/sklearn/naive_bayes.py,583,Count raw events from data before updating the class log prior,not
scikit-learn/sklearn/naive_bayes.py,584,and feature log probas,not
scikit-learn/sklearn/naive_bayes.py,587,XXX: OPTIM: we could introduce a public finalization method to,SATD
scikit-learn/sklearn/naive_bayes.py,588,be called by the user explicitly just once after several consecutive,not
scikit-learn/sklearn/naive_bayes.py,589,calls to partial_fit and prior any call to predict[_[log_]proba],not
scikit-learn/sklearn/naive_bayes.py,590,to avoid computing the smooth log probas at each call to partial fit,not
scikit-learn/sklearn/naive_bayes.py,625,LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.,not
scikit-learn/sklearn/naive_bayes.py,626,We convert it to np.float64 to support sample_weight consistently;,not
scikit-learn/sklearn/naive_bayes.py,627,this means we also don't have to cast X to floating point,not
scikit-learn/sklearn/naive_bayes.py,636,Count raw events from data before updating the class log prior,not
scikit-learn/sklearn/naive_bayes.py,637,and feature log probas,not
scikit-learn/sklearn/naive_bayes.py,652,XXX The following is a stopgap measure; we need to set the dimensions,SATD
scikit-learn/sklearn/naive_bayes.py,653,of class_log_prior_ and feature_log_prob_ correctly.,not
scikit-learn/sklearn/naive_bayes.py,879,"_BaseNB.predict uses argmax, but ComplementNB operates with argmin.",not
scikit-learn/sklearn/naive_bayes.py,1015,Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob,not
scikit-learn/sklearn/naive_bayes.py,1184,we append a column full of zeros for each new category,not
scikit-learn/sklearn/naive_bayes.py,1230,TODO: remove in 0.24,SATD
scikit-learn/sklearn/naive_bayes.py,1237,TODO: remove in 0.24,SATD
scikit-learn/sklearn/base.py,7,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/base.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/base.py,66,XXX: not handling dictionaries,SATD
scikit-learn/sklearn/base.py,91,quick sanity check of the parameters of the clone,not
scikit-learn/sklearn/base.py,118,Do a multi-line justified repr:,not
scikit-learn/sklearn/base.py,126,use str for representing floating point numbers,not
scikit-learn/sklearn/base.py,127,this way we get consistent representation across,not
scikit-learn/sklearn/base.py,128,architectures and versions.,not
scikit-learn/sklearn/base.py,131,use repr of the rest,not
scikit-learn/sklearn/base.py,147,Strip trailing space to avoid nightmare in doctests,not
scikit-learn/sklearn/base.py,165,fetch the constructor or the original constructor before,not
scikit-learn/sklearn/base.py,166,deprecation wrapping if any,not
scikit-learn/sklearn/base.py,169,No explicit constructor to introspect,not
scikit-learn/sklearn/base.py,172,introspect the constructor arguments to find the model parameters,not
scikit-learn/sklearn/base.py,173,to represent,not
scikit-learn/sklearn/base.py,175,Consider the constructor parameters excluding 'self',not
scikit-learn/sklearn/base.py,186,Extract and sort argument names excluding 'self',not
scikit-learn/sklearn/base.py,241,Simple optimization to gain speed (inspect is slow),not
scikit-learn/sklearn/base.py,245,grouped by prefix,not
scikit-learn/sklearn/base.py,266,N_CHAR_MAX is the (approximate) maximum number of non-blank,not
scikit-learn/sklearn/base.py,267,characters to render. We pass it as an optional parameter to ease,not
scikit-learn/sklearn/base.py,268,the tests.,not
scikit-learn/sklearn/base.py,272,number of elements to show in sequences,not
scikit-learn/sklearn/base.py,274,use ellipsis for sequences with a lot of elements,not
scikit-learn/sklearn/base.py,281,Use bruteforce ellipsis when there are a lot of non-blank characters,not
scikit-learn/sklearn/base.py,284,apprx number of chars to keep on both ends,not
scikit-learn/sklearn/base.py,286,The regex '^(\s*\S){%d}' % n,not
scikit-learn/sklearn/base.py,287,matches from the start of the string until the nth non-blank,not
scikit-learn/sklearn/base.py,288,character:,not
scikit-learn/sklearn/base.py,289,- ^ matches the start of string,not
scikit-learn/sklearn/base.py,290,- (pattern){n} matches n repetitions of pattern,not
scikit-learn/sklearn/base.py,291,- \s*\S matches a non-blank char following zero or more blanks,not
scikit-learn/sklearn/base.py,296,The left side and right side aren't on the same line.,not
scikit-learn/sklearn/base.py,297,"To avoid weird cuts, e.g.:",not
scikit-learn/sklearn/base.py,298,"categoric...ore',",not
scikit-learn/sklearn/base.py,299,we need to start the right side with an appropriate newline,not
scikit-learn/sklearn/base.py,300,character so that it renders properly as:,not
scikit-learn/sklearn/base.py,301,categoric...,not
scikit-learn/sklearn/base.py,302,"handle_unknown='ignore',",not
scikit-learn/sklearn/base.py,303,so we add [^\n]*\n which matches until the next \n,not
scikit-learn/sklearn/base.py,309,Only add ellipsis if it results in a shorter repr,not
scikit-learn/sklearn/base.py,347,need the if because mixins might not have _more_tags,not
scikit-learn/sklearn/base.py,348,but might do redundant work in estimators,not
scikit-learn/sklearn/base.py,349,(i.e. calling more tags on BaseEstimator multiple times),not
scikit-learn/sklearn/base.py,424,We need this because some estimators validate X and y,not
scikit-learn/sklearn/base.py,425,"separately, and in general, separately calling check_array()",not
scikit-learn/sklearn/base.py,426,on X and y isn't equivalent to just calling check_X_y(),not
scikit-learn/sklearn/base.py,427,:(,not
scikit-learn/sklearn/base.py,579,non-optimized default implementation; override when a better,not
scikit-learn/sklearn/base.py,580,method is possible for a given clustering algorithm,not
scikit-learn/sklearn/base.py,686,non-optimized default implementation; override when a better,not
scikit-learn/sklearn/base.py,687,method is possible for a given clustering algorithm,not
scikit-learn/sklearn/base.py,689,fit method of arity 1 (unsupervised transformation),not
scikit-learn/sklearn/base.py,692,fit method of arity 2 (supervised transformation),not
scikit-learn/sklearn/base.py,739,override for transductive outlier detectors like LocalOulierFactor,not
scikit-learn/sklearn/dummy.py,1,Author: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/dummy.py,2,Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/dummy.py,3,Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>,not
scikit-learn/sklearn/dummy.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/dummy.py,129,TODO: Remove in 0.24,SATD
scikit-learn/sklearn/dummy.py,159,No input validation is done for X,not
scikit-learn/sklearn/dummy.py,183,Checking in case of constant strategy if the constant,not
scikit-learn/sklearn/dummy.py,184,provided by the user is in y.,not
scikit-learn/sklearn/dummy.py,213,numpy random_state expects Python int and not long as size argument,not
scikit-learn/sklearn/dummy.py,214,under Windows,not
scikit-learn/sklearn/dummy.py,223,Get same type even for self.n_outputs_ == 1,not
scikit-learn/sklearn/dummy.py,228,Compute probability only once,not
scikit-learn/sklearn/dummy.py,291,numpy random_state expects Python int and not long as size argument,not
scikit-learn/sklearn/dummy.py,292,under Windows,not
scikit-learn/sklearn/dummy.py,301,Get same type even for self.n_outputs_ == 1,not
scikit-learn/sklearn/dummy.py,398,mypy error: Decorated property not supported,not
scikit-learn/sklearn/dummy.py,399,type: ignore,not
scikit-learn/sklearn/dummy.py,495,No input validation is done for X,not
scikit-learn/sklearn/dummy.py,628,mypy error: Decorated property not supported,not
scikit-learn/sklearn/dummy.py,629,type: ignore,not
scikit-learn/sklearn/calibration.py,3,Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/calibration.py,4,Balazs Kegl <balazs.kegl@gmail.com>,not
scikit-learn/sklearn/calibration.py,5,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/calibration.py,6,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/calibration.py,7,,not
scikit-learn/sklearn/calibration.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/calibration.py,133,Check that each cross-validation fold can have at least one,not
scikit-learn/sklearn/calibration.py,134,example per class,not
scikit-learn/sklearn/calibration.py,146,we want all classifiers that don't expose a random_state,not
scikit-learn/sklearn/calibration.py,147,to be deterministic (and we don't want to expose this one).,not
scikit-learn/sklearn/calibration.py,207,Compute the arithmetic mean of the predictions of the calibrated,not
scikit-learn/sklearn/calibration.py,208,classifiers,not
scikit-learn/sklearn/calibration.py,377,Normalize the probabilities,not
scikit-learn/sklearn/calibration.py,383,XXX : for some reason all probas can be 0,SATD
scikit-learn/sklearn/calibration.py,386,Deal with cases where the predicted probability minimally exceeds 1.0,not
scikit-learn/sklearn/calibration.py,421,F follows Platt's notations,not
scikit-learn/sklearn/calibration.py,423,Bayesian priors (see Platt end of section 2.2),not
scikit-learn/sklearn/calibration.py,432,From Platt (beginning of Section 2.2),not
scikit-learn/sklearn/calibration.py,441,gradient of the objective function,not
scikit-learn/sklearn/calibration.py,567,"Normalize predicted values into interval [0, 1]",not
scikit-learn/sklearn/calibration.py,579,Determine bin edges by distribution of data,not
scikit-learn/sklearn/kernel_approximation.py,6,Author: Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/kernel_approximation.py,7,,not
scikit-learn/sklearn/kernel_approximation.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/kernel_approximation.py,217,transform by inverse CDF of sech,not
scikit-learn/sklearn/kernel_approximation.py,344,"See reference, figure 2 c)",not
scikit-learn/sklearn/kernel_approximation.py,380,zeroth component,not
scikit-learn/sklearn/kernel_approximation.py,381,1/cosh = sech,not
scikit-learn/sklearn/kernel_approximation.py,382,cosh(0) = 1.0,not
scikit-learn/sklearn/kernel_approximation.py,564,get basis vectors,not
scikit-learn/sklearn/kernel_approximation.py,566,XXX should we just bail?,SATD
scikit-learn/sklearn/kernel_approximation.py,583,sqrt of kernel matrix on basis vectors,not
scikit-learn/sklearn/kernel_ridge.py,3,Authors: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/kernel_ridge.py,4,Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/kernel_ridge.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/kernel_ridge.py,160,Convert data,not
scikit-learn/sklearn/multiclass.py,31,Author: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/multiclass.py,32,Author: Hamzeh Alsalhi <93hamsal@gmail.com>,not
scikit-learn/sklearn/multiclass.py,33,,not
scikit-learn/sklearn/multiclass.py,34,License: BSD 3 clause,not
scikit-learn/sklearn/multiclass.py,98,probabilities of the positive class,not
scikit-learn/sklearn/multiclass.py,229,"A sparse LabelBinarizer, with sparse_output=True, has been shown to",not
scikit-learn/sklearn/multiclass.py,230,outperform or match a dense label binarizer in all cases and has also,not
scikit-learn/sklearn/multiclass.py,231,resulted in less or equal memory consumption in the fit_ovr function,not
scikit-learn/sklearn/multiclass.py,232,overall.,not
scikit-learn/sklearn/multiclass.py,238,In cases where individual estimators are very fast to train setting,not
scikit-learn/sklearn/multiclass.py,239,n_jobs > 1 in can results in slower performance due to the overhead,not
scikit-learn/sklearn/multiclass.py,240,of spawning threads.  See joblib issue #112.,not
scikit-learn/sklearn/multiclass.py,283,"A sparse LabelBinarizer, with sparse_output=True, has been",not
scikit-learn/sklearn/multiclass.py,284,shown to outperform or match a dense label binarizer in all,not
scikit-learn/sklearn/multiclass.py,285,cases and has also resulted in less or equal memory consumption,not
scikit-learn/sklearn/multiclass.py,286,in the fit_ovr function overall.,not
scikit-learn/sklearn/multiclass.py,371,"Y[i, j] gives the probability that sample i has the label j.",not
scikit-learn/sklearn/multiclass.py,372,"In the multi-label case, these are not disjoint.",not
scikit-learn/sklearn/multiclass.py,376,"Only one estimator, but we still want to return probabilities",not
scikit-learn/sklearn/multiclass.py,377,for two classes.,not
scikit-learn/sklearn/multiclass.py,381,"Then, probabilities should be normalized to 1.",not
scikit-learn/sklearn/multiclass.py,448,For consistency with other estimators we raise a AttributeError so,not
scikit-learn/sklearn/multiclass.py,449,that hasattr() fails if the OVR estimator isn't fitted.,not
scikit-learn/sklearn/multiclass.py,818,FIXME: there are more elaborate methods than generating the codebook,SATD
scikit-learn/sklearn/multiclass.py,819,randomly.,not
scikit-learn/sklearn/discriminant_analysis.py,5,Authors: Clemens Brunner,not
scikit-learn/sklearn/discriminant_analysis.py,6,Martin Billinger,not
scikit-learn/sklearn/discriminant_analysis.py,7,Matthieu Perrot,not
scikit-learn/sklearn/discriminant_analysis.py,8,Mathieu Blondel,not
scikit-learn/sklearn/discriminant_analysis.py,10,License: BSD 3-Clause,not
scikit-learn/sklearn/discriminant_analysis.py,54,standardize features,not
scikit-learn/sklearn/discriminant_analysis.py,57,rescale,not
scikit-learn/sklearn/discriminant_analysis.py,254,used only in svd solver,not
scikit-learn/sklearn/discriminant_analysis.py,255,used only in svd solver,not
scikit-learn/sklearn/discriminant_analysis.py,331,within scatter,not
scikit-learn/sklearn/discriminant_analysis.py,332,total scatter,not
scikit-learn/sklearn/discriminant_analysis.py,333,between scatter,not
scikit-learn/sklearn/discriminant_analysis.py,338,sort eigenvectors,not
scikit-learn/sklearn/discriminant_analysis.py,372,1) within (univariate) scaling by with classes std-dev,not
scikit-learn/sklearn/discriminant_analysis.py,374,avoid division by zero in normalization,not
scikit-learn/sklearn/discriminant_analysis.py,378,2) Within variance scaling,not
scikit-learn/sklearn/discriminant_analysis.py,380,SVD of centered (within)scaled data,not
scikit-learn/sklearn/discriminant_analysis.py,384,Scaling of within covariance is: V' 1/S,not
scikit-learn/sklearn/discriminant_analysis.py,387,3) Between variance scaling,not
scikit-learn/sklearn/discriminant_analysis.py,388,Scale weighted centers,not
scikit-learn/sklearn/discriminant_analysis.py,391,Centers are living in a space with n_classes-1 dim (maximum),not
scikit-learn/sklearn/discriminant_analysis.py,392,Use SVD to find projection in the space spanned by the,not
scikit-learn/sklearn/discriminant_analysis.py,393,(n_classes) centers,not
scikit-learn/sklearn/discriminant_analysis.py,434,estimate priors from sample,not
scikit-learn/sklearn/discriminant_analysis.py,435,non-negative ints,not
scikit-learn/sklearn/discriminant_analysis.py,447,Maximum number of components no matter what n_components is,not
scikit-learn/sklearn/discriminant_analysis.py,448,specified:,not
scikit-learn/sklearn/discriminant_analysis.py,472,treat binary case as a special case,not
scikit-learn/sklearn/discriminant_analysis.py,562,Only override for the doc,not
scikit-learn/sklearn/discriminant_analysis.py,711,Xgc = U * S * V.T,not
scikit-learn/sklearn/discriminant_analysis.py,719,cov = V * (S^2 / (n-1)) * V.T,not
scikit-learn/sklearn/discriminant_analysis.py,731,"return log posterior, see eq (4.12) p. 110 of the ESL.",not
scikit-learn/sklearn/discriminant_analysis.py,742,"shape = [len(X), n_classes]",not
scikit-learn/sklearn/discriminant_analysis.py,767,handle special case of two classes,not
scikit-learn/sklearn/discriminant_analysis.py,803,compute the likelihood of the underlying gaussian models,not
scikit-learn/sklearn/discriminant_analysis.py,804,up to a multiplicative constant.,not
scikit-learn/sklearn/discriminant_analysis.py,806,compute posterior probabilities,not
scikit-learn/sklearn/discriminant_analysis.py,822,XXX : can do better to avoid precision overflows,SATD
scikit-learn/sklearn/isotonic.py,1,Authors: Fabian Pedregosa <fabian@fseoane.net>,not
scikit-learn/sklearn/isotonic.py,2,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/isotonic.py,3,Nelle Varoquaux <nelle.varoquaux@gmail.com>,not
scikit-learn/sklearn/isotonic.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/isotonic.py,55,Calculate Spearman rho estimate and set return accordingly.,not
scikit-learn/sklearn/isotonic.py,59,"Run Fisher transform to get the rho CI, but handle rho=+/-1",not
scikit-learn/sklearn/isotonic.py,64,"Use a 95% CI, i.e., +/-1.96 S.E.",not
scikit-learn/sklearn/isotonic.py,65,https://en.wikipedia.org/wiki/Fisher_transformation,not
scikit-learn/sklearn/isotonic.py,69,Warn if the CI spans zero.,not
scikit-learn/sklearn/isotonic.py,124,"Older versions of np.clip don't accept None as a bound, so use np.inf",not
scikit-learn/sklearn/isotonic.py,220,Handle the out_of_bounds argument by setting bounds_error,not
scikit-learn/sklearn/isotonic.py,228,"single y, constant prediction",not
scikit-learn/sklearn/isotonic.py,238,Determine increasing if auto-determination requested,not
scikit-learn/sklearn/isotonic.py,244,"If sample_weights is passed, removed zero-weight values and clean",not
scikit-learn/sklearn/isotonic.py,245,order,not
scikit-learn/sklearn/isotonic.py,260,Handle the left and right bounds on X,not
scikit-learn/sklearn/isotonic.py,264,Remove unnecessary points for faster prediction,not
scikit-learn/sklearn/isotonic.py,266,"Aside from the 1st and last point, remove points whose y values",not
scikit-learn/sklearn/isotonic.py,267,are equal to both the point before and the point after it.,not
scikit-learn/sklearn/isotonic.py,274,The ability to turn off trim_duplicates is only used to it make,not
scikit-learn/sklearn/isotonic.py,275,easier to unit test that removing duplicates in y does not have,not
scikit-learn/sklearn/isotonic.py,276,any impact the resulting interpolation function (besides,not
scikit-learn/sklearn/isotonic.py,277,prediction speed).,not
scikit-learn/sklearn/isotonic.py,310,Transform y by running the isotonic regression algorithm and,not
scikit-learn/sklearn/isotonic.py,311,transform X accordingly.,not
scikit-learn/sklearn/isotonic.py,314,It is necessary to store the non-redundant part of the training set,not
scikit-learn/sklearn/isotonic.py,315,on the model to make it possible to support model persistence via,not
scikit-learn/sklearn/isotonic.py,316,the pickle module as the object built by scipy.interp1d is not,not
scikit-learn/sklearn/isotonic.py,317,picklable directly.,not
scikit-learn/sklearn/isotonic.py,320,Build the interpolation function,not
scikit-learn/sklearn/isotonic.py,348,Handle the out_of_bounds argument by clipping if needed,not
scikit-learn/sklearn/isotonic.py,359,"on scipy 0.17, interp1d up-casts to float64, so we cast back",not
scikit-learn/sklearn/isotonic.py,382,remove interpolation method,not
scikit-learn/sklearn/__init__.py,24,"PEP0440 compatible formatted version, see:",not
scikit-learn/sklearn/__init__.py,25,https://www.python.org/dev/peps/pep-0440/,not
scikit-learn/sklearn/__init__.py,26,,not
scikit-learn/sklearn/__init__.py,27,Generic release markers:,not
scikit-learn/sklearn/__init__.py,28,X.Y,not
scikit-learn/sklearn/__init__.py,29,X.Y.Z   # For bugfix releases,not
scikit-learn/sklearn/__init__.py,30,,not
scikit-learn/sklearn/__init__.py,31,Admissible pre-release markers:,not
scikit-learn/sklearn/__init__.py,32,X.YaN   # Alpha release,not
scikit-learn/sklearn/__init__.py,33,X.YbN   # Beta release,not
scikit-learn/sklearn/__init__.py,34,X.YrcN  # Release Candidate,not
scikit-learn/sklearn/__init__.py,35,X.Y     # Final release,not
scikit-learn/sklearn/__init__.py,36,,not
scikit-learn/sklearn/__init__.py,37,Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.,not
scikit-learn/sklearn/__init__.py,38,'X.Y.dev0' is the canonical version of 'X.Y.dev',not
scikit-learn/sklearn/__init__.py,39,,not
scikit-learn/sklearn/__init__.py,43,"On OSX, we can get a runtime error due to multiple OpenMP libraries loaded",not
scikit-learn/sklearn/__init__.py,44,simultaneously. This can happen for instance when calling BLAS inside a,not
scikit-learn/sklearn/__init__.py,45,prange. Setting the following environment variable allows multiple OpenMP,not
scikit-learn/sklearn/__init__.py,46,libraries to be loaded. It should not degrade performances since we manually,not
scikit-learn/sklearn/__init__.py,47,"take care of potential over-subcription performance issues, in sections of",SATD
scikit-learn/sklearn/__init__.py,48,"the code where nested OpenMP loops can happen, by dynamically reconfiguring",not
scikit-learn/sklearn/__init__.py,49,the inner OpenMP runtime to temporarily disable it while under the scope of,not
scikit-learn/sklearn/__init__.py,50,the outer OpenMP parallel section.,not
scikit-learn/sklearn/__init__.py,53,Workaround issue discovered in intel-openmp 2019.5:,SATD
scikit-learn/sklearn/__init__.py,54,https://github.com/ContinuumIO/anaconda-issues/issues/11294,not
scikit-learn/sklearn/__init__.py,58,This variable is injected in the __builtins__ by the build,not
scikit-learn/sklearn/__init__.py,59,process. It is used to enable importing subpackages of sklearn when,not
scikit-learn/sklearn/__init__.py,60,the binaries are not built,not
scikit-learn/sklearn/__init__.py,61,mypy error: Cannot determine type of '__SKLEARN_SETUP__',not
scikit-learn/sklearn/__init__.py,62,type: ignore,not
scikit-learn/sklearn/__init__.py,68,We are not importing the rest of scikit-learn during the build,not
scikit-learn/sklearn/__init__.py,69,"process, as it may not be compiled yet",not
scikit-learn/sklearn/__init__.py,71,`_distributor_init` allows distributors to run custom init code.,not
scikit-learn/sklearn/__init__.py,72,"For instance, for the Windows wheel, this is used to pre-load the",not
scikit-learn/sklearn/__init__.py,73,vcomp shared library runtime for OpenMP embedded in the sklearn/.libs,not
scikit-learn/sklearn/__init__.py,74,sub-folder.,not
scikit-learn/sklearn/__init__.py,75,It is necessary to do this prior to importing show_versions as the,not
scikit-learn/sklearn/__init__.py,76,later is linked to the OpenMP runtime to make it possible to introspect,not
scikit-learn/sklearn/__init__.py,77,it and importing it first would fail if the OpenMP dll cannot be found.,not
scikit-learn/sklearn/__init__.py,78,noqa: F401,not
scikit-learn/sklearn/__init__.py,79,noqa: F401,not
scikit-learn/sklearn/__init__.py,93,Non-modules:,not
scikit-learn/sklearn/__init__.py,104,"Check if a random seed exists in the environment, if not create one.",not
scikit-learn/sklearn/pipeline.py,5,Author: Edouard Duchesnay,not
scikit-learn/sklearn/pipeline.py,6,Gael Varoquaux,not
scikit-learn/sklearn/pipeline.py,7,Virgile Fritsch,not
scikit-learn/sklearn/pipeline.py,8,Alexandre Gramfort,not
scikit-learn/sklearn/pipeline.py,9,Lars Buitinck,not
scikit-learn/sklearn/pipeline.py,10,License: BSD,not
scikit-learn/sklearn/pipeline.py,106,BaseEstimator interface,not
scikit-learn/sklearn/pipeline.py,147,validate names,not
scikit-learn/sklearn/pipeline.py,150,validate estimators,not
scikit-learn/sklearn/pipeline.py,164,We allow last estimator to be None as an identity transformation,not
scikit-learn/sklearn/pipeline.py,211,"Not an int, try get step by name",not
scikit-learn/sklearn/pipeline.py,221,Use Bunch object to improve autocomplete,not
scikit-learn/sklearn/pipeline.py,253,Estimator interface,not
scikit-learn/sklearn/pipeline.py,256,shallow copy of steps - this should really be steps_,not
scikit-learn/sklearn/pipeline.py,259,Setup the memory,not
scikit-learn/sklearn/pipeline.py,274,joblib >= 0.12,not
scikit-learn/sklearn/pipeline.py,276,we do not clone when caching is disabled to,not
scikit-learn/sklearn/pipeline.py,277,preserve backward compatibility,not
scikit-learn/sklearn/pipeline.py,282,joblib < 0.11,not
scikit-learn/sklearn/pipeline.py,284,we do not clone when caching is disabled to,not
scikit-learn/sklearn/pipeline.py,285,preserve backward compatibility,not
scikit-learn/sklearn/pipeline.py,291,Fit or load from cache the current transformer,not
scikit-learn/sklearn/pipeline.py,297,Replace the transformer of the step with the fitted,not
scikit-learn/sklearn/pipeline.py,298,transformer. This is necessary when loading the transformer,not
scikit-learn/sklearn/pipeline.py,299,from the cache.,not
scikit-learn/sklearn/pipeline.py,540,"_final_estimator is None or has transform, otherwise attribute error",not
scikit-learn/sklearn/pipeline.py,541,XXX: Handling the None case means we can't use if_delegate_has_method,SATD
scikit-learn/sklearn/pipeline.py,570,raise AttributeError if necessary for hasattr behaviour,not
scikit-learn/sklearn/pipeline.py,571,XXX: Handling the None case means we can't use if_delegate_has_method,SATD
scikit-learn/sklearn/pipeline.py,619,check if first estimator expects pairwise input,not
scikit-learn/sklearn/pipeline.py,624,delegate to first step (which will call _check_is_fitted),not
scikit-learn/sklearn/pipeline.py,633,Is an estimator,not
scikit-learn/sklearn/pipeline.py,720,"if we have a weight for this transformer, multiply output",not
scikit-learn/sklearn/pipeline.py,863,validate names,not
scikit-learn/sklearn/pipeline.py,866,validate estimators,not
scikit-learn/sklearn/pipeline.py,868,TODO: Remove in 0.24 when None is removed,SATD
scikit-learn/sklearn/pipeline.py,929,All transformers are None,not
scikit-learn/sklearn/pipeline.py,955,All transformers are None,not
scikit-learn/sklearn/pipeline.py,1004,All transformers are None,not
scikit-learn/sklearn/pipeline.py,1020,X is passed to all transformers so we just delegate to the first one,not
scikit-learn/sklearn/pipeline.py,1073,We do not currently support `transformer_weights` as we may want to,not
scikit-learn/sklearn/pipeline.py,1074,change its type spec in make_union,not
scikit-learn/sklearn/multioutput.py,9,Author: Tim Head <betatim@gmail.com>,not
scikit-learn/sklearn/multioutput.py,10,Author: Hugo Bowne-Anderson <hugobowne@gmail.com>,not
scikit-learn/sklearn/multioutput.py,11,Author: Chris Rivera <chris.richard.rivera@gmail.com>,not
scikit-learn/sklearn/multioutput.py,12,Author: Michael Williamson,not
scikit-learn/sklearn/multioutput.py,13,Author: James Ashton Nichols <james.ashton.nichols@gmail.com>,not
scikit-learn/sklearn/multioutput.py,14,,not
scikit-learn/sklearn/multioutput.py,15,License: BSD 3 clause,not
scikit-learn/sklearn/multioutput.py,421,FIXME,SATD
scikit-learn/sklearn/multioutput.py,793,TODO: remove in 0.24,SATD
scikit-learn/sklearn/setup.py,22,submodules with build utilities,not
scikit-learn/sklearn/setup.py,26,submodules which do not have their own setup.py,not
scikit-learn/sklearn/setup.py,27,we must manually add sub-submodules & tests,not
scikit-learn/sklearn/setup.py,60,submodules which have their own setup.py,not
scikit-learn/sklearn/setup.py,74,add cython extension module for isotonic regression,not
scikit-learn/sklearn/setup.py,81,add the test directory,not
scikit-learn/sklearn/setup.py,84,Skip cythonization as we do not want to include the generated,not
scikit-learn/sklearn/setup.py,85,C/C++ files in the release tarballs as they are not necessarily,not
scikit-learn/sklearn/setup.py,86,forward compatible with future versions of Python for instance.,not
scikit-learn/sklearn/random_projection.py,1,-*- coding: utf8,not
scikit-learn/sklearn/random_projection.py,26,"Authors: Olivier Grisel <olivier.grisel@ensta.org>,",not
scikit-learn/sklearn/random_projection.py,27,Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/random_projection.py,28,License: BSD 3 clause,not
scikit-learn/sklearn/random_projection.py,155,TODO: remove in 0.24,SATD
scikit-learn/sklearn/random_projection.py,202,TODO: remove in 0.24,SATD
scikit-learn/sklearn/random_projection.py,276,skip index generation if totally dense,not
scikit-learn/sklearn/random_projection.py,281,Generate location of non zero elements,not
scikit-learn/sklearn/random_projection.py,286,find the indices of the non-zero components for row i,not
scikit-learn/sklearn/random_projection.py,296,Among non zero components the probability of the sign is 50%/50%,not
scikit-learn/sklearn/random_projection.py,299,build the CSR structure by concatenating the rows,not
scikit-learn/sklearn/random_projection.py,393,"Generate a projection matrix of size [n_components, n_features]",not
scikit-learn/sklearn/random_projection.py,397,Check contract,not
scikit-learn/sklearn/ensemble/_bagging.py,3,Author: Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/sklearn/ensemble/_bagging.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_bagging.py,36,Draw sample indices,not
scikit-learn/sklearn/ensemble/_bagging.py,50,Get valid random state,not
scikit-learn/sklearn/ensemble/_bagging.py,53,Draw indices,not
scikit-learn/sklearn/ensemble/_bagging.py,65,Retrieve settings,not
scikit-learn/sklearn/ensemble/_bagging.py,76,Build estimators,not
scikit-learn/sklearn/ensemble/_bagging.py,89,"Draw random feature, sample indices",not
scikit-learn/sklearn/ensemble/_bagging.py,96,"Draw samples, using sample weights, and then fit",not
scikit-learn/sklearn/ensemble/_bagging.py,138,Resort to voting,not
scikit-learn/sklearn/ensemble/_bagging.py,280,Convert data (X is required to be 2d and indexable),not
scikit-learn/sklearn/ensemble/_bagging.py,288,Remap output,not
scikit-learn/sklearn/ensemble/_bagging.py,293,Check parameters,not
scikit-learn/sklearn/ensemble/_bagging.py,299,Validate max_samples,not
scikit-learn/sklearn/ensemble/_bagging.py,308,Store validated integer row sampling value,not
scikit-learn/sklearn/ensemble/_bagging.py,311,Validate max_features,not
scikit-learn/sklearn/ensemble/_bagging.py,324,Store validated integer feature sampling value,not
scikit-learn/sklearn/ensemble/_bagging.py,327,Other checks,not
scikit-learn/sklearn/ensemble/_bagging.py,340,"Free allocated memory, if any",not
scikit-learn/sklearn/ensemble/_bagging.py,356,Parallel loop,not
scikit-learn/sklearn/ensemble/_bagging.py,361,Advance random state to state after training,not
scikit-learn/sklearn/ensemble/_bagging.py,362,the first n_estimators,not
scikit-learn/sklearn/ensemble/_bagging.py,382,Reduce,not
scikit-learn/sklearn/ensemble/_bagging.py,404,Get drawn indices along both sample and feature axes,not
scikit-learn/sklearn/ensemble/_bagging.py,406,Operations accessing random_state must be performed identically,not
scikit-learn/sklearn/ensemble/_bagging.py,407,to those in `_parallel_build_estimators()`,not
scikit-learn/sklearn/ensemble/_bagging.py,621,Create mask for OOB samples,not
scikit-learn/sklearn/ensemble/_bagging.py,702,Check data,not
scikit-learn/sklearn/ensemble/_bagging.py,714,Parallel loop,not
scikit-learn/sklearn/ensemble/_bagging.py,727,Reduce,not
scikit-learn/sklearn/ensemble/_bagging.py,753,Check data,not
scikit-learn/sklearn/ensemble/_bagging.py,765,Parallel loop,not
scikit-learn/sklearn/ensemble/_bagging.py,777,Reduce,not
scikit-learn/sklearn/ensemble/_bagging.py,811,Check data,not
scikit-learn/sklearn/ensemble/_bagging.py,823,Parallel loop,not
scikit-learn/sklearn/ensemble/_bagging.py,834,Reduce,not
scikit-learn/sklearn/ensemble/_bagging.py,1023,Check data,not
scikit-learn/sklearn/ensemble/_bagging.py,1029,Parallel loop,not
scikit-learn/sklearn/ensemble/_bagging.py,1040,Reduce,not
scikit-learn/sklearn/ensemble/_bagging.py,1059,Create mask for OOB samples,not
scikit-learn/sklearn/ensemble/_gb_losses.py,103,compute leaf for each sample in ``X``.,not
scikit-learn/sklearn/ensemble/_gb_losses.py,106,mask all which are not in sample mask.,not
scikit-learn/sklearn/ensemble/_gb_losses.py,110,update each leaf (= perform line search),not
scikit-learn/sklearn/ensemble/_gb_losses.py,116,update predictions (both in-bag and out-of-bag),not
scikit-learn/sklearn/ensemble/_gb_losses.py,257,update predictions,not
scikit-learn/sklearn/ensemble/_gb_losses.py,578,we only need to fit one tree for binary clf.,not
scikit-learn/sklearn/ensemble/_gb_losses.py,582,"return the most common class, taking into account the samples",not
scikit-learn/sklearn/ensemble/_gb_losses.py,583,weights,not
scikit-learn/sklearn/ensemble/_gb_losses.py,601,"logaddexp(0, v) == log(1.0 + exp(v))",not
scikit-learn/sklearn/ensemble/_gb_losses.py,644,prevents overflow and division by zero,not
scikit-learn/sklearn/ensemble/_gb_losses.py,665,log(x / (1 - x)) is the inverse of the sigmoid (expit) function,not
scikit-learn/sklearn/ensemble/_gb_losses.py,708,create one-hot label encoding,not
scikit-learn/sklearn/ensemble/_gb_losses.py,753,prevents overflow and division by zero,not
scikit-learn/sklearn/ensemble/_gb_losses.py,794,we only need to fit one tree for binary clf.,not
scikit-learn/sklearn/ensemble/_gb_losses.py,849,prevents overflow and division by zero,not
scikit-learn/sklearn/ensemble/_gb_losses.py,869,"according to The Elements of Statistical Learning sec. 10.5, the",not
scikit-learn/sklearn/ensemble/_gb_losses.py,870,minimizer of the exponential loss is .5 * log odds ratio. So this is,not
scikit-learn/sklearn/ensemble/_gb_losses.py,871,the equivalent to .5 * binomial_deviance.get_init_raw_predictions(),not
scikit-learn/sklearn/ensemble/_gb_losses.py,881,"for both, multinomial and binomial",not
scikit-learn/sklearn/ensemble/_stacking.py,3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>,not
scikit-learn/sklearn/ensemble/_stacking.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_stacking.py,74,case where the the estimator returned a 1D array,not
scikit-learn/sklearn/ensemble/_stacking.py,80,Remove the first column when using probabilities in,not
scikit-learn/sklearn/ensemble/_stacking.py,81,binary classification because both features are perfectly,not
scikit-learn/sklearn/ensemble/_stacking.py,82,collinear.,not
scikit-learn/sklearn/ensemble/_stacking.py,135,"all_estimators contains all estimators, the one to be fitted and the",not
scikit-learn/sklearn/ensemble/_stacking.py,136,'drop' string.,not
scikit-learn/sklearn/ensemble/_stacking.py,142,Fit the base estimators on the whole training data. Those,not
scikit-learn/sklearn/ensemble/_stacking.py,143,"base estimators will be used in transform, predict, and",not
scikit-learn/sklearn/ensemble/_stacking.py,144,predict_proba. They are exposed publicly.,not
scikit-learn/sklearn/ensemble/_stacking.py,161,"To train the meta-classifier using the most data as possible, we use",not
scikit-learn/sklearn/ensemble/_stacking.py,162,a cross-validation to obtain the output of the stacked estimators.,not
scikit-learn/sklearn/ensemble/_stacking.py,164,"To ensure that the data provided to each estimator are the same, we",not
scikit-learn/sklearn/ensemble/_stacking.py,165,need to set the random state of the cv if there is one and we need to,not
scikit-learn/sklearn/ensemble/_stacking.py,166,take a copy.,not
scikit-learn/sklearn/ensemble/_stacking.py,187,Only not None or not 'drop' estimators will be used in transform.,not
scikit-learn/sklearn/ensemble/_stacking.py,188,Remove the None from the method as well.,not
scikit-learn/sklearn/ensemble/_stacking.py,509,If final_estimator's default changes then this should be,not
scikit-learn/sklearn/ensemble/_stacking.py,510,updated.,not
scikit-learn/sklearn/ensemble/_stacking.py,688,If final_estimator's default changes then this should be,not
scikit-learn/sklearn/ensemble/_stacking.py,689,updated.,not
scikit-learn/sklearn/ensemble/_forest.py,35,Authors: Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/sklearn/ensemble/_forest.py,36,Brian Holt <bdholt1@gmail.com>,not
scikit-learn/sklearn/ensemble/_forest.py,37,Joly Arnaud <arnaud.v.joly@gmail.com>,not
scikit-learn/sklearn/ensemble/_forest.py,38,Fares Hedayati <fares.hedayati@gmail.com>,not
scikit-learn/sklearn/ensemble/_forest.py,39,,not
scikit-learn/sklearn/ensemble/_forest.py,40,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_forest.py,298,Validate or convert input data,not
scikit-learn/sklearn/ensemble/_forest.py,309,Pre-sort indices to avoid that each individual tree of the,not
scikit-learn/sklearn/ensemble/_forest.py,310,ensemble sorts the indices.,not
scikit-learn/sklearn/ensemble/_forest.py,313,Remap output,not
scikit-learn/sklearn/ensemble/_forest.py,324,reshape is necessary to preserve the data contiguity against vs,not
scikit-learn/sklearn/ensemble/_forest.py,325,"[:, np.newaxis] that does not.",not
scikit-learn/sklearn/ensemble/_forest.py,341,Get bootstrap sample size,not
scikit-learn/sklearn/ensemble/_forest.py,347,Check parameters,not
scikit-learn/sklearn/ensemble/_forest.py,357,"Free allocated memory, if any",not
scikit-learn/sklearn/ensemble/_forest.py,372,We draw from the random state to get the random state we,not
scikit-learn/sklearn/ensemble/_forest.py,373,would have got if we hadn't used a warm_start.,not
scikit-learn/sklearn/ensemble/_forest.py,380,Parallel loop: we prefer the threading backend as the Cython code,not
scikit-learn/sklearn/ensemble/_forest.py,381,for fitting the trees is internally releasing the Python GIL,not
scikit-learn/sklearn/ensemble/_forest.py,382,making threading more efficient than multiprocessing in,not
scikit-learn/sklearn/ensemble/_forest.py,383,"that case. However, for joblib 0.12+ we respect any",not
scikit-learn/sklearn/ensemble/_forest.py,384,"parallel_backend contexts set at a higher level,",not
scikit-learn/sklearn/ensemble/_forest.py,385,since correctness does not rely on using threads.,not
scikit-learn/sklearn/ensemble/_forest.py,394,Collect newly grown trees,not
scikit-learn/sklearn/ensemble/_forest.py,400,Decapsulate classes_ attributes,not
scikit-learn/sklearn/ensemble/_forest.py,413,Default implementation,not
scikit-learn/sklearn/ensemble/_forest.py,636,"all dtypes should be the same, so just take the first",not
scikit-learn/sklearn/ensemble/_forest.py,672,Check data,not
scikit-learn/sklearn/ensemble/_forest.py,675,Assign chunk of trees to jobs,not
scikit-learn/sklearn/ensemble/_forest.py,678,avoid storing the output of every estimator by summing them here,not
scikit-learn/sklearn/ensemble/_forest.py,782,Check data,not
scikit-learn/sklearn/ensemble/_forest.py,785,Assign chunk of trees to jobs,not
scikit-learn/sklearn/ensemble/_forest.py,788,avoid storing the output of every estimator by summing them here,not
scikit-learn/sklearn/ensemble/_forest.py,794,Parallel loop,not
scikit-learn/sklearn/ensemble/_forest.py,874,Note: we don't sum in parallel because the GIL isn't released in,not
scikit-learn/sklearn/ensemble/_forest.py,875,the fast method.,not
scikit-learn/sklearn/ensemble/_forest.py,878,Average over the forest,not
scikit-learn/sklearn/ensemble/_forest.py,2331,Pre-sort indices to avoid that each individual tree of the,not
scikit-learn/sklearn/ensemble/_forest.py,2332,ensemble sorts the indices.,not
scikit-learn/sklearn/ensemble/__init__.py,26,Avoid errors in type checkers (e.g. mypy) for experimental estimators.,not
scikit-learn/sklearn/ensemble/__init__.py,27,TODO: remove this check once the estimator is no longer experimental.,SATD
scikit-learn/sklearn/ensemble/__init__.py,28,noqa,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,19,Authors: Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,20,Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,21,Hamzeh Alsalhi <ha258@cornell.edu>,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,22,Arnaud Joly <arnaud.v.joly@gmail.com>,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,23,,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,24,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,100,Check parameters,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,116,Check parameters,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,119,Clear any previous fit results,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,124,Initializion of the random number instance that will be used to,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,125,generate a seed at each iteration,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,129,Boosting step,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,136,Early termination,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,143,Stop if error is zero,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,149,Stop if the sum of sample weights has become non-positive,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,154,Normalize,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,275,Displace zero probabilities so the log is defined.,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,276,Also fix negative elements which may occur with,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,277,negative sample weights.,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,438,Check that algorithm is supported,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,442,Fit,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,450,SAMME-R requires predict_proba-enabled base estimators,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,505,"elif self.algorithm == ""SAMME"":",not
scikit-learn/sklearn/ensemble/_weight_boosting.py,524,Instances incorrectly classified,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,527,Error fraction,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,531,Stop if classification is perfect,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,535,Construct y coding as described in Zhu et al [2]:,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,536,,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,537,y_k = 1 if c == k else -1 / (K - 1),not
scikit-learn/sklearn/ensemble/_weight_boosting.py,538,,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,539,"where K == n_classes_ and c, k in [0, K) are indices along the second",not
scikit-learn/sklearn/ensemble/_weight_boosting.py,540,axis of the y coding with c being the index corresponding to the true,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,541,class label.,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,547,Displace zero probabilities so the log is defined.,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,548,Also fix negative elements which may occur with,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,549,negative sample weights.,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,550,alias for readability,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,553,Boost weight using multi-class AdaBoost SAMME.R alg,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,558,Only boost the weights if it will fit again,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,560,Only boost positive weights,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,579,Instances incorrectly classified,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,582,Error fraction,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,586,Stop if classification is perfect,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,592,Stop if the error is at least as bad as random guessing,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,601,Boost weight using multi-class AdaBoost SAMME alg,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,606,Only boost the weights if I will fit again,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,608,Only boost positive weights,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,701,The weights are all 1. for SAMME.R,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,704,"self.algorithm == ""SAMME""",not
scikit-learn/sklearn/ensemble/_weight_boosting.py,750,The weights are all 1. for SAMME.R,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,752,"elif self.algorithm == ""SAMME"":",not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1001,Check loss,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1006,Fit,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1058,Weighted sampling of the training set with replacement,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1064,Fit on the bootstrapped sample and obtain a prediction,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1065,for all samples in the training set,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1085,Calculate the average loss,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1089,Stop if fit is perfect,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1093,Discard current estimator only if it isn't the only one,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1100,Boost weight using AdaBoost.R2 alg,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1111,Evaluate predictions of all estimators,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1115,Sort the predictions,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1118,Find index of median prediction for each sample,not
scikit-learn/sklearn/ensemble/_weight_boosting.py,1125,Return median predictions,not
scikit-learn/sklearn/ensemble/setup.py,14,Histogram-based gradient boosting files,not
scikit-learn/sklearn/ensemble/_base.py,3,Authors: Gilles Louppe,not
scikit-learn/sklearn/ensemble/_base.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_base.py,109,overwrite _required_parameters from MetaEstimatorMixin,not
scikit-learn/sklearn/ensemble/_base.py,115,Set parameters,not
scikit-learn/sklearn/ensemble/_base.py,120,Don't instantiate estimators now! Parameters of base_estimator might,not
scikit-learn/sklearn/ensemble/_base.py,121,"still change. Eg., when grid-searching with the nested object syntax.",not
scikit-learn/sklearn/ensemble/_base.py,122,self.estimators_ needs to be filled by the derived classes in fit.,not
scikit-learn/sklearn/ensemble/_base.py,178,Compute the number of jobs,not
scikit-learn/sklearn/ensemble/_base.py,181,Partition estimators between jobs,not
scikit-learn/sklearn/ensemble/_base.py,227,defined by MetaEstimatorMixin,not
scikit-learn/sklearn/ensemble/_base.py,230,FIXME: deprecate the usage of None to drop an estimator from the,SATD
scikit-learn/sklearn/ensemble/_base.py,231,ensemble. Remove in 0.24,not
scikit-learn/sklearn/ensemble/_gb.py,19,"Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,",not
scikit-learn/sklearn/ensemble/_gb.py,20,"Arnaud Joly, Jacob Schreiber",not
scikit-learn/sklearn/ensemble/_gb.py,21,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_gb.py,83,header fields and line format str,not
scikit-learn/sklearn/ensemble/_gb.py,86,do oob?,not
scikit-learn/sklearn/ensemble/_gb.py,93,print the header line,not
scikit-learn/sklearn/ensemble/_gb.py,98,plot verbose info each time i % verbose_mod == 0,not
scikit-learn/sklearn/ensemble/_gb.py,114,we need to take into account if we fit additional estimators.,not
scikit-learn/sklearn/ensemble/_gb.py,115,iteration relative to the start iter,not
scikit-learn/sklearn/ensemble/_gb.py,129,adjust verbose frequency (powers of 10),not
scikit-learn/sklearn/ensemble/_gb.py,178,Need to pass a copy of raw_predictions to negative_gradient(),not
scikit-learn/sklearn/ensemble/_gb.py,179,because raw_predictions is partially updated at the end of the loop,not
scikit-learn/sklearn/ensemble/_gb.py,180,"in update_terminal_regions(), and gradients need to be evaluated at",not
scikit-learn/sklearn/ensemble/_gb.py,181,iteration i - 1.,not
scikit-learn/sklearn/ensemble/_gb.py,191,induce regression tree on residuals,not
scikit-learn/sklearn/ensemble/_gb.py,207,no inplace multiplication!,not
scikit-learn/sklearn/ensemble/_gb.py,214,update tree leaves,not
scikit-learn/sklearn/ensemble/_gb.py,219,add tree to ensemble,not
scikit-learn/sklearn/ensemble/_gb.py,255,init must be an estimator or 'zero',not
scikit-learn/sklearn/ensemble/_gb.py,270,if is_classification,not
scikit-learn/sklearn/ensemble/_gb.py,274,is regression,not
scikit-learn/sklearn/ensemble/_gb.py,288,float,not
scikit-learn/sklearn/ensemble/_gb.py,321,do oob?,not
scikit-learn/sklearn/ensemble/_gb.py,341,self.n_estimators is the number of additional est to fit,not
scikit-learn/sklearn/ensemble/_gb.py,351,if do oob resize arrays or create new if not available,not
scikit-learn/sklearn/ensemble/_gb.py,401,if not warmstart - clear the estimator state,not
scikit-learn/sklearn/ensemble/_gb.py,405,Check input,not
scikit-learn/sklearn/ensemble/_gb.py,406,"Since check_array converts both X and y to the same dtype, but the",not
scikit-learn/sklearn/ensemble/_gb.py,407,"trees use different types for X and y, checking them separately.",not
scikit-learn/sklearn/ensemble/_gb.py,429,We choose to error here. The problem is that the init,not
scikit-learn/sklearn/ensemble/_gb.py,430,"estimator would be trained on y, which has some missing",not
scikit-learn/sklearn/ensemble/_gb.py,431,"classes now, so its predictions would not have the",not
scikit-learn/sklearn/ensemble/_gb.py,432,correct shape.,not
scikit-learn/sklearn/ensemble/_gb.py,444,init state,not
scikit-learn/sklearn/ensemble/_gb.py,447,fit initial model and initialize raw predictions,not
scikit-learn/sklearn/ensemble/_gb.py,452,XXX clean this once we have a support_sample_weight tag,SATD
scikit-learn/sklearn/ensemble/_gb.py,460,regular estimator without SW support,not
scikit-learn/sklearn/ensemble/_gb.py,465,pipeline,not
scikit-learn/sklearn/ensemble/_gb.py,467,regular estimator whose input checking failed,not
scikit-learn/sklearn/ensemble/_gb.py,475,The rng state must be preserved if warm_start is True,not
scikit-learn/sklearn/ensemble/_gb.py,479,add more estimators to fitted model,not
scikit-learn/sklearn/ensemble/_gb.py,480,invariant: warm_start = True,not
scikit-learn/sklearn/ensemble/_gb.py,488,The requirements of _decision_function (called in two lines,not
scikit-learn/sklearn/ensemble/_gb.py,489,below) are more constrained than fit. It accepts only CSR,not
scikit-learn/sklearn/ensemble/_gb.py,490,matrices.,not
scikit-learn/sklearn/ensemble/_gb.py,497,fit the boosting stages,not
scikit-learn/sklearn/ensemble/_gb.py,502,change shape of arrays after fit (early-stopping or additional ests),not
scikit-learn/sklearn/ensemble/_gb.py,537,We create a generator to get the predictions for X_val after,not
scikit-learn/sklearn/ensemble/_gb.py,538,the addition of each successive stage,not
scikit-learn/sklearn/ensemble/_gb.py,541,perform boosting iterations,not
scikit-learn/sklearn/ensemble/_gb.py,545,subsampling,not
scikit-learn/sklearn/ensemble/_gb.py,549,OOB score before adding this stage,not
scikit-learn/sklearn/ensemble/_gb.py,554,fit next stage of trees,not
scikit-learn/sklearn/ensemble/_gb.py,559,track deviance (= loss),not
scikit-learn/sklearn/ensemble/_gb.py,569,no need to fancy index w/ no subsampling,not
scikit-learn/sklearn/ensemble/_gb.py,580,We also provide an early stopping based on the score from,not
scikit-learn/sklearn/ensemble/_gb.py,581,"validation set (X_val, y_val), if n_iter_no_change is set",not
scikit-learn/sklearn/ensemble/_gb.py,583,"By calling next(y_val_pred_iter), we get the predictions",not
scikit-learn/sklearn/ensemble/_gb.py,584,for X_val after the addition of the current stage,not
scikit-learn/sklearn/ensemble/_gb.py,588,Require validation_score to be better (less) than at least,SATD
scikit-learn/sklearn/ensemble/_gb.py,589,one of the last n_iter_no_change evaluations,not
scikit-learn/sklearn/ensemble/_gb.py,598,we don't need _make_estimator,not
scikit-learn/sklearn/ensemble/_gb.py,677,degenerate case where all trees have only one node,not
scikit-learn/sklearn/ensemble/_gb.py,727,'sample_weight' is not utilised but is used for,not
scikit-learn/sklearn/ensemble/_gb.py,728,consistency with similar method _validate_y of GBC,not
scikit-learn/sklearn/ensemble/_gb.py,732,Default implementation,not
scikit-learn/sklearn/ensemble/_gb.py,758,n_classes will be equal to 1 in the binary classification or the,not
scikit-learn/sklearn/ensemble/_gb.py,759,regression case.,not
scikit-learn/sklearn/ensemble/_gb.py,1610,In regression we can directly return the raw value from the trees.,not
scikit-learn/sklearn/ensemble/_iforest.py,1,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,not
scikit-learn/sklearn/ensemble/_iforest.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/ensemble/_iforest.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_iforest.py,204,here above max_features has no links with self.max_features,not
scikit-learn/sklearn/ensemble/_iforest.py,222,"ExtraTreeRegressor releases the GIL, so it's more efficient to use",not
scikit-learn/sklearn/ensemble/_iforest.py,223,a thread-based backend rather than a process-based backend so as,not
scikit-learn/sklearn/ensemble/_iforest.py,224,to avoid suffering from communication overhead and extra memory,not
scikit-learn/sklearn/ensemble/_iforest.py,225,copies.,not
scikit-learn/sklearn/ensemble/_iforest.py,265,Pre-sort indices to avoid that each individual tree of the,not
scikit-learn/sklearn/ensemble/_iforest.py,266,ensemble sorts the indices.,not
scikit-learn/sklearn/ensemble/_iforest.py,272,"ensure that max_sample is in [1, n_samples]:",not
scikit-learn/sklearn/ensemble/_iforest.py,292,float,not
scikit-learn/sklearn/ensemble/_iforest.py,305,0.5 plays a special role as described in the original paper.,not
scikit-learn/sklearn/ensemble/_iforest.py,306,we take the opposite as we consider the opposite of their score.,not
scikit-learn/sklearn/ensemble/_iforest.py,310,"else, define offset_ wrt contamination parameter",not
scikit-learn/sklearn/ensemble/_iforest.py,366,We subtract self.offset_ to make 0 be the threshold value for being,not
scikit-learn/sklearn/ensemble/_iforest.py,367,an outlier:,not
scikit-learn/sklearn/ensemble/_iforest.py,395,code structure from ForestClassifier/predict_proba,not
scikit-learn/sklearn/ensemble/_iforest.py,399,Check data,not
scikit-learn/sklearn/ensemble/_iforest.py,407,Take the opposite of the scores as bigger is better (here less,not
scikit-learn/sklearn/ensemble/_iforest.py,408,abnormal),not
scikit-learn/sklearn/ensemble/_iforest.py,420,We get as many rows as possible within our working_memory budget,not
scikit-learn/sklearn/ensemble/_iforest.py,421,(defined by sklearn.get_config()['working_memory']) to store,not
scikit-learn/sklearn/ensemble/_iforest.py,422,self._max_features in each row during computation.,not
scikit-learn/sklearn/ensemble/_iforest.py,423,,not
scikit-learn/sklearn/ensemble/_iforest.py,424,Note:,not
scikit-learn/sklearn/ensemble/_iforest.py,425,"- this will get at least 1 row, even if 1 row of score will",not
scikit-learn/sklearn/ensemble/_iforest.py,426,exceed working_memory.,not
scikit-learn/sklearn/ensemble/_iforest.py,427,- this does only account for temporary memory usage while loading,not
scikit-learn/sklearn/ensemble/_iforest.py,428,the data needed to compute the scores -- the returned scores,not
scikit-learn/sklearn/ensemble/_iforest.py,429,themselves are 1D.,not
scikit-learn/sklearn/ensemble/_iforest.py,438,compute score on the slices of test samples:,not
scikit-learn/sklearn/ensemble/_voting.py,9,"Authors: Sebastian Raschka <se.raschka@gmail.com>,",not
scikit-learn/sklearn/ensemble/_voting.py,10,"Gilles Louppe <g.louppe@gmail.com>,",not
scikit-learn/sklearn/ensemble/_voting.py,11,Ramil Nugmanov <stsouko@live.ru>,not
scikit-learn/sklearn/ensemble/_voting.py,12,Mohamed Ali Jamaoui <m.ali.jamaoui@gmail.com>,not
scikit-learn/sklearn/ensemble/_voting.py,13,,not
scikit-learn/sklearn/ensemble/_voting.py,14,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/_voting.py,86,Uses None or 'drop' as placeholder for dropped estimators,not
scikit-learn/sklearn/ensemble/_voting.py,96,For consistency with other estimators we raise a AttributeError so,not
scikit-learn/sklearn/ensemble/_voting.py,97,that hasattr() fails if the estimator isn't fitted.,not
scikit-learn/sklearn/ensemble/_voting.py,284,'hard' voting,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/predictor.py,4,Author: Nicolas Hug,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,8,Author: Nicolas Hug,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,58,ignore missing values when computing bin thresholds,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,68,We sort again the data in this case. We could compute,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,69,approximate midpoint percentiles using the output of,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,70,"np.unique(col_data, return_counts) instead but this is more",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,71,work and the performance benefit will be limited because we,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,72,work on a fixed-size subsample of the full data.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,79,We avoid having +inf thresholds: +inf thresholds are only allowed in,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,80,"a ""split on nan"" situation.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/binning.py,160,min is 3: at least 2 distinct bins and a missing values bin,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,7,Author: Nicolas Hug,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,41,This variable indicates whether the loss requires the leaves values to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,42,be updated once the tree has been trained. The trees are trained to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,43,predict a Newton-Raphson step (see grower._finalize_leaf()). But for,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,44,some losses (e.g. least absolute deviation) we need to adjust the tree,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,45,"values to account for the ""line search"" of the gradient descent",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,46,procedure. See the original paper Greedy Function Approximation: A,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,47,Gradient Boosting Machine by Friedman,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,48,(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) for the theory.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,85,"If the hessians are constant, we consider they are equal to 1.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,86,- This is correct for the half LS loss,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,87,"- For LAD loss, hessians are actually 0, but they are always",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,88,ignored anyway.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,160,"If sample weights are provided, the hessians and gradients",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,161,"are multiplied by sample_weight, which means the hessians are",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,162,equal to sample weights.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,166,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,167,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,181,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,182,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,203,"If sample weights are provided, the hessians and gradients",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,204,"are multiplied by sample_weight, which means the hessians are",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,205,equal to sample weights.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,208,This variable indicates whether the loss requires the leaves values to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,209,be updated once the tree has been trained. The trees are trained to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,210,predict a Newton-Raphson step (see grower._finalize_leaf()). But for,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,211,some losses (e.g. least absolute deviation) we need to adjust the tree,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,212,"values to account for the ""line search"" of the gradient descent",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,213,procedure. See the original paper Greedy Function Approximation: A,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,214,Gradient Boosting Machine by Friedman,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,215,(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) for the theory.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,219,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,220,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,237,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,238,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,251,Update the values predicted by the tree with,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,252,median(y_true - raw_predictions).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,253,See note about need_update_leaves_values in BaseLoss.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,255,TODO: ideally this should be computed in parallel over the leaves,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,256,"using something similar to _update_raw_predictions(), but this",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,257,requires a cython version of median(),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,269,Note that the regularization is ignored here,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,290,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,291,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,293,"TODO: For speed, we could remove the constant xlogy(y_true, y_true)",SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,294,Advantage of this form: minimum of zero at raw_predictions = y_true.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,307,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,308,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,335,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,336,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,338,"logaddexp(0, x) = log(1 + exp(x))",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,351,"log(x / 1 - x) is the anti function of sigmoid, or the link function",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,352,of the Binomial model.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,357,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,358,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,366,"shape (1, n_samples) --> (n_samples,). reshape(-1) is more likely to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,367,return a view.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,413,TODO: This could be done in parallel,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/loss.py,414,compute softmax (using exp(log(softmax))),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,2,Author: Nicolas Hug,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,116,time spent finding the best splits,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,117,time spent splitting nodes,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,118,time spent computing histograms,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,119,time spent predicting X for gradient and hessians update,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,125,Do not create unit sample weights by default to later skip some,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,126,computation,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,130,TODO: remove when PDP suports sample weights,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,135,"When warm starting, we want to re-use the same seed that was used",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,136,the first time fit was called (e.g. for subsampling or for the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,137,train/val split).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,143,used for validation in predict,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,145,we need this stateful variable to tell raw_predict() that it was,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,146,"called from fit() (this current method), and that the data it has",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,147,received is pre-binned.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,148,"predicting is faster on pre-binned data, so we want early stopping",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,149,predictions to be made on pre-binned data. Unfortunately the scorer_,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,150,"can only call predict() or predict_proba(), not raw_predict(), and",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,151,there's no way to tell the scorer that it needs to predict binned,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,152,data.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,165,create validation data if needed,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,168,stratify for classification,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,171,Save the state of the RNG for the training and validation split.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,172,This is needed in order to have the same split when using,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,173,warm starting.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,182,"TODO: incorporate sample_weight in sampling here, as well as",SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,183,stratify,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,195,Bin the data,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,196,"For ease of use of the API, the user-facing GBDT classes accept the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,197,"parameter max_bins, which doesn't take into account the bin for",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,198,"missing values (which is always allocated). However, since max_bins",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,199,"isn't the true maximal number of bins, all other private classes",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,200,"(binmapper, histbuilder...) accept n_bins instead, which is the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,201,"actual total number of bins. Everywhere in the code, the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,202,convention is that n_bins == max_bins + 1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,203,+ 1 for missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,217,"First time calling fit, or no warm start",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,219,Clear random state and score attributes,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,222,initialize raw_predictions: those are the accumulated values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,223,predicted by the trees for the training data. raw_predictions has,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,224,"shape (n_trees_per_iteration, n_samples) where",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,225,"n_trees_per_iterations is n_classes in multiclass classification,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,226,else 1.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,236,predictors is a matrix (list of lists) of TreePredictor objects,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,237,"with shape (n_iter_, n_trees_per_iteration)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,240,Initialize structures and attributes related to early stopping,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,241,set if scoring != loss,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,242,set if scoring == loss and use val,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,247,populate train_score and validation_score with the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,248,predictions of the initial model (before the first tree),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,251,we're going to compute scoring w.r.t the loss. As losses,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,252,"take raw predictions as input (unlike the scorers), we",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,253,can optimize a bit and avoid repeating computing the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,254,predictions of the previous trees. We'll re-use,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,255,raw_predictions (as it's needed for training anyway) for,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,256,"evaluating the training loss, and create",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,257,raw_predictions_val for storing the raw predictions of,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,258,the validation data.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,275,"scorer_ is a callable with signature (est, X, y) and",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,276,calls est.predict() or est.predict_proba() depending on,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,277,its nature.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,278,"Unfortunately, each call to scorer_() will compute",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,279,the predictions of all the trees. So we use a subset of,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,280,the training set to compute train scores.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,282,Compute the subsample set,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,296,warm start: this is not the first time fit was called,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,298,Check that the maximum number of iterations is not smaller,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,299,than the number of iterations from the previous fit,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,307,Convert array attributes to lists,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,311,Compute raw predictions,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,319,Compute the subsample set,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,326,Get the predictors from the previous fit,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,331,initialize gradients and hessians (empty arrays).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,332,"shape = (n_trees_per_iteration, n_samples).",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,346,"Update gradients and hessians, inplace",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,351,Append a list since there may be more than 1 predictor per iter,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,354,Build `n_trees_per_iteration` trees.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,383,Update raw_predictions with the predictions of the newly,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,384,created tree.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,393,Update raw_predictions_val with the newest tree(s),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,418,maybe we could also early stop if all the trees are stumps?,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,445,hard delete so we're sure it can't be used anymore,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,464,TODO: incorporate sample_weights here in `resample`,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,553,A higher score is always better. Higher tol means that it will be,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,554,harder for subsequent iteration to be considered an improvement upon,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,555,"the reference score, and therefore it is more likely to early stop",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,556,because of the lack of significant improvement.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,577,F-aligned array,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,579,F-aligned array,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,580,We convert the array to C-contiguous since predicting is faster,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,581,with this layout (training is faster on F-arrays though),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,614,score_ arrays contain the negative loss,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,663,noqa,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,704,Note that the learning rate is already accounted for in the leaves,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,705,values.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,909,Return inverse link of raw predictions after converting,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,910,"shape (n_samples, 1) to (n_samples,)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,914,Just convert y to the expected dtype,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,918,Ensure y >= 0 and sum(y) > 0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1113,TODO: This could be done in parallel,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1155,encode classes into 0 ... n_classes - 1 and sets attributes classes_,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1156,and n_trees_per_iteration_,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1163,"only 1 tree for binary classification. For multiclass classification,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py,1164,we build 1 tree per class.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,7,Author: Nicolas Hug,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,23,to avoid zero division errors,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,79,start and stop indices of the node in the splitter.partition,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,80,"array. Concretely,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,81,self.sample_indices = view(self.splitter.partition[start:stop]),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,82,Please see the comments about splitter.partition and,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,83,splitter.split_indices for more info about this design.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,84,"These 2 attributes are only used in _update_raw_prediction, because we",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,85,need to iterate over the leaves and I don't know how to efficiently,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,86,store the sample_indices views because they're all of different sizes.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,105,"These are bounds for the node's *children* values, not the node's",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,106,value. The bounds are used in the splitter when considering potential,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,107,left and right child.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,248,time spent finding the best splits,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,249,time spent computing histograms,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,250,time spent splitting nodes,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,327,Do not even bother computing any splitting statistics.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,352,no valid split,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,367,Consider the node with the highest loss reduction (a.k.a. gain),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,401,set start and stop indices,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,408,"If no missing values are encountered at fit time, then samples",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,409,with missing values during predict() will go to whichever child,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,410,has the most samples.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,434,Set value bounds for respecting monotonic constraints,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,435,See test_nodes_values() for details,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,446,NEG,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,452,"Compute histograms of children, and compute their best possible split",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,453,(if needed),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,458,We will compute the histograms of both nodes even if one of them,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,459,"is a leaf, since computing the second histogram is very cheap",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,460,(using histogram subtraction).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,470,We use the brute O(n_samples) method on the child that has the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,471,"smallest number of samples, and the subtraction trick O(n_bins)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,472,on the other one.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,539,Leaf node,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,543,Decision node,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,551,"Split is on the last non-missing bin: it's a ""split on nans"". All",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/grower.py,552,"nans go to the right, the rest go to the left.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,21,"create gradients and hessians array, update inplace, and return",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,29,"create gradients and hessians array, update inplace, and return",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,36,hessians aren't updated because they're constant:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,37,the value is 1 (and not 2) because the loss is actually an half,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,38,least squares loss.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,41,hessians aren't updated because they're constant,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,53,The argmin of binary_crossentropy for y_true=0 and y_true=1 is resp. -inf,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,54,"and +inf due to logit, cf. ""complete separation"". Therefore, we use",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,55,0 < y_true < 1.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,67,Check that gradients are zero when the loss is minimized on a single,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,68,value/sample using Halley's method with the first and second order,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,69,derivatives computed by the Loss instance.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,70,Note that methods of Loss instances operate on arrays while the newton,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,71,root finder expects a scalar or a one-element array for this purpose.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,80,Subtract a constant term such that the binary cross entropy,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,81,"has its minimum at zero, which is needed for the newton method.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,96,Need to ravel arrays because assert_allclose requires matching dimensions,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,114,"Make sure gradients and hessians computed in the loss are correct, by",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,115,comparing with their approximations computed with finite central,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,116,differences.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,117,See https://en.wikipedia.org/wiki/Finite_difference.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,133,only take gradients and hessians of first tree / class.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,137,Approximate gradients,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,138,"For multiclass loss, we should only change the predictions of one tree",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,139,"(here the first), hence the use of offset[0, :] += eps",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,140,"As a softmax is computed, offsetting the whole array by a constant would",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,141,"have no effect on the probabilities, and thus on the loss",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,149,Approximate hessians,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,150,need big enough eps as we divide by its square,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,167,scalar,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,169,Make sure baseline prediction is the mean of all targets,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,181,scalar,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,183,Make sure baseline prediction is the median of all targets,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,194,"Sanity check, make sure at least one sample is non-zero so we don't take",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,195,log(0),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,201,Make sure baseline prediction produces the log of the mean of all targets,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,204,Test baseline for y_true = 0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,221,"Make sure baseline prediction is equal to link_function(p), where p",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,222,"is the proba of the positive class. We want predict_proba() to return p,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,223,and by definition,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,224,p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,225,So we want raw_prediction = link_function(p) = log(p / (1 - p)),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,228,scalar,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,246,Same logic as for above test. Here inverse_link_function = softmax and,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,247,link_function = log,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,266,Make sure that passing sample weights to the gradient and hessians,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,267,computation methods is equivalent to multiplying by the weights.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,313,Make sure that passing sample_weight to a loss correctly influences the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,314,"hessians_are_constant attribute, and consequently the shape of the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_loss.py,315,hessians array.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,7,"To use this experimental feature, we need to explicitly ask for it:",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,8,noqa,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,24,Make sure sklearn has the same predictions as lightgbm for easy targets.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,25,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,26,In particular when the size of the trees are bound and the number of,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,27,"samples is large enough, the structure of the prediction trees found by",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,28,LightGBM and sklearn should be exactly identical.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,29,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,30,Notes:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,31,- Several candidate splits may have equal gains when the number of,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,32,samples in a node is low (and because of float errors). Therefore the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,33,predictions on the test set might differ if the structure of the tree,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,34,is not exactly the same. To avoid this issue we only compare the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,35,predictions on the test set when the number of samples is large enough,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,36,and max_leaf_nodes is low enough.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,37,- To ignore  discrepancies caused by small differences the binning,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,38,"strategy, data is pre-binned if n_samples > 255.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,39,- We don't check the least_absolute_deviation loss here. This is because,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,40,LightGBM's computation of the median (used for the initial value of,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,41,raw_prediction) is a bit off (they'll e.g. return midpoints when there,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,42,"is no need to.). Since these tests only run 1 iteration, the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,43,discrepancy between the initial values leads to biggish differences in,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,44,the predictions. These differences are much smaller with more,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,45,iterations.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,57,bin data and convert it to float32 so that the estimator doesn't,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,58,treat it as pre-binned,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,75,"We need X to be treated an numerical data, not pre-binned data.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,80,less than 1% of the predictions are different up to the 3rd decimal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,86,less than 1% of the predictions are different up to the 4th decimal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,98,Same as test_same_predictions_regression but for classification,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,110,bin data and convert it to float32 so that the estimator doesn't,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,111,treat it as pre-binned,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,129,"We need X to be treated an numerical data, not pre-binned data.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,159,Same as test_same_predictions_regression but for classification,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,173,bin data and convert it to float32 so that the estimator doesn't,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,174,treat it as pre-binned,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,192,"We need X to be treated an numerical data, not pre-binned data.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,201,assert more than 75% of the predicted probabilities are the same up to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,202,the second decimal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,217,assert more than 75% of the predicted probabilities are the same up,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py,218,to the second decimal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,24,Init gradients and hessians to that of least squares loss,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,50,Make sure infinite values and infinite thresholds are handled properly.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,51,"In particular, if a value is +inf and the threshold is ALMOST_INF the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,52,sample should go to the right child. If the threshold is inf (split on,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,53,"nan), the +inf sample will go to the left child.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,58,We just construct a simple tree with 1 root and 2 children,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,59,parent node,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,65,left child,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py,69,right child,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,22,Just a redef to avoid having to pass arguments all the time (as the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,23,function is private we don't use default values for parameters),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,60,255 - 1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,75,128 - 1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,122,max_bins is the number of bins for non-missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,138,Check that the binned data is approximately balanced across bins.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,155,max_bins is the number of bins for non-missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,173,max_bins is the number of bins for non-missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,196,Adding more bins to the mapper yields the same results (same thresholds),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,211,max_bins is the number of bins for non-missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,239,Check that n_bins_non_missing is n_unique_values when,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,240,"there are not a lot of unique values, else n_bins - 1.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,251,Make sure bin thresholds are different when applying subsampling,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,263,255 <=> missing value,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,269,2 <=> missing value,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,276,check for missing values: make sure nans are mapped to the last bin,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,277,and that the _BinMapper attributes are correct,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py,304,Make sure infinite values are properly handled.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,16,Generate some test data directly binned so as to test the grower code,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,17,independently of the binning logic.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,37,Assume a square loss applied to an initial model that always predicts 0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,38,(hardcoded for this test):,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,47,Make sure the samples are correctly dispatched from a parent to its,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,48,children,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,52,each sample from the parent is propagated to one of the two children,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,59,"samples are sent either to the left or the right node, never to both",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,91,"The root node is not yet splitted, but the best possible split has",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,92,already been evaluated:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,101,Calling split next applies the next split and computes the best split,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,102,for each of the two newly introduced children nodes.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,105,"All training samples have ben splitted in the two nodes, approximately",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,106,50%/50%,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,112,The left node is too pure: there is no gain to split it further.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,116,"The right node can still be splitted further, this time on feature #1",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,124,The right split has not been applied yet. Let's do it now:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,134,"All the leafs are pure, it is not possible to split any further:",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,139,Check the values of the leaves:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,147,Build a tree on the toy 3-leaf dataset to extract the predictor.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,155,(2 decision nodes + 3 leaves),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,157,Check that the node structure can be converted into a predictor,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,158,object to perform predictions at scale,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,163,Probe some predictions for each leaf of the tree,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,164,each group of 3 samples corresponds to a condition in _make_training_data,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,183,Check that training set can be recovered exactly:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,203,"data = linear target, 3 features, 1 irrelevant.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,237,Make sure root node isn't split if n_samples is not at least twice,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,238,min_samples_leaf,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,243,"data = linear target, 3 features, 1 irrelevant.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,263,To assert that stumps are created when max_depth=1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,271,Make sure max_depth parameter works as expected,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,277,"data = linear target, 3 features, 1 irrelevant.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,326,Make sure that missing values are supported at predict time even if they,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,327,were not encountered in the training data: the missing values are,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,328,assigned to whichever child has the most samples.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,344,"go from root to a leaf, always following node with the most samples.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,345,That's the path nans are supposed to take,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,354,"now build X_test with only nans, and make sure all predictions are equal",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,355,to prediction_main_path,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,361,Make sure the split on nan situations are respected even when there are,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,362,samples with +inf values (we set the threshold to +inf when we have a,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,363,split on nan so this test makes sure this does not introduce edge-case,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,364,bugs). We need to use the private API so that we can also test,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,365,predict_binned().,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,368,the gradient values will force a split on nan situation,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,388,sanity check: this was a split on nan,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,392,Make sure in particular that the +inf sample is mapped to the left child,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,393,"Note that lightgbm ""fails"" here and will assign the inf sample to the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py,394,"right child, even though it's a ""split on nan"" situation.",SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,13,noqa,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,27,make sure leaves values (from left to right) are either all increasing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,28,or all decreasing (or neither) depending on the monotonic constraint.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,43,start at root (0),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,49,"some increasing, some decreasing",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,52,all increasing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,54,NEG,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,55,all decreasing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,60,Make sure siblings values respect the monotonic constraints. Left should,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,61,be lower (resp greater) than right child if constraint is POS (resp.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,62,NEG).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,63,"Note that this property alone isn't enough to ensure full monotonicity,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,64,since we also need to guanrantee that all the descendents of the left,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,65,"child won't be greater (resp. lower) than the right child, or its",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,66,descendents. That's why we need to bound the predicted values (this is,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,67,tested in assert_children_values_bounded),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,87,NEG,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,92,Make sure that the values of the children of a node are bounded by the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,93,middle value between that node and its sibling (if there is a monotonic,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,94,constraint).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,95,"As a bonus, we also check that the siblings values are properly ordered",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,96,which is slightly redundant with assert_children_values_monotonic (but,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,97,this check is done on the grower nodes whereas,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,98,assert_children_values_monotonic is done on the predictor nodes),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,107,on the right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,117,NEG,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,139,"Build a single tree with only one feature, and make sure the nodes",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,140,values respect the monotonic constraints.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,142,"Considering the following tree with a monotonic POS constraint, we",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,143,should have:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,144,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,145,root,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,146,/    \,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,147,5     10    # middle = 7.5,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,148,/ \   / \,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,149,a  b  c  d,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,150,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,151,a <= b and c <= d  (assert_children_values_monotonic),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,152,"a, b <= middle <= c, d (assert_children_values_bounded)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,153,a <= b <= c <= d (assert_leaves_values_monotonic),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,154,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,155,"The last one is a consequence of the others, but can't hurt to check",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,172,grow() will shrink the leaves values at the very end. For our comparison,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,173,"tests, we need to revert the shrinkage of the leaves, else we would",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,174,compare the value of a leaf (shrunk) with a node (not shrunk) and the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,175,test would not be correct.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,179,The consistency of the bounds can only be checked on the tree grower,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,180,as the node bounds are not copied into the predictor tree. The,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,181,consistency checks on the values of node children and leaves can be,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,182,done either on the grower tree or on the predictor tree. We only,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,183,do those checks on the predictor tree as the latter is derived from,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,184,the former.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,193,Train a model with a POS constraint on the first feature and a NEG,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,194,"constraint on the second feature, and make sure the constraints are",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,195,respected by checking the predictions.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,196,"test adapted from lightgbm's test_monotone_constraint(), itself inspired",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,197,by https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,202,positive correlation with y,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,203,negative correslation with y,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,217,"We now assert the predictions properly respect the constraints, on each",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,218,feature. When testing for a feature we need to set the other one to a,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,219,"constant, because the monotonic constraints are only a ""all else being",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,220,"equal"" type of constraints:",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,221,a constraint on the first feature only means that,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,222,"x0 < x0' => f(x0, x1) < f(x0', x1)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,223,while x1 stays constant.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,224,The constraint does not guanrantee that,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,225,"x0 < x0' => f(x0, x1) < f(x0', x1')",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,227,First feature (POS),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,228,assert pred is all increasing when f_0 is all increasing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,232,assert pred actually follows the variations of f_0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,237,Second feature (NEG),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,238,assert pred is all decreasing when f_1 is all increasing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,242,assert pred actually follows the inverse variations of f_1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,274,The purpose of this test is to show that when computing the gain at a,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,275,"given split, the value of the current node should be properly bounded to",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,276,"respect the monotonic constraints, because it strongly interacts with",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,277,"min_gain_to_split. We build a simple example where gradients are [1, 1,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,278,"100, 1, 1] (hessians are all ones). The best split happens on the 3rd",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,279,"bin, and depending on whether the value of the node is bounded or not,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,280,the min_gain_to_split constraint is or isn't satisfied.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,312,"Since the gradient array is [1, 1, 100, 1, 1]",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,313,the max possible gain happens on the 3rd bin (or equivalently in the 2nd),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,314,"and is equal to about 1307, which less than min_gain_to_split = 2000, so",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,315,the node is considered unsplittable (gain = -1),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,320,the unbounded value is equal to -sum_gradients / sum_hessians,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,326,min_gain_to_split not respected,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,328,here again the max possible gain is on the 3rd bin but we now cap the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,329,"value of the node into [-10, inf].",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,330,This means the gain is now about 2430 which is more than the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py,331,min_gain_to_split constraint.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,10,"To use this experimental feature, we need to explicitly ask for it:",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,11,noqa,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,23,Check identical nodes for each tree,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,28,Check identical predictions,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,37,Check that a ValueError is raised when the maximum number of iterations,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,38,is smaller than the number of iterations from the previous fit when warm,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,39,start is True.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,56,Make sure that fitting 50 iterations and then 25 with warm start is,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,57,equivalent to fitting 75 iterations.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,70,Check that both predictors are equal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,79,Test if possible to fit trees of different depth in ensemble.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,86,First 20 trees have max_depth == 2,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,89,Last 10 trees have max_depth == 3,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,100,Make sure that early stopping occurs after a small number of iterations,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,101,when fitting a second time with warm starting.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,120,Test if warm start with equal n_estimators does nothing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,129,Check that both predictors are equal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,138,Test if fit clears state.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,144,inits state,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,146,clears old state and equals est,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,148,Check that both predictors have the same train_score_ and,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,149,validation_score_ attributes,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,153,Check that both predictors are equal,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,163,Make sure the seeds for train/val split and small trainset subsampling,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,164,are correctly set in a warm start context.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,166,Helper to avoid consuming rngs,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,182,"clear the old state, different seed",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,188,inits state,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,190,clears old state and equals est,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,193,"Without warm starting, the seeds should be",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,194,* all different if random state is None,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,195,* all equal if random state is an integer,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,196,* different when refitting and equal with a new estimator (because,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,197,the random state is mutated),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py,205,"With warm starting, the seeds must be equal",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,76,Constant hessian: 1. per sample.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,83,This test checks that the values of gradients and hessians are,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,84,consistent in different places:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,85,- in split_info: si.sum_gradient_left + si.sum_gradient_right must be,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,86,equal to the gradient at the node. Same for hessians.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,87,- in the histograms: summing 'sum_gradients' over the bins must be,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,88,"constant across all features, and those sums must be equal to the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,89,node's gradient. Same for hessians.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,154,make sure that si.sum_gradient_left + si.sum_gradient_right have their,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,155,"expected value, same for hessians",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,171,"make sure sum of gradients in histograms are the same for all features,",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,172,and make sure they're equal to their expected value,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,180,"note: gradients and hessians have shape (n_features,),",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,181,we're comparing them to *scalars*. This has the benefit of also,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,182,making sure that all the entries are equal across features.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,183,"shape = (n_features,)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,184,scalar,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,187,"0 is not the actual hessian, but it's not computed in this case",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,197,Check that split_indices returns the correct splits and that,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,198,splitter.partition is consistent with what is returned.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,208,split will happen on feature 1 and on bin 3,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,250,sanity checks for best split,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,262,Check that the resulting split indices sizes are consistent with the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,263,count statistics anticipated when looking for the best split.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,269,"Try to split a pure node (all gradients are equal, same for hessians)",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,270,with min_gain_to_split = 0 and make sure that the node is not split (best,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,271,"possible gain = -1). Note: before the strict inequality comparison, this",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,272,test would fail because the node would be split with a gain of 0.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,316,basic sanity check with no missing values: given the gradient,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,317,"values, the split must occur on bin_idx=3",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,318,X_binned,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,319,gradients,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,320,no missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,321,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,322,don't split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,323,expected_bin_idx,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,326,We replace 2 samples by NaNs (bin_idx=8),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,327,"These 2 samples were mapped to the left node before, so they should",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,328,be mapped to left node again,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,329,Notice how the bin_idx threshold changes from 3 to 1.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,330,8 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,332,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,333,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,334,don't split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,335,cut on bin_idx=1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,336,missing values go to left,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,338,"same as above, but with non-consecutive missing_values_bin",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,339,9 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,341,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,342,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,343,don't split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,344,cut on bin_idx=1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,345,missing values go to left,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,347,this time replacing 2 samples that were on the right.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,348,8 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,350,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,351,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,352,don't split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,353,cut on bin_idx=3 (like in first case),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,354,missing values go to right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,356,"same as above, but with non-consecutive missing_values_bin",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,357,9 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,359,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,360,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,361,don't split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,362,cut on bin_idx=3 (like in first case),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,363,missing values go to right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,365,"For the following cases, split_on_nans is True (we replace all of",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,366,"the samples with nans, instead of just 2).",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,367,4 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,369,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,370,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,371,split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,372,cut on bin_idx=3,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,373,missing values go to right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,375,"same as above, but with non-consecutive missing_values_bin",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,376,9 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,378,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,379,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,380,split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,381,cut on bin_idx=3,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,382,missing values go to right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,384,6 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,386,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,387,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,388,split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,389,cut on bin_idx=5,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,390,missing values go to right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,392,"same as above, but with non-consecutive missing_values_bin",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,393,9 <=> missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,395,missing values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,396,n_bins_non_missing,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,397,split on nans,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,398,cut on bin_idx=5,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,399,missing values go to right,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,406,Make sure missing values are properly supported.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,407,we build an artificial example with gradients such that the best split,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,408,"is on bin_idx=3, when there are no missing values.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,409,Then we introduce missing values and:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,410,- make sure the chosen bin is correct (find_best_bin()): it's,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,411,"still the same split, even though the index of the bin may change",SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,412,- make sure the missing values are mapped to the correct child,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,413,(split_indices()),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,461,Make sure the split is properly computed.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,462,This also make sure missing values are properly assigned to the correct,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,463,child in split_indices(),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,468,"When we don't split on nans, the split should always be the same.",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,472,"When we split on nans, samples with missing values are always mapped",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py,473,to the right child.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,13,"To use this experimental feature, we need to explicitly ask for it:",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,14,noqa,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,76,use scorer,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,77,use scorer on train,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,78,same with default scorer,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,80,use loss,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,81,use loss on training data,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,82,no early stopping,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,92,just for coverage,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,93,easier to overfit fast,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,117,use scorer,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,118,use scorer on training data,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,119,same with default scorer,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,121,use loss,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,122,use loss on training data,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,123,no early stopping,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,133,just for coverage,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,134,easier to overfit fast,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,158,Test that early stopping is enabled by default if and only if there,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,159,are more than 10000 samples,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,171,not enough iterations,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,172,not enough iterations,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,173,not enough iterations,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,174,significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,175,significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,176,significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,177,significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,178,no significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,179,no significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,180,no significant improvement,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,192,For coverage only.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,202,Test that ValueError is raised if either one y_i < 0 or sum(y_i) <= 0.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,210,"For Poisson distributed target, Poisson loss should give better results",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,211,than least squares measured in Poisson deviance as metric.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,216,We create a log-linear Poisson model and downscale coef as it will get,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,217,exponentiated.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,231,least_squares might produce non-positive predictions => clip,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,240,Make sure training and validation data are binned separately.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,241,See issue 13926,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,253,Note that since the data is small there is no subsampling and the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,254,random_state doesn't matter,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,266,sanity check for missing values support. With only one feature and,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,267,"y == isnan(X), the gbdt is supposed to reach perfect accuracy on the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,268,training set.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,294,Make sure the estimators can deal with missing values and still yield,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,295,decent predictions,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,325,non regression test for issue #14018,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,326,make sure we avoid zero division errors when computing the leaves values.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,328,"If the learning rate is too high, the raw predictions are bad and will",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,329,saturate the softmax (or sigmoid in binary classif). This leads to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,330,"probabilities being exactly 0 or 1, gradients being constant, and",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,331,hessians being zero.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,338,Make sure that the small trainset is stratified and has the expected,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,339,length (10k samples),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,349,Compute the small training set,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,353,Compute the class distribution in the small training set,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,358,Test that the small training set has the expected length,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,362,Test that the class distributions in the whole dataset and in the small,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,363,training set are identical,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,368,Compare the buit-in missing value handling of Histogram GBC with an,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,369,a-priori missing value imputation strategy that should yield the same,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,370,results in terms of decision function.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,371,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,372,Each feature (containing NaNs) is replaced by 2 features:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,373,- one where the nans are replaced by min(feature) - 1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,374,- one where the nans are replaced by max(feature) + 1,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,375,A split where nans go to the left has an equivalent split in the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,376,"first (min) feature, and a split where nans go to the right has an",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,377,equivalent split in the second (max) feature.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,378,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,379,Assuming the data is such that there is never a tie to select the best,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,380,"feature to split on during training, the learned decision trees should be",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,381,strictly equivalent (learn a sequence of splits that encode the same,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,382,decision function).,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,383,,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,384,The MinMaxImputer transformer is meant to be a toy implementation of the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,385,"""Missing In Attributes"" (MIA) missing value handling for decision trees",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,386,https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,387,The implementation of MIA as an imputation transformer was suggested by,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,388,"""Remark 3"" in https://arxiv.org/abs/1902.06931",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,413,Pre-bin the data to ensure a deterministic handling by the 2,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,414,strategies and also make it easier to insert np.nan in a structured,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,415,way:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,418,First feature has missing values completely at random:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,422,Second and third features have missing values for extreme values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,423,(censoring missingness):,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,430,Make the last feature nan pattern very informative:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,436,Check that there is at least one missing value in each feature:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,440,Let's use a test set to check that the learned decision function is,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,441,the same as evaluated on unseen data. Otherwise it could just be the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,442,case that we find two independent ways to overfit the training set.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,445,n_samples need to be large enough to minimize the likelihood of having,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,446,several candidate splits with the same gain value in a given tree.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,450,Use a small number of leaf nodes and iterations so as to keep,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,451,under-fitting models to minimize the likelihood of ties when training the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,452,model.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,461,Check that the model reach the same score:,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,468,Check the individual prediction match as a finer grained,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,469,decision function check.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,475,Basic test for infinite values,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,500,High level test making sure that inf and nan values are properly handled,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,501,when both are present. This is similar to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,502,"test_split_on_nan_with_infinite_values() in test_grower.py, though we",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,503,cannot check the predictions for binned values here.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,517,categorical_crossentropy should only be used if there are more than two,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,518,classes present. PR #14869,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,529,Regression tests for #14709 where the targets need to be encoded before,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,530,to compute the score,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,539,Make sure setting a SW to zero amounts to ignoring the corresponding,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,540,sample,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,547,ignore the first 2 training samples by setting their weight to 0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,555,Make sure setting a SW to zero amounts to ignoring the corresponding,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,556,sample,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,563,ignore the first 2 training samples by setting their weight to 0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,576,ignore the first 2 training samples by setting their weight to 0,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,591,High level test to make sure that duplicating a sample is equivalent to,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,592,giving it weight of 2.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,594,fails for n_samples > 255 because binning does not take sample weights,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,595,into account. Keeping n_samples <= 255 makes,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,596,sure only unique values are used so SW have no effect on binning.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,611,This test can't pass if min_samples_leaf > 1 because that would force 2,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,612,"samples to be in the same node in est_sw, while these samples would be",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,613,free to be separate in est_dup: est_dup would just group together the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,614,duplicated samples.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,617,Create dataset with duplicate and corresponding sample weights,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,630,checking raw_predict is stricter than just predict for classification,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,638,"For losses with constant hessians, the sum_hessians field of the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,639,histograms must be equal to the sum of the sample weight of samples at,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,640,the corresponding bin.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,659,build sum_sample_weight which contains the sum of the sample weights at,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,660,each bin (for each feature). This must be equal to the sum_hessians,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,661,field of the corresponding histogram,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,668,Build histogram,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,681,Non regression test for,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,682,https://github.com/scikit-learn/scikit-learn/issues/16179,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,683,there was a bug when the max_depth and the max_leaf_nodes criteria were,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,684,"met at the same time, which would lead to max_leaf_nodes not being",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,685,respected.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,691,would be 4 prior to bug fix,SATD
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,695,Non regression test for #16661 where second fit fails with,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,696,"warm_start=True, early_stopping is on, and no validation set",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,703,does not raise on second call,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,711,Make sure it's still possible to build single-node trees. In that case,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,712,the value of the root is set to 0. That's a correct value: if the tree is,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,713,single-node that's because min_gain_to_split is not respected right from,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,714,"the root, so we don't want the tree to have any impact on the",not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,715,predictions.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,718,constant target will lead to a single root node,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py,726,Still gives correct predictions thanks to the baseline prediction,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,25,Small sample_indices (below unrolling threshold),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,38,Larger sample_indices (above unrolling threshold),not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,53,Make sure the order of the samples has no impact on the histogram,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,54,computations,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,100,Make sure the different unrolled histogram computations give the same,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,101,results as the naive one.,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,146,Make sure the histogram subtraction trick gives the same result as the,not
scikit-learn/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py,147,classical method.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,33,Common random state,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,36,Toy sample,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,38,test string class labels,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,44,Load the iris dataset and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,49,Load the boston dataset and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,56,Test the `_samme_proba` helper function.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,58,Define some example (bad) `predict_proba` output.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,65,_samme_proba calls estimator.predict_proba.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,66,Make a mock object so I can control what gets returned.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,78,Make sure that the correct elements come out as smallest --,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,79,`_samme_proba` should preserve the ordering in each example.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,85,Test predict_proba robustness for one class label input.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,86,In response to issue #7501,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,87,https://github.com/scikit-learn/scikit-learn/issues/7501,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,95,Check classification on a toy dataset.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,105,Check classification on a toy dataset.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,112,Check consistency on dataset iris.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,132,Check we used multiple estimators,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,134,Check for distinct random states (see issue #7408),not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,138,Somewhat hacky regression test: prior to,SATD
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,139,"ae7adc880d624615a34bafdb1d75ef67051b8200,",not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,140,predict_proba returned SAMME.R values for SAMME.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,148,Check consistency on dataset boston house prices.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,154,Check we used multiple estimators,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,156,Check for distinct random states (see issue #7408),not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,163,Check staged predictions.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,187,AdaBoost regression,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,205,Check that base trees can be grid-searched.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,206,AdaBoost classification,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,214,AdaBoost regression,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,224,Check pickability.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,227,Adaboost classifier,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,239,Adaboost regressor,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,252,Check variable importances.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,272,Test that it gives proper exception on deficient input.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,287,Test different base estimators.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,290,XXX doesn't work with y_class because RF doesn't support classes_,SATD
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,291,Shouldn't AdaBoost run a LabelBinarizer?,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,306,"Check that an empty discrete ensemble fails in fit, not predict.",not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,315,Check classification with sparse input.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,329,Flatten y to a 1d array,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,339,Trained on sparse format,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,346,Trained on dense format,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,353,predict,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,358,decision_function,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,363,predict_log_proba,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,368,predict_proba,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,373,score,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,378,staged_decision_function,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,385,staged_predict,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,391,staged_predict_proba,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,397,staged_score,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,404,Verify sparsity of data is maintained during training,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,412,Check regression with sparse input.,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,433,Trained on sparse format,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,439,Trained on dense format,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,445,predict,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,450,staged_predict,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,502,TODO: Remove in 0.24 when DummyClassifier's `strategy` default changes,SATD
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,518,check that giving weight will have an influence on the error computed,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,519,for a weak learner,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,525,add an arbitrary outlier,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,529,random_state=0 ensure that the underlying bootstrap will use the outlier,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,536,fit 3 models:,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,537,- a model containing the outlier,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,538,- a model without the outlier,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,539,- a model containing the outlier but with a null sample-weight,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,556,check that predict_proba and predict give consistent results,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,557,regression test for:,not
scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py,558,https://github.com/scikit-learn/scikit-learn/issues/14084,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,5,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,6,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,33,load the iris dataset,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,34,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,40,also load the diabetes dataset,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,41,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,77,Trained on sparse format,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,82,Trained on dense format,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,94,Test max_samples,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,101,"The dataset has less than 256 samples, explicitly setting",not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,102,max_samples > n_samples should result in a warning. If not set,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,103,explicitly there should be no warning,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,107,note that assert_no_warnings does not apply since it enables a,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,108,PendingDeprecationWarning triggered by scipy.sparse's use of,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,109,np.matrix. See issue #11251.,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,124,test X_test n_features match X_train one:,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,127,test that behaviour='old' will raise an error,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,183,Generate train/test data,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,189,Generate some abnormal novel observations,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,194,fit the model,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,197,"predict scores (the lower, the more normal)",not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,200,check that there is at most 6 errors (false positive or false negative),not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,206,toy sample (the last two samples are outliers),not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,209,Test IsolationForest,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,214,assert detect outliers:,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,220,Make sure validated max_samples in iforest and BaseBagging are identical,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,227,It tests non-regression for #5732 which failed at predict.,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,238,It tests non-regression for #8549 which used the wrong formula,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,239,"for average path length, strictly for the integer case",not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,240,Updated to check average path length when input is <= 2 (issue #11839),not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,252,_average_path_length is increasing,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,275,fit first 10 trees,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,279,remember the 1st tree,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,281,fit another 10 trees,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,284,expecting 20 fitted trees and no overwritten trees,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,289,mock get_chunk_n_rows to actually test more than one chunk (here one,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,290,chunk = 3 rows:,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,305,idem with chunk_size = 5 rows,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,330,2-d array of all 1s,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,342,2-d array where columns contain the same value across rows,not
scikit-learn/sklearn/ensemble/tests/test_iforest.py,351,Single row,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,5,Author: Gilles Louppe,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,40,also load the iris dataset,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,41,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,47,also load the diabetes dataset,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,48,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,55,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,SATD
scikit-learn/sklearn/ensemble/tests/test_bagging.py,58,Check classification for various parameter settings.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,81,Check classification for various parameter settings on sparse input.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,117,Trained on sparse format,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,125,Trained on dense format,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,141,Check regression for various parameter settings.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,163,Check regression for various parameter settings on sparse input.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,199,Trained on sparse format,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,207,Trained on dense format,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,230,Test that bootstrapping samples generate non-perfect base estimators.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,238,"without bootstrap, all trees are perfect on the training set",not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,247,"with bootstrap, trees are no longer perfect on the training set",not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,256,check that each sampling correspond to a complete bootstrap resample.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,257,the size of each bootstrap should be the same as the input data but,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,258,the data should be different (checked using the hash of the data).,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,269,Test that bootstrapping features may generate duplicate features.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,293,Predict probabilities.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,300,Normal case,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,311,"Degenerate case, where some classes are missing",not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,325,Check that oob prediction is a good estimation of the generalization,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,326,error.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,343,Test with few estimators,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,355,Check that oob prediction is a good estimation of the generalization,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,356,error.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,372,Test with few estimators,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,384,Check singleton ensembles.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,402,Test that it gives proper exception on deficient input.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,406,Test max_samples,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,418,Test max_features,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,430,Test support of decision_function,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,435,Check parallel classification.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,438,Classification,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,447,predict_proba,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,461,decision_function,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,488,Check parallel regression.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,514,Check that bagging ensembles can be grid-searched.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,515,Transform iris into a binary classification task,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,519,Grid search with scoring based on decision_function,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,529,Check base_estimator and its default values.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,532,Classification,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,555,Regression,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,606,Test if fitting incrementally with warm start gives a forest of the,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,607,right size and the same results as a normal fit.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,630,Test if warm start'ed second fit with smaller n_estimators raises error.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,639,Test that nothing happens when fitting without increasing n_estimators,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,647,"modify X to nonsense values, this should not change anything",not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,657,warm started classifier with 5+5 estimators should be equivalent to,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,658,one classifier with 10 estimators,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,678,Check using oob_score and warm_start simultaneously fails,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,697,"Make sure OOB scores are identical when random_state, estimator, and",not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,698,training data are fixed and fitting is done twice,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,707,Check that format of estimators_samples_ is correct and that results,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,708,generated at fit time can be identically reproduced at a later time,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,709,using data saved in object attributes.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,716,Get relevant attributes,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,721,Test for correct formatting,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,726,Re-fit single estimator to test for consistent sampling,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,743,This test is a regression test to check that with a random step,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,744,"(e.g. SparseRandomProjection) and a given random state, the results",not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,745,generated at fit time can be identically reproduced at a later time using,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,746,data saved in object attributes. Check issue #9524 for full discussion.,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,771,Make sure validated max_samples and original max_samples are identical,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,772,when valid integer max_samples supplied by user,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,783,Make sure the oob_score doesn't change when the labels change,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,784,See: https://github.com/scikit-learn/scikit-learn/issues/8933,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,806,Check that BaggingRegressor can accept X with missing/infinite data,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,834,Verify that exceptions can be raised by wrapper regressor,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,843,Check that BaggingClassifier can accept X with missing/infinite data,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,864,Verify that exceptions can be raised by wrapper classifier,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,873,Check that Bagging estimator can accept low fractional max_features,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,884,Check that Bagging estimator can generate sample indices properly,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,885,Non-regression test for:,not
scikit-learn/sklearn/ensemble/tests/test_bagging.py,886,https://github.com/scikit-learn/scikit-learn/issues/16436,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,3,Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,60,prescale the data to avoid convergence warning without using a pipeline,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,61,for later assert,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,86,LogisticRegression has decision_function method,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,97,check that a column is dropped in binary classification,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,103,both classifiers implement 'predict_proba' and will both drop one column,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,112,LinearSVC does not implement 'predict_proba' and will not drop one column,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,122,prescale the data to avoid convergence warning without using a pipeline,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,123,for later assert,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,145,prescale the data to avoid convergence warning without using a pipeline,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,146,for later assert,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,178,prescale the data to avoid convergence warning without using a pipeline,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,179,for later assert,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,213,Check passthrough behavior on a sparse X matrix,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,232,Check passthrough behavior on a sparse X matrix,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,250,check that classifier will drop one of the probability column for,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,251,binary classification problem,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,253,Select only the 2 first classes,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,342,keep only classes 0 and 1,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,350,checking that fixing the random state of the CV will lead to the same,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,351,results,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,369,These warnings are raised due to _BaseComposition,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,388,check that we stratify the classes for the default CV,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,393,"since iris is not shuffled, a simple k-fold would not contain the",not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,394,3 classes during training,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,415,check that sample weights has an influence on the fitting,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,416,note: ConvergenceWarning are catch since we are not worrying about the,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,417,convergence here,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,444,check sample_weight is passed to all invocations of fit,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,472,check that the stacking affects the fit of the final estimator but not,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,473,the fit of the base estimators,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,474,note: ConvergenceWarning are catch since we are not worrying about the,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,475,convergence here,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,485,the base estimators should be identical,not
scikit-learn/sklearn/ensemble/tests/test_stacking.py,490,the final estimator should be different,not
scikit-learn/sklearn/ensemble/tests/test_common.py,40,"check that the behavior of `estimators`, `estimators_`,",not
scikit-learn/sklearn/ensemble/tests/test_common.py,41,"`named_estimators`, `named_estimators_` is consistent across all",not
scikit-learn/sklearn/ensemble/tests/test_common.py,42,ensemble classes and when using `set_params()`.,not
scikit-learn/sklearn/ensemble/tests/test_common.py,44,before fit,not
scikit-learn/sklearn/ensemble/tests/test_common.py,49,check fitted attributes,not
scikit-learn/sklearn/ensemble/tests/test_common.py,56,check that set_params() does not add a new attribute,not
scikit-learn/sklearn/ensemble/tests/test_common.py,66,check the behavior when setting an dropping an estimator,not
scikit-learn/sklearn/ensemble/tests/test_common.py,76,check that the correspondence is correct,not
scikit-learn/sklearn/ensemble/tests/test_common.py,79,check that we can set the parameters of the underlying classifier,not
scikit-learn/sklearn/ensemble/tests/test_common.py,93,check that ensemble will fail during validation if the underlying,not
scikit-learn/sklearn/ensemble/tests/test_common.py,94,estimators are not of the same type (i.e. classifier or regressor),not
scikit-learn/sklearn/ensemble/tests/test_common.py,118,raise an error when the name contains dunder,not
scikit-learn/sklearn/ensemble/tests/test_common.py,129,raise an error when the name is not unique,not
scikit-learn/sklearn/ensemble/tests/test_common.py,142,raise an error when the name conflicts with the parameters,not
scikit-learn/sklearn/ensemble/tests/test_common.py,168,check that we raise a consistent error when all estimators are,not
scikit-learn/sklearn/ensemble/tests/test_common.py,169,dropped,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,5,"Authors: Gilles Louppe,",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,6,"Brian Holt,",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,7,"Andreas Mueller,",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,8,Arnaud Joly,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,56,toy sample,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,62,Larger classification sample used for testing feature importances,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,67,also load the iris dataset,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,68,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,75,also load the boston dataset,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,76,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,82,also make a hastie_10_2 dataset,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,86,Get the default backend in joblib to test parallelism and interaction with,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,87,different backends,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,127,also test apply,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,138,Check consistency on dataset iris.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,163,Check consistency on dataset boston house prices.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,188,Regression models should not have a classes_ attribute.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,204,Predict probabilities.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,222,cast as dype,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,233,The forest estimator can detect that only the first 3 features of the,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,234,dataset are informative:,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,240,Check with parallel,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,246,Check with sample weights,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,276,Check whether variable importances of totally randomized trees,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,277,"converge towards their theoretical values (See Louppe et al,",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,278,"Understanding variable importances in forests of randomized trees, 2013).",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,304,Weight of each B of size k,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,307,For all B of size k,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,309,For all values B=b,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,327,P(B=b),not
scikit-learn/sklearn/ensemble/tests/test_forest.py,348,Compute true importances,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,354,Estimate importances with totally randomized trees,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,363,Check correctness,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,378,Check that oob prediction is a good estimation of the generalization,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,379,error.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,381,Proper behavior,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,394,Check warning if not enough estimators,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,405,csc matrix,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,408,non-contiguous targets in classification,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,416,csc matrix,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,431,Unfitted /  no bootstrap / no oob_score,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,438,No bootstrap,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,456,Check that base trees can be grid-searched.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,486,Check pickability.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,511,Check estimators on multi-output problems.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,544,Check estimators on multi-output problems with string outputs.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,573,Test that n_classes_ and classes_ have proper shape.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,576,"Classification, single output",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,582,"Classification, multi-output",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,596,Test that the `sparse_output` parameter of RandomTreesEmbedding,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,597,works by returning a dense array.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,599,Create the RTE with sparse=False,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,604,"Assert that type is ndarray, not scipy.sparse.csr.csr_matrix",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,609,Test that the `sparse_output` parameter of RandomTreesEmbedding,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,610,works by returning the same array for both argument values.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,612,Create the RTEs,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,621,Assert that dense and sparse hashers have same array.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,625,Ignore warnings from switching to more power iterations in randomized_svd,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,628,test random forest hashing on circles dataset,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,629,make sure that it is linearly separable.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,630,even after projected to two SVD dimensions,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,631,Note: Not all random_states produce perfect results.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,636,test fit and transform:,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,641,one leaf active per data point per forest,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,680,Single variable with 4 values,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,698,"On a single variable problem where X_0 has 4 equiprobable values, there",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,699,"are 5 ways to build a random tree. The more compact (0,1/0,0/--0,2/--) of",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,700,them has probability 1/3 while the 4 others have probability 1/6.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,703,Rough approximation of 1/6.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,710,"Two variables, one with 2 values, one with 3 values",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,733,Test precedence of max_leaf_nodes over max_depth.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,753,test boundary value,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,787,Test if leaves contain more than leaf_count training examples,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,790,test boundary value,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,800,drop inner nodes,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,809,drop inner nodes,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,823,Test if leaves contain at least min_weight_fraction_leaf of the,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,824,training set,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,830,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,831,by setting max_leaf_nodes,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,841,drop inner nodes,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,891,Check that it works no matter the memory layout,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,895,Nothing,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,900,C-order,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,905,F-order,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,910,Contiguous,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,916,csr matrix,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,921,csc_matrix,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,926,coo_matrix,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,931,Strided,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,967,Check class_weights resemble sample_weights behavior.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,970,"Iris is balanced, so no effect expected for using 'balanced' weights",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,977,Make a multi-output problem with three copies of Iris,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,979,Create user-defined weights that should balance over the outputs,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,986,"Check against multi-output ""balanced"" which should also have no effect",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,991,"Inflate importance of class 1, check against user-defined weights",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1001,Check that sample_weight and class_weight are multiplicative,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1015,"Test class_weight works for multi-output""""""",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1023,smoke test for balanced subsample,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1034,Test if class_weight raises errors and warnings when expected.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1038,Invalid preset string,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1043,Warning warm_start with preset,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1049,Not a list or preset for multi-output,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1053,Incorrect length list for multi-output,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1064,Test if fitting incrementally with warm start gives a forest of the,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1065,right size and the same results as a normal fit.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1096,Test if fit clears state and grows a new forest when warm_start==False.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1105,inits state,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1107,clears old state and equals est,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1118,Test if warm start second fit with smaller n_estimators raises error.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1133,Test if warm start with equal n_estimators does nothing and returns the,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1134,same forest and raises a warning.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1144,Now est_2 equals est.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1148,If we had fit the trees again we would have got a different forest as we,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1149,changed the random state.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1159,Test that the warm start computes oob score when asked.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1162,Use 15 estimators to avoid 'some inputs do not have OOB scores' warning.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1177,Test that oob_score is computed even if we don't need to train,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1178,additional trees.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1220,Assert that leaves index are correct,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1234,Test if min_impurity_split of base estimators is set,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1235,Regression test for #8006,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1258,Simply check if the parameter is passed on correctly. Tree tests,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1259,will suffice for the actual working of this param,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1263,"mypy error: Variable ""DEFAULT_JOBLIB_BACKEND"" is not valid type",not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1264,type: ignore,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1288,predict_proba requires shared memory. Ensure that's honored.,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1304,build a forest of single node trees. See #13636,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1335,Check invalid `max_samples` values,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1359,First fit with no restriction on max samples,not
scikit-learn/sklearn/ensemble/tests/test_forest.py,1366,Second fit with max samples restricted to just 2,not
scikit-learn/sklearn/ensemble/tests/test_base.py,5,Authors: Gilles Louppe,not
scikit-learn/sklearn/ensemble/tests/test_base.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/ensemble/tests/test_base.py,23,Check BaseEnsemble methods.,not
scikit-learn/sklearn/ensemble/tests/test_base.py,29,empty the list and create estimators manually,not
scikit-learn/sklearn/ensemble/tests/test_base.py,52,Check that instantiating a BaseEnsemble with n_estimators<=0 raises,not
scikit-learn/sklearn/ensemble/tests/test_base.py,53,a ValueError.,not
scikit-learn/sklearn/ensemble/tests/test_base.py,63,Check that instantiating a BaseEnsemble with a string as n_estimators,not
scikit-learn/sklearn/ensemble/tests/test_base.py,64,raises a ValueError demanding n_estimators to be supplied as an integer.,not
scikit-learn/sklearn/ensemble/tests/test_base.py,79,Linear Discriminant Analysis doesn't have random state: smoke test,not
scikit-learn/sklearn/ensemble/tests/test_base.py,84,check random_state is None still sets,not
scikit-learn/sklearn/ensemble/tests/test_base.py,88,check random_state fixes results in consistent initialisation,not
scikit-learn/sklearn/ensemble/tests/test_base.py,95,nested random_state,not
scikit-learn/sklearn/ensemble/tests/test_base.py,108,ensure multiple random_state parameters are invariant to get_params(),not
scikit-learn/sklearn/ensemble/tests/test_base.py,109,iteration order,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,32,Load datasets,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,308,check that an error is raised and indicative if sample_weight is not,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,309,supported.,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,319,check that _fit_single_estimator will raise the right error,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,320,it should raise the original error if this is not linked to sample_weight,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,339,Should not raise an error.,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,344,check equivalence in the output when setting underlying estimators,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,361,TODO: Remove parametrization in 0.24 when None is removed in Voting*,SATD
scikit-learn/sklearn/ensemble/tests/test_voting.py,366,Test predict,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,400,Test soft voting transform,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,426,Test estimator weights inputs as list and array,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,473,TODO: Remove drop=None in 0.24 when None is removed in Voting*,SATD
scikit-learn/sklearn/ensemble/tests/test_voting.py,485,TODO: remove the parametrization on 'drop' when support for None is,SATD
scikit-learn/sklearn/ensemble/tests/test_voting.py,486,removed.,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,487,check that an estimator can be set to 'drop' and passing some weight,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,488,regression test for,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,489,https://github.com/scikit-learn/scikit-learn/issues/13777,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,511,FIXME: to be removed when meta-estimators can specified themselves,SATD
scikit-learn/sklearn/ensemble/tests/test_voting.py,512,their testing parameters (for required parameters).,not
scikit-learn/sklearn/ensemble/tests/test_voting.py,560,TODO: Remove in 0.24 when None is removed in Voting*,SATD
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,47,toy sample,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,54,also load the boston dataset,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,55,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,61,also load the iris dataset,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,62,and randomly permute it,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,70,Check classification on a toy dataset.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,93,Check input parameter validation for GradientBoostingClassifier.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,141,test fit before feature importance,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,145,deviance requires ``n_classes >= 2``.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,153,Check input parameter validation for GradientBoostingRegressor,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,187,Test GradientBoostingClassifier on synthetic dataset used by,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,188,Hastie et al. in ESLII Example 12.7.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,216,Check consistency on dataset boston house prices with least squares,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,217,and least absolute deviation.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,251,Check consistency on dataset iris.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,273,"Test on synthetic regression datasets used in Leo Breiman,",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,274,`Bagging Predictors?. Machine Learning 24(2): 123-140 (1996).,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,280,Friedman1,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,292,Friedman2,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,302,Friedman3,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,324,Predict probabilities.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,332,"check if probabilities are in [0, 1].",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,337,derive predictions from probabilities,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,343,Test input checks (shape and type of X and y).,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,355,X has wrong shape,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,382,check that predict_stages through an error if the type of X is not,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,383,supported,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,401,test if max_features is valid.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,416,Test to make sure random state is set properly.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,450,The most important feature is the median income by far.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,453,The three subsequent features are the following. Their relative ordering,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,454,might change a bit depending on the randomness of the trees and the,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,455,train / test split.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,460,Test if max features is set properly for floats and str.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,494,Test whether staged decision function eventually gives,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,495,the same prediction.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,501,test raise ValueError if not fitted,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,508,test if prediction for last stage equals ``predict``,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,516,Test whether staged predict proba eventually gives,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,517,the same prediction.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,523,test raise NotFittedError if not fitted,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,529,test if prediction for last stage equals ``predict``,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,535,test if prediction for last stage equals ``predict_proba``,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,545,test that staged_functions make defensive copies,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,548,don't predict zeros,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,554,regressor has no staged_predict_proba,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,563,Check model serialization.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,583,Check if we can fit even though all targets are equal.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,586,classifier should raise exception,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,597,Check if quantile loss with alpha=0.5 equals lad.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,614,Test with non-integer class labels.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,625,Test with float class labels.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,637,Test with float class labels.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,643,This will raise a DataConversionWarning that we want to,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,644,"""always"" raise, elsewhere the warnings gets ignored in the",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,645,"later tests, and the tests that check for this warning fail",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,652,Test with different memory layouts of X and y,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,681,Test if oob improvement has correct shape and regression test.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,686,hard-coded regression test - change if modification in OOB computation,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,693,Test if oob improvement has correct shape.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,701,Check OOB improvement on multi-class dataset.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,708,hard-coded regression test - change if modification in OOB computation,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,709,FIXME: the following snippet does not yield the same results on 32 bits,SATD
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,710,"assert_array_almost_equal(clf.oob_improvement_[:5],",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,711,"np.array([12.68, 10.45, 8.18, 6.43, 5.13]),",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,712,decimal=2),not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,716,Check verbose=1 does not cause error.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,728,check output,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,731,with OOB,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,737,one for 1-10 and then 9 for 20-100,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,742,Check verbose=2 does not cause error.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,753,check output,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,756,no OOB,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,762,100 lines for n_estimators==100,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,768,Test if warm start equals fit.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,781,Random state is preserved and hence predict_proba must also be,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,782,same,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,790,Test if warm start equals fit - set n_estimators.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,805,Test if possible to fit trees of different depth in ensemble.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,812,last 10 trees have different depth,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,820,Test if fit clears state.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,826,inits state,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,828,clears old state and equals est,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,835,Test if warm start with zero n_estimators raises error,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,845,Test if warm start with smaller n_estimators raises error,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,855,Test if warm start with equal n_estimators does nothing,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,869,Test if oob can be turned on during warm start.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,877,the last 10 are not zeros,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,884,Test if warm start OOB equals fit.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,902,Test that all sparse matrix types are supported,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,931,Test that feeding a X in Fortran-ordered is giving the same results as,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,932,in C-ordered,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,959,Test if monitor return value works.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,964,this is not altered,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,969,try refit,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,984,try refit,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,994,Test greedy trees with max_depth + 1 leafs.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1010,Test greedy trees with max_depth + 1 leafs.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1024,Test if init='zero' works for regression.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1039,Test if init='zero' works for classification.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1049,binary clf,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1065,Test precedence of max_leaf_nodes over max_depth.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1081,Test if min_impurity_split of base estimators is set,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1082,Regression test for #8006,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1100,Simply check if the parameter is passed on correctly. Tree tests,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1101,will suffice for the actual working of this param,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1106,Test if warm_start does nothing if n_estimators is not changed.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1107,Regression test for #3513.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1116,Predict probabilities.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1125,"check if probabilities are in [0, 1].",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1132,derive predictions from probabilities,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1143,ignore the first 2 training samples by setting their weight to 0,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1158,ignore the first 2 training samples by setting their weight to 0,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1225,Check if early_stopping works as expected,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1234,Without early stopping,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1265,Check if validation_fraction has an effect,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1274,Check if n_estimators_ increase monotonically with n_iter_no_change,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1275,Set validation,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1283,Make sure data splitting for early stopping is stratified,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1298,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,SATD
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1307,Check that GradientBoostingRegressor works when init is a sklearn,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1308,estimator.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1309,Check that an error is raised if trying to fit with sample weight but,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1310,initial estimator does not support sample weight,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1315,init supports sample weights,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1319,init does not support sample weights,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1321,ok no sample weights,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1328,Check that the init estimator can be a pipeline (see issue #13466),not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1333,pipeline without sample_weight works fine,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1341,Passing sample_weight to a pipeline raises a ValueError. This test makes,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1342,sure we make the distinction between ValueError raised by a pipeline that,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1343,"was passed sample_weight, and a ValueError raised by a regular estimator",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1344,whose input checking failed.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1348,Note that NuSVR properly supports sample_weight,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1359,Make sure error is raised if init estimators don't have the required,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1360,"methods (fit, predict, predict_proba)",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1369,"when doing early stopping (_, , y_train, _ = train_test_split(X, y))",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1370,there might be classes in y that are missing in y_train. As the init,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1371,"estimator will be trained on y_train, we need to raise an error if this",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1372,happens.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1375,only 2 negative class over 10 samples,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1383,No error if we let training data be big enough,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py,1389,growing an ensemble of single node trees. See #13620,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,24,Check binomial deviance loss.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,25,Check against alternative definitions in ESLII.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,28,"pred has the same BD for y in {0, 1}",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,38,check if same results as alternative definition of deviance (from ESLII),not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,51,check the gradient against the,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,62,least squares,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,70,Smoke test for init estimators with sample weights.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,88,skip multiclass,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,102,check if predictions match,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,137,Non regression test for the QuantileLossFunction object,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,138,There was a sign problem when evaluating the function,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,139,for negative values of 'ytrue - ypred',not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,147,Test if deviance supports sample weights.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,168,one-hot encoding,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,180,Make sure get_init_raw_predictions returns float64 arrays with shape,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,181,"(n_samples, K) where K is 1 for binary classification and regression, and",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,182,K = n_classes for multiclass classification,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,215,Make sure the get_init_raw_predictions() returns the expected values for,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,216,each loss.,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,223,Least squares loss,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,227,Make sure baseline prediction is the mean of all targets,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,230,Least absolute and huber loss,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,235,Make sure baseline prediction is the median of all targets,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,238,Quantile loss,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,243,Make sure baseline prediction is the alpha-quantile of all targets,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,248,Binomial deviance,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,251,"Make sure baseline prediction is equal to link_function(p), where p",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,252,"is the proba of the positive class. We want predict_proba() to return p,",not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,253,and by definition,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,254,p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction),not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,255,So we want raw_prediction = link_function(p) = log(p / (1 - p)),not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,260,Exponential loss,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,267,Multinomial deviance loss,not
scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py,280,Make sure quantile loss with alpha = .5 is equivalent to LAD,not
scikit-learn/sklearn/__check_build/__init__.py,17,Raise a comprehensible error and list the contents of the,not
scikit-learn/sklearn/__check_build/__init__.py,18,directory to help debugging on the mailing list.,not
scikit-learn/sklearn/__check_build/__init__.py,22,Picking up the local install: this will work only if the,not
scikit-learn/sklearn/__check_build/__init__.py,23,install is an 'inplace build',not
scikit-learn/sklearn/__check_build/__init__.py,44,noqa,not
scikit-learn/sklearn/__check_build/setup.py,1,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/__check_build/setup.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_species_distributions.py,35,Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/sklearn/datasets/_species_distributions.py,36,Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/datasets/_species_distributions.py,37,,not
scikit-learn/sklearn/datasets/_species_distributions.py,38,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_species_distributions.py,56,The original data can be found at:,not
scikit-learn/sklearn/datasets/_species_distributions.py,57,https://biodiversityinformatics.amnh.org/open_source/maxent/samples.zip,not
scikit-learn/sklearn/datasets/_species_distributions.py,64,The original data can be found at:,not
scikit-learn/sklearn/datasets/_species_distributions.py,65,https://biodiversityinformatics.amnh.org/open_source/maxent/coverages.zip,not
scikit-learn/sklearn/datasets/_species_distributions.py,127,"x,y coordinates for corner cells",not
scikit-learn/sklearn/datasets/_species_distributions.py,133,x coordinates of the grid cells,not
scikit-learn/sklearn/datasets/_species_distributions.py,135,y coordinates of the grid cells,not
scikit-learn/sklearn/datasets/_species_distributions.py,216,Define parameters for the data files.  These should not be changed,not
scikit-learn/sklearn/datasets/_species_distributions.py,217,unless the data model changes.  They will be saved in the npz file,not
scikit-learn/sklearn/datasets/_species_distributions.py,218,with the downloaded data.,not
scikit-learn/sklearn/datasets/_species_distributions.py,234,samples.zip is a valid npz,not
scikit-learn/sklearn/datasets/_species_distributions.py,246,coverages.zip is a valid npz,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,24,Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,25,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,52,The original data can be found at:,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,53,https://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,80,Store a zipped pickle,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,297,Sort the categories to have the ordering of the labels,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,303,searchsorted to have continuous labels,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,306,Use an object array to shuffle: avoids memory copy,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,317,Use an object array to shuffle: avoids memory copy,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,418,we shuffle but use a fixed seed for the memoization,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,443,the data is stored as int16 for compactness,not
scikit-learn/sklearn/datasets/_twenty_newsgroups.py,444,but normalize needs floats,not
scikit-learn/sklearn/datasets/_kddcup99.py,29,The original data can be found at:,not
scikit-learn/sklearn/datasets/_kddcup99.py,30,https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz,not
scikit-learn/sklearn/datasets/_kddcup99.py,37,The original data can be found at:,not
scikit-learn/sklearn/datasets/_kddcup99.py,38,https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data_10_percent.gz,not
scikit-learn/sklearn/datasets/_kddcup99.py,133,selected abnormal samples:,not
scikit-learn/sklearn/datasets/_kddcup99.py,143,select all samples with positive logged_in attribute:,not
scikit-learn/sklearn/datasets/_kddcup99.py,291,XXX bug when compress!=0:,SATD
scikit-learn/sklearn/datasets/_kddcup99.py,292,(error: 'Incorrect data length while decompressing[...] the file,not
scikit-learn/sklearn/datasets/_kddcup99.py,293,could be corrupted.'),not
scikit-learn/sklearn/datasets/_lfw.py,8,Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/datasets/_lfw.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_lfw.py,27,The original data can be found in:,not
scikit-learn/sklearn/datasets/_lfw.py,28,http://vis-www.cs.umass.edu/lfw/lfw.tgz,not
scikit-learn/sklearn/datasets/_lfw.py,35,The original funneled data can be found in:,not
scikit-learn/sklearn/datasets/_lfw.py,36,http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz,not
scikit-learn/sklearn/datasets/_lfw.py,43,The original target data can be found in:,not
scikit-learn/sklearn/datasets/_lfw.py,44,"http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt',",not
scikit-learn/sklearn/datasets/_lfw.py,45,"http://vis-www.cs.umass.edu/lfw/pairsDevTest.txt',",not
scikit-learn/sklearn/datasets/_lfw.py,46,"http://vis-www.cs.umass.edu/lfw/pairs.txt',",not
scikit-learn/sklearn/datasets/_lfw.py,68,,not
scikit-learn/sklearn/datasets/_lfw.py,69,Common private utilities for data fetching from the original LFW website,not
scikit-learn/sklearn/datasets/_lfw.py,70,"local disk caching, and image decoding.",not
scikit-learn/sklearn/datasets/_lfw.py,71,,not
scikit-learn/sklearn/datasets/_lfw.py,119,import PIL only when needed,not
scikit-learn/sklearn/datasets/_lfw.py,122,compute the portion of the images to load to respect the slice_ parameter,not
scikit-learn/sklearn/datasets/_lfw.py,123,given by the caller,not
scikit-learn/sklearn/datasets/_lfw.py,139,allocate some contiguous memory to host the decoded image slices,not
scikit-learn/sklearn/datasets/_lfw.py,146,iterate over the collected file path to load the jpeg files as numpy,not
scikit-learn/sklearn/datasets/_lfw.py,147,arrays,not
scikit-learn/sklearn/datasets/_lfw.py,152,Checks if jpeg reading worked. Refer to issue #3594 for more,not
scikit-learn/sklearn/datasets/_lfw.py,153,details.,not
scikit-learn/sklearn/datasets/_lfw.py,161,"scale uint8 coded colors to the [0.0, 1.0] floats",not
scikit-learn/sklearn/datasets/_lfw.py,165,average the color channels to compute a gray levels,not
scikit-learn/sklearn/datasets/_lfw.py,166,representation,not
scikit-learn/sklearn/datasets/_lfw.py,174,,not
scikit-learn/sklearn/datasets/_lfw.py,175,Task #1:  Face Identification on picture with names,not
scikit-learn/sklearn/datasets/_lfw.py,176,,not
scikit-learn/sklearn/datasets/_lfw.py,184,scan the data folder content to retain people with more that,not
scikit-learn/sklearn/datasets/_lfw.py,185,`min_faces_per_person` face pictures,not
scikit-learn/sklearn/datasets/_lfw.py,208,shuffle the faces with a deterministic RNG scheme to avoid having,not
scikit-learn/sklearn/datasets/_lfw.py,209,"all faces of the same person in a row, as it would break some",not
scikit-learn/sklearn/datasets/_lfw.py,210,cross validation and learning algorithms such as SGD and online,not
scikit-learn/sklearn/datasets/_lfw.py,211,k-means that make an IID assumption,not
scikit-learn/sklearn/datasets/_lfw.py,305,wrap the loader in a memoizing function that will return memmaped data,not
scikit-learn/sklearn/datasets/_lfw.py,306,arrays for optimal memory usage,not
scikit-learn/sklearn/datasets/_lfw.py,308,Deal with change of API in joblib,not
scikit-learn/sklearn/datasets/_lfw.py,314,load and memoize the pairs as np arrays,not
scikit-learn/sklearn/datasets/_lfw.py,328,pack the results as a Bunch instance,not
scikit-learn/sklearn/datasets/_lfw.py,334,,not
scikit-learn/sklearn/datasets/_lfw.py,335,Task #2:  Face Verification on pairs of face pictures,not
scikit-learn/sklearn/datasets/_lfw.py,336,,not
scikit-learn/sklearn/datasets/_lfw.py,345,parse the index file to find the number of pairs to be able to allocate,not
scikit-learn/sklearn/datasets/_lfw.py,346,the right amount of memory before starting to decode the jpeg files,not
scikit-learn/sklearn/datasets/_lfw.py,352,iterating over the metadata lines for each pair to find the filename to,not
scikit-learn/sklearn/datasets/_lfw.py,353,decode and load in memory,not
scikit-learn/sklearn/datasets/_lfw.py,478,wrap the loader in a memoizing function that will return memmaped data,not
scikit-learn/sklearn/datasets/_lfw.py,479,arrays for optimal memory usage,not
scikit-learn/sklearn/datasets/_lfw.py,481,Deal with change of API in joblib,not
scikit-learn/sklearn/datasets/_lfw.py,487,select the right metadata file according to the requested subset,not
scikit-learn/sklearn/datasets/_lfw.py,498,load and memoize the pairs as np arrays,not
scikit-learn/sklearn/datasets/_lfw.py,507,pack the results as a Bunch instance,not
scikit-learn/sklearn/datasets/_california_housing.py,21,Authors: Peter Prettenhofer,not
scikit-learn/sklearn/datasets/_california_housing.py,22,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_california_housing.py,42,The original data can be found at:,not
scikit-learn/sklearn/datasets/_california_housing.py,43,https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz,not
scikit-learn/sklearn/datasets/_california_housing.py,141,Columns are not in the same order compared to the previous,not
scikit-learn/sklearn/datasets/_california_housing.py,142,URL resource on lib.stat.cmu.edu,not
scikit-learn/sklearn/datasets/_california_housing.py,157,avg rooms = total rooms / households,not
scikit-learn/sklearn/datasets/_california_housing.py,160,avg bed rooms = total bed rooms / households,not
scikit-learn/sklearn/datasets/_california_housing.py,163,avg occupancy = population / households,not
scikit-learn/sklearn/datasets/_california_housing.py,166,"target in units of 100,000",not
scikit-learn/sklearn/datasets/_samples_generator.py,5,"Authors: B. Thirion, G. Varoquaux, A. Gramfort, V. Michel, O. Grisel,",not
scikit-learn/sklearn/datasets/_samples_generator.py,6,"G. Louppe, J. Nothman",not
scikit-learn/sklearn/datasets/_samples_generator.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_samples_generator.py,160,"Count features, clusters and samples",not
scikit-learn/sklearn/datasets/_samples_generator.py,165,Use log2 to avoid overflow errors,not
scikit-learn/sklearn/datasets/_samples_generator.py,188,Distribute samples among clusters by weight,not
scikit-learn/sklearn/datasets/_samples_generator.py,196,Initialize X and y,not
scikit-learn/sklearn/datasets/_samples_generator.py,200,Build the polytope whose vertices become cluster centroids,not
scikit-learn/sklearn/datasets/_samples_generator.py,209,Initially draw informative features from the standard normal,not
scikit-learn/sklearn/datasets/_samples_generator.py,212,Create each cluster; a variant of make_blobs,not
scikit-learn/sklearn/datasets/_samples_generator.py,216,assign labels,not
scikit-learn/sklearn/datasets/_samples_generator.py,217,slice a view of the cluster,not
scikit-learn/sklearn/datasets/_samples_generator.py,220,introduce random covariance,not
scikit-learn/sklearn/datasets/_samples_generator.py,222,shift the cluster to a vertex,not
scikit-learn/sklearn/datasets/_samples_generator.py,224,Create redundant features,not
scikit-learn/sklearn/datasets/_samples_generator.py,230,Repeat some features,not
scikit-learn/sklearn/datasets/_samples_generator.py,236,Fill useless features,not
scikit-learn/sklearn/datasets/_samples_generator.py,240,Randomly replace labels,not
scikit-learn/sklearn/datasets/_samples_generator.py,245,Randomly shift and scale,not
scikit-learn/sklearn/datasets/_samples_generator.py,255,Randomly permute samples,not
scikit-learn/sklearn/datasets/_samples_generator.py,258,Randomly permute features,not
scikit-learn/sklearn/datasets/_samples_generator.py,371,pick a nonzero number of labels per document by rejection sampling,not
scikit-learn/sklearn/datasets/_samples_generator.py,376,pick n classes,not
scikit-learn/sklearn/datasets/_samples_generator.py,379,pick a class with probability P(c),not
scikit-learn/sklearn/datasets/_samples_generator.py,385,pick a non-zero document length by rejection sampling,not
scikit-learn/sklearn/datasets/_samples_generator.py,390,generate a document of length n_words,not
scikit-learn/sklearn/datasets/_samples_generator.py,392,"if sample does not belong to any class, generate noise word",not
scikit-learn/sklearn/datasets/_samples_generator.py,396,sample words with replacement from selected classes,not
scikit-learn/sklearn/datasets/_samples_generator.py,417,return_indicator can be True due to backward compatibility,not
scikit-learn/sklearn/datasets/_samples_generator.py,559,Randomly generate a well conditioned input set,not
scikit-learn/sklearn/datasets/_samples_generator.py,563,"Randomly generate a low rank, fat tail input set",not
scikit-learn/sklearn/datasets/_samples_generator.py,570,Generate a ground truth model with only n_informative features being non,not
scikit-learn/sklearn/datasets/_samples_generator.py,571,zeros (the other features are not correlated to y and should be ignored,not
scikit-learn/sklearn/datasets/_samples_generator.py,572,by a sparsifying regularizers such as L1 or elastic net),not
scikit-learn/sklearn/datasets/_samples_generator.py,579,Add noise,not
scikit-learn/sklearn/datasets/_samples_generator.py,583,Randomly permute samples and features,not
scikit-learn/sklearn/datasets/_samples_generator.py,657,"so as not to have the first point = last point, we set endpoint=False",not
scikit-learn/sklearn/datasets/_samples_generator.py,826,Set n_centers by looking at centers arg,not
scikit-learn/sklearn/datasets/_samples_generator.py,841,Set n_centers by looking at [n_samples] arg,not
scikit-learn/sklearn/datasets/_samples_generator.py,859,"stds: if cluster_std is given as list, it must be consistent",not
scikit-learn/sklearn/datasets/_samples_generator.py,860,with the n_centers,not
scikit-learn/sklearn/datasets/_samples_generator.py,1150,Random (ortho normal) vectors,not
scikit-learn/sklearn/datasets/_samples_generator.py,1154,Index of the singular values,not
scikit-learn/sklearn/datasets/_samples_generator.py,1157,Build the singular profile by assembling signal and noise components,not
scikit-learn/sklearn/datasets/_samples_generator.py,1211,generate dictionary,not
scikit-learn/sklearn/datasets/_samples_generator.py,1215,generate code,not
scikit-learn/sklearn/datasets/_samples_generator.py,1223,encode signal,not
scikit-learn/sklearn/datasets/_samples_generator.py,1373,Permute the lines: we don't want to have asymmetries in the final,not
scikit-learn/sklearn/datasets/_samples_generator.py,1374,SPD matrix,not
scikit-learn/sklearn/datasets/_samples_generator.py,1381,Form the diagonal vector into a row matrix,not
scikit-learn/sklearn/datasets/_samples_generator.py,1554,Build multivariate normal distribution,not
scikit-learn/sklearn/datasets/_samples_generator.py,1558,Sort by distance from origin,not
scikit-learn/sklearn/datasets/_samples_generator.py,1562,Label by quantile,not
scikit-learn/sklearn/datasets/_samples_generator.py,1643,row and column clusters of approximately equal sizes,not
scikit-learn/sklearn/datasets/_samples_generator.py,1738,row and column clusters of approximately equal sizes,not
scikit-learn/sklearn/datasets/_covtype.py,13,Author: Lars Buitinck,not
scikit-learn/sklearn/datasets/_covtype.py,14,Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/sklearn/datasets/_covtype.py,15,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_covtype.py,34,The original data can be found in:,not
scikit-learn/sklearn/datasets/_covtype.py,35,https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz,not
scikit-learn/sklearn/datasets/_covtype.py,117,delete archive,not
scikit-learn/sklearn/datasets/_openml.py,25,noqa,not
scikit-learn/sklearn/datasets/_openml.py,102,"potentially, the directory has been created already",not
scikit-learn/sklearn/datasets/_openml.py,118,"XXX: First time, decompression will not be necessary (by using fsrc), but",SATD
scikit-learn/sklearn/datasets/_openml.py,119,it will happen nonetheless,not
scikit-learn/sklearn/datasets/_openml.py,163,"412 is an OpenML specific error code, indicating a generic error",not
scikit-learn/sklearn/datasets/_openml.py,164,"(e.g., data not found)",not
scikit-learn/sklearn/datasets/_openml.py,168,"412 error, not in except for nicer traceback",not
scikit-learn/sklearn/datasets/_openml.py,208,"turns the sparse data back into an array (can't use toarray() function,",not
scikit-learn/sklearn/datasets/_openml.py,209,as this does only work on numeric data),not
scikit-learn/sklearn/datasets/_openml.py,214,TODO: improve for efficiency,SATD
scikit-learn/sklearn/datasets/_openml.py,269,This should never happen,not
scikit-learn/sklearn/datasets/_openml.py,280,"only numeric, integer, real are left",not
scikit-learn/sklearn/datasets/_openml.py,283,cast to floats when there are any missing values,not
scikit-learn/sklearn/datasets/_openml.py,314,calculate chunksize,not
scikit-learn/sklearn/datasets/_openml.py,321,read arff data with chunks,not
scikit-learn/sklearn/datasets/_openml.py,366,situation in which we return the oldest active version,not
scikit-learn/sklearn/datasets/_openml.py,379,an integer version has been provided,not
scikit-learn/sklearn/datasets/_openml.py,384,we can do this in 1 function call if OpenML does not require the,not
scikit-learn/sklearn/datasets/_openml.py,385,"specification of the dataset status (i.e., return datasets with a",not
scikit-learn/sklearn/datasets/_openml.py,386,"given name / version regardless of active, deactivated, etc. )",not
scikit-learn/sklearn/datasets/_openml.py,387,TODO: feature request OpenML.,SATD
scikit-learn/sklearn/datasets/_openml.py,398,OpenML API function: https://www.openml.org/api_docs#!/data/get_data_id,not
scikit-learn/sklearn/datasets/_openml.py,407,OpenML function:,not
scikit-learn/sklearn/datasets/_openml.py,408,https://www.openml.org/api_docs#!/data/get_data_features_id,not
scikit-learn/sklearn/datasets/_openml.py,417,OpenML API function:,not
scikit-learn/sklearn/datasets/_openml.py,418,https://www.openml.org/api_docs#!/data/get_data_qualities_id,not
scikit-learn/sklearn/datasets/_openml.py,426,"the qualities might not be available, but we still try to process",not
scikit-learn/sklearn/datasets/_openml.py,427,the data,not
scikit-learn/sklearn/datasets/_openml.py,445,"If the data qualities are unavailable, we return -1",not
scikit-learn/sklearn/datasets/_openml.py,461,"Note that if the data is dense, no reading is done until the data",not
scikit-learn/sklearn/datasets/_openml.py,462,generator is iterated.,not
scikit-learn/sklearn/datasets/_openml.py,474,NB: this function is long in order to handle retry for any failure,not
scikit-learn/sklearn/datasets/_openml.py,475,during the streaming parse of the ARFF.,not
scikit-learn/sklearn/datasets/_openml.py,477,Prepare which columns and data types should be returned for the X and y,not
scikit-learn/sklearn/datasets/_openml.py,480,XXX: col_slice_y should be all nominal or all numeric,SATD
scikit-learn/sklearn/datasets/_openml.py,496,Access an ARFF file on the OpenML server. Documentation:,not
scikit-learn/sklearn/datasets/_openml.py,497,https://www.openml.org/api_data_docs#!/data/get_download_id,not
scikit-learn/sklearn/datasets/_openml.py,510,type:ignore,not
scikit-learn/sklearn/datasets/_openml.py,522,nominal attributes is a dict mapping from the attribute name to,not
scikit-learn/sklearn/datasets/_openml.py,523,the possible values. Includes also the target column (which will,not
scikit-learn/sklearn/datasets/_openml.py,524,"be popped off below, before it will be packed in the Bunch",not
scikit-learn/sklearn/datasets/_openml.py,525,object),not
scikit-learn/sklearn/datasets/_openml.py,531,type:ignore,not
scikit-learn/sklearn/datasets/_openml.py,535,No target,not
scikit-learn/sklearn/datasets/_openml.py,549,"reshape y back to 1-D array, if there is only 1 target column;",not
scikit-learn/sklearn/datasets/_openml.py,550,back to None if there are not target columns,not
scikit-learn/sklearn/datasets/_openml.py,571,verifies the data type of the y array in case there are multiple targets,not
scikit-learn/sklearn/datasets/_openml.py,572,(throws an error if these targets do not comply with sklearn support),not
scikit-learn/sklearn/datasets/_openml.py,585,"note: we compare to a string, not boolean",not
scikit-learn/sklearn/datasets/_openml.py,599,logic for determining on which columns can be learned. Note that from the,not
scikit-learn/sklearn/datasets/_openml.py,600,OpenML guide follows that columns that have the `is_row_identifier` or,not
scikit-learn/sklearn/datasets/_openml.py,601,"`is_ignore` flag, these can not be learned on. Also target columns are",not
scikit-learn/sklearn/datasets/_openml.py,602,excluded.,not
scikit-learn/sklearn/datasets/_openml.py,724,no caching will be applied,not
scikit-learn/sklearn/datasets/_openml.py,727,"check valid function arguments. data_id XOR (name, version) should be",not
scikit-learn/sklearn/datasets/_openml.py,728,provided,not
scikit-learn/sklearn/datasets/_openml.py,730,"OpenML is case-insensitive, but the caching mechanism is not",not
scikit-learn/sklearn/datasets/_openml.py,731,convert all data names (str) to lower case,not
scikit-learn/sklearn/datasets/_openml.py,741,"from the previous if statement, it is given that name is None",not
scikit-learn/sklearn/datasets/_openml.py,774,"download data features, meta-info about column types",not
scikit-learn/sklearn/datasets/_openml.py,786,determines the default target based on the data feature results,not
scikit-learn/sklearn/datasets/_openml.py,787,(which is currently more reliable than the data description;,not
scikit-learn/sklearn/datasets/_openml.py,788,see issue: https://github.com/openml/OpenML/issues/768),not
scikit-learn/sklearn/datasets/_openml.py,792,"for code-simplicity, make target_column by default a list",not
scikit-learn/sklearn/datasets/_openml.py,805,determine arff encoding to return,not
scikit-learn/sklearn/datasets/_openml.py,807,The shape must include the ignored features to keep the right indexes,not
scikit-learn/sklearn/datasets/_openml.py,808,during the arff data conversion.,not
scikit-learn/sklearn/datasets/_openml.py,814,obtain the data,not
scikit-learn/sklearn/datasets/_olivetti_faces.py,13,Copyright (c) 2011 David Warde-Farley <wardefar at iro dot umontreal dot ca>,not
scikit-learn/sklearn/datasets/_olivetti_faces.py,14,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_olivetti_faces.py,30,The original data can be found at:,not
scikit-learn/sklearn/datasets/_olivetti_faces.py,31,https://cs.nyu.edu/~roweis/data/olivettifaces.mat,not
scikit-learn/sklearn/datasets/_olivetti_faces.py,113,delete raw .mat data,not
scikit-learn/sklearn/datasets/_olivetti_faces.py,122,"We want floating point data, but float32 is enough (there is only",not
scikit-learn/sklearn/datasets/_olivetti_faces.py,123,one byte of precision in the original uint8s anyway),not
scikit-learn/sklearn/datasets/_olivetti_faces.py,128,"10 images per class, 400 images total, each class is contiguous.",not
scikit-learn/sklearn/datasets/_rcv1.py,8,Author: Tom Dupre la Tour,not
scikit-learn/sklearn/datasets/_rcv1.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_rcv1.py,31,The original vectorized data can be found at:,not
scikit-learn/sklearn/datasets/_rcv1.py,32,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt0.dat.gz,not
scikit-learn/sklearn/datasets/_rcv1.py,33,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt1.dat.gz,not
scikit-learn/sklearn/datasets/_rcv1.py,34,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt2.dat.gz,not
scikit-learn/sklearn/datasets/_rcv1.py,35,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt3.dat.gz,not
scikit-learn/sklearn/datasets/_rcv1.py,36,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_train.dat.gz,not
scikit-learn/sklearn/datasets/_rcv1.py,37,while the original stemmed token files can be found,not
scikit-learn/sklearn/datasets/_rcv1.py,38,"in the README, section B.12.i.:",not
scikit-learn/sklearn/datasets/_rcv1.py,39,http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm,not
scikit-learn/sklearn/datasets/_rcv1.py,68,The original data can be found at:,not
scikit-learn/sklearn/datasets/_rcv1.py,69,http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a08-topic-qrels/rcv1-v2.topics.qrels.gz,not
scikit-learn/sklearn/datasets/_rcv1.py,167,load data (X) and sample_id,not
scikit-learn/sklearn/datasets/_rcv1.py,178,Training data is before testing data,not
scikit-learn/sklearn/datasets/_rcv1.py,186,delete archives,not
scikit-learn/sklearn/datasets/_rcv1.py,194,"load target (y), categories, and sample_id_bis",not
scikit-learn/sklearn/datasets/_rcv1.py,201,parse the target file,not
scikit-learn/sklearn/datasets/_rcv1.py,224,delete archive,not
scikit-learn/sklearn/datasets/_rcv1.py,227,"Samples in X are ordered with sample_id,",not
scikit-learn/sklearn/datasets/_rcv1.py,228,"whereas in y, they are ordered with sample_id_bis.",not
scikit-learn/sklearn/datasets/_rcv1.py,232,"save category names in a list, with same order than y",not
scikit-learn/sklearn/datasets/_rcv1.py,237,reorder categories in lexicographic order,not
scikit-learn/sklearn/datasets/_rcv1.py,281,s[p] = i,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,13,Authors: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,14,Lars Buitinck,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,15,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,16,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,166,file descriptor,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,194,"convert from array.array, give data the right dtype",not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,199,never empty,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,202,"no-op for float{32,64}",not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,295,disable heuristic search to avoid getting inconsistent results on,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,296,different segments of the file,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,443,Convert comment string to list of lines in UTF-8.,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,444,"If a byte string is passed, then check whether it's ASCII;",not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,445,"if a user wants to get fancy, they'll have to decode themselves.",not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,446,Avoid mention of str and unicode types for Python 3.x compat.,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,448,just for the exception,not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,471,"We had some issues with CSR matrices with unsorted indices (e.g. #1501),",not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,472,"so sort them here, but first make sure we don't modify the user's X.",not
scikit-learn/sklearn/datasets/_svmlight_format_io.py,473,TODO We can do this cheaper; sorted_indices copies the whole matrix.,SATD
scikit-learn/sklearn/datasets/_base.py,5,Copyright (c) 2007 David Cournapeau <cournape@gmail.com>,not
scikit-learn/sklearn/datasets/_base.py,6,2010 Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/datasets/_base.py,7,2010 Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/datasets/_base.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/datasets/_base.py,200,convert to array for fancy indexing,not
scikit-learn/sklearn/datasets/_base.py,913,Read data,not
scikit-learn/sklearn/datasets/_base.py,917,Read header,not
scikit-learn/sklearn/datasets/_base.py,1018,names of features,not
scikit-learn/sklearn/datasets/_base.py,1030,last column is target value,not
scikit-learn/sklearn/datasets/_base.py,1069,import PIL only when needed,not
scikit-learn/sklearn/datasets/_base.py,1078,Load image data for each image in the source folder.,not
scikit-learn/sklearn/datasets/tests/test_olivetti_faces.py,25,test the return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,61,generate some random jpeg files for each person,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,78,add some random file pollution to test robustness,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,82,generate some pairing metadata files using the same format as LFW,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,126,The data is croped around the center as a rectangular bounding box,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,127,around the face. Colors are converted to gray levels:,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,131,the target is array of person integer ids,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,134,names of the persons can be found using the target_names array,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,138,It is possible to ask for the original data without any croping or color,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,139,conversion and not limit on the number of picture per person,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,145,the ids and class names are the same as previously,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,152,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,176,The data is croped around the center as a rectangular bounding box,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,177,around the face. Colors are converted to gray levels:,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,180,the target is whether the person is the same or not,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,183,names of the persons can be found using the target_names array,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,187,It is possible to ask for the original data without any croping or color,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,188,conversion,not
scikit-learn/sklearn/datasets/tests/test_lfw.py,194,the ids and class names are the same as previously,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,32,test X's shape,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,38,test X's non-zero values,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,45,tests X's zero values,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,52,test can change X's values,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,56,test y,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,61,test loading from file descriptor,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,96,test X'shape,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,101,test X's non-zero values,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,107,21 features in file,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,116,necessary under windows,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,121,"because we ""close"" it manually and write to it,",not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,122,we need to remove it manually.,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,128,necessary under windows,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,133,"because we ""close"" it manually and write to it,",not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,134,we need to remove it manually.,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,172,load svmfile with qid attribute,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,207,in python 3 integers are valid file opening arguments (taken as unix,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,208,file descriptors),not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,223,"slicing a csr_matrix can unsort its .indices, so test that we sort",not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,224,those correctly,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,233,we need to pass a comment to get the version info in;,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,234,LibSVM doesn't grok comments so they're not put in by,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,235,default anymore.,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,238,"make sure y's shape is: (n_samples, n_labels)",not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,239,when it is sparse,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,242,"Note: with dtype=np.int32 we are performing unsafe casts,",not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,243,where X.astype(dtype) overflows. The result is,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,244,then platform dependent and X_dense.astype(dtype) may be,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,245,different from X_sparse.astype(dtype).asarray().,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,274,allow a rounding error at the last decimal place,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,280,allow a rounding error at the last decimal place,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,297,make sure it dumps multilabel correctly,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,308,loses the last decimal place,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,319,make sure it's using the most concise format possible,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,327,make sure it's correct too :),not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,346,XXX we have to update this to support Python 3.x,SATD
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,380,test dumping a file with query_id,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,395,load svmfile with longint qid attribute,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,458,put some marks that are likely to happen anywhere in a row,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,465,load the original sparse matrix into 3 independent CSR matrices,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,501,load the same data in 2 parts with all the possible byte offsets to,not
scikit-learn/sklearn/datasets/tests/test_svmlight_format.py,502,locate the split so has to test for particular boundary cases,SATD
scikit-learn/sklearn/datasets/tests/test_california_housing.py,15,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_california_housing.py,32,Check that pandas is imported lazily and that an informative error,not
scikit-learn/sklearn/datasets/tests/test_california_housing.py,33,message is raised when pandas is missing:,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,18,test sparsity,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,24,test shapes,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,30,test ordering of categories,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,34,test number of sample for some categories,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,41,test shuffling and subset,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,46,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,50,The first 23149 samples are the training samples,not
scikit-learn/sklearn/datasets/tests/test_rcv1.py,53,test some precise values,not
scikit-learn/sklearn/datasets/tests/test_covtype.py,23,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_20news.py,17,Extract a reduced dataset,not
scikit-learn/sklearn/datasets/tests/test_20news.py,20,Check that the ordering of the target_names is the same,not
scikit-learn/sklearn/datasets/tests/test_20news.py,21,as the ordering in the full dataset,not
scikit-learn/sklearn/datasets/tests/test_20news.py,23,Assert that we have only 0 and 1 as labels,not
scikit-learn/sklearn/datasets/tests/test_20news.py,26,Check that the number of filenames is consistent with data/target,not
scikit-learn/sklearn/datasets/tests/test_20news.py,30,Check that the first entry of the reduced dataset corresponds to,not
scikit-learn/sklearn/datasets/tests/test_20news.py,31,the first entry of the corresponding category in the full dataset,not
scikit-learn/sklearn/datasets/tests/test_20news.py,38,check that return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_20news.py,49,Extract the full dataset,not
scikit-learn/sklearn/datasets/tests/test_20news.py,57,test subset = train,not
scikit-learn/sklearn/datasets/tests/test_20news.py,64,test subset = test,not
scikit-learn/sklearn/datasets/tests/test_20news.py,71,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_20news.py,75,test subset = all,not
scikit-learn/sklearn/datasets/tests/test_common.py,9,noqa,not
scikit-learn/sklearn/datasets/tests/test_common.py,12,Check that pandas is imported lazily and that an informative error,not
scikit-learn/sklearn/datasets/tests/test_common.py,13,message is raised when pandas is missing:,not
scikit-learn/sklearn/datasets/tests/test_openml.py,31,"if True, urlopen will be monkey patched to only use local files",not
scikit-learn/sklearn/datasets/tests/test_openml.py,36,XXX Test is intended to verify/ensure correct decoding behavior,SATD
scikit-learn/sklearn/datasets/tests/test_openml.py,37,Not usable with sparse data or datasets that have columns marked as,not
scikit-learn/sklearn/datasets/tests/test_openml.py,38,"{row_identifier, ignore}",not
scikit-learn/sklearn/datasets/tests/test_openml.py,42,"XXX: This would be faster with np.take, although it does not",SATD
scikit-learn/sklearn/datasets/tests/test_openml.py,43,handle missing values fast (also not with mode='wrap'),not
scikit-learn/sklearn/datasets/tests/test_openml.py,49,non-nominal attribute,not
scikit-learn/sklearn/datasets/tests/test_openml.py,54,also obtain decoded arff,not
scikit-learn/sklearn/datasets/tests/test_openml.py,70,"XXX: Test per column, as this makes it easier to avoid problems with",SATD
scikit-learn/sklearn/datasets/tests/test_openml.py,71,missing values,not
scikit-learn/sklearn/datasets/tests/test_openml.py,83,"fetches a dataset in three various ways from OpenML, using the",not
scikit-learn/sklearn/datasets/tests/test_openml.py,84,"fetch_openml function, and does various checks on the validity of the",not
scikit-learn/sklearn/datasets/tests/test_openml.py,85,result. Note that this function can be mocked (by invoking,not
scikit-learn/sklearn/datasets/tests/test_openml.py,86,_monkey_patch_webbased_functions before invoking this function),not
scikit-learn/sklearn/datasets/tests/test_openml.py,91,"Please note that cache=False is crucial, as the monkey patched files are",not
scikit-learn/sklearn/datasets/tests/test_openml.py,92,not consistent with reality,not
scikit-learn/sklearn/datasets/tests/test_openml.py,94,"without specifying the version, there is no guarantee that the data id",not
scikit-learn/sklearn/datasets/tests/test_openml.py,95,will be the same,not
scikit-learn/sklearn/datasets/tests/test_openml.py,97,fetch with dataset id,not
scikit-learn/sklearn/datasets/tests/test_openml.py,103,"single target, so target is vector",not
scikit-learn/sklearn/datasets/tests/test_openml.py,107,"multi target, so target is array",not
scikit-learn/sklearn/datasets/tests/test_openml.py,117,TODO: pass in a list of expected nominal features,SATD
scikit-learn/sklearn/datasets/tests/test_openml.py,125,check whether the data by id and data by id target are equal,not
scikit-learn/sklearn/datasets/tests/test_openml.py,138,np.isnan doesn't work on CSR matrix,not
scikit-learn/sklearn/datasets/tests/test_openml.py,142,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_openml.py,152,monkey patches the urlopen function. Important note: Do NOT use this,not
scikit-learn/sklearn/datasets/tests/test_openml.py,153,"in combination with a regular cache directory, as the files that are",not
scikit-learn/sklearn/datasets/tests/test_openml.py,154,stored as cache should not be mixed up with real openml datasets,not
scikit-learn/sklearn/datasets/tests/test_openml.py,240,"load the file itself, to simulate a http error",not
scikit-learn/sklearn/datasets/tests/test_openml.py,269,XXX: Global variable,SATD
scikit-learn/sklearn/datasets/tests/test_openml.py,300,classification dataset with numeric only columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,341,as_frame = True returns the same underlying data as as_frame = False,not
scikit-learn/sklearn/datasets/tests/test_openml.py,360,classification dataset with numeric only columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,402,classification dataset with numeric and categorical columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,439,regression dataset with numeric and categorical columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,536,Check because of the numeric row attribute (issue #12329),not
scikit-learn/sklearn/datasets/tests/test_openml.py,569,"JvR: very important check, as this dataset defined several row ids",not
scikit-learn/sklearn/datasets/tests/test_openml.py,570,"and ignore attributes. Note that data_features json has 82 attributes,",not
scikit-learn/sklearn/datasets/tests/test_openml.py,571,"and row id (1), ignore attributes (3) have been removed.",not
scikit-learn/sklearn/datasets/tests/test_openml.py,609,classification dataset with multiple targets (natively),not
scikit-learn/sklearn/datasets/tests/test_openml.py,647,dataset with strings,not
scikit-learn/sklearn/datasets/tests/test_openml.py,704,classification dataset with numeric only columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,741,classification dataset with numeric only columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,760,classification dataset with numeric and categorical columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,765,Not all original instances included for space reasons,not
scikit-learn/sklearn/datasets/tests/test_openml.py,785,classification dataset with numeric and categorical columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,790,Not all original instances included for space reasons,not
scikit-learn/sklearn/datasets/tests/test_openml.py,804,regression dataset with numeric and categorical columns,not
scikit-learn/sklearn/datasets/tests/test_openml.py,828,sparse dataset,not
scikit-learn/sklearn/datasets/tests/test_openml.py,829,Australian is the only sparse dataset that is reasonably small,not
scikit-learn/sklearn/datasets/tests/test_openml.py,830,"as it is inactive, we need to catch the warning. Due to mocking",not
scikit-learn/sklearn/datasets/tests/test_openml.py,831,"framework, it is not deactivated in our tests",not
scikit-learn/sklearn/datasets/tests/test_openml.py,836,Not all original instances included for space reasons,not
scikit-learn/sklearn/datasets/tests/test_openml.py,854,numpy specific check,not
scikit-learn/sklearn/datasets/tests/test_openml.py,860,Check because of the numeric row attribute (issue #12329),not
scikit-learn/sklearn/datasets/tests/test_openml.py,865,Not all original instances included for space reasons,not
scikit-learn/sklearn/datasets/tests/test_openml.py,879,"JvR: very important check, as this dataset defined several row ids",not
scikit-learn/sklearn/datasets/tests/test_openml.py,880,"and ignore attributes. Note that data_features json has 82 attributes,",not
scikit-learn/sklearn/datasets/tests/test_openml.py,881,"and row id (1), ignore attributes (3) have been removed (and target is",not
scikit-learn/sklearn/datasets/tests/test_openml.py,882,stored in data.target),not
scikit-learn/sklearn/datasets/tests/test_openml.py,887,Not all original instances included for space reasons,not
scikit-learn/sklearn/datasets/tests/test_openml.py,901,classification dataset with multiple targets (natively),not
scikit-learn/sklearn/datasets/tests/test_openml.py,933,first fill the cache,not
scikit-learn/sklearn/datasets/tests/test_openml.py,935,assert file exists,not
scikit-learn/sklearn/datasets/tests/test_openml.py,938,"redownload, to utilize cache",not
scikit-learn/sklearn/datasets/tests/test_openml.py,978,The first call will raise an error since location exists,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1045,fetch inactive dataset by id,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1051,fetch inactive dataset by name and version,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1061,there is no active version of glass2,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1064,Note that we only want to search by name (not data id),not
scikit-learn/sklearn/datasets/tests/test_openml.py,1074,Note that we only want to search by name (not data id),not
scikit-learn/sklearn/datasets/tests/test_openml.py,1087,single column test,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1096,multi column test,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1111,single column test,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1182,Regression test for #14340,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1183,62 is the ID of the ZOO dataset,not
scikit-learn/sklearn/datasets/tests/test_openml.py,1189,"The dataset has 17 features, including 1 ignored (animal),",not
scikit-learn/sklearn/datasets/tests/test_openml.py,1190,so we assert that we don't have the ignored feature in the final Bunch,not
scikit-learn/sklearn/datasets/tests/test_base.py,73,get_data_home will point to a pre-existing folder,not
scikit-learn/sklearn/datasets/tests/test_base.py,78,clear_data_home will delete both the content and the folder it-self,not
scikit-learn/sklearn/datasets/tests/test_base.py,82,if the folder is missing it will be created again,not
scikit-learn/sklearn/datasets/tests/test_base.py,134,assert is china image,not
scikit-learn/sklearn/datasets/tests/test_base.py,137,assert is flower image,not
scikit-learn/sklearn/datasets/tests/test_base.py,150,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,184,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,197,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,209,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,220,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,232,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,271,test return_X_y option,not
scikit-learn/sklearn/datasets/tests/test_base.py,284,This reproduces a problem when Bunch pickles have been created,not
scikit-learn/sklearn/datasets/tests/test_base.py,285,with scikit-learn 0.16 and are read with 0.17. Basically there,not
scikit-learn/sklearn/datasets/tests/test_base.py,286,is a surprising behaviour because reading bunch.key uses,not
scikit-learn/sklearn/datasets/tests/test_base.py,287,bunch.__dict__ (which is non empty for 0.16 Bunch objects),not
scikit-learn/sklearn/datasets/tests/test_base.py,288,whereas assigning into bunch.key uses bunch.__setattr__. See,not
scikit-learn/sklearn/datasets/tests/test_base.py,289,https://github.com/scikit-learn/scikit-learn/issues/6196 for,not
scikit-learn/sklearn/datasets/tests/test_base.py,290,more details,not
scikit-learn/sklearn/datasets/tests/test_base.py,293,After loading from pickle the __dict__ should have been ignored,not
scikit-learn/sklearn/datasets/tests/test_base.py,296,Making sure that changing the attr does change the value,not
scikit-learn/sklearn/datasets/tests/test_base.py,297,associated with __getitem__ as well,not
scikit-learn/sklearn/datasets/tests/test_base.py,304,check that dir (important for autocomplete) shows attributes,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,52,Test for n_features > 30,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,70,Create very separate clusters; check that vertices are unique and,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,71,correspond to classes,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,98,"Cluster by sign, viewed as strings to allow uniquing",not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,121,Ensure on vertices of hypercube,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,201,Also test return_distributions and return_indicator with True,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,254,"Test that y ~= np.dot(X, c) + bias + N(0, 1.0).",not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,257,Test with small number of features.,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,258,n_informative=3,not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,272,"Test that y ~= np.dot(X, c) + bias + N(0, 1.0)",not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,520,"Testing odd and even case, because in the past make_circles always",not
scikit-learn/sklearn/datasets/tests/test_samples_generator.py,521,created an even number of samples.,not
scikit-learn/sklearn/utils/_joblib.py,5,joblib imports may raise DeprecationWarning on certain Python,not
scikit-learn/sklearn/utils/_joblib.py,6,versions,not
scikit-learn/sklearn/utils/_pprint.py,4,"Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,",not
scikit-learn/sklearn/utils/_pprint.py,5,"2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation;",not
scikit-learn/sklearn/utils/_pprint.py,6,All Rights Reserved,not
scikit-learn/sklearn/utils/_pprint.py,8,"Authors: Fred L. Drake, Jr. <fdrake@acm.org> (built-in CPython pprint module)",not
scikit-learn/sklearn/utils/_pprint.py,9,Nicolas Hug (scikit-learn specific changes),not
scikit-learn/sklearn/utils/_pprint.py,11,License: PSF License version 2 (see below),not
scikit-learn/sklearn/utils/_pprint.py,13,PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2,not
scikit-learn/sklearn/utils/_pprint.py,14,--------------------------------------------,not
scikit-learn/sklearn/utils/_pprint.py,16,"1. This LICENSE AGREEMENT is between the Python Software Foundation (""PSF""),",not
scikit-learn/sklearn/utils/_pprint.py,17,"and the Individual or Organization (""Licensee"") accessing and otherwise",not
scikit-learn/sklearn/utils/_pprint.py,18,"using this software (""Python"") in source or binary form and its associated",not
scikit-learn/sklearn/utils/_pprint.py,19,documentation.,not
scikit-learn/sklearn/utils/_pprint.py,21,"2. Subject to the terms and conditions of this License Agreement, PSF hereby",not
scikit-learn/sklearn/utils/_pprint.py,22,"grants Licensee a nonexclusive, royalty-free, world-wide license to",not
scikit-learn/sklearn/utils/_pprint.py,23,"reproduce, analyze, test, perform and/or display publicly, prepare",not
scikit-learn/sklearn/utils/_pprint.py,24,"derivative works, distribute, and otherwise use Python alone or in any",not
scikit-learn/sklearn/utils/_pprint.py,25,"derivative version, provided, however, that PSF's License Agreement and",not
scikit-learn/sklearn/utils/_pprint.py,26,"PSF's notice of copyright, i.e., ""Copyright (c) 2001, 2002, 2003, 2004,",not
scikit-learn/sklearn/utils/_pprint.py,27,"2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016,",not
scikit-learn/sklearn/utils/_pprint.py,28,"2017, 2018 Python Software Foundation; All Rights Reserved"" are retained in",not
scikit-learn/sklearn/utils/_pprint.py,29,Python alone or in any derivative version prepared by Licensee.,not
scikit-learn/sklearn/utils/_pprint.py,31,3. In the event Licensee prepares a derivative work that is based on or,not
scikit-learn/sklearn/utils/_pprint.py,32,"incorporates Python or any part thereof, and wants to make the derivative",not
scikit-learn/sklearn/utils/_pprint.py,33,"work available to others as provided herein, then Licensee hereby agrees to",not
scikit-learn/sklearn/utils/_pprint.py,34,include in any such work a brief summary of the changes made to Python.,not
scikit-learn/sklearn/utils/_pprint.py,36,"4. PSF is making Python available to Licensee on an ""AS IS"" basis. PSF MAKES",not
scikit-learn/sklearn/utils/_pprint.py,37,"NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT",not
scikit-learn/sklearn/utils/_pprint.py,38,"NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF",not
scikit-learn/sklearn/utils/_pprint.py,39,MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF,not
scikit-learn/sklearn/utils/_pprint.py,40,PYTHON WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.,not
scikit-learn/sklearn/utils/_pprint.py,42,5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON FOR ANY,not
scikit-learn/sklearn/utils/_pprint.py,43,"INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF",not
scikit-learn/sklearn/utils/_pprint.py,44,"MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON, OR ANY DERIVATIVE",not
scikit-learn/sklearn/utils/_pprint.py,45,"THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.",not
scikit-learn/sklearn/utils/_pprint.py,47,6. This License Agreement will automatically terminate upon a material,not
scikit-learn/sklearn/utils/_pprint.py,48,breach of its terms and conditions.,not
scikit-learn/sklearn/utils/_pprint.py,50,7. Nothing in this License Agreement shall be deemed to create any,not
scikit-learn/sklearn/utils/_pprint.py,51,"relationship of agency, partnership, or joint venture between PSF and",not
scikit-learn/sklearn/utils/_pprint.py,52,Licensee. This License Agreement does not grant permission to use PSF,not
scikit-learn/sklearn/utils/_pprint.py,53,trademarks or trade name in a trademark sense to endorse or promote products,not
scikit-learn/sklearn/utils/_pprint.py,54,"or services of Licensee, or any third party.",not
scikit-learn/sklearn/utils/_pprint.py,56,"8. By copying, installing or otherwise using Python, Licensee agrees to be",not
scikit-learn/sklearn/utils/_pprint.py,57,bound by the terms and conditions of this License Agreement.,not
scikit-learn/sklearn/utils/_pprint.py,60,Brief summary of changes to original code:,not
scikit-learn/sklearn/utils/_pprint.py,61,"- ""compact"" parameter is supported for dicts, not just lists or tuples",not
scikit-learn/sklearn/utils/_pprint.py,62,"- estimators have a custom handler, they're not just treated as objects",not
scikit-learn/sklearn/utils/_pprint.py,63,"- long sequences (lists, tuples, dict items) with more than N elements are",not
scikit-learn/sklearn/utils/_pprint.py,64,"shortened using ellipsis (', ...') at the end.",not
scikit-learn/sklearn/utils/_pprint.py,78,needed for _dispatch[tuple.__repr__] not to be overridden,not
scikit-learn/sklearn/utils/_pprint.py,161,ignore indent param,not
scikit-learn/sklearn/utils/_pprint.py,163,"Max number of elements in a list, dict, tuple until we start using",not
scikit-learn/sklearn/utils/_pprint.py,164,ellipsis. This also affects the number of arguments of an estimators,not
scikit-learn/sklearn/utils/_pprint.py,165,(they are treated as dicts),not
scikit-learn/sklearn/utils/_pprint.py,324,Note: need to copy _dispatch to prevent instances of the builtin,not
scikit-learn/sklearn/utils/_pprint.py,325,PrettyPrinter class to call methods of _EstimatorPrettyPrinter (see issue,not
scikit-learn/sklearn/utils/_pprint.py,326,12906),not
scikit-learn/sklearn/utils/_pprint.py,327,"mypy error: ""Type[PrettyPrinter]"" has no attribute ""_dispatch""",not
scikit-learn/sklearn/utils/_pprint.py,328,type: ignore,not
scikit-learn/sklearn/utils/fixes.py,6,Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,not
scikit-learn/sklearn/utils/fixes.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/utils/fixes.py,8,Fabian Pedregosa <fpedregosa@acm.org>,not
scikit-learn/sklearn/utils/fixes.py,9,Lars Buitinck,not
scikit-learn/sklearn/utils/fixes.py,10,,not
scikit-learn/sklearn/utils/fixes.py,11,License: BSD 3 clause,not
scikit-learn/sklearn/utils/fixes.py,19,noqa,not
scikit-learn/sklearn/utils/fixes.py,28,x may be of the form dev-1ea1592,not
scikit-learn/sklearn/utils/fixes.py,40,"Backport of lobpcg functionality from scipy 1.4.0, can be removed",not
scikit-learn/sklearn/utils/fixes.py,41,"once support for sp_version < (1, 4) is dropped",not
scikit-learn/sklearn/utils/fixes.py,42,mypy error: Name 'lobpcg' already defined (possibly by an import),not
scikit-learn/sklearn/utils/fixes.py,43,type: ignore  # noqa,not
scikit-learn/sklearn/utils/fixes.py,50,"TODO: replace by copy=False, when only scipy > 1.1 is supported.",SATD
scikit-learn/sklearn/utils/estimator_checks.py,75,Check that all estimator yield informative messages when,not
scikit-learn/sklearn/utils/estimator_checks.py,76,trained on empty datasets,not
scikit-learn/sklearn/utils/estimator_checks.py,83,"cross-decomposition's ""transform"" returns X and Y",not
scikit-learn/sklearn/utils/estimator_checks.py,87,Test that all estimators check their input for NaN's and infs,not
scikit-learn/sklearn/utils/estimator_checks.py,91,Check that pairwise estimator throws error on non-square input,not
scikit-learn/sklearn/utils/estimator_checks.py,100,"Test that estimators can be pickled, and once pickled",not
scikit-learn/sklearn/utils/estimator_checks.py,101,give the same answer as before.,not
scikit-learn/sklearn/utils/estimator_checks.py,108,test classifiers can handle non-array data and pandas objects,not
scikit-learn/sklearn/utils/estimator_checks.py,110,test classifiers trained on a single label always return this label,not
scikit-learn/sklearn/utils/estimator_checks.py,116,basic consistency testing,not
scikit-learn/sklearn/utils/estimator_checks.py,133,test if predict_proba is a monotonic transformation of decision_function,not
scikit-learn/sklearn/utils/estimator_checks.py,139,Checks that the Estimator targets are not NaN.,not
scikit-learn/sklearn/utils/estimator_checks.py,162,TODO: test with intercept,SATD
scikit-learn/sklearn/utils/estimator_checks.py,163,TODO: test with multiple responses,SATD
scikit-learn/sklearn/utils/estimator_checks.py,164,basic testing,not
scikit-learn/sklearn/utils/estimator_checks.py,178,check that the regressor handles int input,not
scikit-learn/sklearn/utils/estimator_checks.py,186,All transformers should either deal with sparse data or raise an,not
scikit-learn/sklearn/utils/estimator_checks.py,187,exception with type TypeError and an intelligible error message,not
scikit-learn/sklearn/utils/estimator_checks.py,190,"these don't actually fit the data, so don't raise errors",not
scikit-learn/sklearn/utils/estimator_checks.py,195,Dependent on external solvers and hence accessing the iter,not
scikit-learn/sklearn/utils/estimator_checks.py,196,param is non-trivial.,not
scikit-learn/sklearn/utils/estimator_checks.py,206,this is clustering on the features,not
scikit-learn/sklearn/utils/estimator_checks.py,207,let's not test that here.,not
scikit-learn/sklearn/utils/estimator_checks.py,216,checks for outlier detectors that have a fit_predict method,not
scikit-learn/sklearn/utils/estimator_checks.py,220,checks for estimators that can be used on a test set,not
scikit-learn/sklearn/utils/estimator_checks.py,224,test outlier detectors can handle non-array data,not
scikit-learn/sklearn/utils/estimator_checks.py,226,test if NotFittedError is raised,not
scikit-learn/sklearn/utils/estimator_checks.py,344,"check isn't part of the xfail_checks tags, just return it",not
scikit-learn/sklearn/utils/estimator_checks.py,347,"check is in the tag, mark it as xfail for pytest",not
scikit-learn/sklearn/utils/estimator_checks.py,468,the only SkipTest thrown currently results from not,not
scikit-learn/sklearn/utils/estimator_checks.py,469,being able to import pandas.,not
scikit-learn/sklearn/utils/estimator_checks.py,491,set parameters to speed up some estimators and,not
scikit-learn/sklearn/utils/estimator_checks.py,492,avoid deprecated behaviour,not
scikit-learn/sklearn/utils/estimator_checks.py,500,"LinearSVR, LinearSVC",not
scikit-learn/sklearn/utils/estimator_checks.py,503,NMF,not
scikit-learn/sklearn/utils/estimator_checks.py,506,MLP,not
scikit-learn/sklearn/utils/estimator_checks.py,510,randomized lasso,not
scikit-learn/sklearn/utils/estimator_checks.py,515,RANSAC,not
scikit-learn/sklearn/utils/estimator_checks.py,518,K-Means,not
scikit-learn/sklearn/utils/estimator_checks.py,522,TruncatedSVD doesn't run with n_components = n_features,not
scikit-learn/sklearn/utils/estimator_checks.py,523,This is ugly :-/,SATD
scikit-learn/sklearn/utils/estimator_checks.py,533,be tolerant of noisy datasets (not actually speed),not
scikit-learn/sklearn/utils/estimator_checks.py,540,"Due to the jl lemma and often very few samples, the number",not
scikit-learn/sklearn/utils/estimator_checks.py,541,of components of the random matrix projection will be probably,not
scikit-learn/sklearn/utils/estimator_checks.py,542,greater than the number of features.,not
scikit-learn/sklearn/utils/estimator_checks.py,543,"So we impose a smaller number (avoid ""auto"" mode)",not
scikit-learn/sklearn/utils/estimator_checks.py,547,SelectKBest has a default of k=10,not
scikit-learn/sklearn/utils/estimator_checks.py,548,which is more feature than we have in most case.,not
scikit-learn/sklearn/utils/estimator_checks.py,553,The default min_samples_leaf (20) isn't appropriate for small,not
scikit-learn/sklearn/utils/estimator_checks.py,554,datasets (only very shallow trees are built) that the checks use.,not
scikit-learn/sklearn/utils/estimator_checks.py,557,Speed-up by reducing the number of CV or splits for CV estimators,not
scikit-learn/sklearn/utils/estimator_checks.py,593,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/estimator_checks.py,667,Generate large indices matrix only if its supported by scipy,not
scikit-learn/sklearn/utils/estimator_checks.py,691,catch deprecation warnings,not
scikit-learn/sklearn/utils/estimator_checks.py,696,catch deprecation warnings,not
scikit-learn/sklearn/utils/estimator_checks.py,701,fit and predict,not
scikit-learn/sklearn/utils/estimator_checks.py,740,check that estimators will accept a 'sample_weight' parameter of,not
scikit-learn/sklearn/utils/estimator_checks.py,741,type pandas.Series in the 'fit' function.,not
scikit-learn/sklearn/utils/estimator_checks.py,767,check that estimators will accept a 'sample_weight' parameter of,not
scikit-learn/sklearn/utils/estimator_checks.py,768,type _NotAnArray in the 'fit' function.,not
scikit-learn/sklearn/utils/estimator_checks.py,784,check that estimators will accept a 'sample_weight' parameter of,not
scikit-learn/sklearn/utils/estimator_checks.py,785,type list in the 'fit' function.,not
scikit-learn/sklearn/utils/estimator_checks.py,798,Test that estimators don't raise any exception,not
scikit-learn/sklearn/utils/estimator_checks.py,804,check that estimators raise an error if sample_weight,not
scikit-learn/sklearn/utils/estimator_checks.py,805,shape mismatches the input,not
scikit-learn/sklearn/utils/estimator_checks.py,829,check that the estimators yield same results for,not
scikit-learn/sklearn/utils/estimator_checks.py,830,unit weights and no weights,not
scikit-learn/sklearn/utils/estimator_checks.py,834,We skip pairwise because the data is not pairwise,not
scikit-learn/sklearn/utils/estimator_checks.py,867,check that estimators treat dtype object as numeric if possible,not
scikit-learn/sklearn/utils/estimator_checks.py,897,Estimators supporting string will not call np.asarray to convert the,not
scikit-learn/sklearn/utils/estimator_checks.py,898,"data to numeric and therefore, the error will not be raised.",not
scikit-learn/sklearn/utils/estimator_checks.py,899,Checking for each element dtype in the input array will be costly.,not
scikit-learn/sklearn/utils/estimator_checks.py,900,Refer to #11401 for full discussion.,not
scikit-learn/sklearn/utils/estimator_checks.py,905,check that estimators raise an exception on providing complex data,not
scikit-learn/sklearn/utils/estimator_checks.py,916,this estimator raises,not
scikit-learn/sklearn/utils/estimator_checks.py,917,"ValueError: Found array with 0 feature(s) (shape=(23, 0))",not
scikit-learn/sklearn/utils/estimator_checks.py,918,while a minimum of 1 is required.,not
scikit-learn/sklearn/utils/estimator_checks.py,919,error,not
scikit-learn/sklearn/utils/estimator_checks.py,966,check that fit method only changes or sets private attributes,not
scikit-learn/sklearn/utils/estimator_checks.py,968,to not check deprecated classes,not
scikit-learn/sklearn/utils/estimator_checks.py,996,check that fit doesn't add any public attribute,not
scikit-learn/sklearn/utils/estimator_checks.py,1005,check that fit doesn't change any public attribute,not
scikit-learn/sklearn/utils/estimator_checks.py,1021,check by fitting a 2d array and predicting with a 1d array,not
scikit-learn/sklearn/utils/estimator_checks.py,1040,FIXME this is a bit loose,SATD
scikit-learn/sklearn/utils/estimator_checks.py,1051,apply function on the whole set and on mini batches,not
scikit-learn/sklearn/utils/estimator_checks.py,1057,func can output tuple (e.g. score_samples),not
scikit-learn/sklearn/utils/estimator_checks.py,1071,check that method gives invariant results if applied,not
scikit-learn/sklearn/utils/estimator_checks.py,1072,on mini batches or the whole set,not
scikit-learn/sklearn/utils/estimator_checks.py,1105,Check that fitting a 2d array with only one sample either works or,not
scikit-learn/sklearn/utils/estimator_checks.py,1106,returns an informative message. The error message should either mention,not
scikit-learn/sklearn/utils/estimator_checks.py,1107,the number of samples or the number of classes.,not
scikit-learn/sklearn/utils/estimator_checks.py,1123,min_cluster_size cannot be less than the data size for OPTICS.,not
scikit-learn/sklearn/utils/estimator_checks.py,1139,check fitting a 2d array with only 1 feature either works or returns,not
scikit-learn/sklearn/utils/estimator_checks.py,1140,informative message,not
scikit-learn/sklearn/utils/estimator_checks.py,1152,ensure two labels in subsample for RandomizedLogisticRegression,not
scikit-learn/sklearn/utils/estimator_checks.py,1155,ensure non skipped trials for RANSACRegressor,not
scikit-learn/sklearn/utils/estimator_checks.py,1173,check fitting 1d X array raises a ValueError,not
scikit-learn/sklearn/utils/estimator_checks.py,1180,FIXME this is a bit loose,SATD
scikit-learn/sklearn/utils/estimator_checks.py,1212,"We need to make sure that we have non negative data, for things",not
scikit-learn/sklearn/utils/estimator_checks.py,1213,like NMF,not
scikit-learn/sklearn/utils/estimator_checks.py,1219,try the same with some list,not
scikit-learn/sklearn/utils/estimator_checks.py,1240,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1251,fit_transform method should work on non fitted estimator,not
scikit-learn/sklearn/utils/estimator_checks.py,1259,check for consistent n_samples,not
scikit-learn/sklearn/utils/estimator_checks.py,1299,raises error on malformed input for transform,not
scikit-learn/sklearn/utils/estimator_checks.py,1304,"If it's not an array, it does not have a 'T' property",not
scikit-learn/sklearn/utils/estimator_checks.py,1319,check that make_pipeline(est) gives same score as est,not
scikit-learn/sklearn/utils/estimator_checks.py,1344,check that all estimators accept an optional y,not
scikit-learn/sklearn/utils/estimator_checks.py,1345,in fit and score so they can be used in pipelines,not
scikit-learn/sklearn/utils/estimator_checks.py,1365,if_delegate_has_method makes methods into functions,not
scikit-learn/sklearn/utils/estimator_checks.py,1366,"with an explicit ""self"", so need to shift arguments",not
scikit-learn/sklearn/utils/estimator_checks.py,1405,The precise message can change depending on whether X or y is,not
scikit-learn/sklearn/utils/estimator_checks.py,1406,validated first. Let us test the type of exception only:,not
scikit-learn/sklearn/utils/estimator_checks.py,1414,the following y should be accepted by both classifiers and regressors,not
scikit-learn/sklearn/utils/estimator_checks.py,1415,and ignored by unsupervised models,not
scikit-learn/sklearn/utils/estimator_checks.py,1424,Checks that Estimator X's do not contain NaN or inf.,not
scikit-learn/sklearn/utils/estimator_checks.py,1441,catch deprecation warnings,not
scikit-learn/sklearn/utils/estimator_checks.py,1445,try to fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1459,actually fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1462,predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1477,transform,not
scikit-learn/sklearn/utils/estimator_checks.py,1515,some estimators can't do features less than 0,not
scikit-learn/sklearn/utils/estimator_checks.py,1520,include NaN values when the estimator should deal with them,not
scikit-learn/sklearn/utils/estimator_checks.py,1522,set randomly 10 elements to np.nan,not
scikit-learn/sklearn/utils/estimator_checks.py,1534,pickle and unpickle!,not
scikit-learn/sklearn/utils/estimator_checks.py,1552,check if number of features changes between calls to partial_fit.,not
scikit-learn/sklearn/utils/estimator_checks.py,1670,catch deprecation and neighbors warnings,not
scikit-learn/sklearn/utils/estimator_checks.py,1678,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1680,with lists,not
scikit-learn/sklearn/utils/estimator_checks.py,1693,fit_predict(X) and labels_ should be of type int,not
scikit-learn/sklearn/utils/estimator_checks.py,1697,Add noise to X to test the possible values of the labels,not
scikit-learn/sklearn/utils/estimator_checks.py,1700,There should be at least one sample in every cluster. Equivalently,not
scikit-learn/sklearn/utils/estimator_checks.py,1701,labels_ should contain all the consecutive values between its,not
scikit-learn/sklearn/utils/estimator_checks.py,1702,min and its max.,not
scikit-learn/sklearn/utils/estimator_checks.py,1707,Labels are expected to start at 0 (no noise) or -1 (if noise),not
scikit-learn/sklearn/utils/estimator_checks.py,1709,Labels should be less than n_clusters - 1,not
scikit-learn/sklearn/utils/estimator_checks.py,1713,else labels should be less than max(labels_) which is necessarily true,not
scikit-learn/sklearn/utils/estimator_checks.py,1724,MiniBatchKMeans,not
scikit-learn/sklearn/utils/estimator_checks.py,1740,catch deprecation warnings,not
scikit-learn/sklearn/utils/estimator_checks.py,1743,try to fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1757,predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1765,Warnings are raised by decision function,not
scikit-learn/sklearn/utils/estimator_checks.py,1772,generate binary problem from multi-class one,not
scikit-learn/sklearn/utils/estimator_checks.py,1798,raises error on malformed input for fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1809,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1811,with lists,not
scikit-learn/sklearn/utils/estimator_checks.py,1817,training set performance,not
scikit-learn/sklearn/utils/estimator_checks.py,1821,raises error on malformed input for predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1840,decision_function agrees with predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1853,raises error on malformed input for decision_function,not
scikit-learn/sklearn/utils/estimator_checks.py,1867,predict_proba agrees with predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1871,check that probas for all classes sum to one,not
scikit-learn/sklearn/utils/estimator_checks.py,1875,raises error on malformed input for predict_proba,not
scikit-learn/sklearn/utils/estimator_checks.py,1885,predict_log_proba is a transformation of predict_proba,not
scikit-learn/sklearn/utils/estimator_checks.py,1892,Check for deviation from the precise given contamination level that may,not
scikit-learn/sklearn/utils/estimator_checks.py,1893,be due to ties in the anomaly scores.,not
scikit-learn/sklearn/utils/estimator_checks.py,1901,"ensure that all values in the 'critical area' are tied,",not
scikit-learn/sklearn/utils/estimator_checks.py,1902,leading to the observed discrepancy between provided,not
scikit-learn/sklearn/utils/estimator_checks.py,1903,and actual contamination levels.,not
scikit-learn/sklearn/utils/estimator_checks.py,1923,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,1925,with lists,not
scikit-learn/sklearn/utils/estimator_checks.py,1939,raises error on malformed input for predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1942,decision_function agrees with predict,not
scikit-learn/sklearn/utils/estimator_checks.py,1947,raises error on malformed input for decision_function,not
scikit-learn/sklearn/utils/estimator_checks.py,1950,decision_function is a translation of score_samples,not
scikit-learn/sklearn/utils/estimator_checks.py,1954,raises error on malformed input for score_samples,not
scikit-learn/sklearn/utils/estimator_checks.py,1957,contamination parameter (not for OneClassSVM which has the nu parameter),not
scikit-learn/sklearn/utils/estimator_checks.py,1960,proportion of outliers equal to contamination parameter when not,not
scikit-learn/sklearn/utils/estimator_checks.py,1961,set to 'auto'. This is true for the training set and cannot thus be,not
scikit-learn/sklearn/utils/estimator_checks.py,1962,checked as follows for estimators with a novelty parameter such as,not
scikit-learn/sklearn/utils/estimator_checks.py,1963,LocalOutlierFactor (tested in check_outliers_fit_predict),not
scikit-learn/sklearn/utils/estimator_checks.py,1971,num_outliers should be equal to expected_outliers unless,not
scikit-learn/sklearn/utils/estimator_checks.py,1972,there are ties in the decision_function values. this can,not
scikit-learn/sklearn/utils/estimator_checks.py,1973,only be tested for estimators with a decision_function,not
scikit-learn/sklearn/utils/estimator_checks.py,1974,"method, i.e. all estimators except LOF which is already",not
scikit-learn/sklearn/utils/estimator_checks.py,1975,excluded from this if branch.,not
scikit-learn/sklearn/utils/estimator_checks.py,1980,"raises error when contamination is a scalar and not in [0,1]",not
scikit-learn/sklearn/utils/estimator_checks.py,2029,some want non-negative input,not
scikit-learn/sklearn/utils/estimator_checks.py,2049,"Common test for Regressors, Classifiers and Outlier detection estimators",not
scikit-learn/sklearn/utils/estimator_checks.py,2063,"These only work on 2d, so this test makes no sense",not
scikit-learn/sklearn/utils/estimator_checks.py,2077,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,2082,"Check that when a 2D y is given, a DataConversionWarning is",not
scikit-learn/sklearn/utils/estimator_checks.py,2083,raised,not
scikit-learn/sklearn/utils/estimator_checks.py,2092,check that we warned if we don't support multi-output,not
scikit-learn/sklearn/utils/estimator_checks.py,2130,training set performance,not
scikit-learn/sklearn/utils/estimator_checks.py,2132,This is a pathological data set for ComplementNB.,not
scikit-learn/sklearn/utils/estimator_checks.py,2133,For some specific cases 'ComplementNB' predicts less classes,not
scikit-learn/sklearn/utils/estimator_checks.py,2134,than expected,not
scikit-learn/sklearn/utils/estimator_checks.py,2143,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/estimator_checks.py,2160,"We need to make sure that we have non negative data, for things",not
scikit-learn/sklearn/utils/estimator_checks.py,2161,like NMF,not
scikit-learn/sklearn/utils/estimator_checks.py,2199,separate estimators to control random seeds,not
scikit-learn/sklearn/utils/estimator_checks.py,2211,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,2225,X is already scaled,not
scikit-learn/sklearn/utils/estimator_checks.py,2240,"linear regressors need to set alpha, but not generalized CV ones",not
scikit-learn/sklearn/utils/estimator_checks.py,2245,raises error on malformed input for fit,not
scikit-learn/sklearn/utils/estimator_checks.py,2252,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,2259,TODO: find out why PLS and CCA fail. RANSAC is random,SATD
scikit-learn/sklearn/utils/estimator_checks.py,2260,"and furthermore assumes the presence of outliers, hence",not
scikit-learn/sklearn/utils/estimator_checks.py,2261,skipped,not
scikit-learn/sklearn/utils/estimator_checks.py,2268,checks whether regressors have decision_function or predict_proba,not
scikit-learn/sklearn/utils/estimator_checks.py,2277,"FIXME CCA, PLS is not robust to rank 1 effects",SATD
scikit-learn/sklearn/utils/estimator_checks.py,2285,doesn't have function,not
scikit-learn/sklearn/utils/estimator_checks.py,2287,has function. Should raise deprecation warning,not
scikit-learn/sklearn/utils/estimator_checks.py,2301,create a very noisy dataset,not
scikit-learn/sklearn/utils/estimator_checks.py,2306,"can't use gram_if_pairwise() here, setting up gram matrix manually",not
scikit-learn/sklearn/utils/estimator_checks.py,2332,"XXX: Generally can use 0.89 here. On Windows, LinearSVC gets",SATD
scikit-learn/sklearn/utils/estimator_checks.py,2333,0.88 (Issue #9111),not
scikit-learn/sklearn/utils/estimator_checks.py,2360,"this is run on classes, not instances, though this should be changed",not
scikit-learn/sklearn/utils/estimator_checks.py,2368,"This is a very small dataset, default n_iter are likely to prevent",not
scikit-learn/sklearn/utils/estimator_checks.py,2369,convergence,not
scikit-learn/sklearn/utils/estimator_checks.py,2377,Let the model compute the class frequencies,not
scikit-learn/sklearn/utils/estimator_checks.py,2381,Count each label occurrence to reweight manually,not
scikit-learn/sklearn/utils/estimator_checks.py,2403,some want non-negative input,not
scikit-learn/sklearn/utils/estimator_checks.py,2411,Make a physical copy of the original estimator parameters before fitting.,not
scikit-learn/sklearn/utils/estimator_checks.py,2415,Fit the model,not
scikit-learn/sklearn/utils/estimator_checks.py,2418,Compare the state of the model parameters with the original parameters,not
scikit-learn/sklearn/utils/estimator_checks.py,2423,We should never change or mutate the internal state of input,not
scikit-learn/sklearn/utils/estimator_checks.py,2424,parameters by default. To check this we use the joblib.hash function,not
scikit-learn/sklearn/utils/estimator_checks.py,2425,that introspects recursively any subobjects to compute a checksum.,not
scikit-learn/sklearn/utils/estimator_checks.py,2426,The only exception to this rule of immutable constructor parameters,not
scikit-learn/sklearn/utils/estimator_checks.py,2427,is possible RandomState instance but in this check we explicitly,not
scikit-learn/sklearn/utils/estimator_checks.py,2428,fixed the random_state params recursively to be integer seeds.,not
scikit-learn/sklearn/utils/estimator_checks.py,2444,__init__ signature has additional objects in PyPy,not
scikit-learn/sklearn/utils/estimator_checks.py,2453,Test for no setting apart from parameters during init,not
scikit-learn/sklearn/utils/estimator_checks.py,2460,Ensure that each parameter is set in init,not
scikit-learn/sklearn/utils/estimator_checks.py,2479,test sparsify with dense inputs,not
scikit-learn/sklearn/utils/estimator_checks.py,2485,pickle and unpickle with sparse coef_,not
scikit-learn/sklearn/utils/estimator_checks.py,2520,separate estimators to control random seeds,not
scikit-learn/sklearn/utils/estimator_checks.py,2533,Here pandas objects (Series and DataFrame) are tested explicitly,not
scikit-learn/sklearn/utils/estimator_checks.py,2534,because some estimators may handle them (especially their indexing),not
scikit-learn/sklearn/utils/estimator_checks.py,2535,specially.,not
scikit-learn/sklearn/utils/estimator_checks.py,2549,fit,not
scikit-learn/sklearn/utils/estimator_checks.py,2558,test default-constructibility,not
scikit-learn/sklearn/utils/estimator_checks.py,2559,get rid of deprecation warnings,not
scikit-learn/sklearn/utils/estimator_checks.py,2565,test cloning,not
scikit-learn/sklearn/utils/estimator_checks.py,2567,test __repr__,not
scikit-learn/sklearn/utils/estimator_checks.py,2569,test that set_params returns self,not
scikit-learn/sklearn/utils/estimator_checks.py,2572,test if init does nothing but set parameters,not
scikit-learn/sklearn/utils/estimator_checks.py,2573,this is important for grid_search etc.,not
scikit-learn/sklearn/utils/estimator_checks.py,2574,We get the default parameters from init and then,not
scikit-learn/sklearn/utils/estimator_checks.py,2575,compare these against the actual values of the attributes.,not
scikit-learn/sklearn/utils/estimator_checks.py,2577,this comes from getattr. Gets rid of deprecation decorator.,not
scikit-learn/sklearn/utils/estimator_checks.py,2592,init is not a python function.,not
scikit-learn/sklearn/utils/estimator_checks.py,2593,true for mixins,not
scikit-learn/sklearn/utils/estimator_checks.py,2596,they can need a non-default argument,not
scikit-learn/sklearn/utils/estimator_checks.py,2611,"deprecated parameter, not in get_params",not
scikit-learn/sklearn/utils/estimator_checks.py,2620,Allows to set default parameters to np.nan,not
scikit-learn/sklearn/utils/estimator_checks.py,2626,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/estimator_checks.py,2634,Estimators with a `requires_positive_y` tag only accept strictly positive,not
scikit-learn/sklearn/utils/estimator_checks.py,2635,data,not
scikit-learn/sklearn/utils/estimator_checks.py,2637,"Create strictly positive y. The minimal increment above 0 is 1, as",not
scikit-learn/sklearn/utils/estimator_checks.py,2638,y could be of integer dtype.,not
scikit-learn/sklearn/utils/estimator_checks.py,2640,Estimators in mono_output_task_error raise ValueError if y is of 1-D,not
scikit-learn/sklearn/utils/estimator_checks.py,2641,Convert into a 2-D y for those estimators.,not
scikit-learn/sklearn/utils/estimator_checks.py,2648,Estimators with a `_pairwise` tag only accept,not
scikit-learn/sklearn/utils/estimator_checks.py,2649,"X of shape (`n_samples`, `n_samples`)",not
scikit-learn/sklearn/utils/estimator_checks.py,2652,Estimators with `1darray` in `X_types` tag only accept,not
scikit-learn/sklearn/utils/estimator_checks.py,2653,"X of shape (`n_samples`,)",not
scikit-learn/sklearn/utils/estimator_checks.py,2656,Estimators with a `requires_positive_X` tag only accept,not
scikit-learn/sklearn/utils/estimator_checks.py,2657,strictly positive data,not
scikit-learn/sklearn/utils/estimator_checks.py,2665,Test that estimators that are not transformers with a parameter,not
scikit-learn/sklearn/utils/estimator_checks.py,2666,"max_iter, return the attribute of n_iter_ at least 1.",not
scikit-learn/sklearn/utils/estimator_checks.py,2668,These models are dependent on external solvers like,not
scikit-learn/sklearn/utils/estimator_checks.py,2669,libsvm and accessing the iter parameter is non-trivial.,not
scikit-learn/sklearn/utils/estimator_checks.py,2675,Tested in test_transformer_n_iter,not
scikit-learn/sklearn/utils/estimator_checks.py,2680,LassoLars stops early for the default alpha=1.0 the iris dataset.,not
scikit-learn/sklearn/utils/estimator_checks.py,2699,"Test that transformers with a parameter max_iter, return the",not
scikit-learn/sklearn/utils/estimator_checks.py,2700,attribute of n_iter_ at least 1.,not
scikit-learn/sklearn/utils/estimator_checks.py,2704,Check using default data,not
scikit-learn/sklearn/utils/estimator_checks.py,2715,These return a n_iter per component.,not
scikit-learn/sklearn/utils/estimator_checks.py,2725,Checks if get_params(deep=False) is a subset of get_params(deep=True),not
scikit-learn/sklearn/utils/estimator_checks.py,2737,Check that get_params() returns the same thing,not
scikit-learn/sklearn/utils/estimator_checks.py,2738,before and after set_params() with some fuzz,not
scikit-learn/sklearn/utils/estimator_checks.py,2750,some fuzz values,not
scikit-learn/sklearn/utils/estimator_checks.py,2762,"Exception occurred, possibly parameter validation",not
scikit-learn/sklearn/utils/estimator_checks.py,2791,Check if classifier throws an exception when fed regression targets,not
scikit-learn/sklearn/utils/estimator_checks.py,2802,Check whether an estimator having both decision_function and,not
scikit-learn/sklearn/utils/estimator_checks.py,2803,predict_proba methods has outputs with perfect rank correlation.,not
scikit-learn/sklearn/utils/estimator_checks.py,2815,Since the link function from decision_function() to predict_proba(),not
scikit-learn/sklearn/utils/estimator_checks.py,2816,"is sometimes not precise enough (typically expit), we round to the",not
scikit-learn/sklearn/utils/estimator_checks.py,2817,10th decimal to avoid numerical issues.,not
scikit-learn/sklearn/utils/estimator_checks.py,2824,Check fit_predict for outlier detectors.,not
scikit-learn/sklearn/utils/estimator_checks.py,2839,check fit_predict = fit.predict when the estimator has both a predict and,not
scikit-learn/sklearn/utils/estimator_checks.py,2840,a fit_predict method. recall that it is already assumed here that the,not
scikit-learn/sklearn/utils/estimator_checks.py,2841,estimator has a fit_predict method,not
scikit-learn/sklearn/utils/estimator_checks.py,2847,proportion of outliers equal to contamination parameter when not,not
scikit-learn/sklearn/utils/estimator_checks.py,2848,set to 'auto',not
scikit-learn/sklearn/utils/estimator_checks.py,2855,num_outliers should be equal to expected_outliers unless,not
scikit-learn/sklearn/utils/estimator_checks.py,2856,there are ties in the decision_function values. this can,not
scikit-learn/sklearn/utils/estimator_checks.py,2857,only be tested for estimators with a decision_function,not
scikit-learn/sklearn/utils/estimator_checks.py,2858,method,not
scikit-learn/sklearn/utils/estimator_checks.py,2864,"raises error when contamination is a scalar and not in [0,1]",not
scikit-learn/sklearn/utils/estimator_checks.py,2871,Check that proper warning is raised for non-negative X,not
scikit-learn/sklearn/utils/estimator_checks.py,2872,when tag requires_positive_X is present,not
scikit-learn/sklearn/utils/estimator_checks.py,2881,Check that est.fit(X) is the same as est.fit(X).fit(X). Ideally we would,not
scikit-learn/sklearn/utils/estimator_checks.py,2882,check that the estimated parameters during training (e.g. coefs_) are,not
scikit-learn/sklearn/utils/estimator_checks.py,2883,"the same, but having a universal comparison function for those",not
scikit-learn/sklearn/utils/estimator_checks.py,2884,attributes is difficult and full of edge cases. So instead we check that,not
scikit-learn/sklearn/utils/estimator_checks.py,2885,"predict(), predict_proba(), decision_function() and transform() return",not
scikit-learn/sklearn/utils/estimator_checks.py,2886,the same results.,not
scikit-learn/sklearn/utils/estimator_checks.py,2910,Fit for the first time,not
scikit-learn/sklearn/utils/estimator_checks.py,2917,Fit again,not
scikit-learn/sklearn/utils/estimator_checks.py,2936,Make sure that n_features_in_ attribute doesn't exist until fit is,not
scikit-learn/sklearn/utils/estimator_checks.py,2937,"called, and that its value is correct.",not
scikit-learn/sklearn/utils/estimator_checks.py,2968,noqa,not
scikit-learn/sklearn/utils/estimator_checks.py,2974,Make sure that an estimator with requires_y=True fails gracefully when,not
scikit-learn/sklearn/utils/estimator_checks.py,2975,given y=None,not
scikit-learn/sklearn/utils/_show_versions.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,76,outer_class inner_class,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,92,check if estimator looks like a meta estimator wraps estimators,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,96,Only look at the estimators in the first layer,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,133,parallel,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,135,wrap element in a serial visualblock,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,139,sk-parallel-item,not
scikit-learn/sklearn/utils/_estimator_html_repr.py,285,noqa,not
scikit-learn/sklearn/utils/graph.py,8,Authors: Aric Hagberg <hagberg@lanl.gov>,not
scikit-learn/sklearn/utils/graph.py,9,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/utils/graph.py,10,Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/utils/graph.py,11,License: BSD 3 clause,not
scikit-learn/sklearn/utils/graph.py,15,noqa,not
scikit-learn/sklearn/utils/graph.py,18,,not
scikit-learn/sklearn/utils/graph.py,19,Path and connected component analysis.,not
scikit-learn/sklearn/utils/graph.py,20,Code adapted from networkx,not
scikit-learn/sklearn/utils/graph.py,55,level (number of hops) when seen in BFS,not
scikit-learn/sklearn/utils/graph.py,56,the current level,not
scikit-learn/sklearn/utils/graph.py,57,dict of nodes to check at next level,not
scikit-learn/sklearn/utils/graph.py,59,advance to next level,not
scikit-learn/sklearn/utils/graph.py,60,and start a new list (fringe),not
scikit-learn/sklearn/utils/graph.py,63,set the level of vertex v,not
scikit-learn/sklearn/utils/graph.py,68,return all path lengths as dictionary,not
scikit-learn/sklearn/utils/deprecation.py,31,"Adapted from https://wiki.python.org/moin/PythonDecoratorLibrary,",not
scikit-learn/sklearn/utils/deprecation.py,32,but with many changes.,not
scikit-learn/sklearn/utils/deprecation.py,47,Note that this is only triggered properly if the `property`,not
scikit-learn/sklearn/utils/deprecation.py,48,"decorator comes before the `deprecated` decorator, like so:",not
scikit-learn/sklearn/utils/deprecation.py,49,,not
scikit-learn/sklearn/utils/deprecation.py,50,@deprecated(msg),not
scikit-learn/sklearn/utils/deprecation.py,51,@property,not
scikit-learn/sklearn/utils/deprecation.py,52,def deprecated_attribute_(self):,not
scikit-learn/sklearn/utils/deprecation.py,53,...,not
scikit-learn/sklearn/utils/deprecation.py,63,FIXME: we should probably reset __new__ for full generality,SATD
scikit-learn/sklearn/utils/deprecation.py,90,Add a reference to the wrapped function so that we can introspect,not
scikit-learn/sklearn/utils/deprecation.py,91,on function arguments in Python 2 (already works in Python 3),not
scikit-learn/sklearn/utils/deprecation.py,128,Raise a deprecation warning with standardized deprecation message.,not
scikit-learn/sklearn/utils/deprecation.py,129,Useful because we are now deprecating # anything that isn't explicitly,not
scikit-learn/sklearn/utils/deprecation.py,130,in an __init__ file.,not
scikit-learn/sklearn/utils/deprecation.py,132,TODO: remove in 0.24 since this shouldn't be needed anymore.,SATD
scikit-learn/sklearn/utils/metaestimators.py,2,Author: Joel Nothman,not
scikit-learn/sklearn/utils/metaestimators.py,3,Andreas Mueller,not
scikit-learn/sklearn/utils/metaestimators.py,4,License: BSD,not
scikit-learn/sklearn/utils/metaestimators.py,40,Ensure strict ordering of parameter setting:,not
scikit-learn/sklearn/utils/metaestimators.py,41,1. All steps,not
scikit-learn/sklearn/utils/metaestimators.py,44,2. Step replacement,not
scikit-learn/sklearn/utils/metaestimators.py,52,3. Step parameters and other initialisation arguments,not
scikit-learn/sklearn/utils/metaestimators.py,57,assumes `name` is a valid estimator name,not
scikit-learn/sklearn/utils/metaestimators.py,99,update the docstring of the descriptor,not
scikit-learn/sklearn/utils/metaestimators.py,103,raise an AttributeError if the attribute is not present on the object,not
scikit-learn/sklearn/utils/metaestimators.py,105,"delegate only on instances, not the classes.",not
scikit-learn/sklearn/utils/metaestimators.py,106,this is to allow access to the docstrings.,not
scikit-learn/sklearn/utils/metaestimators.py,118,"lambda, but not partial, allows help() to work with update_wrapper",not
scikit-learn/sklearn/utils/metaestimators.py,120,update the docstring of the returned function,not
scikit-learn/sklearn/utils/metaestimators.py,196,X is a precomputed square kernel matrix,not
scikit-learn/sklearn/utils/sparsefuncs.py,1,Authors: Manoj Kumar,not
scikit-learn/sklearn/utils/sparsefuncs.py,2,Thomas Unterthiner,not
scikit-learn/sklearn/utils/sparsefuncs.py,3,Giorgio Patrini,not
scikit-learn/sklearn/utils/sparsefuncs.py,4,,not
scikit-learn/sklearn/utils/sparsefuncs.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/utils/sparsefuncs.py,261,The following swapping makes life easier since m is assumed to be the,not
scikit-learn/sklearn/utils/sparsefuncs.py,262,smaller integer below.,not
scikit-learn/sklearn/utils/sparsefuncs.py,275,Modify indptr first,not
scikit-learn/sklearn/utils/sparsefuncs.py,345,"reduceat tries casts X.indptr to intp, which errors",not
scikit-learn/sklearn/utils/sparsefuncs.py,346,if it is int64 on a 32 bit system.,not
scikit-learn/sklearn/utils/sparsefuncs.py,347,"Reinitializing prevents this where possible, see #13737",not
scikit-learn/sklearn/utils/sparsefuncs.py,463,We rely here on the fact that np.diff(Y.indptr) for a CSR,not
scikit-learn/sklearn/utils/sparsefuncs.py,464,will return the number of nonzero entries in each row.,not
scikit-learn/sklearn/utils/sparsefuncs.py,465,A bincount over Y.indices will return the number of nonzeros,not
scikit-learn/sklearn/utils/sparsefuncs.py,466,in each column. See ``csr_matrix.getnnz`` in scipy >= 0.14.,not
scikit-learn/sklearn/utils/sparsefuncs.py,475,astype here is for consistency with axis=0 dtype,not
scikit-learn/sklearn/utils/sparsefuncs.py,541,Prevent modifying X in place,not
scikit-learn/sklearn/utils/extmath.py,4,Authors: Gael Varoquaux,not
scikit-learn/sklearn/utils/extmath.py,5,Alexandre Gramfort,not
scikit-learn/sklearn/utils/extmath.py,6,Alexandre T. Passos,not
scikit-learn/sklearn/utils/extmath.py,7,Olivier Grisel,not
scikit-learn/sklearn/utils/extmath.py,8,Lars Buitinck,not
scikit-learn/sklearn/utils/extmath.py,9,Stefan van der Walt,not
scikit-learn/sklearn/utils/extmath.py,10,Kyle Kastner,not
scikit-learn/sklearn/utils/extmath.py,11,Giorgio Patrini,not
scikit-learn/sklearn/utils/extmath.py,12,License: BSD 3 clause,not
scikit-learn/sklearn/utils/extmath.py,136,sparse is always 2D. Implies b is 3D+,not
scikit-learn/sklearn/utils/extmath.py,137,"[i, j] @ [k, ..., l, m, n] -> [i, k, ..., l, n]",not
scikit-learn/sklearn/utils/extmath.py,143,sparse is always 2D. Implies a is 3D+,not
scikit-learn/sklearn/utils/extmath.py,144,"[k, ..., l, m] @ [i, j] -> [k, ..., l, j]",not
scikit-learn/sklearn/utils/extmath.py,211,"Generating normal random vectors with shape: (A.shape[1], size)",not
scikit-learn/sklearn/utils/extmath.py,214,Ensure f32 is preserved as f32,not
scikit-learn/sklearn/utils/extmath.py,217,"Deal with ""auto"" mode",not
scikit-learn/sklearn/utils/extmath.py,224,Perform power iterations with Q to further 'imprint' the top,not
scikit-learn/sklearn/utils/extmath.py,225,singular vectors of A in Q,not
scikit-learn/sklearn/utils/extmath.py,237,Sample the range of A using by linear projection of Q,not
scikit-learn/sklearn/utils/extmath.py,238,Extract an orthonormal basis,not
scikit-learn/sklearn/utils/extmath.py,335,Checks if the number of iterations is explicitly specified,not
scikit-learn/sklearn/utils/extmath.py,336,Adjust n_iter. 7 was found a good compromise for PCA. See #5299,not
scikit-learn/sklearn/utils/extmath.py,342,this implementation is a bit faster with smaller shape[1],not
scikit-learn/sklearn/utils/extmath.py,348,project M to the (k + p) dimensional space using the basis vectors,not
scikit-learn/sklearn/utils/extmath.py,351,compute the SVD on the thin matrix: (k + p) wide,not
scikit-learn/sklearn/utils/extmath.py,361,In case of transpose u_based_decision=false,not
scikit-learn/sklearn/utils/extmath.py,362,to actually flip based on u and not v.,not
scikit-learn/sklearn/utils/extmath.py,366,transpose back the results according to the input convention,not
scikit-learn/sklearn/utils/extmath.py,429,get ALL unique values,not
scikit-learn/sklearn/utils/extmath.py,526,"columns of u, rows of v",not
scikit-learn/sklearn/utils/extmath.py,532,"rows of v, columns of u",not
scikit-learn/sklearn/utils/extmath.py,678,Use at least float64 for the accumulating functions to avoid precision issue,not
scikit-learn/sklearn/utils/extmath.py,679,see https://github.com/numpy/numpy/issues/9393. The float64 is also retained,not
scikit-learn/sklearn/utils/extmath.py,680,as it is in case the float overflows,not
scikit-learn/sklearn/utils/extmath.py,756,old = stats until now,not
scikit-learn/sklearn/utils/extmath.py,757,new = the current increment,not
scikit-learn/sklearn/utils/extmath.py,758,updated = the aggregated stats,not
scikit-learn/sklearn/utils/multiclass.py,1,"Author: Arnaud Joly, Joel Nothman, Hamzeh Alsalhi",not
scikit-learn/sklearn/utils/multiclass.py,2,,not
scikit-learn/sklearn/utils/multiclass.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/utils/multiclass.py,75,Check that we don't mix label format,not
scikit-learn/sklearn/utils/multiclass.py,86,Check consistency for the indicator format,not
scikit-learn/sklearn/utils/multiclass.py,94,Get the unique set of labels,not
scikit-learn/sklearn/utils/multiclass.py,101,Check that we don't mix string type with number type,not
scikit-learn/sklearn/utils/multiclass.py,149,"bool, int, uint",not
scikit-learn/sklearn/utils/multiclass.py,154,"bool, int, uint",not
scikit-learn/sklearn/utils/multiclass.py,256,Known to fail in numpy 1.3 for array of arrays,not
scikit-learn/sklearn/utils/multiclass.py,259,The old sequence of sequences format,not
scikit-learn/sklearn/utils/multiclass.py,271,Invalid inputs,not
scikit-learn/sklearn/utils/multiclass.py,274,"[[[1, 2]]] or [obj_1] and not [""label_1""]",not
scikit-learn/sklearn/utils/multiclass.py,277,[[]],not
scikit-learn/sklearn/utils/multiclass.py,280,"[[1, 2], [1, 2]]",not
scikit-learn/sklearn/utils/multiclass.py,282,"[1, 2, 3] or [[1], [2], [3]]",not
scikit-learn/sklearn/utils/multiclass.py,284,check float and contains non-integer float values,not
scikit-learn/sklearn/utils/multiclass.py,286,"[.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]",not
scikit-learn/sklearn/utils/multiclass.py,291,"[1, 2, 3] or [[1., 2., 3]] or [[1, 2]]",not
scikit-learn/sklearn/utils/multiclass.py,293,"[1, 2] or [[""a""], [""b""]]",not
scikit-learn/sklearn/utils/multiclass.py,322,This is the first call to partial_fit,not
scikit-learn/sklearn/utils/multiclass.py,326,classes is None and clf.classes_ has already previously been set:,not
scikit-learn/sklearn/utils/multiclass.py,327,nothing to do,not
scikit-learn/sklearn/utils/multiclass.py,368,separate sample weights for zero and non-zero elements,not
scikit-learn/sklearn/utils/multiclass.py,381,"An explicit zero was found, combine its weight with the weight",not
scikit-learn/sklearn/utils/multiclass.py,382,of the implicit zeros,not
scikit-learn/sklearn/utils/multiclass.py,386,If an there is an implicit zero and it is not in classes and,not
scikit-learn/sklearn/utils/multiclass.py,387,"class_prior, make an entry for it",not
scikit-learn/sklearn/utils/multiclass.py,439,"Monotonically transform the sum_of_confidences to (-1/3, 1/3)",not
scikit-learn/sklearn/utils/multiclass.py,440,and add it with votes. The monotonic transformation  is,not
scikit-learn/sklearn/utils/multiclass.py,441,"f: x -> x / (3 * (|x| + 1)), it uses 1/3 instead of 1/2",not
scikit-learn/sklearn/utils/multiclass.py,442,to ensure that we won't reach the limits and change vote order.,not
scikit-learn/sklearn/utils/multiclass.py,443,The motivation is to use confidence levels as a way to break ties in,not
scikit-learn/sklearn/utils/multiclass.py,444,the votes without switching any decision made based on a difference,not
scikit-learn/sklearn/utils/multiclass.py,445,of 1 vote.,not
scikit-learn/sklearn/utils/random.py,1,Author: Hamzeh Alsalhi <ha258@cornell.edu>,not
scikit-learn/sklearn/utils/random.py,2,,not
scikit-learn/sklearn/utils/random.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/utils/random.py,59,use uniform distribution if no class_probability is given,not
scikit-learn/sklearn/utils/random.py,77,If 0 is not present in the classes insert it with a probability 0.0,not
scikit-learn/sklearn/utils/random.py,82,If there are nonzero classes choose randomly using class_probability,not
scikit-learn/sklearn/utils/random.py,92,Normalize probabilities for the nonzero elements,not
scikit-learn/sklearn/utils/_testing.py,3,"Copyright (c) 2011, 2012",not
scikit-learn/sklearn/utils/_testing.py,4,"Authors: Pietro Berkes,",not
scikit-learn/sklearn/utils/_testing.py,5,Andreas Muller,not
scikit-learn/sklearn/utils/_testing.py,6,Mathieu Blondel,not
scikit-learn/sklearn/utils/_testing.py,7,Olivier Grisel,not
scikit-learn/sklearn/utils/_testing.py,8,Arnaud Joly,not
scikit-learn/sklearn/utils/_testing.py,9,Denis Engemann,not
scikit-learn/sklearn/utils/_testing.py,10,Giorgio Patrini,not
scikit-learn/sklearn/utils/_testing.py,11,Thierry Guillemot,not
scikit-learn/sklearn/utils/_testing.py,12,License: BSD 3 clause,not
scikit-learn/sklearn/utils/_testing.py,34,WindowsError only exist on Windows,not
scikit-learn/sklearn/utils/_testing.py,83,assert_raises_regexp is deprecated in Python 3.4 in favor of,not
scikit-learn/sklearn/utils/_testing.py,84,assert_raises_regex but lets keep the backward compat in scikit-learn with,not
scikit-learn/sklearn/utils/_testing.py,85,the old name for now,not
scikit-learn/sklearn/utils/_testing.py,111,Cause all warnings to always be triggered.,not
scikit-learn/sklearn/utils/_testing.py,113,Trigger a warning.,not
scikit-learn/sklearn/utils/_testing.py,116,Filter out numpy-specific warnings in numpy >= 1.9,not
scikit-learn/sklearn/utils/_testing.py,120,Verify some things,not
scikit-learn/sklearn/utils/_testing.py,133,very important to avoid uncontrolled state propagation,not
scikit-learn/sklearn/utils/_testing.py,159,Cause all warnings to always be triggered.,not
scikit-learn/sklearn/utils/_testing.py,162,Let's not catch the numpy internal DeprecationWarnings,not
scikit-learn/sklearn/utils/_testing.py,164,Trigger a warning.,not
scikit-learn/sklearn/utils/_testing.py,166,Verify some things,not
scikit-learn/sklearn/utils/_testing.py,178,Checks the message of all warnings belong to warning_class,not
scikit-learn/sklearn/utils/_testing.py,180,"substring will match, the entire message with typo won't",not
scikit-learn/sklearn/utils/_testing.py,181,For Python 3 compatibility,not
scikit-learn/sklearn/utils/_testing.py,183,add support for certain tests,not
scikit-learn/sklearn/utils/_testing.py,216,This platform does not report numpy divide by zeros,not
scikit-learn/sklearn/utils/_testing.py,223,To remove when we support numpy 1.7,not
scikit-learn/sklearn/utils/_testing.py,232,very important to avoid uncontrolled state propagation,not
scikit-learn/sklearn/utils/_testing.py,238,Filter out numpy-specific warnings in numpy >= 1.9,not
scikit-learn/sklearn/utils/_testing.py,276,Avoid common pitfall of passing category as the first positional,not
scikit-learn/sklearn/utils/_testing.py,277,argument which result in the test not being run,not
scikit-learn/sklearn/utils/_testing.py,379,concatenate exception names,not
scikit-learn/sklearn/utils/_testing.py,423,both dense,not
scikit-learn/sklearn/utils/_testing.py,430,TODO: Remove in 0.24. This class is now in utils.__init__.,SATD
scikit-learn/sklearn/utils/_testing.py,463,get parent folder,not
scikit-learn/sklearn/utils/_testing.py,472,Ignore deprecation warnings triggered at import time.,not
scikit-learn/sklearn/utils/_testing.py,483,get rid of abstract base classes,not
scikit-learn/sklearn/utils/_testing.py,490,copy,not
scikit-learn/sklearn/utils/_testing.py,508,"drop duplicates, sort for reproducibility",not
scikit-learn/sklearn/utils/_testing.py,509,itemgetter is used to ensure the sort does not extend to the 2nd item of,not
scikit-learn/sklearn/utils/_testing.py,510,the tuple,not
scikit-learn/sklearn/utils/_testing.py,542,Decorator for tests involving both BLAS calls and multiprocessing.,not
scikit-learn/sklearn/utils/_testing.py,543,,not
scikit-learn/sklearn/utils/_testing.py,544,"Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction",not
scikit-learn/sklearn/utils/_testing.py,545,with some implementation of BLAS (or other libraries that manage an,not
scikit-learn/sklearn/utils/_testing.py,546,internal posix thread pool) can cause a crash or a freeze of the Python,not
scikit-learn/sklearn/utils/_testing.py,547,process.,not
scikit-learn/sklearn/utils/_testing.py,548,,not
scikit-learn/sklearn/utils/_testing.py,549,In practice all known packaged distributions (from Linux distros or,not
scikit-learn/sklearn/utils/_testing.py,550,Anaconda) of BLAS under Linux seems to be safe. So we this problem seems,not
scikit-learn/sklearn/utils/_testing.py,551,to only impact OSX users.,not
scikit-learn/sklearn/utils/_testing.py,552,,not
scikit-learn/sklearn/utils/_testing.py,553,This wrapper makes it possible to skip tests that can possibly cause,not
scikit-learn/sklearn/utils/_testing.py,554,this crash under OS X with.,not
scikit-learn/sklearn/utils/_testing.py,555,,not
scikit-learn/sklearn/utils/_testing.py,556,Under Python 3.4+ it is possible to use the `forkserver` start method,not
scikit-learn/sklearn/utils/_testing.py,557,for multiprocessing to avoid this issue. However it can cause pickling,not
scikit-learn/sklearn/utils/_testing.py,558,errors on interactively defined functions. It therefore not enabled by,not
scikit-learn/sklearn/utils/_testing.py,559,default.,not
scikit-learn/sklearn/utils/_testing.py,580,"This can fail under windows,",not
scikit-learn/sklearn/utils/_testing.py,581,but will succeed when called by atexit,not
scikit-learn/sklearn/utils/_testing.py,626,Utils to test docstrings,not
scikit-learn/sklearn/utils/_testing.py,635,Error on builtin C function,not
scikit-learn/sklearn/utils/_testing.py,700,Don't check docstring for property-functions,not
scikit-learn/sklearn/utils/_testing.py,703,Don't check docstring for setup / teardown pytest functions,not
scikit-learn/sklearn/utils/_testing.py,706,Dont check estimator_checks module,not
scikit-learn/sklearn/utils/_testing.py,709,Get the arguments from the function signature,not
scikit-learn/sklearn/utils/_testing.py,711,drop self,not
scikit-learn/sklearn/utils/_testing.py,715,Analyze function's docstring,not
scikit-learn/sklearn/utils/_testing.py,728,Type hints are empty only if parameter name ended with :,not
scikit-learn/sklearn/utils/_testing.py,739,Create a list of parameters to compare with the parameters gotten,not
scikit-learn/sklearn/utils/_testing.py,740,from the func signature,not
scikit-learn/sklearn/utils/_testing.py,744,If one of the docstring's parameters had an error then return that,not
scikit-learn/sklearn/utils/_testing.py,745,incorrect message,not
scikit-learn/sklearn/utils/_testing.py,749,Remove the parameters that should be ignored from list,not
scikit-learn/sklearn/utils/_testing.py,752,"The following is derived from pytest, Copyright (c) 2004-2017 Holger",not
scikit-learn/sklearn/utils/_testing.py,753,"Krekel and others, Licensed under MIT License. See",not
scikit-learn/sklearn/utils/_testing.py,754,https://github.com/pytest-dev/pytest,not
scikit-learn/sklearn/utils/_testing.py,774,If there wasn't any difference in the parameters themselves between,not
scikit-learn/sklearn/utils/_testing.py,775,docstring and signature including having the same length then return,not
scikit-learn/sklearn/utils/_testing.py,776,empty list,not
scikit-learn/sklearn/utils/_testing.py,795,Prepend function name,not
scikit-learn/sklearn/utils/_testing.py,833,"If coverage is running, pass the config file to the subprocess",not
scikit-learn/sklearn/utils/optimize.py,11,This is a modified file from scipy.optimize,not
scikit-learn/sklearn/utils/optimize.py,12,"Original authors: Travis Oliphant, Eric Jones",not
scikit-learn/sklearn/utils/optimize.py,13,"Modifications by Gael Varoquaux, Mathieu Blondel and Tom Dupre la Tour",not
scikit-learn/sklearn/utils/optimize.py,14,License: BSD,not
scikit-learn/sklearn/utils/optimize.py,46,line search failed: try different one.,not
scikit-learn/sklearn/utils/optimize.py,92,check curvature,not
scikit-learn/sklearn/utils/optimize.py,100,fall back to steepest descent direction,not
scikit-learn/sklearn/utils/optimize.py,110,"update np.dot(ri,ri) for next time.",not
scikit-learn/sklearn/utils/optimize.py,178,Outer loop: our Newton iteration,not
scikit-learn/sklearn/utils/optimize.py,180,Compute a search direction pk by applying the CG method to,not
scikit-learn/sklearn/utils/optimize.py,181,del2 f(xk) p = - fgrad f(xk) starting from 0.,not
scikit-learn/sklearn/utils/optimize.py,192,"Inner loop: solve the Newton update by conjugate gradient, to",not
scikit-learn/sklearn/utils/optimize.py,193,avoid inverting the Hessian,not
scikit-learn/sklearn/utils/optimize.py,207,upcast if necessary,not
scikit-learn/sklearn/utils/optimize.py,234,handle both scipy and scikit-learn solver names,not
scikit-learn/sklearn/utils/optimize.py,248,"In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.",not
scikit-learn/sklearn/utils/optimize.py,249,See https://github.com/scipy/scipy/issues/7854,not
scikit-learn/sklearn/utils/__init__.py,38,Do not deprecate parallel_backend and register_parallel_backend as they are,not
scikit-learn/sklearn/utils/__init__.py,39,needed to tune `scikit-learn` behavior and have different effect if called,not
scikit-learn/sklearn/utils/__init__.py,40,from the vendored version or or the site-package version. The other are,not
scikit-learn/sklearn/utils/__init__.py,41,utilities that are independent of scikit-learn so they are not part of,not
scikit-learn/sklearn/utils/__init__.py,42,scikit-learn public API.,not
scikit-learn/sklearn/utils/__init__.py,101,Bunch pickles generated with scikit-learn 0.16.* have an non,not
scikit-learn/sklearn/utils/__init__.py,102,empty __dict__. This causes a surprising behaviour when,not
scikit-learn/sklearn/utils/__init__.py,103,loading these pickles scikit-learn 0.17: reading bunch.key,not
scikit-learn/sklearn/utils/__init__.py,104,uses __dict__ but assigning to bunch.key use __setattr__ and,not
scikit-learn/sklearn/utils/__init__.py,105,only changes bunch['key']. More details can be found at:,not
scikit-learn/sklearn/utils/__init__.py,106,https://github.com/scikit-learn/scikit-learn/issues/6196.,not
scikit-learn/sklearn/utils/__init__.py,107,Overriding __setstate__ to be a noop has the effect of,not
scikit-learn/sklearn/utils/__init__.py,108,ignoring the pickled __dict__,not
scikit-learn/sklearn/utils/__init__.py,175,FIXME: Remove the check for NumPy when using >= 1.12,SATD
scikit-learn/sklearn/utils/__init__.py,176,check if we have an boolean array-likes to make the proper indexing,not
scikit-learn/sklearn/utils/__init__.py,187,Work-around for indexing with read-only key in pandas,SATD
scikit-learn/sklearn/utils/__init__.py,188,FIXME: solved in pandas 0.25,SATD
scikit-learn/sklearn/utils/__init__.py,193,check whether we should index with loc or iloc,not
scikit-learn/sklearn/utils/__init__.py,201,key is a slice or a scalar,not
scikit-learn/sklearn/utils/__init__.py,204,key is a boolean array-like,not
scikit-learn/sklearn/utils/__init__.py,206,key is a integer array-like of key,not
scikit-learn/sklearn/utils/__init__.py,273,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/__init__.py,411,we get an empty list,not
scikit-learn/sklearn/utils/__init__.py,414,Convert key into positive indexes,not
scikit-learn/sklearn/utils/__init__.py,436,pandas indexing with strings is endpoint included,not
scikit-learn/sklearn/utils/__init__.py,580,Code adapted from StratifiedShuffleSplit(),not
scikit-learn/sklearn/utils/__init__.py,583,"for multi-label y, map each distinct row to a string repr",not
scikit-learn/sklearn/utils/__init__.py,584,using join because str(row) uses an ellipsis if len(row) > 1000,not
scikit-learn/sklearn/utils/__init__.py,592,Find the sorted list of instances for each class:,not
scikit-learn/sklearn/utils/__init__.py,593,"(np.unique above performs a sort, so code is O(n logn) already)",not
scikit-learn/sklearn/utils/__init__.py,609,convert sparse matrices to CSR for row-based indexing,not
scikit-learn/sklearn/utils/__init__.py,613,syntactic sugar for the unit argument case,not
scikit-learn/sklearn/utils/__init__.py,927,adapted from joblib.logger.short_format_time without the Windows -.1s,not
scikit-learn/sklearn/utils/__init__.py,928,adjustment,not
scikit-learn/sklearn/utils/__init__.py,1031,convert from numpy.bool_ to python bool to ensure that testing,not
scikit-learn/sklearn/utils/__init__.py,1032,is_scalar_nan(x) is True does not fail.,not
scikit-learn/sklearn/utils/__init__.py,1077,this computes a bad approximation to the mode of the,not
scikit-learn/sklearn/utils/__init__.py,1078,multivariate hypergeometric given by class_counts and n_draws,not
scikit-learn/sklearn/utils/__init__.py,1080,"floored means we don't overshoot n_samples, but probably undershoot",not
scikit-learn/sklearn/utils/__init__.py,1082,"we add samples according to how much ""left over"" probability",SATD
scikit-learn/sklearn/utils/__init__.py,1083,"they had, until we arrive at n_samples",not
scikit-learn/sklearn/utils/__init__.py,1088,"add according to remainder, but break ties",not
scikit-learn/sklearn/utils/__init__.py,1089,randomly to avoid biases,not
scikit-learn/sklearn/utils/__init__.py,1092,if we need_to_add less than what's in inds,not
scikit-learn/sklearn/utils/__init__.py,1093,we draw randomly from them.,not
scikit-learn/sklearn/utils/__init__.py,1094,"if we need to add more, we add them all and",not
scikit-learn/sklearn/utils/__init__.py,1095,go to the next value,not
scikit-learn/sklearn/utils/__init__.py,1117,noqa,not
scikit-learn/sklearn/utils/__init__.py,1138,noqa,not
scikit-learn/sklearn/utils/__init__.py,1169,lazy import to avoid circular imports from sklearn.base,not
scikit-learn/sklearn/utils/__init__.py,1183,sklearn package,not
scikit-learn/sklearn/utils/__init__.py,1184,Ignore deprecation warnings triggered at import time and from walking,not
scikit-learn/sklearn/utils/__init__.py,1185,packages,not
scikit-learn/sklearn/utils/__init__.py,1198,TODO: Remove when FeatureHasher is implemented in PYPY,SATD
scikit-learn/sklearn/utils/__init__.py,1199,Skips FeatureHasher for PYPY,not
scikit-learn/sklearn/utils/__init__.py,1211,get rid of abstract base classes,not
scikit-learn/sklearn/utils/__init__.py,1218,copy,not
scikit-learn/sklearn/utils/__init__.py,1236,"drop duplicates, sort for reproducibility",not
scikit-learn/sklearn/utils/__init__.py,1237,itemgetter is used to ensure the sort does not extend to the 2nd item of,not
scikit-learn/sklearn/utils/__init__.py,1238,the tuple,not
scikit-learn/sklearn/utils/setup.py,49,generate _seq_dataset from template,not
scikit-learn/sklearn/utils/_mocking.py,26,have shape and length but don't support indexing.,not
scikit-learn/sklearn/utils/_mocking.py,32,ugly hack to make iloc work.,SATD
scikit-learn/sklearn/utils/_mocking.py,39,Pandas data frames also are array-like: we want to make sure that,not
scikit-learn/sklearn/utils/_mocking.py,40,input validation in cross-validation does not try to call that,not
scikit-learn/sklearn/utils/_mocking.py,41,method.,not
scikit-learn/sklearn/utils/_mask.py,13,can't have NaNs in integer array.,not
scikit-learn/sklearn/utils/_mask.py,16,np.isnan does not work on object dtypes.,not
scikit-learn/sklearn/utils/stats.py,12,Find index of median prediction for each sample,not
scikit-learn/sklearn/utils/stats.py,16,"in rare cases, percentile_idx equals to len(sorted_idx)",not
scikit-learn/sklearn/utils/validation.py,3,Authors: Olivier Grisel,not
scikit-learn/sklearn/utils/validation.py,4,Gael Varoquaux,not
scikit-learn/sklearn/utils/validation.py,5,Andreas Mueller,not
scikit-learn/sklearn/utils/validation.py,6,Lars Buitinck,not
scikit-learn/sklearn/utils/validation.py,7,Alexandre Gramfort,not
scikit-learn/sklearn/utils/validation.py,8,Nicolas Tresegnie,not
scikit-learn/sklearn/utils/validation.py,9,Sylvain Marie,not
scikit-learn/sklearn/utils/validation.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/utils/validation.py,21,mypy error: Module 'numpy.core.numeric' has no attribute 'ComplexWarning',not
scikit-learn/sklearn/utils/validation.py,22,type: ignore,not
scikit-learn/sklearn/utils/validation.py,35,Silenced by default to reduce verbosity. Turn on at runtime for,not
scikit-learn/sklearn/utils/validation.py,36,performance profiling.,not
scikit-learn/sklearn/utils/validation.py,65,ignore first 'self' argument for instance methods,not
scikit-learn/sklearn/utils/validation.py,80,validation is also imported in extmath,not
scikit-learn/sklearn/utils/validation.py,86,"First try an O(n) time, O(1) space solution for the common case that",not
scikit-learn/sklearn/utils/validation.py,87,everything is finite; fall back to O(n) space np.isfinite to prevent,not
scikit-learn/sklearn/utils/validation.py,88,false positives from overflow in sum method. The sum is also calculated,not
scikit-learn/sklearn/utils/validation.py,89,safely to reduce dtype induced overflows.,not
scikit-learn/sklearn/utils/validation.py,103,"for object dtype data, we only check for NaNs (GH-13254)",not
scikit-learn/sklearn/utils/validation.py,165,is numpy array,not
scikit-learn/sklearn/utils/validation.py,186,Don't get num_samples from an ensembles length!,not
scikit-learn/sklearn/utils/validation.py,199,Check that shape is returning an integer or default to len,not
scikit-learn/sklearn/utils/validation.py,200,Dask dataframes may not return numeric shape[0] value,not
scikit-learn/sklearn/utils/validation.py,351,Indices dtype validation,not
scikit-learn/sklearn/utils/validation.py,363,ensure correct sparse format,not
scikit-learn/sklearn/utils/validation.py,365,create new with correct sparse,not
scikit-learn/sklearn/utils/validation.py,369,any other type,not
scikit-learn/sklearn/utils/validation.py,375,convert dtype,not
scikit-learn/sklearn/utils/validation.py,378,force copy,not
scikit-learn/sklearn/utils/validation.py,487,store reference to original array to check if copy is needed when,not
scikit-learn/sklearn/utils/validation.py,488,function returns,not
scikit-learn/sklearn/utils/validation.py,491,store whether originally we wanted numeric dtype,not
scikit-learn/sklearn/utils/validation.py,496,not a data type (e.g. a column named dtype in a pandas DataFrame),not
scikit-learn/sklearn/utils/validation.py,499,check if the object contains several dtypes (typically a pandas,not
scikit-learn/sklearn/utils/validation.py,500,"DataFrame), and store them. If not, store None.",not
scikit-learn/sklearn/utils/validation.py,504,"throw warning if columns are sparse. If all columns are sparse, then",not
scikit-learn/sklearn/utils/validation.py,505,array.sparse exists and sparsity will be perserved (later).,not
scikit-learn/sklearn/utils/validation.py,516,pandas boolean dtype __array__ interface coerces bools to objects,not
scikit-learn/sklearn/utils/validation.py,521,"name looks like an Integer Extension Array, now check for",not
scikit-learn/sklearn/utils/validation.py,522,the dtype,not
scikit-learn/sklearn/utils/validation.py,539,"if input is object, convert to float.",not
scikit-learn/sklearn/utils/validation.py,546,no dtype conversion required,not
scikit-learn/sklearn/utils/validation.py,549,dtype conversion required. Let's select the first element of the,not
scikit-learn/sklearn/utils/validation.py,550,list of accepted types.,not
scikit-learn/sklearn/utils/validation.py,554,"If there are any pandas integer extension arrays,",not
scikit-learn/sklearn/utils/validation.py,570,"When all dataframe columns are sparse, convert to a sparse array",not
scikit-learn/sklearn/utils/validation.py,572,DataFrame.sparse only supports `to_coo`,not
scikit-learn/sklearn/utils/validation.py,582,"If np.array(..) gives ComplexWarning, then we convert the warning",not
scikit-learn/sklearn/utils/validation.py,583,to an error. This is needed because specifying a non complex,not
scikit-learn/sklearn/utils/validation.py,584,"dtype to the function converts complex to real dtype,",not
scikit-learn/sklearn/utils/validation.py,585,thereby passing the test made in the lines following the scope,not
scikit-learn/sklearn/utils/validation.py,586,of warnings context manager.,not
scikit-learn/sklearn/utils/validation.py,591,Conversion float -> int should not contain NaN or,not
scikit-learn/sklearn/utils/validation.py,592,inf (numpy#14412). We cannot use casting='safe' because,not
scikit-learn/sklearn/utils/validation.py,593,then conversion float -> int would be disallowed.,not
scikit-learn/sklearn/utils/validation.py,605,It is possible that the np.array(..) gave no warning. This happens,not
scikit-learn/sklearn/utils/validation.py,606,"when no dtype conversion happened, for example dtype = None. The",not
scikit-learn/sklearn/utils/validation.py,607,result is that np.array(..) produces an array of complex dtype,not
scikit-learn/sklearn/utils/validation.py,608,and we need to catch and raise exception for such cases.,not
scikit-learn/sklearn/utils/validation.py,612,If input is scalar raise error,not
scikit-learn/sklearn/utils/validation.py,619,If input is 1D raise error,not
scikit-learn/sklearn/utils/validation.py,627,in the future np.flexible dtypes will be handled like object dtypes,not
scikit-learn/sklearn/utils/validation.py,638,make sure we actually converted to numeric:,not
scikit-learn/sklearn/utils/validation.py,934,"only csr, csc, and coo have `data` attribute",not
scikit-learn/sklearn/utils/validation.py,1036,avoid X.min() on sparse matrix since it also sorts the indices,not
scikit-learn/sklearn/utils/validation.py,1177,note: the minimum value available is,not
scikit-learn/sklearn/utils/validation.py,1178,- single-precision: np.finfo('float32').eps = 1.2e-07,not
scikit-learn/sklearn/utils/validation.py,1179,- double-precision: np.finfo('float64').eps = 2.2e-16,not
scikit-learn/sklearn/utils/validation.py,1181,the various thresholds used for validation,not
scikit-learn/sklearn/utils/validation.py,1182,we may wish to change the value according to precision.,not
scikit-learn/sklearn/utils/validation.py,1188,Check that there are no significant imaginary parts,not
scikit-learn/sklearn/utils/validation.py,1200,warn about imaginary parts being removed,not
scikit-learn/sklearn/utils/validation.py,1210,Remove all imaginary parts (even if zero),not
scikit-learn/sklearn/utils/validation.py,1213,Check that there are no significant negative eigenvalues,not
scikit-learn/sklearn/utils/validation.py,1231,Remove all negative values and warn about it,not
scikit-learn/sklearn/utils/validation.py,1242,Check for conditioning (small positive non-zeros),not
scikit-learn/sklearn/utils/validation.py,1369,Non-indexable pass-through (for now for backward-compatibility).,not
scikit-learn/sklearn/utils/validation.py,1370,https://github.com/scikit-learn/scikit-learn/issues/15805,not
scikit-learn/sklearn/utils/validation.py,1373,Any other fit_params should support indexing,not
scikit-learn/sklearn/utils/validation.py,1374,(e.g. for cross-validation).,not
scikit-learn/sklearn/utils/class_weight.py,1,Authors: Andreas Mueller,not
scikit-learn/sklearn/utils/class_weight.py,2,Manoj Kumar,not
scikit-learn/sklearn/utils/class_weight.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/utils/class_weight.py,39,Import error caused by circular imports.,not
scikit-learn/sklearn/utils/class_weight.py,46,uniform class weights,not
scikit-learn/sklearn/utils/class_weight.py,49,Find the weight of each class as present in y.,not
scikit-learn/sklearn/utils/class_weight.py,59,user-defined dictionary,not
scikit-learn/sklearn/utils/class_weight.py,149,"Get class weights for the subsample, covering all classes in",not
scikit-learn/sklearn/utils/class_weight.py,150,case some labels that were present in the original data are,not
scikit-learn/sklearn/utils/class_weight.py,151,missing from the sample.,not
scikit-learn/sklearn/utils/class_weight.py,171,Make missing classes' weight zero,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,35,Test checking logic and labeling,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,51,Test estimators that are represnted by strings,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,156,top level estimators show estimator with changes,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,162,low level estimators do not show changes,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,171,feature union,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,184,voting classifer,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,200,If final_estimator's default changes from LogisticRegression,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,201,this should be updated,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,223,Test duck typing meta estimators with Birch,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,227,inner estimators do not show changes,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,232,outer estimator contains all changes,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,237,Test duck typing metaestimators with OVO,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,241,inner estimators do not show changes,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,246,outter estimator,not
scikit-learn/sklearn/utils/tests/test_estimator_html_repr.py,251,Test duck typing metaestimators with GP,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,184,Convert data,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,190,Function is only called after we verify that pandas is installed,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,214,Intentionally modify the balanced class_weight,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,215,to simulate a bug and raise an exception,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,219,Simply assigning coef_ to the class_weight,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,236,Convert data,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,245,return 1 if X has more than one element else return 0,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,309,"Toy classifier that only supports binary classification, will fail tests.",not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,318,Toy classifier that only supports binary classification.,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,342,always returns True,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,347,Tests that check_fit_score_takes_y works on a class with,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,348,a deprecated fit method,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,360,"tests that the estimator actually fails on ""bad"" estimators.",not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,361,"not a complete test of all checks, which are very extensive.",not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,363,check that we have a set_params and can clone,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,368,check that values returned by get_params match set_params,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,375,check that we have a fit method,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,378,check that fit does input validation,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,382,check that sample_weights in fit accepts pandas.Series type,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,384,noqa,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,391,check that predict does input validation (doesn't accept dicts in input),not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,395,check that estimator state does not change,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,396,at transform/predict/predict_proba time,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,399,check that `fit` only changes attribures that,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,400,are private (start with an _ or end with a _).,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,406,check that `fit` doesn't add any public attribute,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,413,check for invariant method,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,420,check for sparse matrix input handling,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,423,"the check for sparse input handling prints to the stdout,",not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,424,"instead of raising an error, so as not to remove the original traceback.",not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,425,that means we need to jump through some hoops to catch it.,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,437,Large indices test on bad estimator,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,443,does error on binary_only untagged estimator,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,448,non-regression test for estimators transforming to sparse data,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,451,doesn't error on actual estimator,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,456,doesn't error on binary_only tagged estimator,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,459,Check regressor with requires_positive_y estimator tag,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,466,should raise AssertionError,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,469,should pass,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,475,check that TransformerMixin is not required for transformer tests to run,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,481,check that check_estimator doesn't modify the estimator it receives,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,489,when 'est = SGDClassifier()',not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,493,without fitting,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,499,when 'est = SGDClassifier()',not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,503,with fitting,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,511,check that a ValueError/AttributeError is raised when calling predict,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,512,on an unfitted estimator,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,517,check that CorrectNotFittedError inherit from either ValueError,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,518,or AttributeError,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,549,check that check_estimator() works on estimator with _pairwise,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,550,kernel or metric,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,552,test precomputed kernel,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,556,test precomputed metric,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,591,check that ill-computed balanced weights raises an exception,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,601,all_estimator should not fail when pytest is not installed and return,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,602,only public estimators,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,609,This module is run as a script to check that we have no dependency on,not
scikit-learn/sklearn/utils/tests/test_estimator_checks.py,610,pytest for estimator checks.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,26,Sparsify the array a little bit,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,58,Sparsify the array a little bit,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,96,default params for incr_mean_variance,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,101,Test errors,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,112,Test _incr_mean_and_var with a 1 row input,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,118,X.shape[axis] picks # samples,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,127,Test _incremental_mean_and_var with whole data,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,166,non-regression test for:,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,167,https://github.com/scikit-learn/scikit-learn/issues/16448,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,168,check that computing the incremental mean and variance is equivalent to,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,169,computing the mean and variance on the stacked dataset.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,186,check the behaviour when we update the variance with an empty matrix,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,195,update statistic with a column which should ignored,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,222,we avoid creating specific data for axis 0 and 1: translating the data is,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,223,enough.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,228,take a copy of the old statistics since they are modified in place.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,243,Sparsify the array a little bit,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,526,Check dtypes with large sparse matrices too,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,527,XXX: test fails on 32bit (Windows/Linux),SATD
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,543,Test csc_row_median actually calculates the median.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,545,Test that it gives the same output when X is dense.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,553,Test that it gives the same output when X is sparse,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,563,Test for toy data.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,571,Test that it raises an Error for non-csc matrices.,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,586,"csr_matrix will use int32 indices by default,",not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,587,up-casting those to int64 when necessary,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,602,checks that csr_row_norms returns the same output as,not
scikit-learn/sklearn/utils/tests/test_sparsefuncs.py,603,"scipy.sparse.linalg.norm, and that the dype is the same as X.dtype.",not
scikit-learn/sklearn/utils/tests/test_murmurhash.py,1,Author: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/utils/tests/test_murmurhash.py,2,,not
scikit-learn/sklearn/utils/tests/test_murmurhash.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/utils/tests/test_fixes.py,1,Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/utils/tests/test_fixes.py,2,Justin Vincent,not
scikit-learn/sklearn/utils/tests/test_fixes.py,3,Lars Buitinck,not
scikit-learn/sklearn/utils/tests/test_fixes.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/utils/tests/test_fixes.py,25,arguments are simply passed through,not
scikit-learn/sklearn/utils/tests/test_fixes.py,31,arguments are mapped to the corresponding backend,not
scikit-learn/sklearn/utils/tests/test_fixes.py,71,"Test the basics; right bounds, right size",not
scikit-learn/sklearn/utils/tests/test_fixes.py,75,Test that it's actually (fairly) uniform,not
scikit-learn/sklearn/utils/tests/test_fixes.py,81,Test that random_state works,not
scikit-learn/sklearn/utils/tests/test_validation.py,57,Test function for as_float_array,not
scikit-learn/sklearn/utils/tests/test_validation.py,62,Another test,not
scikit-learn/sklearn/utils/tests/test_validation.py,65,Checking that the array wasn't overwritten,not
scikit-learn/sklearn/utils/tests/test_validation.py,68,Test int dtypes <= 32bit,not
scikit-learn/sklearn/utils/tests/test_validation.py,77,Test object dtype,not
scikit-learn/sklearn/utils/tests/test_validation.py,82,"Here, X is of the right type, it shouldn't be modified",not
scikit-learn/sklearn/utils/tests/test_validation.py,85,Test that if X is fortran ordered it stays,not
scikit-learn/sklearn/utils/tests/test_validation.py,89,Test the copy parameter with some matrices,not
scikit-learn/sklearn/utils/tests/test_validation.py,113,Confirm that input validation code does not return np.matrix,not
scikit-learn/sklearn/utils/tests/test_validation.py,122,Confirm that input validation code doesn't copy memory mapped arrays,not
scikit-learn/sklearn/utils/tests/test_validation.py,138,Check that ordering is enforced correctly by validation utilities.,not
scikit-learn/sklearn/utils/tests/test_validation.py,139,"We need to check each validation utility, because a 'copy' without",not
scikit-learn/sklearn/utils/tests/test_validation.py,140,'order=K' will kill the ordering.,not
scikit-learn/sklearn/utils/tests/test_validation.py,220,casting a float array containing NaN or inf to int dtype should,not
scikit-learn/sklearn/utils/tests/test_validation.py,221,raise an error irrespective of the force_all_finite parameter.,not
scikit-learn/sklearn/utils/tests/test_validation.py,228,accept_sparse == False,not
scikit-learn/sklearn/utils/tests/test_validation.py,229,raise error on sparse inputs,not
scikit-learn/sklearn/utils/tests/test_validation.py,235,ensure_2d=False,not
scikit-learn/sklearn/utils/tests/test_validation.py,238,ensure_2d=True with 1d array,not
scikit-learn/sklearn/utils/tests/test_validation.py,243,ensure_2d=True with scalar array,not
scikit-learn/sklearn/utils/tests/test_validation.py,248,don't allow ndim > 3,not
scikit-learn/sklearn/utils/tests/test_validation.py,252,doesn't raise,not
scikit-learn/sklearn/utils/tests/test_validation.py,254,dtype and order enforcement.,not
scikit-learn/sklearn/utils/tests/test_validation.py,279,doesn't copy if it was already good,not
scikit-learn/sklearn/utils/tests/test_validation.py,285,allowed sparse != None,not
scikit-learn/sklearn/utils/tests/test_validation.py,300,XXX unreached code as of v0.22,SATD
scikit-learn/sklearn/utils/tests/test_validation.py,312,no change if allowed,not
scikit-learn/sklearn/utils/tests/test_validation.py,315,got converted,not
scikit-learn/sklearn/utils/tests/test_validation.py,320,doesn't copy if it was already good,not
scikit-learn/sklearn/utils/tests/test_validation.py,324,other input formats,not
scikit-learn/sklearn/utils/tests/test_validation.py,325,convert lists to arrays,not
scikit-learn/sklearn/utils/tests/test_validation.py,328,raise on too deep lists,not
scikit-learn/sklearn/utils/tests/test_validation.py,331,doesn't raise,not
scikit-learn/sklearn/utils/tests/test_validation.py,333,convert weird stuff to arrays,not
scikit-learn/sklearn/utils/tests/test_validation.py,338,"deprecation warning if string-like array with dtype=""numeric""",not
scikit-learn/sklearn/utils/tests/test_validation.py,345,"deprecation warning if byte-like array with dtype=""numeric""",not
scikit-learn/sklearn/utils/tests/test_validation.py,359,Test pandas IntegerArray with pd.NA,not
scikit-learn/sklearn/utils/tests/test_validation.py,366,Creates dataframe with IntegerArrays with pd.NA,not
scikit-learn/sklearn/utils/tests/test_validation.py,368,column c has no nans,not
scikit-learn/sklearn/utils/tests/test_validation.py,384,test that data-frame like objects with dtype object,not
scikit-learn/sklearn/utils/tests/test_validation.py,385,get converted,not
scikit-learn/sklearn/utils/tests/test_validation.py,390,"smoke-test against dataframes with column named ""dtype""",not
scikit-learn/sklearn/utils/tests/test_validation.py,396,test that data-frames with homogeneous dtype are not upcast,not
scikit-learn/sklearn/utils/tests/test_validation.py,410,"float16, int16, float32 casts to float32",not
scikit-learn/sklearn/utils/tests/test_validation.py,415,"float16, int16, float16 casts to float32",not
scikit-learn/sklearn/utils/tests/test_validation.py,421,we're not using upcasting rules for determining,not
scikit-learn/sklearn/utils/tests/test_validation.py,422,"the target type yet, so we cast to the default of float64",not
scikit-learn/sklearn/utils/tests/test_validation.py,425,check that we handle pandas dtypes in a semi-reasonable way,not
scikit-learn/sklearn/utils/tests/test_validation.py,426,this is actually tricky because we can't really know that this,not
scikit-learn/sklearn/utils/tests/test_validation.py,427,should be integer ahead of converting it.,not
scikit-learn/sklearn/utils/tests/test_validation.py,444,test that lists with ints don't get converted to floats,not
scikit-learn/sklearn/utils/tests/test_validation.py,532,When large sparse are allowed,not
scikit-learn/sklearn/utils/tests/test_validation.py,537,When large sparse are not allowed,not
scikit-learn/sklearn/utils/tests/test_validation.py,545,empty list is considered 2D by default:,not
scikit-learn/sklearn/utils/tests/test_validation.py,551,"If considered a 1D collection when ensure_2d=False, then the minimum",not
scikit-learn/sklearn/utils/tests/test_validation.py,552,number of samples will break:,not
scikit-learn/sklearn/utils/tests/test_validation.py,557,Invalid edge case when checking the default minimum sample of a scalar,not
scikit-learn/sklearn/utils/tests/test_validation.py,563,Simulate a model that would need at least 2 samples to be well defined,not
scikit-learn/sklearn/utils/tests/test_validation.py,571,The same message is raised if the data has 2 dimensions even if this is,not
scikit-learn/sklearn/utils/tests/test_validation.py,572,not mandatory,not
scikit-learn/sklearn/utils/tests/test_validation.py,576,Simulate a model that would require at least 3 features (e.g. SelectKBest,not
scikit-learn/sklearn/utils/tests/test_validation.py,577,with k=3),not
scikit-learn/sklearn/utils/tests/test_validation.py,585,Only the feature check is enabled whenever the number of dimensions is 2,not
scikit-learn/sklearn/utils/tests/test_validation.py,586,even if allow_nd is enabled:,not
scikit-learn/sklearn/utils/tests/test_validation.py,590,Simulate a case where a pipeline stage as trimmed all the features of a,not
scikit-learn/sklearn/utils/tests/test_validation.py,591,2D dataset.,not
scikit-learn/sklearn/utils/tests/test_validation.py,599,nd-data is not checked for any minimum number of features by default:,not
scikit-learn/sklearn/utils/tests/test_validation.py,612,list of lists,not
scikit-learn/sklearn/utils/tests/test_validation.py,617,tuple of tuples,not
scikit-learn/sklearn/utils/tests/test_validation.py,622,list of np arrays,not
scikit-learn/sklearn/utils/tests/test_validation.py,628,tuple of np arrays,not
scikit-learn/sklearn/utils/tests/test_validation.py,634,dataframe,not
scikit-learn/sklearn/utils/tests/test_validation.py,640,sparse matrix,not
scikit-learn/sklearn/utils/tests/test_validation.py,675,check error for bad inputs,not
scikit-learn/sklearn/utils/tests/test_validation.py,679,check that asymmetric arrays are properly symmetrized,not
scikit-learn/sklearn/utils/tests/test_validation.py,681,Check for warnings and errors,not
scikit-learn/sklearn/utils/tests/test_validation.py,696,Check is TypeError raised when non estimator instance passed,not
scikit-learn/sklearn/utils/tests/test_validation.py,713,NotFittedError is a subclass of both ValueError and AttributeError,not
scikit-learn/sklearn/utils/tests/test_validation.py,769,Does not raise,not
scikit-learn/sklearn/utils/tests/test_validation.py,772,Raises when using attribute that is not defined,not
scikit-learn/sklearn/utils/tests/test_validation.py,791,Despite ensembles having __len__ they must raise TypeError,not
scikit-learn/sklearn/utils/tests/test_validation.py,794,"XXX: We should have a test with a string, but what is correct behaviour?",SATD
scikit-learn/sklearn/utils/tests/test_validation.py,798,check pandas dataframe with 'fit' column does not raise error,not
scikit-learn/sklearn/utils/tests/test_validation.py,799,https://github.com/scikit-learn/scikit-learn/issues/8415,not
scikit-learn/sklearn/utils/tests/test_validation.py,821,regression test that check_array works on pandas Series,not
scikit-learn/sklearn/utils/tests/test_validation.py,826,with categorical dtype (not a numpy dtype) (GH12699),not
scikit-learn/sklearn/utils/tests/test_validation.py,833,"pandas dataframe will coerce a boolean into a object, this is a mismatch",not
scikit-learn/sklearn/utils/tests/test_validation.py,834,with np.result_type which will return a float,not
scikit-learn/sklearn/utils/tests/test_validation.py,835,check_array needs to explicitly check for bool dtype in a dataframe for,not
scikit-learn/sklearn/utils/tests/test_validation.py,836,this situation,not
scikit-learn/sklearn/utils/tests/test_validation.py,837,https://github.com/scikit-learn/scikit-learn/issues/15787,not
scikit-learn/sklearn/utils/tests/test_validation.py,930,check that it gives a good error if there's no __len__,not
scikit-learn/sklearn/utils/tests/test_validation.py,1003,Test that ``_check_psd_eigenvalues`` returns the right output for valid,not
scikit-learn/sklearn/utils/tests/test_validation.py,1004,"input, possibly raising the right warning",not
scikit-learn/sklearn/utils/tests/test_validation.py,1039,Test that ``_check_psd_eigenvalues`` raises the right error for invalid,not
scikit-learn/sklearn/utils/tests/test_validation.py,1040,input,not
scikit-learn/sklearn/utils/tests/test_validation.py,1047,check array order,not
scikit-learn/sklearn/utils/tests/test_validation.py,1053,check None input,not
scikit-learn/sklearn/utils/tests/test_validation.py,1057,check numbers input,not
scikit-learn/sklearn/utils/tests/test_validation.py,1061,check wrong number of dimensions,not
scikit-learn/sklearn/utils/tests/test_validation.py,1066,check incorrect n_samples,not
scikit-learn/sklearn/utils/tests/test_validation.py,1071,float32 dtype is preserved,not
scikit-learn/sklearn/utils/tests/test_validation.py,1077,int dtype will be converted to float64 instead,not
scikit-learn/sklearn/utils/tests/test_validation.py,1132,The * is place before a keyword only argument without a default value,not
scikit-learn/sklearn/utils/tests/test_validation.py,1201,check_array converts pandas dataframe with only sparse arrays into,not
scikit-learn/sklearn/utils/tests/test_validation.py,1202,sparse matrix,not
scikit-learn/sklearn/utils/tests/test_validation.py,1210,by default pandas converts to coo when accept_sparse is True,not
scikit-learn/sklearn/utils/tests/test_utils.py,34,toy array,not
scikit-learn/sklearn/utils/tests/test_utils.py,39,Check the check_random_state utility function behavior,not
scikit-learn/sklearn/utils/tests/test_utils.py,57,Make sure gen_batches errors on invalid batch_size,not
scikit-learn/sklearn/utils/tests/test_utils.py,73,Test whether the deprecated decorator issues appropriate warnings,not
scikit-learn/sklearn/utils/tests/test_utils.py,74,Copied almost verbatim from https://docs.python.org/library/warnings.html,not
scikit-learn/sklearn/utils/tests/test_utils.py,76,First a function...,not
scikit-learn/sklearn/utils/tests/test_utils.py,86,function must remain usable,not
scikit-learn/sklearn/utils/tests/test_utils.py,92,... then a class.,not
scikit-learn/sklearn/utils/tests/test_utils.py,110,Border case not worth mentioning in doctests,not
scikit-learn/sklearn/utils/tests/test_utils.py,113,Check that invalid arguments yield ValueError,not
scikit-learn/sklearn/utils/tests/test_utils.py,121,"Issue:6581, n_samples can be more when replace is True (default).",not
scikit-learn/sklearn/utils/tests/test_utils.py,126,Make sure resample can stratify,not
scikit-learn/sklearn/utils/tests/test_utils.py,139,"all 1s, one 0",not
scikit-learn/sklearn/utils/tests/test_utils.py,143,Make sure stratified resampling supports the replace parameter,not
scikit-learn/sklearn/utils/tests/test_utils.py,156,make sure n_samples can be greater than X.shape[0] if we sample with,not
scikit-learn/sklearn/utils/tests/test_utils.py,157,replacement,not
scikit-learn/sklearn/utils/tests/test_utils.py,165,Make sure y can be 2d when stratifying,not
scikit-learn/sklearn/utils/tests/test_utils.py,175,resample must be ndarray,not
scikit-learn/sklearn/utils/tests/test_utils.py,302,validation of the indices,not
scikit-learn/sklearn/utils/tests/test_utils.py,303,we make a copy because indices is mutable and shared between tests,not
scikit-learn/sklearn/utils/tests/test_utils.py,427,sparse matrix are keeping the 2D shape,not
scikit-learn/sklearn/utils/tests/test_utils.py,458,check that we are raising an error if the array-like passed is 1D and,not
scikit-learn/sklearn/utils/tests/test_utils.py,459,we try to index on the 2nd dimension,not
scikit-learn/sklearn/utils/tests/test_utils.py,510,to make the inner arrays hashable,not
scikit-learn/sklearn/utils/tests/test_utils.py,513,"A.shape = (2,2,2)",not
scikit-learn/sklearn/utils/tests/test_utils.py,515,shouldn't raise a ValueError for dim = 3,not
scikit-learn/sklearn/utils/tests/test_utils.py,520,Check that shuffle does not try to convert to numpy arrays with float,not
scikit-learn/sklearn/utils/tests/test_utils.py,521,dtypes can let any indexable datastructure pass-through.,not
scikit-learn/sklearn/utils/tests/test_utils.py,553,check that gen_even_slices contains all samples,not
scikit-learn/sklearn/utils/tests/test_utils.py,559,check that passing negative n_chunks raises an error,not
scikit-learn/sklearn/utils/tests/test_utils.py,679,Only parallel_backend and register_parallel_backend are not deprecated in,not
scikit-learn/sklearn/utils/tests/test_utils.py,680,sklearn.utils,not
scikit-learn/sklearn/utils/tests/conftest.py,10,reset to default,not
scikit-learn/sklearn/utils/tests/test_fast_dict.py,27,Test the argmin implementation on the IntFloatDict,not
scikit-learn/sklearn/utils/tests/test_testing.py,43,0.24,not
scikit-learn/sklearn/utils/tests/test_testing.py,51,0.24,not
scikit-learn/sklearn/utils/tests/test_testing.py,59,0.24,not
scikit-learn/sklearn/utils/tests/test_testing.py,68,0.24,not
scikit-learn/sklearn/utils/tests/test_testing.py,79,Linear Discriminant Analysis doesn't have random state: smoke test,not
scikit-learn/sklearn/utils/tests/test_testing.py,90,basic compare,not
scikit-learn/sklearn/utils/tests/test_testing.py,132,multiple exceptions in a tuple,not
scikit-learn/sklearn/utils/tests/test_testing.py,139,This check that ignore_warning decorateur and context manager are working,not
scikit-learn/sklearn/utils/tests/test_testing.py,140,as expected,not
scikit-learn/sklearn/utils/tests/test_testing.py,148,Check the function directly,not
scikit-learn/sklearn/utils/tests/test_testing.py,164,Check the decorator,not
scikit-learn/sklearn/utils/tests/test_testing.py,197,Check the context manager,not
scikit-learn/sklearn/utils/tests/test_testing.py,229,Check that passing warning class as first positional argument,not
scikit-learn/sklearn/utils/tests/test_testing.py,254,test that assert_warns doesn't have side effects on warnings,not
scikit-learn/sklearn/utils/tests/test_testing.py,255,filters,not
scikit-learn/sklearn/utils/tests/test_testing.py,269,Should raise an AssertionError,not
scikit-learn/sklearn/utils/tests/test_testing.py,271,"assert_warns has a special handling of ""FutureWarning"" that",not
scikit-learn/sklearn/utils/tests/test_testing.py,272,pytest.warns does not have,not
scikit-learn/sklearn/utils/tests/test_testing.py,284,Tests for docstrings:,not
scikit-learn/sklearn/utils/tests/test_testing.py,665,0.24,not
scikit-learn/sklearn/utils/tests/test_pprint.py,14,Ignore flake8 (lots of line too long issues),not
scikit-learn/sklearn/utils/tests/test_pprint.py,15,flake8: noqa,not
scikit-learn/sklearn/utils/tests/test_pprint.py,17,Constructors excerpted to test pprinting,not
scikit-learn/sklearn/utils/tests/test_pprint.py,178,Basic pprint test,not
scikit-learn/sklearn/utils/tests/test_pprint.py,187,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,192,Make sure the changed_only param is correctly used when True (default),not
scikit-learn/sklearn/utils/tests/test_pprint.py,197,Check with a repr that doesn't fit on a single line,not
scikit-learn/sklearn/utils/tests/test_pprint.py,203,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,210,"Defaults to np.NaN, trying with float('NaN')",not
scikit-learn/sklearn/utils/tests/test_pprint.py,215,make sure array parameters don't throw error (see #13583),not
scikit-learn/sklearn/utils/tests/test_pprint.py,220,Render a pipeline object,not
scikit-learn/sklearn/utils/tests/test_pprint.py,236,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,241,Render a deeply nested estimator,not
scikit-learn/sklearn/utils/tests/test_pprint.py,273,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,278,render a gridsearch,not
scikit-learn/sklearn/utils/tests/test_pprint.py,298,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,303,render a pipeline inside a gridsearch,not
scikit-learn/sklearn/utils/tests/test_pprint.py,364,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,366,Remove address of '<function chi2 at 0x.....>' for reproducibility,not
scikit-learn/sklearn/utils/tests/test_pprint.py,380,No ellipsis,not
scikit-learn/sklearn/utils/tests/test_pprint.py,397,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,400,Now with ellipsis,not
scikit-learn/sklearn/utils/tests/test_pprint.py,417,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,420,Also test with lists,not
scikit-learn/sklearn/utils/tests/test_pprint.py,437,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,440,Now with ellipsis,not
scikit-learn/sklearn/utils/tests/test_pprint.py,457,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,462,Check that the bruteforce ellipsis (used when the number of non-blank,not
scikit-learn/sklearn/utils/tests/test_pprint.py,463,characters exceeds N_CHAR_MAX) renders correctly.,not
scikit-learn/sklearn/utils/tests/test_pprint.py,467,test when the left and right side of the ellipsis aren't on the same,not
scikit-learn/sklearn/utils/tests/test_pprint.py,468,line.,not
scikit-learn/sklearn/utils/tests/test_pprint.py,476,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,479,test with very small N_CHAR_MAX,not
scikit-learn/sklearn/utils/tests/test_pprint.py,480,"Note that N_CHAR_MAX is not strictly enforced, but it's normal: to avoid",not
scikit-learn/sklearn/utils/tests/test_pprint.py,481,weird reprs we still keep the whole line of the right part (after the,not
scikit-learn/sklearn/utils/tests/test_pprint.py,482,ellipsis).,not
scikit-learn/sklearn/utils/tests/test_pprint.py,487,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,490,test with N_CHAR_MAX == number of non-blank characters: In this case we,not
scikit-learn/sklearn/utils/tests/test_pprint.py,491,don't want ellipsis,not
scikit-learn/sklearn/utils/tests/test_pprint.py,497,test with N_CHAR_MAX == number of non-blank characters - 10: the left and,not
scikit-learn/sklearn/utils/tests/test_pprint.py,498,right side of the ellispsis are on different lines. In this case we,not
scikit-learn/sklearn/utils/tests/test_pprint.py,499,want to expend the whole line of the right side,not
scikit-learn/sklearn/utils/tests/test_pprint.py,506,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,509,test with N_CHAR_MAX == number of non-blank characters - 10: the left and,not
scikit-learn/sklearn/utils/tests/test_pprint.py,510,right side of the ellispsis are on the same line. In this case we don't,not
scikit-learn/sklearn/utils/tests/test_pprint.py,511,"want to expend the whole line of the right side, just add the ellispsis",not
scikit-learn/sklearn/utils/tests/test_pprint.py,512,between the 2 sides.,not
scikit-learn/sklearn/utils/tests/test_pprint.py,519,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,522,test with N_CHAR_MAX == number of non-blank characters - 2: the left and,not
scikit-learn/sklearn/utils/tests/test_pprint.py,523,"right side of the ellispsis are on the same line, but adding the ellipsis",not
scikit-learn/sklearn/utils/tests/test_pprint.py,524,would actually make the repr longer. So we don't add the ellipsis.,not
scikit-learn/sklearn/utils/tests/test_pprint.py,531,remove first \n,not
scikit-learn/sklearn/utils/tests/test_pprint.py,536,non regression test than ensures we can still use the builtin,not
scikit-learn/sklearn/utils/tests/test_pprint.py,537,PrettyPrinter class for estimators (as done e.g. by joblib).,not
scikit-learn/sklearn/utils/tests/test_pprint.py,538,Used to be a bug,SATD
scikit-learn/sklearn/utils/tests/test_random.py,11,,not
scikit-learn/sklearn/utils/tests/test_random.py,12,test custom sampling without replacement algorithm,not
scikit-learn/sklearn/utils/tests/test_random.py,13,,not
scikit-learn/sklearn/utils/tests/test_random.py,36,n_population < n_sample,not
scikit-learn/sklearn/utils/tests/test_random.py,42,n_population == n_samples,not
scikit-learn/sklearn/utils/tests/test_random.py,47,n_population >= n_samples,not
scikit-learn/sklearn/utils/tests/test_random.py,51,n_population < 0 or n_samples < 0,not
scikit-learn/sklearn/utils/tests/test_random.py,59,This test is heavily inspired from test_random.py of python-core.,not
scikit-learn/sklearn/utils/tests/test_random.py,60,,not
scikit-learn/sklearn/utils/tests/test_random.py,61,"For the entire allowable range of 0 <= k <= N, validate that",not
scikit-learn/sklearn/utils/tests/test_random.py,62,the sample is of the correct length and contains only unique items,not
scikit-learn/sklearn/utils/tests/test_random.py,72,test edge case n_population == n_samples == 0,not
scikit-learn/sklearn/utils/tests/test_random.py,77,This test is heavily inspired from test_random.py of python-core.,not
scikit-learn/sklearn/utils/tests/test_random.py,78,,not
scikit-learn/sklearn/utils/tests/test_random.py,79,"For the entire allowable range of 0 <= k <= N, validate that",not
scikit-learn/sklearn/utils/tests/test_random.py,80,sample generates all possible permutations,not
scikit-learn/sklearn/utils/tests/test_random.py,83,a large number of trials prevents false negatives without slowing normal,not
scikit-learn/sklearn/utils/tests/test_random.py,84,case,not
scikit-learn/sklearn/utils/tests/test_random.py,88,Counting the number of combinations is not as good as counting the,not
scikit-learn/sklearn/utils/tests/test_random.py,89,"the number of permutations. However, it works with sampling algorithm",not
scikit-learn/sklearn/utils/tests/test_random.py,90,that does not provide a random permutation of the subset of integer.,not
scikit-learn/sklearn/utils/tests/test_random.py,107,Explicit class probabilities,not
scikit-learn/sklearn/utils/tests/test_random.py,119,Implicit class probabilities,not
scikit-learn/sklearn/utils/tests/test_random.py,120,test for array-like support,not
scikit-learn/sklearn/utils/tests/test_random.py,132,Edge case probabilities 1.0 and 0.0,not
scikit-learn/sklearn/utils/tests/test_random.py,145,One class target data,not
scikit-learn/sklearn/utils/tests/test_random.py,146,test for array-like support,not
scikit-learn/sklearn/utils/tests/test_random.py,160,the length of an array in classes and class_probabilities is mismatched,not
scikit-learn/sklearn/utils/tests/test_random.py,166,the class dtype is not supported,not
scikit-learn/sklearn/utils/tests/test_random.py,172,the class dtype is not supported,not
scikit-learn/sklearn/utils/tests/test_random.py,178,Given probabilities don't sum to 1,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,14,Test (and demo) compute_class_weight.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,19,total effect of samples is preserved,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,26,Raise error when y does not contain all class labels,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,31,Fix exception in error message formatting when missing label is a string,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,32,https://github.com/scikit-learn/scikit-learn/issues/8312,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,36,Raise error when y has items not in classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,50,"When the user specifies class weights, compute_class_weights should just",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,51,return them.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,54,"When a class weight is specified that isn't in classes, a ValueError",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,55,should get raised,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,68,"Test that results with class_weight=""balanced"" is invariant wrt",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,69,class imbalance if the number of samples is identical.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,70,The test uses a balanced two class dataset with 100 datapoints.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,71,"It creates three versions, one where class 1 is duplicated",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,72,"resulting in 150 points of class 1 and 50 of class 0,",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,73,"one where there are 50 points in class 1 and 150 in class 0,",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,74,and one where there are 100 points of each class (this one is balanced,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,75,again).,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,76,"With balancing class weights, all three should give the same model.",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,78,create dataset where class 1 is duplicated twice,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,81,create dataset where class 0 is duplicated twice,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,84,duplicate everything,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,87,results should be identical,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,96,Test compute_class_weight when labels are negative,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,97,Test with balanced class labels.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,105,Test with unbalanced class labels.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,116,Test compute_class_weight when classes are unordered,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,127,Test for the case where no weight is given for a present class.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,128,Current behaviour is to assign the unweighted classes a weight of 1.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,133,Test for non specified weights,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,138,Tests for partly specified weights,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,149,Test (and demo) compute_sample_weight.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,150,Test with balanced classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,155,Test with user-defined weights,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,159,Test with column vector of balanced classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,164,Test with unbalanced classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,171,Test with `None` weights,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,175,Test with multi-output of balanced classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,180,Test with multi-output with user-defined weights,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,185,Test with multi-output of unbalanced classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,192,Test compute_sample_weight with subsamples specified.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,193,Test with balanced classes and all samples present,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,198,Test with column vector of balanced classes and all samples present,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,203,Test with a subsample,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,209,Test with a bootstrap subsample,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,216,Test with a bootstrap subsample for multi-output,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,222,Test with a missing class,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,227,Test with a missing class for multi-output,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,234,Test compute_sample_weight raises errors expected.,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,235,Invalid preset string,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,248,"Not ""balanced"" for subsample",not
scikit-learn/sklearn/utils/tests/test_class_weight.py,252,Not a list or preset for multi-output,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,256,Incorrect length list for multi-output,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,262,Non-regression smoke test for #12146,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,263,more than 32 distinct classes,not
scikit-learn/sklearn/utils/tests/test_class_weight.py,264,use subsampling,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,12,set nonzero entries to infinity,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,15,set diagonal to zero,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,32,sparse grid of distances,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,36,make symmetric: distances are not direction-dependent,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,39,make graph sparse,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,43,set diagonal to zero,not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,71,We compare path length and not costs (-> set distances to 0 or 1),not
scikit-learn/sklearn/utils/tests/test_shortest_path.py,80,Non-reachable nodes have distance 0 in graph_py,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,35,"valid when the data is formatted as sparse or dense, identified",not
scikit-learn/sklearn/utils/tests/test_multiclass.py,36,by CSR format when the testing takes place,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,48,Only valid when data is dense,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,119,sequence of sequences that weren't supported even before deprecation,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,125,and also confusable as sequences of sequences,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,128,empty second dimension,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,131,3d,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,154,Empty iterable,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,158,Multiclass problem,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,163,Multilabel indicator,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,173,Several arrays passed,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,179,Border line case with binary indicator matrix,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,190,Test unique_labels with a variety of collected examples,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,192,Smoke test for all supported format,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,197,We don't support those format at the moment,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,210,Mix with binary or multiclass and multilabel,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,242,Only mark explicitly defined sparse examples as valid sparse,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,243,multilabel-indicators,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,265,Densify sparse examples before testing,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,286,@ignore_warnings,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,328,Define the sparse matrix with a mix of implicit and explicit zeros,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,355,Test again with explicit sample weights,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,400,test properties for ovr decision function,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,416,check that the decision values are within 0.5 range of the votes,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,424,check that the prediction are what we expect,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,425,highest vote or highest confidence if there is a tie.,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,426,for the second sample we have a tie (should be won by 1),not
scikit-learn/sklearn/utils/tests/test_multiclass.py,430,third and fourth sample have the same vote but third sample,not
scikit-learn/sklearn/utils/tests/test_multiclass.py,431,"has higher confidence, this should reflect on the decision values",not
scikit-learn/sklearn/utils/tests/test_multiclass.py,434,assert subset invariance.,not
scikit-learn/sklearn/utils/tests/test_optimize.py,10,Test that newton_cg gives same result as scipy's fmin_ncg,not
scikit-learn/sklearn/utils/tests/test_deprecation.py,1,Authors: Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/sklearn/utils/tests/test_deprecation.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/utils/tests/test_deprecation.py,49,Test if _is_deprecated helper identifies wrapping via deprecated,not
scikit-learn/sklearn/utils/tests/test_deprecation.py,50,NOTE it works only for class methods and functions,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,19,This file tests the utils that are deprecated,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,22,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,29,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,36,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,43,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,50,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,57,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,64,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,85,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,92,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,99,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,101,Non-regression test for:,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,102,https://github.com/scikit-learn/scikit-learn/issues/15842,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,105,noqa,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,107,Calling all_estimators() also triggers a recursive import of all,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,108,"submodules, including deprecated ones.",not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,115,TODO: remove in 0.24,SATD
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,117,Non-regression test for:,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,118,https://github.com/scikit-learn/scikit-learn/issues/15842,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,121,noqa,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,123,Calling all_estimators() also triggers a recursive import of all,not
scikit-learn/sklearn/utils/tests/test_deprecated_utils.py,124,"submodules, including deprecated ones.",not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,1,Author: Tom Dupre la Tour,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,2,Joan Massich <mailsik@gmail.com>,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,3,,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,67,next sample,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,75,random sample,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,90,not shuffled,not
scikit-learn/sklearn/utils/tests/test_seq_dataset.py,129,next sample,not
scikit-learn/sklearn/utils/tests/test_extmath.py,1,Authors: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/utils/tests/test_extmath.py,2,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/utils/tests/test_extmath.py,3,Denis Engemann <denis-alexander.engemann@inria.fr>,not
scikit-learn/sklearn/utils/tests/test_extmath.py,4,,not
scikit-learn/sklearn/utils/tests/test_extmath.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/utils/tests/test_extmath.py,55,"with uniform weights, results should be identical to stats.mode",not
scikit-learn/sklearn/utils/tests/test_extmath.py,69,"set this up so that each row should have a weighted mode of 6,",not
scikit-learn/sklearn/utils/tests/test_extmath.py,70,with a score that is easily reproduced,not
scikit-learn/sklearn/utils/tests/test_extmath.py,87,Check that extmath.randomized_svd is consistent with linalg.svd,not
scikit-learn/sklearn/utils/tests/test_extmath.py,95,generate a matrix X of approximate effective rank `rank` and no noise,not
scikit-learn/sklearn/utils/tests/test_extmath.py,96,component (very structured signal):,not
scikit-learn/sklearn/utils/tests/test_extmath.py,102,compute the singular values of X using the slow exact method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,105,Convert the singular values to the specific dtype,not
scikit-learn/sklearn/utils/tests/test_extmath.py,110,'none' would not be stable,not
scikit-learn/sklearn/utils/tests/test_extmath.py,111,compute the singular values of X using the fast approximate method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,115,"If the input dtype is float, then the output dtype is float of the",not
scikit-learn/sklearn/utils/tests/test_extmath.py,116,same bit size (f32 is not upcast to f64),not
scikit-learn/sklearn/utils/tests/test_extmath.py,117,"But if the input dtype is int, the output dtype is float64",not
scikit-learn/sklearn/utils/tests/test_extmath.py,131,ensure that the singular values of both methods are equal up to the,not
scikit-learn/sklearn/utils/tests/test_extmath.py,132,real rank of the matrix,not
scikit-learn/sklearn/utils/tests/test_extmath.py,135,check the singular vectors too (while not checking the sign),not
scikit-learn/sklearn/utils/tests/test_extmath.py,139,check the sparse matrix representation,not
scikit-learn/sklearn/utils/tests/test_extmath.py,142,compute the singular values of X using the fast approximate method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,182,"csr_matrix will use int32 indices by default,",not
scikit-learn/sklearn/utils/tests/test_extmath.py,183,up-casting those to int64 when necessary,not
scikit-learn/sklearn/utils/tests/test_extmath.py,196,Check that extmath.randomized_svd can handle noisy matrices,not
scikit-learn/sklearn/utils/tests/test_extmath.py,202,generate a matrix X wity structure approximate rank `rank` and an,not
scikit-learn/sklearn/utils/tests/test_extmath.py,203,important noisy component,not
scikit-learn/sklearn/utils/tests/test_extmath.py,209,compute the singular values of X using the slow exact method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,213,compute the singular values of X using the fast approximate,not
scikit-learn/sklearn/utils/tests/test_extmath.py,214,method without the iterated power method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,219,the approximation does not tolerate the noise:,not
scikit-learn/sklearn/utils/tests/test_extmath.py,222,compute the singular values of X using the fast approximate,not
scikit-learn/sklearn/utils/tests/test_extmath.py,223,method with iterated power method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,228,the iterated power method is helping getting rid of the noise:,not
scikit-learn/sklearn/utils/tests/test_extmath.py,233,Check that extmath.randomized_svd can handle noisy matrices,not
scikit-learn/sklearn/utils/tests/test_extmath.py,239,let us try again without 'low_rank component': just regularly but slowly,not
scikit-learn/sklearn/utils/tests/test_extmath.py,240,decreasing singular values: the rank of the data matrix is infinite,not
scikit-learn/sklearn/utils/tests/test_extmath.py,246,compute the singular values of X using the slow exact method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,249,compute the singular values of X using the fast approximate method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,250,without the iterated power method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,254,the approximation does not tolerate the noise:,not
scikit-learn/sklearn/utils/tests/test_extmath.py,257,compute the singular values of X using the fast approximate method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,258,with iterated power method,not
scikit-learn/sklearn/utils/tests/test_extmath.py,262,the iterated power method is still managing to get most of the,not
scikit-learn/sklearn/utils/tests/test_extmath.py,263,structure at the requested rank,not
scikit-learn/sklearn/utils/tests/test_extmath.py,268,Check that transposing the design matrix has limited impact,not
scikit-learn/sklearn/utils/tests/test_extmath.py,296,in this case 'auto' is equivalent to transpose,not
scikit-learn/sklearn/utils/tests/test_extmath.py,301,randomized_svd with power_iteration_normalized='none' diverges for,not
scikit-learn/sklearn/utils/tests/test_extmath.py,302,large number of power iterations on this dataset,not
scikit-learn/sklearn/utils/tests/test_extmath.py,308,Check that it diverges with many (non-normalized) power iterations,not
scikit-learn/sklearn/utils/tests/test_extmath.py,336,randomized_svd throws a warning for lil and dok matrix,not
scikit-learn/sklearn/utils/tests/test_extmath.py,351,"Check that svd_flip works in both situations, and reconstructs input.",not
scikit-learn/sklearn/utils/tests/test_extmath.py,357,Check matrix reconstruction,not
scikit-learn/sklearn/utils/tests/test_extmath.py,362,Check transposed matrix reconstruction,not
scikit-learn/sklearn/utils/tests/test_extmath.py,368,Check that different flip methods are equivalent under reconstruction,not
scikit-learn/sklearn/utils/tests/test_extmath.py,388,Check if the randomized_svd sign flipping is always done based on u,not
scikit-learn/sklearn/utils/tests/test_extmath.py,389,irrespective of transpose.,not
scikit-learn/sklearn/utils/tests/test_extmath.py,390,See https://github.com/scikit-learn/scikit-learn/issues/5608,not
scikit-learn/sklearn/utils/tests/test_extmath.py,391,for more details.,not
scikit-learn/sklearn/utils/tests/test_extmath.py,403,Without transpose,not
scikit-learn/sklearn/utils/tests/test_extmath.py,409,With transpose,not
scikit-learn/sklearn/utils/tests/test_extmath.py,419,Check if cartesian product delivers the right results,not
scikit-learn/sklearn/utils/tests/test_extmath.py,439,check single axis,not
scikit-learn/sklearn/utils/tests/test_extmath.py,445,Check correctness and robustness of logistic sigmoid implementation,not
scikit-learn/sklearn/utils/tests/test_extmath.py,457,Test Youngs and Cramer incremental variance formulas.,not
scikit-learn/sklearn/utils/tests/test_extmath.py,458,Doggie data from https://www.mathsisfun.com/data/standard-deviation.html,not
scikit-learn/sklearn/utils/tests/test_extmath.py,504,Test Youngs and Cramer incremental variance formulas.,not
scikit-learn/sklearn/utils/tests/test_extmath.py,509,Naive one pass variance computation - not numerically stable,not
scikit-learn/sklearn/utils/tests/test_extmath.py,510,https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance,not
scikit-learn/sklearn/utils/tests/test_extmath.py,517,"Two-pass algorithm, stable.",not
scikit-learn/sklearn/utils/tests/test_extmath.py,518,We use it as a benchmark. It is not an online algorithm,not
scikit-learn/sklearn/utils/tests/test_extmath.py,519,https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm,not
scikit-learn/sklearn/utils/tests/test_extmath.py,525,Naive online implementation,not
scikit-learn/sklearn/utils/tests/test_extmath.py,526,https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm,not
scikit-learn/sklearn/utils/tests/test_extmath.py,527,This works only for chunks for size 1,not
scikit-learn/sklearn/utils/tests/test_extmath.py,537,We want to show a case when one_pass_var has error > 1e-3 while,not
scikit-learn/sklearn/utils/tests/test_extmath.py,538,_batch_mean_variance_update has less.,not
scikit-learn/sklearn/utils/tests/test_extmath.py,548,Naive one pass var: >tol (=1063),not
scikit-learn/sklearn/utils/tests/test_extmath.py,551,Starting point for online algorithms: after A0,not
scikit-learn/sklearn/utils/tests/test_extmath.py,553,Naive implementation: >tol (436),not
scikit-learn/sklearn/utils/tests/test_extmath.py,559,the mean is also slightly unstable,not
scikit-learn/sklearn/utils/tests/test_extmath.py,563,Robust implementation: <tol (177),not
scikit-learn/sklearn/utils/tests/test_extmath.py,576,Test that degrees of freedom parameter for calculations are correct.,not
scikit-learn/sklearn/utils/tests/test_extmath.py,590,Assign this twice so that the test logic is consistent,not
scikit-learn/sklearn/utils/tests/test_extmath.py,611,Testing that sign flip is working & largest value has positive sign,not
scikit-learn/sklearn/utils/tests/test_extmath.py,634,test axis parameter,not
scikit-learn/sklearn/utils/tests/test_extmath.py,669,dense ND / sparse,not
scikit-learn/sklearn/utils/tests/test_extmath.py,677,sparse / dense ND,not
scikit-learn/sklearn/utils/tests/test_extmath.py,693,2D @ 1D,not
scikit-learn/sklearn/utils/tests/test_extmath.py,700,1D @ 2D,not
scikit-learn/sklearn/inspection/_partial_dependence.py,3,Authors: Peter Prettenhofer,not
scikit-learn/sklearn/inspection/_partial_dependence.py,4,Trevor Stephens,not
scikit-learn/sklearn/inspection/_partial_dependence.py,5,Nicolas Hug,not
scikit-learn/sklearn/inspection/_partial_dependence.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/inspection/_partial_dependence.py,18,noqa,not
scikit-learn/sklearn/inspection/_partial_dependence.py,85,feature has low resolution use unique vals,not
scikit-learn/sklearn/inspection/_partial_dependence.py,88,create axis based on percentiles and grid resolution,not
scikit-learn/sklearn/inspection/_partial_dependence.py,109,"reshape to (1, n_points) for consistency with",not
scikit-learn/sklearn/inspection/_partial_dependence.py,110,_partial_dependence_brute,not
scikit-learn/sklearn/inspection/_partial_dependence.py,119,"define the prediction_method (predict, predict_proba, decision_function).",not
scikit-learn/sklearn/inspection/_partial_dependence.py,126,"try predict_proba, then decision_function if it doesn't exist",not
scikit-learn/sklearn/inspection/_partial_dependence.py,157,Note: predictions is of shape,not
scikit-learn/sklearn/inspection/_partial_dependence.py,158,"(n_points,) for non-multioutput regressors",not
scikit-learn/sklearn/inspection/_partial_dependence.py,159,"(n_points, n_tasks) for multioutput regressors",not
scikit-learn/sklearn/inspection/_partial_dependence.py,160,"(n_points, 1) for the regressors in cross_decomposition (I think)",not
scikit-learn/sklearn/inspection/_partial_dependence.py,161,"(n_points, 2) for binary classification",not
scikit-learn/sklearn/inspection/_partial_dependence.py,162,"(n_points, n_classes) for multiclass classification",not
scikit-learn/sklearn/inspection/_partial_dependence.py,164,average over samples,not
scikit-learn/sklearn/inspection/_partial_dependence.py,167,"reshape to (n_targets, n_points) where n_targets is:",not
scikit-learn/sklearn/inspection/_partial_dependence.py,168,- 1 for non-multioutput regression and binary classification (shape is,not
scikit-learn/sklearn/inspection/_partial_dependence.py,169,already correct in those cases),not
scikit-learn/sklearn/inspection/_partial_dependence.py,170,- n_tasks for multi-output regression,not
scikit-learn/sklearn/inspection/_partial_dependence.py,171,- n_classes for multiclass classification.,not
scikit-learn/sklearn/inspection/_partial_dependence.py,174,"non-multioutput regression, shape is (n_points,)",not
scikit-learn/sklearn/inspection/_partial_dependence.py,177,"Binary classification, shape is (2, n_points).",not
scikit-learn/sklearn/inspection/_partial_dependence.py,178,we output the effect of **positive** class,not
scikit-learn/sklearn/inspection/_partial_dependence.py,313,TODO: to be removed if/when pipeline get a `steps_` attributes,SATD
scikit-learn/sklearn/inspection/_partial_dependence.py,314,assuming Pipeline is the only estimator that does not store a new,not
scikit-learn/sklearn/inspection/_partial_dependence.py,315,attribute,not
scikit-learn/sklearn/inspection/_partial_dependence.py,317,FIXME: remove the None option when it will be deprecated,SATD
scikit-learn/sklearn/inspection/_partial_dependence.py,329,Use check_array only on lists and other non-array-likes / sparse. Do not,not
scikit-learn/sklearn/inspection/_partial_dependence.py,330,convert DataFrame into a NumPy array.,not
scikit-learn/sklearn/inspection/_partial_dependence.py,390,"_get_column_indices() supports negative indexing. Here, we limit",not
scikit-learn/sklearn/inspection/_partial_dependence.py,391,the indexing to be positive. The upper bound will be checked,not
scikit-learn/sklearn/inspection/_partial_dependence.py,392,by _get_column_indices(),not
scikit-learn/sklearn/inspection/_partial_dependence.py,416,reshape averaged_predictions to,not
scikit-learn/sklearn/inspection/_partial_dependence.py,417,"(n_outputs, n_values_feature_0, n_values_feature_1, ...)",not
scikit-learn/sklearn/inspection/_permutation_importance.py,18,Work on a copy of X to to ensure thread-safety in case of threading based,not
scikit-learn/sklearn/inspection/_permutation_importance.py,19,"parallelism. Furthermore, making a copy is also useful when the joblib",not
scikit-learn/sklearn/inspection/_permutation_importance.py,20,"backend is 'loky' (default) or the old 'multiprocessing': in those cases,",not
scikit-learn/sklearn/inspection/_permutation_importance.py,21,if X is large it will be automatically be backed by a readonly memory map,not
scikit-learn/sklearn/inspection/_permutation_importance.py,22,(memmap). X.copy() on the other hand is always guaranteed to return a,not
scikit-learn/sklearn/inspection/_permutation_importance.py,23,writable data-structure whose columns can be shuffled inplace.,not
scikit-learn/sklearn/inspection/_permutation_importance.py,124,Precompute random seed from the random state to be used,not
scikit-learn/sklearn/inspection/_permutation_importance.py,125,to get a fresh independent RandomState instance for each,not
scikit-learn/sklearn/inspection/_permutation_importance.py,126,"parallel call to _calculate_permutation_scores, irrespective of",not
scikit-learn/sklearn/inspection/_permutation_importance.py,127,the fact that variables are shared or not depending on the active,not
scikit-learn/sklearn/inspection/_permutation_importance.py,128,"joblib backend (sequential, thread-based or process-based).",not
scikit-learn/sklearn/inspection/__init__.py,3,TODO: remove me in 0.24 (as well as the noqa markers) and,SATD
scikit-learn/sklearn/inspection/__init__.py,4,import the partial_dependence func directly from the,not
scikit-learn/sklearn/inspection/__init__.py,5,._partial_dependence module instead.,not
scikit-learn/sklearn/inspection/__init__.py,6,Pre-cache the import of the deprecated module so that import,not
scikit-learn/sklearn/inspection/__init__.py,7,sklearn.inspection.partial_dependence returns the function as in,not
scikit-learn/sklearn/inspection/__init__.py,8,"0.21, instead of the module",not
scikit-learn/sklearn/inspection/__init__.py,9,https://github.com/scikit-learn/scikit-learn/issues/15842,not
scikit-learn/sklearn/inspection/__init__.py,15,noqa,not
scikit-learn/sklearn/inspection/__init__.py,17,noqa,not
scikit-learn/sklearn/inspection/__init__.py,18,noqa,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,18,noqa,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,45,toy sample,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,50,"(X, y), n_targets  <-- as expected in the output of partial_dep()",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,61,iris,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,83,Check that partial_dependence has consistent output shape for different,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,84,kinds of estimators:,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,85,- classifiers with binary and multiclass settings,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,86,- regressors,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,87,- multi-task regressors,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,91,n_target corresponds to the number of classes (1 for binary classif) or,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,92,the number of tasks / outputs in multi task settings. It's equal to 1 for,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,93,classical regression_data.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,111,"tests for _grid_from_X: sanity check for output, and for shapes.",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,113,Make sure that the grid is a cartesian product of the input (it will use,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,114,the unique values instead of the percentiles),not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,126,test shapes of returned objects depending on the number of unique values,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,127,for a feature.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,131,n_unique_values > grid_resolution,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,137,"n_unique_values < grid_resolution, will use actual values",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,140,just to make sure the order is irrelevant,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,143,axes is a list of arrays of different shapes,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,175,Check that what is returned by _partial_dependence_brute or,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,176,_partial_dependence_recursion is equivalent to manually setting a target,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,177,"feature to a given value, and computing the average prediction over all",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,178,samples.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,179,This also checks that the brute and recursion methods give the same,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,180,output.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,181,"Note that even on the trainset, the brute and the recursion methods",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,182,"aren't always strictly equivalent, in particular when the slow method",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,183,generates unrealistic samples that have low mass in the joint,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,184,"distribution of the input features, and when some of the features are",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,185,dependent. Hence the high tolerance on the checks.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,188,The 'init' estimator for GBDT (here the average prediction) isn't taken,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,189,"into account with the recursion method, for technical reasons. We set",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,190,the mean to 0 to that this 'bug' doesn't have any effect.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,194,target feature will be set to .5 and then to 123,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,211,"(shape is (1, 2) so make it (2,))",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,213,allow for greater margin for error with recursion method,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,220,Make sure that the recursion method gives the same results on a,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,221,DecisionTreeRegressor and a GradientBoostingRegressor or a,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,222,RandomForestRegressor with 1 tree and equivalent parameters.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,226,Purely random dataset to avoid correlated features,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,232,The 'init' estimator for GBDT (here the average prediction) isn't taken,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,233,"into account with the recursion method, for technical reasons. We set",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,234,the mean to 0 to that this 'bug' doesn't have any effect.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,237,set max_depth not too high to avoid splits with same gain but different,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,238,features,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,245,The forest will use ensemble.base._set_random_states to set the,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,246,random_state of the tree sub-estimator. We simulate this here to have,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,247,equivalent estimators.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,260,"sanity check: if the trees aren't the same, the PD values won't be equal",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,265,"For some reason the trees aren't exactly equal on 32bits, so the PDs",not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,266,cannot be equal either. See,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,267,https://github.com/scikit-learn/scikit-learn/issues/8853,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,289,Make sure the recursion method (implicitly uses decision_function) has,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,290,the same result as using brute method with,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,291,response_method=decision_function,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,295,make sure the init estimator predicts 0 anyway,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,318,If the target y only depends on one feature in an obvious way (linear or,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,319,quadratic) then the partial dependence for that feature should reflect,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,320,it.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,321,We here fit a linear regression_data model (with polynomial features if,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,322,needed) and compute r_squared to check that the partial dependence,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,323,correctly reflects the target.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,338,add polynomial features if needed,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,355,Make sure error is raised for multiclass-multioutput classifiers,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,357,make multiclass-multioutput dataset,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,373,simulate that we have some classes,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,471,check that array-like objects are accepted,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,477,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,SATD
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,480,make sure that passing a non-constant init parameter to a GBDT and using,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,481,recursion method yields a warning.,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,498,Test near perfect correlation between partial dependence and diagonal,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,499,when sample weights emphasize y = x predictions,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,500,non-regression test for #13193,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,501,TODO: extend to HistGradientBoosting once sample_weight is supported,SATD
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,507,set y = x on mask and y = -x outside,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,511,sample weights to emphasize data points where y = x,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,524,TODO: remove/fix when PDP supports HGBT with sample weights,SATD
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,533,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,SATD
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,536,check that the partial dependence support pipeline,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,584,check that the partial dependence support dataframe and pipeline,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,585,including a column transformer,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,595,the column transformer will reorder the column when transforming,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,596,we mixed the index to be sure that we are computing the partial,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,597,dependence of the right columns,not
scikit-learn/sklearn/inspection/tests/test_partial_dependence.py,631,check all possible features type supported in PDP,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,31,Make sure that feature highly correlated to the target have a higher,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,32,importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,50,the correlated feature with y was added as the last column and should,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,51,have the highest importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,60,Make sure that feature highly correlated to the target have a higher,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,61,importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,70,Adds feature correlated with y as the last column,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,82,the correlated feature with y was added as the last column and should,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,83,have the highest importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,89,Permutation variable importance should not be affected by the high,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,90,"cardinality bias of traditional feature importances, especially when",not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,91,computed on a held-out test set:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,100,Generate a multiclass classification dataset and a set of informative,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,101,binary features that can be used to predict some classes of y exactly,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,102,while leaving some classes unexplained to make the problem harder.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,109,Not all target classes are explained by the binary class indicator,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,110,features:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,113,Add 10 other noisy features with high cardinality (numerical) values,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,114,that can be used to overfit the training data.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,118,Split the dataset to be able to evaluate on a held-out test set. The,SATD
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,119,Test size should be large enough for importance measurements to be,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,120,stable:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,126,Variable importances computed by impurity decrease on the tree node,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,127,splits often use the noisy features in splits. This can give misleading,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,128,impression that high cardinality noisy variables are the most important:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,134,Let's check that permutation-based feature importances do not have this,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,135,problem.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,141,Split the importances between informative and noisy features,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,145,"Because we do not have a binary variable explaining each target classes,",not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,146,the RF model will have to use the random variable to make some,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,147,(overfitting) splits (as max_depth is not set). Therefore the noisy,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,148,variables will be non-zero but with small values oscillating around,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,149,zero:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,153,The binary features correlated with y should have a higher importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,154,than the high cardinality noisy features.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,155,"The maximum test accuracy is 2 / 5 == 0.4, each informative feature",not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,156,contributing approximately a bit more than 0.2 of accuracy.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,164,Last column is correlated with y,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,175,the correlated feature with y is the last column and should,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,176,have the highest importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,179,use another random state,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,187,the correlated feature with y is the last column and should,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,188,have the highest importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,197,Last column is correlated with y,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,214,the correlated feature with y is the last column and should,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,215,have the highest importance,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,227,this relationship can be computed in closed form,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,237,regression test to make sure that sequential and parallel calls will,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,238,output the same results.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,246,First check that the problem is structured enough and that the model is,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,247,"complex enough to not yield trivial, constant importances:",not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,252,The actually check that parallelism does not impact the results,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,253,either with shared memory (threading) or without isolated memory,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,254,via process-based parallelism using the default backend,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,255,('loky' or 'multiprocessing') depending on the joblib version:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,257,process-based parallelism (by default):,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,265,thread-based parallelism:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,278,This test checks that the column shuffling logic has the same behavior,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,279,both a dataframe and a simple numpy array.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,282,regression test to make sure that sequential and parallel calls will,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,283,output the same results.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,287,Add a categorical feature that is statistically linked to y:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,291,Concatenate the extra column to the numpy array: integers will be,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,292,cast to float values,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,296,Insert extra column as a non-numpy-native dtype (while keeping backward,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,297,compat for old pandas versions):,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,306,Stich an aribtrary index to the dataframe:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,317,First check that the problem is structured enough and that the model is,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,318,"complex enough to not yield trivial, constant importances:",not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,323,Now check that importances computed on dataframe matche the values,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,324,of those computed on the array with the same data.,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,336,"Smoke, non-regression test for:",not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,337,https://github.com/scikit-learn/scikit-learn/issues/15810,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,341,trigger joblib memmaping,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,346,Actual smoke test: should not raise any error:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,350,Auxiliary check: DummyClassifier is feature independent:,not
scikit-learn/sklearn/inspection/tests/test_permutation_importance.py,351,permutating feature should not change the predictions,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,14,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,201,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,202,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,203,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,204,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,205,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,207,set target_idx for multi-class estimators,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,217,regression and binary classification,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,220,Use check_array only on lists and other non-array-likes / sparse. Do not,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,221,convert DataFrame into a NumPy array.,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,226,convert feature_names to list,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,229,get the column names for a pandas dataframe,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,232,define a list of numbered indices for a numpy array,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,235,convert numpy array or pandas index to a list,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,248,convert features into a seq of int tuples,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,266,Early exit if the axes does not have the correct number of axes,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,279,compute averaged predictions,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,288,"For multioutput regression, we can only check the validity of target",not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,289,now that we have the predictions.,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,290,"Also note: as multiclass-multioutput classifiers are not supported,",not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,291,multiclass and multioutput scenario are mutually exclusive. So there is,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,292,no risk of overwriting target_idx here.,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,293,checking the first result is enough,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,303,get global min and max average predictions of PD grouped by plot type,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,473,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,474,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,475,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,476,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,477,noqa,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,493,"If ax was set off, it has most likely been set to off",not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,494,by a previous call to plot.,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,516,array-like,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,531,create contour levels for two-way plots,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,539,Create 1d views of these 2d arrays for easy indexing,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,554,contour plot,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,573,Set xlabel if it is not already set,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,584,contour plot,not
scikit-learn/sklearn/inspection/_plot/partial_dependence.py,590,hline erases xlim,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,18,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,SATD
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,38,Test partial dependence plot function.,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,61,"deciles lines: always show on xaxis, only show on yaxis if 2-way PDP",not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,95,two feature position,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,130,check with str features and array feature names and single column,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,154,line,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,168,contour,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,203,contour,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,262,with axes object,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,268,The first call to plot_partial_dependence will create two new axes to,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,269,"place in the space of the passed in axes, which results in a total of",not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,270,three axes in the figure.,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,271,Currently the API does not allow for the second call to,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,272,"plot_partial_dependence to use the same axes again, because it will",not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,273,create two new axes in the space resulting in five axes. To get the,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,274,expected behavior one needs to pass the generated axes into the second,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,275,call:,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,276,disp1 = plot_partial_dependence(...),not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,277,"disp2 = plot_partial_dependence(..., ax=disp1.axes_)",not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,297,second call to plot does not change the feature names from the first,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,298,call,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,318,Test partial dependence plot function on multi-class input.,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,332,now with symbol labels,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,355,check that the pd plots are different for another target,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,370,Test partial dependence plot function on multi-output input.,not
scikit-learn/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py,460,Make sure fig object is correctly used if not None,not
scikit-learn/sklearn/cluster/_birch.py,1,Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com>,not
scikit-learn/sklearn/cluster/_birch.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/cluster/_birch.py,3,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/cluster/_birch.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_birch.py,145,"The list of subclusters, centroids and squared norms",not
scikit-learn/sklearn/cluster/_birch.py,146,to manipulate throughout.,not
scikit-learn/sklearn/cluster/_birch.py,160,Keep centroids and squared norm as views. In this way,not
scikit-learn/sklearn/cluster/_birch.py,161,"if we change init_centroids and init_sq_norm_, it is",not
scikit-learn/sklearn/cluster/_birch.py,162,"sufficient,",not
scikit-learn/sklearn/cluster/_birch.py,185,We need to find the closest subcluster among all the,not
scikit-learn/sklearn/cluster/_birch.py,186,subclusters so that we can insert our new subcluster.,not
scikit-learn/sklearn/cluster/_birch.py,193,"If the subcluster has a child, we need a recursive strategy.",not
scikit-learn/sklearn/cluster/_birch.py,199,"If it is determined that the child need not be split, we",not
scikit-learn/sklearn/cluster/_birch.py,200,can just update the closest_subcluster,not
scikit-learn/sklearn/cluster/_birch.py,208,things not too good. we need to redistribute the subclusters in,not
scikit-learn/sklearn/cluster/_birch.py,209,"our child node, and add a new subcluster in the parent",not
scikit-learn/sklearn/cluster/_birch.py,210,subcluster to accommodate the new child.,not
scikit-learn/sklearn/cluster/_birch.py,221,good to go!,not
scikit-learn/sklearn/cluster/_birch.py,232,"not close to any other subclusters, and we still",not
scikit-learn/sklearn/cluster/_birch.py,233,"have space, so add.",not
scikit-learn/sklearn/cluster/_birch.py,238,We do not have enough space nor is it closer to an,not
scikit-learn/sklearn/cluster/_birch.py,239,other subcluster. We need to split.,not
scikit-learn/sklearn/cluster/_birch.py,474,"If partial_fit is called for the first time or fit is called, we",not
scikit-learn/sklearn/cluster/_birch.py,475,start a new tree.,not
scikit-learn/sklearn/cluster/_birch.py,479,The first root is the leaf. Manipulate this object throughout.,not
scikit-learn/sklearn/cluster/_birch.py,485,To enable getting back subclusters.,not
scikit-learn/sklearn/cluster/_birch.py,492,Cannot vectorize. Enough to convince to use cython.,not
scikit-learn/sklearn/cluster/_birch.py,557,Perform just the final global clustering step.,not
scikit-learn/sklearn/cluster/_birch.py,626,Preprocessing for the global clustering.,not
scikit-learn/sklearn/cluster/_birch.py,631,There is no need to perform the global clustering step.,not
scikit-learn/sklearn/cluster/_birch.py,639,To use in predict to avoid recalculation.,not
scikit-learn/sklearn/cluster/_birch.py,651,The global clustering step that clusters the subclusters of,not
scikit-learn/sklearn/cluster/_birch.py,652,the leaves. It assumes the centroids of the subclusters as,not
scikit-learn/sklearn/cluster/_birch.py,653,samples and finds the final centroids.,not
scikit-learn/sklearn/cluster/_agglomerative.py,24,mypy error: Module 'sklearn.cluster' has no attribute '_hierarchical_fast',not
scikit-learn/sklearn/cluster/_agglomerative.py,25,type: ignore,not
scikit-learn/sklearn/cluster/_agglomerative.py,31,,not
scikit-learn/sklearn/cluster/_agglomerative.py,32,For non fully-connected graphs,not
scikit-learn/sklearn/cluster/_agglomerative.py,49,Make the connectivity matrix symmetric:,not
scikit-learn/sklearn/cluster/_agglomerative.py,52,Convert connectivity matrix to LIL,not
scikit-learn/sklearn/cluster/_agglomerative.py,59,Compute the number of nodes,not
scikit-learn/sklearn/cluster/_agglomerative.py,67,XXX: Can we do without completing the matrix?,SATD
scikit-learn/sklearn/cluster/_agglomerative.py,93,explicitly cast connectivity to ensure safety,not
scikit-learn/sklearn/cluster/_agglomerative.py,97,"Ensure zero distances aren't ignored by setting them to ""epsilon""",not
scikit-learn/sklearn/cluster/_agglomerative.py,101,Use scipy.sparse.csgraph to generate a minimum spanning tree,not
scikit-learn/sklearn/cluster/_agglomerative.py,104,Convert the graph to scipy.cluster.hierarchy array format,not
scikit-learn/sklearn/cluster/_agglomerative.py,107,Undo the epsilon values,not
scikit-learn/sklearn/cluster/_agglomerative.py,112,Sort edges of the min_spanning_tree by weight,not
scikit-learn/sklearn/cluster/_agglomerative.py,115,Convert edge list into standard hierarchical clustering format,not
scikit-learn/sklearn/cluster/_agglomerative.py,119,Compute parents,not
scikit-learn/sklearn/cluster/_agglomerative.py,135,,not
scikit-learn/sklearn/cluster/_agglomerative.py,136,Hierarchical tree building functions,not
scikit-learn/sklearn/cluster/_agglomerative.py,225,imports PIL,not
scikit-learn/sklearn/cluster/_agglomerative.py,257,create inertia matrix,not
scikit-learn/sklearn/cluster/_agglomerative.py,263,We keep only the upper triangular for the moments,not
scikit-learn/sklearn/cluster/_agglomerative.py,264,Generator expressions are faster than arrays on the following,not
scikit-learn/sklearn/cluster/_agglomerative.py,272,build moments as a list,not
scikit-learn/sklearn/cluster/_agglomerative.py,283,prepare the main fields,not
scikit-learn/sklearn/cluster/_agglomerative.py,292,recursive merge loop,not
scikit-learn/sklearn/cluster/_agglomerative.py,294,identify the merge,not
scikit-learn/sklearn/cluster/_agglomerative.py,302,store inertia value,not
scikit-learn/sklearn/cluster/_agglomerative.py,305,update the moments,not
scikit-learn/sklearn/cluster/_agglomerative.py,309,update the structure matrix A and the inertia matrix,not
scikit-learn/sklearn/cluster/_agglomerative.py,315,List comprehension is faster than a for loop,not
scikit-learn/sklearn/cluster/_agglomerative.py,327,List comprehension is faster than a for loop,not
scikit-learn/sklearn/cluster/_agglomerative.py,331,Separate leaves in children (empty lists up to now),not
scikit-learn/sklearn/cluster/_agglomerative.py,333,sort children to get consistent output with unstructured version,not
scikit-learn/sklearn/cluster/_agglomerative.py,335,return numpy array for efficient caching,not
scikit-learn/sklearn/cluster/_agglomerative.py,338,2 is scaling factor to compare w/ unstructured version,not
scikit-learn/sklearn/cluster/_agglomerative.py,345,single average and complete linkage,not
scikit-learn/sklearn/cluster/_agglomerative.py,430,Single linkage is handled differently,not
scikit-learn/sklearn/cluster/_agglomerative.py,443,imports PIL,not
scikit-learn/sklearn/cluster/_agglomerative.py,455,for the linkage function of hierarchy to work on precomputed,not
scikit-learn/sklearn/cluster/_agglomerative.py,456,"data, provide as first argument an ndarray of the shape returned",not
scikit-learn/sklearn/cluster/_agglomerative.py,457,by sklearn.metrics.pairwise_distances.,not
scikit-learn/sklearn/cluster/_agglomerative.py,466,Translate to something understood by scipy,not
scikit-learn/sklearn/cluster/_agglomerative.py,479,We need the fast cythonized metric from neighbors,not
scikit-learn/sklearn/cluster/_agglomerative.py,482,The Cython routines used require contiguous arrays,not
scikit-learn/sklearn/cluster/_agglomerative.py,486,Sort edges of the min_spanning_tree by weight,not
scikit-learn/sklearn/cluster/_agglomerative.py,489,Convert edge list into standard hierarchical clustering format,not
scikit-learn/sklearn/cluster/_agglomerative.py,504,Put the diagonal to zero,not
scikit-learn/sklearn/cluster/_agglomerative.py,515,"FIXME We compute all the distances, while we could have only computed",SATD
scikit-learn/sklearn/cluster/_agglomerative.py,516,"the ""interesting"" distances",not
scikit-learn/sklearn/cluster/_agglomerative.py,535,create inertia heap and connection matrix,not
scikit-learn/sklearn/cluster/_agglomerative.py,539,"LIL seems to the best format to access the rows quickly,",SATD
scikit-learn/sklearn/cluster/_agglomerative.py,540,without the numpy overhead of slicing CSR indices and data.,not
scikit-learn/sklearn/cluster/_agglomerative.py,542,We are storing the graph in a list of IntFloatDict,not
scikit-learn/sklearn/cluster/_agglomerative.py,547,We keep only the upper triangular for the heap,not
scikit-learn/sklearn/cluster/_agglomerative.py,548,Generator expressions are faster than arrays on the following,not
scikit-learn/sklearn/cluster/_agglomerative.py,555,prepare the main fields,not
scikit-learn/sklearn/cluster/_agglomerative.py,560,recursive merge loop,not
scikit-learn/sklearn/cluster/_agglomerative.py,562,identify the merge,not
scikit-learn/sklearn/cluster/_agglomerative.py,571,store distances,not
scikit-learn/sklearn/cluster/_agglomerative.py,576,Keep track of the number of elements per cluster,not
scikit-learn/sklearn/cluster/_agglomerative.py,582,update the structure matrix A and the inertia matrix,not
scikit-learn/sklearn/cluster/_agglomerative.py,583,"a clever 'min', or 'max' operation between A[i] and A[j]",not
scikit-learn/sklearn/cluster/_agglomerative.py,587,Here we use the information from coord_col (containing the,not
scikit-learn/sklearn/cluster/_agglomerative.py,588,distances) to update the heap,not
scikit-learn/sklearn/cluster/_agglomerative.py,591,Clear A[i] and A[j] to save memory,not
scikit-learn/sklearn/cluster/_agglomerative.py,594,Separate leaves in children (empty lists up to now),not
scikit-learn/sklearn/cluster/_agglomerative.py,597,# return numpy array for efficient caching,not
scikit-learn/sklearn/cluster/_agglomerative.py,605,Matching names to tree-building strategies,not
scikit-learn/sklearn/cluster/_agglomerative.py,628,,not
scikit-learn/sklearn/cluster/_agglomerative.py,629,Functions for cutting hierarchical clustering tree,not
scikit-learn/sklearn/cluster/_agglomerative.py,660,"In this function, we store nodes as a heap to avoid recomputing",not
scikit-learn/sklearn/cluster/_agglomerative.py,661,the max of the nodes: the first element is always the smallest,not
scikit-learn/sklearn/cluster/_agglomerative.py,662,"We use negated indices as heaps work on smallest elements, and we",not
scikit-learn/sklearn/cluster/_agglomerative.py,663,are interested in largest elements,not
scikit-learn/sklearn/cluster/_agglomerative.py,664,children[-1] is the root of the tree,not
scikit-learn/sklearn/cluster/_agglomerative.py,667,"As we have a heap, nodes[0] is the smallest element",not
scikit-learn/sklearn/cluster/_agglomerative.py,669,Insert the 2 children and remove the largest node,not
scikit-learn/sklearn/cluster/_agglomerative.py,678,,not
scikit-learn/sklearn/cluster/_agglomerative.py,862,Early stopping is likely to give a speed up only for,not
scikit-learn/sklearn/cluster/_agglomerative.py,863,a large number of clusters. The actual threshold,not
scikit-learn/sklearn/cluster/_agglomerative.py,864,implemented here is heuristic,not
scikit-learn/sklearn/cluster/_agglomerative.py,870,Construct the tree,not
scikit-learn/sklearn/cluster/_agglomerative.py,895,Cut the tree,not
scikit-learn/sklearn/cluster/_agglomerative.py,901,copy to avoid holding a reference on the original array,not
scikit-learn/sklearn/cluster/_agglomerative.py,903,Reassign cluster numbers,not
scikit-learn/sklearn/cluster/_agglomerative.py,1070,"save n_features_in_ attribute here to reset it after, because it will",not
scikit-learn/sklearn/cluster/_agglomerative.py,1071,be overridden in AgglomerativeClustering since we passed it X.T.,not
scikit-learn/sklearn/cluster/_feature_agglomeration.py,5,"Author: V. Michel, A. Gramfort",not
scikit-learn/sklearn/cluster/_feature_agglomeration.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_feature_agglomeration.py,15,,not
scikit-learn/sklearn/cluster/_feature_agglomeration.py,16,Mixin class for feature agglomeration.,not
scikit-learn/sklearn/cluster/_feature_agglomeration.py,48,a fast way to compute the mean of grouped features,not
scikit-learn/sklearn/cluster/_bicluster.py,2,Authors : Kemal Eren,not
scikit-learn/sklearn/cluster/_bicluster.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_bicluster.py,56,"According to paper, this can also be done more efficiently with",not
scikit-learn/sklearn/cluster/_bicluster.py,57,deviation reduction and balancing algorithms.,not
scikit-learn/sklearn/cluster/_bicluster.py,144,"some eigenvalues of A * A.T are negative, causing",not
scikit-learn/sklearn/cluster/_bicluster.py,145,sqrt() to be np.nan. This causes some vectors in vt,not
scikit-learn/sklearn/cluster/_bicluster.py,146,to be np.nan.,not
scikit-learn/sklearn/cluster/_bicluster.py,149,"initialize with [-1,1] as in ARPACK",not
scikit-learn/sklearn/cluster/_bicluster.py,156,"initialize with [-1,1] as in ARPACK",not
scikit-learn/sklearn/cluster/_dbscan.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/cluster/_dbscan.py,6,Author: Robert Layton <robertlayton@gmail.com>,not
scikit-learn/sklearn/cluster/_dbscan.py,7,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/cluster/_dbscan.py,8,Lars Buitinck,not
scikit-learn/sklearn/cluster/_dbscan.py,9,,not
scikit-learn/sklearn/cluster/_dbscan.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_dbscan.py,319,Calculate neighborhood for all samples. This leaves the original,not
scikit-learn/sklearn/cluster/_dbscan.py,320,"point in, which needs to be considered later (i.e. point i is in the",not
scikit-learn/sklearn/cluster/_dbscan.py,321,"neighborhood of point i. While True, its useless information)",not
scikit-learn/sklearn/cluster/_dbscan.py,323,"set the diagonal to explicit values, as a point is its own",not
scikit-learn/sklearn/cluster/_dbscan.py,324,neighbor,not
scikit-learn/sklearn/cluster/_dbscan.py,327,XXX: modifies X's internals in-place,SATD
scikit-learn/sklearn/cluster/_dbscan.py,334,This has worst case O(n^2) memory complexity,not
scikit-learn/sklearn/cluster/_dbscan.py,345,"Initially, all samples are noise.",not
scikit-learn/sklearn/cluster/_dbscan.py,348,A list of all core samples found.,not
scikit-learn/sklearn/cluster/_dbscan.py,357,fix for scipy sparse indexing issue,not
scikit-learn/sklearn/cluster/_dbscan.py,360,no core samples,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,3,Author: Alexandre Gramfort alexandre.gramfort@inria.fr,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,4,Gael Varoquaux gael.varoquaux@normalesup.org,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,24,Create mask to ignore diagonal of S,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,131,"It makes no sense to run the algorithm in this case, so return 1 or",not
scikit-learn/sklearn/cluster/_affinity_propagation.py,132,"n_samples clusters, depending on preferences",not
scikit-learn/sklearn/cluster/_affinity_propagation.py,155,Place preference on the diagonal of S,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,159,Initialize messages,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,160,Intermediate results,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,163,Remove degeneracies,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,167,Execute parallel affinity propagation updates,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,173,tmp = A + S; compute responsibilities,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,176,"np.max(A + S, axis=1)",not
scikit-learn/sklearn/cluster/_affinity_propagation.py,180,tmp = Rnew,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,184,Damping,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,189,tmp = Rp; compute availabilities,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,193,tmp = -Anew,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,199,Damping,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,204,Check for convergence,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,224,Identify exemplars,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,228,Identify clusters,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,229,Refine the final set of exemplars and clusters and return results,not
scikit-learn/sklearn/cluster/_affinity_propagation.py,238,"Reduce labels to a sorted, gapless, list",not
scikit-learn/sklearn/cluster/_affinity_propagation.py,253,,not
scikit-learn/sklearn/cluster/_mean_shift.py,12,Authors: Conrad Lee <conradlee@gmail.com>,not
scikit-learn/sklearn/cluster/_mean_shift.py,13,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/cluster/_mean_shift.py,14,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/cluster/_mean_shift.py,15,Martino Sorbaro <martino.sorbaro@ed.ac.uk>,not
scikit-learn/sklearn/cluster/_mean_shift.py,72,cannot fit NearestNeighbors with n_neighbors = 0,not
scikit-learn/sklearn/cluster/_mean_shift.py,86,separate function for each seed's iterative loop,not
scikit-learn/sklearn/cluster/_mean_shift.py,88,"For each seed, climb gradient until convergence or max_iter",not
scikit-learn/sklearn/cluster/_mean_shift.py,90,when mean has converged,not
scikit-learn/sklearn/cluster/_mean_shift.py,93,Find mean of points within bandwidth,not
scikit-learn/sklearn/cluster/_mean_shift.py,98,Depending on seeding strategy this condition may occur,not
scikit-learn/sklearn/cluster/_mean_shift.py,99,save the old mean,not
scikit-learn/sklearn/cluster/_mean_shift.py,101,"If converged or at max_iter, adds the cluster",not
scikit-learn/sklearn/cluster/_mean_shift.py,220,Bin points,not
scikit-learn/sklearn/cluster/_mean_shift.py,226,Select only those bins as seeds which have enough members,not
scikit-learn/sklearn/cluster/_mean_shift.py,388,We use n_jobs=1 because this will be used in nested calls under,not
scikit-learn/sklearn/cluster/_mean_shift.py,389,parallel calls to _mean_shift_single_seed so there is no need for,not
scikit-learn/sklearn/cluster/_mean_shift.py,390,for further parallelism.,not
scikit-learn/sklearn/cluster/_mean_shift.py,393,execute iterations on all seeds in parallel,not
scikit-learn/sklearn/cluster/_mean_shift.py,397,copy results in a dictionary,not
scikit-learn/sklearn/cluster/_mean_shift.py,399,i.e. len(points_within) > 0,not
scikit-learn/sklearn/cluster/_mean_shift.py,405,nothing near seeds,not
scikit-learn/sklearn/cluster/_mean_shift.py,411,POST PROCESSING: remove near duplicate points,not
scikit-learn/sklearn/cluster/_mean_shift.py,412,"If the distance between two kernels is less than the bandwidth,",not
scikit-learn/sklearn/cluster/_mean_shift.py,413,then we have to remove one because it is a duplicate. Remove the,not
scikit-learn/sklearn/cluster/_mean_shift.py,414,one with fewer points.,not
scikit-learn/sklearn/cluster/_mean_shift.py,428,leave the current point as unique,not
scikit-learn/sklearn/cluster/_mean_shift.py,431,ASSIGN LABELS: a point belongs to the cluster that it is closest to,not
scikit-learn/sklearn/cluster/_spectral.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/cluster/_spectral.py,4,Author: Gael Varoquaux gael.varoquaux@normalesup.org,not
scikit-learn/sklearn/cluster/_spectral.py,5,Brian Cheung,not
scikit-learn/sklearn/cluster/_spectral.py,6,Wei LI <kuantkid@gmail.com>,not
scikit-learn/sklearn/cluster/_spectral.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_spectral.py,84,Normalize the eigenvectors to an equal length of a vector of ones.,not
scikit-learn/sklearn/cluster/_spectral.py,85,Reorient the eigenvectors to point in the negative direction with respect,not
scikit-learn/sklearn/cluster/_spectral.py,86,to the first element.  This may have to do with constraining the,not
scikit-learn/sklearn/cluster/_spectral.py,87,eigenvectors to lie in a specific quadrant to make the discretization,not
scikit-learn/sklearn/cluster/_spectral.py,88,search easier.,not
scikit-learn/sklearn/cluster/_spectral.py,96,Normalize the rows of the eigenvectors.  Samples should lie on the unit,not
scikit-learn/sklearn/cluster/_spectral.py,97,hypersphere centered at the origin.  This transforms the samples in the,not
scikit-learn/sklearn/cluster/_spectral.py,98,embedding space to the space of partition matrices.,not
scikit-learn/sklearn/cluster/_spectral.py,104,If there is an exception we try to randomize and rerun SVD again,not
scikit-learn/sklearn/cluster/_spectral.py,105,do this max_svd_restarts times.,not
scikit-learn/sklearn/cluster/_spectral.py,108,Initialize first column of rotation matrix with a row of the,not
scikit-learn/sklearn/cluster/_spectral.py,109,eigenvectors,not
scikit-learn/sklearn/cluster/_spectral.py,113,"To initialize the rest of the rotation matrix, find the rows",not
scikit-learn/sklearn/cluster/_spectral.py,114,of the eigenvectors that are as orthogonal to each other as,not
scikit-learn/sklearn/cluster/_spectral.py,115,possible,not
scikit-learn/sklearn/cluster/_spectral.py,118,Accumulate c to ensure row is as orthogonal as possible to,not
scikit-learn/sklearn/cluster/_spectral.py,119,previous picks as well as current one,not
scikit-learn/sklearn/cluster/_spectral.py,150,otherwise calculate rotation and continue,not
scikit-learn/sklearn/cluster/_spectral.py,258,The first eigen vector is constant only for fully connected graphs,not
scikit-learn/sklearn/cluster/_spectral.py,259,and should be kept for spectral clustering (drop_first = False),not
scikit-learn/sklearn/cluster/_spectral.py,260,See spectral_embedding documentation.,not
scikit-learn/sklearn/cluster/setup.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/cluster/setup.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_kmeans.py,3,Authors: Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/cluster/_kmeans.py,4,Thomas Rueckstiess <ruecksti@in.tum.de>,not
scikit-learn/sklearn/cluster/_kmeans.py,5,James Bergstra <james.bergstra@umontreal.ca>,not
scikit-learn/sklearn/cluster/_kmeans.py,6,Jan Schlueter <scikit-learn@jan-schlueter.de>,not
scikit-learn/sklearn/cluster/_kmeans.py,7,Nelle Varoquaux,not
scikit-learn/sklearn/cluster/_kmeans.py,8,Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/sklearn/cluster/_kmeans.py,9,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/cluster/_kmeans.py,10,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/cluster/_kmeans.py,11,Robert Layton <robertlayton@gmail.com>,not
scikit-learn/sklearn/cluster/_kmeans.py,12,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/_kmeans.py,42,,not
scikit-learn/sklearn/cluster/_kmeans.py,43,Initialization heuristic,not
scikit-learn/sklearn/cluster/_kmeans.py,87,Set the number of local seeding trials if none is given,not
scikit-learn/sklearn/cluster/_kmeans.py,89,"This is what Arthur/Vassilvitskii tried, but did not report",not
scikit-learn/sklearn/cluster/_kmeans.py,90,specific results for other than mentioning in the conclusion,not
scikit-learn/sklearn/cluster/_kmeans.py,91,that it helped.,not
scikit-learn/sklearn/cluster/_kmeans.py,94,Pick first center randomly,not
scikit-learn/sklearn/cluster/_kmeans.py,101,Initialize list of closest distances and calculate current potential,not
scikit-learn/sklearn/cluster/_kmeans.py,107,Pick the remaining n_clusters-1 points,not
scikit-learn/sklearn/cluster/_kmeans.py,109,Choose center candidates by sampling with probability proportional,not
scikit-learn/sklearn/cluster/_kmeans.py,110,to the squared distance to the closest existing center,not
scikit-learn/sklearn/cluster/_kmeans.py,114,XXX: numerical imprecision can result in a candidate_id out of range,SATD
scikit-learn/sklearn/cluster/_kmeans.py,118,Compute distances to center candidates,not
scikit-learn/sklearn/cluster/_kmeans.py,122,update closest distances squared and potential for each candidate,not
scikit-learn/sklearn/cluster/_kmeans.py,127,Decide which candidate is the best,not
scikit-learn/sklearn/cluster/_kmeans.py,133,Permanently add best center candidate found in local tries,not
scikit-learn/sklearn/cluster/_kmeans.py,142,,not
scikit-learn/sklearn/cluster/_kmeans.py,143,K-means batch estimation by EM (expectation maximization),not
scikit-learn/sklearn/cluster/_kmeans.py,176,normalize the weights to sum up to n_samples,not
scikit-learn/sklearn/cluster/_kmeans.py,177,an array of 1 (i.e. samples_weight is None) is already normalized,not
scikit-learn/sklearn/cluster/_kmeans.py,402,init,not
scikit-learn/sklearn/cluster/_kmeans.py,438,compute new pairwise distances between centers and closest other,not
scikit-learn/sklearn/cluster/_kmeans.py,439,center of each center for next iterations,not
scikit-learn/sklearn/cluster/_kmeans.py,459,rerun E-step so that predicted labels match cluster centers,not
scikit-learn/sklearn/cluster/_kmeans.py,548,init,not
scikit-learn/sklearn/cluster/_kmeans.py,586,rerun E-step so that predicted labels match cluster centers,not
scikit-learn/sklearn/cluster/_kmeans.py,721,ensure that the centers have the same dtype as X,not
scikit-learn/sklearn/cluster/_kmeans.py,722,this is a requirement of fused types of cython,not
scikit-learn/sklearn/cluster/_kmeans.py,989,verify that the number of samples given is larger than k,not
scikit-learn/sklearn/cluster/_kmeans.py,996,Validate init array,not
scikit-learn/sklearn/cluster/_kmeans.py,1009,subtract of mean of x for more accurate distance computations,not
scikit-learn/sklearn/cluster/_kmeans.py,1012,The copy was already done above,not
scikit-learn/sklearn/cluster/_kmeans.py,1018,precompute squared norms of data points,not
scikit-learn/sklearn/cluster/_kmeans.py,1040,seeds for the initializations of the kmeans runs.,not
scikit-learn/sklearn/cluster/_kmeans.py,1044,run a k-means once,not
scikit-learn/sklearn/cluster/_kmeans.py,1050,determine if these results are the best so far,not
scikit-learn/sklearn/cluster/_kmeans.py,1123,"Currently, this just skips a copy of the data if it is not in",not
scikit-learn/sklearn/cluster/_kmeans.py,1124,np.array or CSR format already.,not
scikit-learn/sklearn/cluster/_kmeans.py,1125,"XXX This skips _check_test_data, which may change the dtype;",SATD
scikit-learn/sklearn/cluster/_kmeans.py,1126,we should refactor the input validation.,not
scikit-learn/sklearn/cluster/_kmeans.py,1279,Perform label assignment to nearest centers,not
scikit-learn/sklearn/cluster/_kmeans.py,1285,Reassign clusters that have very low weight,not
scikit-learn/sklearn/cluster/_kmeans.py,1287,pick at most .5 * batch_size samples as new centers,not
scikit-learn/sklearn/cluster/_kmeans.py,1294,Pick new clusters amongst observations with uniform probability,not
scikit-learn/sklearn/cluster/_kmeans.py,1308,"reset counts of reassigned centers, but don't reset them too small",not
scikit-learn/sklearn/cluster/_kmeans.py,1309,to avoid instant reassignment. This is a pretty dirty hack as it,not
scikit-learn/sklearn/cluster/_kmeans.py,1310,also modifies the learning rates.,not
scikit-learn/sklearn/cluster/_kmeans.py,1313,implementation for the sparse CSR representation completely written in,not
scikit-learn/sklearn/cluster/_kmeans.py,1314,cython,not
scikit-learn/sklearn/cluster/_kmeans.py,1320,dense variant in mostly numpy (not as memory efficient though),not
scikit-learn/sklearn/cluster/_kmeans.py,1324,find points from minibatch that are assigned to this center,not
scikit-learn/sklearn/cluster/_kmeans.py,1332,inplace remove previous count scaling,not
scikit-learn/sklearn/cluster/_kmeans.py,1335,inplace sum with new points members of this cluster,not
scikit-learn/sklearn/cluster/_kmeans.py,1340,update the count statistics for this center,not
scikit-learn/sklearn/cluster/_kmeans.py,1343,inplace rescale to compute mean of all points (old and new),not
scikit-learn/sklearn/cluster/_kmeans.py,1344,Note: numpy >= 1.10 does not support '/=' for the following,not
scikit-learn/sklearn/cluster/_kmeans.py,1345,expression for a mixture of int and float (see numpy issue #6464),not
scikit-learn/sklearn/cluster/_kmeans.py,1348,update the squared diff if necessary,not
scikit-learn/sklearn/cluster/_kmeans.py,1360,Normalize inertia to be able to compare values when,not
scikit-learn/sklearn/cluster/_kmeans.py,1361,batch_size changes,not
scikit-learn/sklearn/cluster/_kmeans.py,1365,Compute an Exponentially Weighted Average of the squared,not
scikit-learn/sklearn/cluster/_kmeans.py,1366,diff to monitor the convergence while discarding,not
scikit-learn/sklearn/cluster/_kmeans.py,1367,minibatch-local stochastic variability:,not
scikit-learn/sklearn/cluster/_kmeans.py,1368,https://en.wikipedia.org/wiki/Moving_average,not
scikit-learn/sklearn/cluster/_kmeans.py,1380,Log progress to be able to monitor convergence,not
scikit-learn/sklearn/cluster/_kmeans.py,1389,Early stopping based on absolute tolerance on squared change of,not
scikit-learn/sklearn/cluster/_kmeans.py,1390,centers position (using EWA smoothing),not
scikit-learn/sklearn/cluster/_kmeans.py,1397,Early stopping heuristic due to lack of improvement on smoothed inertia,SATD
scikit-learn/sklearn/cluster/_kmeans.py,1414,update the convergence context to maintain state across successive calls:,not
scikit-learn/sklearn/cluster/_kmeans.py,1627,using tol-based early stopping needs the allocation of a,not
scikit-learn/sklearn/cluster/_kmeans.py,1628,dedicated before which can be expensive for high dim data:,not
scikit-learn/sklearn/cluster/_kmeans.py,1629,hence we allocate it outside of the main loop,not
scikit-learn/sklearn/cluster/_kmeans.py,1633,no need for the center buffer if tol-based early stopping is,not
scikit-learn/sklearn/cluster/_kmeans.py,1634,disabled,not
scikit-learn/sklearn/cluster/_kmeans.py,1653,perform several inits with random sub-sets,not
scikit-learn/sklearn/cluster/_kmeans.py,1661,TODO: once the `k_means` function works with sparse input we,SATD
scikit-learn/sklearn/cluster/_kmeans.py,1662,should refactor the following init to use it instead.,not
scikit-learn/sklearn/cluster/_kmeans.py,1664,Initialize the centers using only a fraction of the data as we,not
scikit-learn/sklearn/cluster/_kmeans.py,1665,expect n_samples to be very large when using MiniBatchKMeans,not
scikit-learn/sklearn/cluster/_kmeans.py,1672,Compute the label assignment on the init dataset,not
scikit-learn/sklearn/cluster/_kmeans.py,1679,Keep only the best cluster centers across independent inits on,SATD
scikit-learn/sklearn/cluster/_kmeans.py,1680,the common validation set,not
scikit-learn/sklearn/cluster/_kmeans.py,1692,Empty context to be used inplace by the convergence check routine,not
scikit-learn/sklearn/cluster/_kmeans.py,1695,Perform the iterative optimization until the final convergence,not
scikit-learn/sklearn/cluster/_kmeans.py,1696,criterion,not
scikit-learn/sklearn/cluster/_kmeans.py,1698,Sample a minibatch from the full dataset,not
scikit-learn/sklearn/cluster/_kmeans.py,1702,Perform the actual update step on the minibatch data,not
scikit-learn/sklearn/cluster/_kmeans.py,1708,Here we randomly choose whether to perform,not
scikit-learn/sklearn/cluster/_kmeans.py,1709,random reassignment: the choice is done as a function,not
scikit-learn/sklearn/cluster/_kmeans.py,1710,"of the iteration index, and the minimum number of",not
scikit-learn/sklearn/cluster/_kmeans.py,1711,"counts, in order to force this reassignment to happen",not
scikit-learn/sklearn/cluster/_kmeans.py,1712,every once in a while,not
scikit-learn/sklearn/cluster/_kmeans.py,1719,Monitor convergence and do early stopping if necessary,not
scikit-learn/sklearn/cluster/_kmeans.py,1803,this is the first call partial_fit on this object:,not
scikit-learn/sklearn/cluster/_kmeans.py,1804,initialize the cluster centers,not
scikit-learn/sklearn/cluster/_kmeans.py,1815,"The lower the minimum count is, the more we do random",not
scikit-learn/sklearn/cluster/_kmeans.py,1816,"reassignment, however, we don't want to do random",not
scikit-learn/sklearn/cluster/_kmeans.py,1817,"reassignment too often, to allow for building up counts",not
scikit-learn/sklearn/cluster/_kmeans.py,1822,Raise error if partial_fit called on data with different number,not
scikit-learn/sklearn/cluster/_kmeans.py,1823,of features.,not
scikit-learn/sklearn/cluster/_optics.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/cluster/_optics.py,261,Extract clusters from the calculated orders and reachability,not
scikit-learn/sklearn/cluster/_optics.py,304,OPTICS helper functions,not
scikit-learn/sklearn/cluster/_optics.py,453,Start all points as 'unprocessed',not
scikit-learn/sklearn/cluster/_optics.py,468,"Here we first do a kNN query for each point, this differs from",not
scikit-learn/sklearn/cluster/_optics.py,469,the original OPTICS that only used epsilon range queries.,not
scikit-learn/sklearn/cluster/_optics.py,470,TODO: handle working_memory somehow?,SATD
scikit-learn/sklearn/cluster/_optics.py,474,"OPTICS puts an upper limit on these, use inf for undefined.",not
scikit-learn/sklearn/cluster/_optics.py,477,Main OPTICS loop. Not parallelizable. The order that entries are,not
scikit-learn/sklearn/cluster/_optics.py,478,written to the 'ordering_' list is important!,not
scikit-learn/sklearn/cluster/_optics.py,479,"Note that this implementation is O(n^2) theoretically, but",not
scikit-learn/sklearn/cluster/_optics.py,480,supposedly with very low constant factors.,not
scikit-learn/sklearn/cluster/_optics.py,484,Choose next based on smallest reachability distance,not
scikit-learn/sklearn/cluster/_optics.py,485,"(And prefer smaller ids on ties, possibly np.inf!)",not
scikit-learn/sklearn/cluster/_optics.py,510,Assume that radius_neighbors is faster without distances,not
scikit-learn/sklearn/cluster/_optics.py,511,"and we don't need all distances, nevertheless, this means",not
scikit-learn/sklearn/cluster/_optics.py,512,we may be doing some work twice.,not
scikit-learn/sklearn/cluster/_optics.py,516,Getting indices of neighbors that have not been processed,not
scikit-learn/sklearn/cluster/_optics.py,518,Neighbors of current point are already processed.,not
scikit-learn/sklearn/cluster/_optics.py,522,Only compute distances to unprocessed neighbors:,not
scikit-learn/sklearn/cluster/_optics.py,528,"the same logic as neighbors, p is ignored if explicitly set",not
scikit-learn/sklearn/cluster/_optics.py,529,in the dict params,not
scikit-learn/sklearn/cluster/_optics.py,693,find a maximal area,not
scikit-learn/sklearn/cluster/_optics.py,699,"it's not a steep point, but still goes up.",not
scikit-learn/sklearn/cluster/_optics.py,701,region should include no more than min_samples consecutive,not
scikit-learn/sklearn/cluster/_optics.py,702,non steep xward points.,not
scikit-learn/sklearn/cluster/_optics.py,786,Our implementation adds an inf to the end of reachability plot,not
scikit-learn/sklearn/cluster/_optics.py,787,this helps to find potential clusters at the end of the,not
scikit-learn/sklearn/cluster/_optics.py,788,reachability plot even if there's no upward region at the end of it.,not
scikit-learn/sklearn/cluster/_optics.py,792,"steep down areas, introduced in section 4.3.2 of the paper",not
scikit-learn/sklearn/cluster/_optics.py,795,"maximum in between, section 4.3.2",not
scikit-learn/sklearn/cluster/_optics.py,797,Our implementation corrects a mistake in the original,not
scikit-learn/sklearn/cluster/_optics.py,798,"paper, i.e., in Definition 9 steep downward point,",not
scikit-learn/sklearn/cluster/_optics.py,799,r(p) * (1 - x1) <= r(p + 1) should be,not
scikit-learn/sklearn/cluster/_optics.py,800,r(p) * (1 - x1) >= r(p + 1),not
scikit-learn/sklearn/cluster/_optics.py,808,the following loop is is almost exactly as Figure 19 of the paper.,not
scikit-learn/sklearn/cluster/_optics.py,809,it jumps over the areas which are not either steep down or up areas,not
scikit-learn/sklearn/cluster/_optics.py,811,just continue if steep_index has been a part of a discovered xward,not
scikit-learn/sklearn/cluster/_optics.py,812,area.,not
scikit-learn/sklearn/cluster/_optics.py,818,steep downward areas,not
scikit-learn/sklearn/cluster/_optics.py,830,steep upward areas,not
scikit-learn/sklearn/cluster/_optics.py,845,"line (**), sc2*",not
scikit-learn/sklearn/cluster/_optics.py,849,Definition 11: criterion 4,not
scikit-learn/sklearn/cluster/_optics.py,852,Find the first index from the left side which is almost,not
scikit-learn/sklearn/cluster/_optics.py,853,at the same level as the end of the detected cluster.,not
scikit-learn/sklearn/cluster/_optics.py,859,Find the first index from the right side which is almost,not
scikit-learn/sklearn/cluster/_optics.py,860,at the same level as the beginning of the detected,not
scikit-learn/sklearn/cluster/_optics.py,861,cluster.,not
scikit-learn/sklearn/cluster/_optics.py,862,Our implementation corrects a mistake in the original,not
scikit-learn/sklearn/cluster/_optics.py,863,"paper, i.e., in Definition 11 4c, r(x) < r(sD) should be",not
scikit-learn/sklearn/cluster/_optics.py,864,r(x) > r(sD).,not
scikit-learn/sklearn/cluster/_optics.py,869,predecessor correction,not
scikit-learn/sklearn/cluster/_optics.py,879,Definition 11: criterion 3.a,not
scikit-learn/sklearn/cluster/_optics.py,883,Definition 11: criterion 1,not
scikit-learn/sklearn/cluster/_optics.py,887,Definition 11: criterion 2,not
scikit-learn/sklearn/cluster/_optics.py,893,add smaller clusters first.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,5,"Authors: Vincent Michel, 2010, Gael Varoquaux 2012,",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,6,Matteo Visconti di Oleggio Castello 2014,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,41,Misc tests on linkage,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,53,Smoke test FeatureAgglomeration,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,56,test hierarchical clustering on a precomputed distances matrix,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,62,test hierarchical clustering on a precomputed distances matrix,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,68,Check that we obtain the correct solution for structured linkage trees.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,71,Avoiding a mask with only 'True' entries,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,80,Check that ward_tree raises a ValueError with a connectivity matrix,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,81,of the wrong shape,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,84,Check that fitting with no samples raises an error,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,90,Check that we obtain the correct solution for unstructured linkage trees.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,94,With specified a number of clusters just for the sake of,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,95,raising a warning and testing the warning code,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,113,Check that the height of the results of linkage tree is sorted.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,125,Test either if an error is raised when memory is not,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,126,either a str or a joblib.Memory instance,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,137,Check that zero vectors in X produce an error when,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,138,'cosine' affinity is used,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,146,Check that we obtain the correct number of clusters with,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,147,agglomerative clustering.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,158,test caching,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,170,Turn caching off now,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,173,Check that we obtain the same solution with early-stopping of the,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,174,tree building,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,182,Check that we raise a TypeError on dense matrices,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,191,Test that using ward with another metric than euclidean raises an,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,192,exception,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,201,Test using another metric than euclidean works with linkage complete,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,203,Compare our (structured) implementation to scipy,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,220,Test that using a distance matrix (affinity = 'precomputed') has same,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,221,results (with connectivity constraints),not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,236,Check that we obtain the correct solution in a simplistic case,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,251,Check that fitting with no samples raises a ValueError,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,257,Check that we get the correct result in two emblematic cases,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,285,Test scikit linkage with full connectivity (i.e. unstructured) vs scipy,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,289,"Not using a lil_matrix here, just to check that non sparse",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,290,matrices are well handled,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,303,Sort the order of child nodes per row for consistency,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,313,Test error management in _hc_cut,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,318,Make sure our custom mst_linkage_core gives,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,319,the same results as scipy's builtin,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,333,Sort the order of child nodes per row for consistency,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,346,Ensure identical points are handled correctly when using mst with,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,347,a sparse connectivity matrix,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,369,Check that connectivity in the ward tree is propagated correctly during,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,370,merging.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,379,"If changes are not propagated correctly, fit crashes with an",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,380,IndexError,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,385,Check that children are ordered in the same way for both structured and,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,386,unstructured versions of ward_tree.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,388,test on five random datasets,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,405,Test return_distance option on linkage and ward trees,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,407,"test that return_distance when set true, gives same",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,408,output on both structured and unstructured clustering.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,422,get children,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,426,check if we got the same clusters,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,429,check if the distances are the same,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,449,test on the following dataset where we know the truth,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,450,taken from scipy/cluster/tests/hierarchy_test_data.py,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,457,truth,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,485,check that the labels are the same,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,489,check that the distances are correct,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,502,check that the labels are the same,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,506,check that the distances are correct,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,512,Check non regression of a bug if a non item assignable connectivity is,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,513,provided with more than one component.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,514,create dummy data,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,516,create a mask with several components to force connectivity fixing,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,535,Complete smoke test,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,566,Test that the full tree is computed if n_clusters is small,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,571,"When n_clusters is less, the full tree should be built",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,572,that is the number of merges should be n_samples - 1,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,579,"When n_clusters is large, greater than max of 100 and 0.02 * n_samples.",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,580,we should stop when there are n_clusters.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,593,"Test n_components returned by linkage, average and ward tree",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,597,Connectivity matrix having five components.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,605,Test that an error is raised when n_clusters <= 0,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,617,Test that the affinity parameter is actually passed to the pairwise,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,618,function,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,645,Check that we obtain the correct number of clusters with,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,646,agglomerative clustering with distance_threshold.,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,652,test when distance threshold is set to 10,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,662,test if the clusters produced match the point in the linkage tree,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,663,where the distance exceeds the threshold,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,670,test number of clusters produced,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,672,test clusters produced,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,684,"this should result in all data in their own clusters, given that",not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,685,their pairwise distances are bigger than .1 (which may not be the case,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,686,with a different random seed).,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,691,check that the pairwise distances are indeed all larger than .1,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,702,check the distances within the clusters and with other clusters,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,710,to avoid taking the 0 diagonal in min(),not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,718,single data point clusters only have that inf diagonal here,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,729,test boundary case of distance_threshold matching the distance,not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,757,Check that an error is raised when affinity='precomputed',not
scikit-learn/sklearn/cluster/tests/test_hierarchical.py,758,and a non square matrix is passed (PR #16257).,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,24,noqa,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,60,Test that SpectralClustering fails with an unknown mode set.,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,68,Distance matrix,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,69,Similarity matrix,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,77,Test that SpectralClustering fails with an unknown assign_labels set.,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,85,Distance matrix,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,86,Similarity matrix,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,107,Test precomputed graph filtering when containing too many neighbors,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,126,"Note: in the following, random_state has been selected to have",not
scikit-learn/sklearn/cluster/tests/test_spectral.py,127,a dataset that yields a stable eigen decomposition both when built,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,128,on OSX and Linux,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,131,nearest neighbors affinity,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,145,Additive chi^2 gives a negative similarity matrix which,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,146,doesn't make sense for spectral clustering,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,159,Histogram kernel implemented as a callable.,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,160,no kernel_params that we didn't ask for,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,167,raise error on unknown affinity,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,175,Test the discretize using a noise assignment matrix,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,178,random class labels,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,181,noise class assignment matrix,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,194,TODO: Remove when pyamg does replaces sp.rand call with np.random.rand,SATD
scikit-learn/sklearn/cluster/tests/test_spectral.py,195,https://github.com/scikit-learn/scikit-learn/issues/15913,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,199,Test that spectral_clustering is the same for arpack and amg solver,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,200,Based on toy example from plot_segmentation_toy.py,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,202,a small two coin image,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,234,"Test that after adding n_components, result is different and",not
scikit-learn/sklearn/cluster/tests/test_spectral.py,235,n_components = n_clusters by default,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,240,set n_components = n_cluster and test if result is the same,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,243,test that n_components=n_clusters by default,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,246,test that n_components affect result,not
scikit-learn/sklearn/cluster/tests/test_spectral.py,247,"n_clusters=8 by default, and set n_components=2",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,30,Test estimate_bandwidth,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,36,"Test estimate_bandwidth when n_samples=1 and quantile<1, so that",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,37,n_neighbors is set to 1.,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,46,Test MeanShift algorithm,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,71,Test estimate_bandwidth with sparse matrix,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,93,Test MeanShift.predict,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,101,"init away from the data, crash with a sensible warning",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,108,"Non-regression: before fit, there should be not fitted attributes.",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,127,Test the bin seeding technique which can be used in the mean shift,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,128,algorithm,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,129,Data is just 6 points in the plane,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,133,"With a bin coarseness of 1.0 and min_bin_freq of 1, 3 bins should be",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,134,found,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,140,"With a bin coarseness of 1.0 and min_bin_freq of 2, 2 bins should be",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,141,found,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,147,"With a bin size of 0.01 and min_bin_freq of 1, 6 bins should be found",not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,148,we bail and use the whole data here.,not
scikit-learn/sklearn/cluster/tests/test_mean_shift.py,153,"tight clusters around [0, 0] and [1, 1], only get two bins",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,30,Affinity Propagation algorithm,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,31,Compute similarities,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,34,Compute Affinity Propagation,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,58,Test also with no copy,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,63,Test input validation,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,76,Test AffinityPropagation.predict,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,84,Test exception in AffinityPropagation.predict,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,85,Not fitted.,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,90,"Predict not supported when affinity=""precomputed"".",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,99,"In case of non-convergence of affinity_propagation(), the cluster",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,100,centers should be an empty array and training samples should be labelled,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,101,as noise (-1),not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,104,Force non-convergence by allowing only a single iteration,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,116,setting preference > similarity,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,120,expect every sample to become an exemplar,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,124,setting preference < similarity,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,128,"expect one cluster, with arbitrary (first) sample as exemplar",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,132,setting different preferences,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,136,"expect one cluster, with highest-preference sample as exemplar",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,142,"In case of non-convergence of affinity_propagation(), the cluster",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,143,centers should be an empty array,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,146,Force non-convergence by allowing only a single iteration,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,151,"At prediction time, consider new samples as noise since there are no",not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,152,clusters,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,168,Unequal distances,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,176,Equal distances,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,180,Different preferences,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,183,Same preferences,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,189,Significance of random_state parameter,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,190,Generate sample data,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,194,random_state = 0,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,199,random_state = 76,not
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,207,FIXME: to be removed in 0.25,SATD
scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py,209,test that a warning is raised when random_state is not defined.,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,27,Mock object for testing get_submatrix.,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,32,Overridden to reproduce old get_submatrix test.,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,55,Test get_shape and get_indices on fitted model.,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,64,Test Dhillon's Spectral CoClustering on a simple problem.,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,73,needs to be nonnegative before making it sparse,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,74,threshold some values,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,92,Test Kluger methods on a checkerboard dataset.,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,114,cannot take log of sparse matrix,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,175,adding any constant to a log-scaled matrix should make it,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,176,bistochastic,not
scikit-learn/sklearn/cluster/tests/test_bicluster.py,208,XXX Previously failed on build bot (not reproducible),SATD
scikit-learn/sklearn/cluster/tests/test_bicluster.py,270,FIXME: remove in 0.25,SATD
scikit-learn/sklearn/cluster/tests/test_birch.py,24,Sanity check for the number of samples in leaves and roots,not
scikit-learn/sklearn/cluster/tests/test_birch.py,36,Test that fit is equivalent to calling partial_fit multiple times,not
scikit-learn/sklearn/cluster/tests/test_birch.py,46,Test that same global labels are obtained after calling partial_fit,not
scikit-learn/sklearn/cluster/tests/test_birch.py,47,with None,not
scikit-learn/sklearn/cluster/tests/test_birch.py,54,Test the predict method predicts the nearest centroid.,not
scikit-learn/sklearn/cluster/tests/test_birch.py,59,n_samples * n_samples_per_cluster,not
scikit-learn/sklearn/cluster/tests/test_birch.py,72,Test that n_clusters param works properly,not
scikit-learn/sklearn/cluster/tests/test_birch.py,79,Test that n_clusters = Agglomerative Clustering gives,not
scikit-learn/sklearn/cluster/tests/test_birch.py,80,the same results.,not
scikit-learn/sklearn/cluster/tests/test_birch.py,87,Test that the wrong global clustering step raises an Error.,not
scikit-learn/sklearn/cluster/tests/test_birch.py,93,Test that a small number of clusters raises a warning.,not
scikit-learn/sklearn/cluster/tests/test_birch.py,99,Test that sparse and dense data give same results,not
scikit-learn/sklearn/cluster/tests/test_birch.py,122,Test that nodes have at max branching_factor number of subclusters,not
scikit-learn/sklearn/cluster/tests/test_birch.py,126,Purposefully set a low threshold to maximize the subclusters.,not
scikit-learn/sklearn/cluster/tests/test_birch.py,136,Raises error when branching_factor is set to one.,not
scikit-learn/sklearn/cluster/tests/test_birch.py,153,Test that the leaf subclusters have a threshold lesser than radius,not
scikit-learn/sklearn/cluster/tests/test_birch.py,165,"Check that birch supports n_clusters with np.int64 dtype, for instance",not
scikit-learn/sklearn/cluster/tests/test_birch.py,166,coming from np.arange. #16484,not
scikit-learn/sklearn/cluster/tests/test_optics.py,1,Authors: Shane Grigsby <refuge@rocktalus.com>,not
scikit-learn/sklearn/cluster/tests/test_optics.py,2,Adrin Jalali <adrin.jalali@gmail.com>,not
scikit-learn/sklearn/cluster/tests/test_optics.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/cluster/tests/test_optics.py,81,small and easy test (no clusters around other clusters),not
scikit-learn/sklearn/cluster/tests/test_optics.py,82,but with a clear noise data.,not
scikit-learn/sklearn/cluster/tests/test_optics.py,103,check float min_samples and min_cluster_size,not
scikit-learn/sklearn/cluster/tests/test_optics.py,117,this may fail if the predecessor correction is not at work!,not
scikit-learn/sklearn/cluster/tests/test_optics.py,148,in 'auto' mode,not
scikit-learn/sklearn/cluster/tests/test_optics.py,152,Parameters chosen specifically for this task.,not
scikit-learn/sklearn/cluster/tests/test_optics.py,153,Compute OPTICS,not
scikit-learn/sklearn/cluster/tests/test_optics.py,156,"number of clusters, ignoring noise if present",not
scikit-learn/sklearn/cluster/tests/test_optics.py,160,check attribute types and sizes,not
scikit-learn/sklearn/cluster/tests/test_optics.py,176,test that we check a minimum number of samples,not
scikit-learn/sklearn/cluster/tests/test_optics.py,179,Compute OPTICS,not
scikit-learn/sklearn/cluster/tests/test_optics.py,183,Run the fit,not
scikit-learn/sklearn/cluster/tests/test_optics.py,188,Test an extraction of eps too close to original eps,not
scikit-learn/sklearn/cluster/tests/test_optics.py,194,Compute OPTICS,not
scikit-learn/sklearn/cluster/tests/test_optics.py,213,Test extract where extraction eps is close to scaled max_eps,not
scikit-learn/sklearn/cluster/tests/test_optics.py,219,Compute OPTICS,not
scikit-learn/sklearn/cluster/tests/test_optics.py,222,Cluster ordering starts at 0; max cluster label = 2 is 3 clusters,not
scikit-learn/sklearn/cluster/tests/test_optics.py,229,Test that OPTICS clustering labels are <= 5% difference of DBSCAN,not
scikit-learn/sklearn/cluster/tests/test_optics.py,235,calculate optics with dbscan extract at 0.3 epsilon,not
scikit-learn/sklearn/cluster/tests/test_optics.py,239,calculate dbscan labels,not
scikit-learn/sklearn/cluster/tests/test_optics.py,249,verify label mismatch is <= 5% labels,not
scikit-learn/sklearn/cluster/tests/test_optics.py,279,try arbitrary minimum sizes,not
scikit-learn/sklearn/cluster/tests/test_optics.py,282,reduce for speed,not
scikit-learn/sklearn/cluster/tests/test_optics.py,287,check behaviour is the same when min_cluster_size is a fraction,not
scikit-learn/sklearn/cluster/tests/test_optics.py,308,"Ensure that we consider all unprocessed points,",not
scikit-learn/sklearn/cluster/tests/test_optics.py,309,not only direct neighbors. when picking the next point.,not
scikit-learn/sklearn/cluster/tests/test_optics.py,318,"Expected values, computed with (future) ELKI 0.7.5 using:",not
scikit-learn/sklearn/cluster/tests/test_optics.py,319,java -jar elki.jar cli -dbc.in csv -dbc.filter FixedDBIDsFilter,not
scikit-learn/sklearn/cluster/tests/test_optics.py,320,-algorithm clustering.optics.OPTICSHeap -optics.minpts 5,not
scikit-learn/sklearn/cluster/tests/test_optics.py,321,where the FixedDBIDsFilter gives 0-indexed ids.,not
scikit-learn/sklearn/cluster/tests/test_optics.py,351,Tests against known extraction array,not
scikit-learn/sklearn/cluster/tests/test_optics.py,352,"Does NOT work with metric='euclidean', because sklearn euclidean has",not
scikit-learn/sklearn/cluster/tests/test_optics.py,353,worse numeric precision. 'minkowski' is slower but more accurate.,not
scikit-learn/sklearn/cluster/tests/test_optics.py,359,ELKI currently does not print the core distances (which are not used much,not
scikit-learn/sklearn/cluster/tests/test_optics.py,360,"in literature, but we can at least ensure to have this consistency:",not
scikit-learn/sklearn/cluster/tests/test_optics.py,365,"Expected values, computed with (future) ELKI 0.7.5 using",not
scikit-learn/sklearn/cluster/tests/test_optics.py,406,testing an easy dbscan case. Not including clusters with different,not
scikit-learn/sklearn/cluster/tests/test_optics.py,407,densities.,not
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,4,Authors: Sergul Aydore 2017,not
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,13,"(n_samples, n_features)",not
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,26,Test transform,not
scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py,34,Test inverse transform,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,39,"non centered, sparse centers to check the",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,56,cheks that kmeans works as intended,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,59,"will be rescaled to [1.5, 0.5, 0.5, 1.5]",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,81,check that empty clusters are relocated as expected,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,84,second center too far from others points will be empty at first iter,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,103,test for the _relocate_empty_clusters_(dense/sparse) helpers,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,105,Synthetic dataset with 3 obvious clusters of different sizes,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,112,centers all initialized to the first point of X,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,115,"With this initialization, all points will be assigned to the first center",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,116,At this point a center in centers_new is the weighted sum of the points,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,117,"it contains if it's not empty, otherwise it is the same as before.",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,131,The relocation scheme will take the 2 points farthest from the center and,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,132,"assign them to the 2 empty clusters, i.e. points at 10 and at 9.9. The",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,133,first center will be updated to contain the other 8 points.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,141,check that results are identical between lloyd and elkan algorithms,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,164,Check that KMeans stops when convergence is reached when tol=0. (#16075),not
scikit-learn/sklearn/cluster/tests/test_k_means.py,176,check that results are identical between lloyd and elkan algorithms,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,177,with sparse input,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,197,pure numpy implementation as easily auditable reference gold,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,198,implementation,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,214,perform label assignment using the dense array input,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,221,perform label assignment using the sparse CSR input,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,230,Check that dense and sparse minibatch update give the same results,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,246,extract a small minibatch,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,254,step 1: compute the dense minibatch update,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,260,compute the new inertia on the same batch to check that it decreased,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,266,check that the incremental difference computation is matching the,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,267,final observed value,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,271,step 2: compute the sparse minibatch update,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,277,compute the new inertia on the same batch to check that it decreased,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,283,check that the incremental difference computation is matching the,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,284,final observed value,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,288,step 3: check that sparse and dense updates lead to the same results,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,297,check that the number of clusters centers and distinct labels match,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,298,the expectation,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,305,check that the labels assignment are perfect (up to a permutation),not
scikit-learn/sklearn/cluster/tests/test_k_means.py,309,check error on dataset being too small,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,315,Explore the part of the code where a new center is reassigned,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,332,"Reorder the labels so that the first instance is in cluster 0,",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,333,"the second in cluster 1, ...",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,350,two regression tests on bad n_init argument,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,351,previous bug: n_init <= 0 threw non-informative TypeError (#3858),SATD
scikit-learn/sklearn/cluster/tests/test_k_means.py,360,test for sensible errors when giving explicit init,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,361,with wrong number of features or clusters,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,365,mismatch of number of features,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,370,for callable init,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,376,mismatch of number of clusters,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,381,for callable init,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,390,"Check the KMeans will work well, even if X is a fortran-aligned data.",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,404,strict non-convergence,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,405,loose non-convergence,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,406,strict convergence,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,407,loose convergence,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,410,check that fit.predict gives same result as fit_predict,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,411,There's a very small chance of failure with elkan on unstructured dataset,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,412,because predict method uses fast euclidean distances computation which,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,413,may cause small numerical instabilities.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,414,NB: This test is largely redundant with respect to test_predict and,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,415,test_predict_equal_labels.  This test has the added effect of,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,416,testing idempotence of the fittng procesdure which appears to,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,417,be where it fails on some MacOS setups.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,435,Due to randomness in the order in which chunks of data are processed when,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,436,"using more than one thread, the absolute values of the labels can be",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,437,different between the 2 strategies but they should correspond to the same,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,438,clustering.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,455,"Check that a warning is raised, as the number clusters is larger",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,456,than the init_size,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,476,check if identical initial clusters are reassigned,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,477,also a regression test for when there are more desired reassignments than,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,478,samples.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,485,there should not be too many exact zero cluster centers,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,488,do the same with batch-size > X.shape[0] (regression test),not
scikit-learn/sklearn/cluster/tests/test_k_means.py,492,there should not be too many exact zero cluster centers,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,503,there should not be too many exact zero cluster centers,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,508,"Give a perfect initialization, but a large reassignment_ratio,",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,509,as a result all the centers should be reassigned and the model,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,510,should no longer be good,SATD
scikit-learn/sklearn/cluster/tests/test_k_means.py,521,Turn on verbosity to smoke test the display code,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,533,"Give a perfect initialization, with a small reassignment_ratio,",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,534,no center should be reassigned,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,541,Turn on verbosity to smoke test the display code,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,553,Test for the case that the number of clusters to reassign is bigger,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,554,than the batch_size,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,558,Check that the fit works if n_clusters is bigger than the batch_size.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,559,"Run the test with 550 clusters and 550 samples, because it turned out",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,560,that this values ensure that the number of clusters to reassign,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,561,is always bigger than the batch_size,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,574,Small test to check that giving the wrong number of centers,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,575,raises a meaningful error,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,580,Now check that the fit actually works,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,589,use the partial_fit API for online learning,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,593,compute the labeling on the complete dataset,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,629,Check if copy_x=False returns nearly equal X after de-centering.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,635,check if my_X is centered,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,640,Check k_means with a bad initialization does not yield a singleton,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,641,Starting with bad centers that are quickly ignored should not,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,642,result in a repositioning of the centers to the center of mass that,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,643,would lead to collapsed centers which in turns make the clustering,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,644,dependent of the numerical unstabilities.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,650,centers must not been collapsed,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,661,Check that fitting k-means with multiple inits gives better score,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,678,sanity check: re-predict labeling for training set samples,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,681,sanity check: predict centroid labels,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,685,re-predict labels for training set using fit_predict,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,692,check that models trained on sparse input also works for dense input at,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,693,predict time,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,710,mini batch kmeans is very unstable on such a small dataset hence,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,711,we use many inits,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,764,Check that increasing the number of init increases the quality,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,782,test calling the k_means function directly,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,783,catch output,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,798,check that the labels assignment are perfect (up to a permutation),not
scikit-learn/sklearn/cluster/tests/test_k_means.py,802,check warning when centers are passed,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,806,to many clusters desired,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,812,Test that x_squared_norms can be None in _init_centroids,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,845,dtype of cluster centers has to be the dtype of the input,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,846,data,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,851,ensure the extracted row is a 2d array,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,855,dtype of cluster centers has to stay the same after,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,856,partial_fit,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,859,compare arrays with low precision since the difference between,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,860,32 and 64 bit sometimes makes a difference up to the 4th decimal,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,861,place,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,871,This test is used to check KMeans won't mutate the user provided input,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,872,array silently even if input data and init centers have the same type,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,887,Get a local optimum,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,890,Fit starting from a local optimum shouldn't change the solution,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,902,Get a local optimum,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,905,Test that a ValueError is raised for validate_center_shape,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,918,last point is duplicated,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,922,"only three distinct points, so only three clusters",not
scikit-learn/sklearn/cluster/tests/test_k_means.py,923,can have points assigned to them,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,926,k_means should warn that fewer labels than cluster,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,927,centers have been used,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,939,a sample weight of N should yield the same result as an N-fold,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,940,repetition of the sample,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,964,not passing any sample weights should be equivalent,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,965,to all weights equal to one,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,977,scaling all sample weights by a common factor,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,978,shouldn't change the result,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,990,check that an error is raised when passing sample weights,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,991,with an incompatible shape,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1008,Regression test on bad n_iter_ value. Previous bug n_iter_ was one off,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1009,it's right value (#11340).,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1016,check that empty clusters are correctly relocated when using sample,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1017,weights (#13486),not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1030,Issue GH #14314,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1038,Check that KMeans gives the same results in parallel mode than in,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1039,sequential mode.,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1054,FIXME: remove in 0.25,SATD
scikit-learn/sklearn/cluster/tests/test_k_means.py,1067,FIXME: remove in 0.25,SATD
scikit-learn/sklearn/cluster/tests/test_k_means.py,1104,check the results after a single iteration (E-step M-step E-step) by,not
scikit-learn/sklearn/cluster/tests/test_k_means.py,1105,comparing against a pure python implementation.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,27,Tests the DBSCAN algorithm with a similarity array.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,28,Parameters chosen specifically for this task.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,31,Compute similarities,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,34,Compute DBSCAN,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,37,"number of clusters, ignoring noise if present",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,50,Tests the DBSCAN algorithm with a feature vector array.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,51,Parameters chosen specifically for this task.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,52,"Different eps to other test, because distance is not normalised.",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,56,Compute DBSCAN,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,57,parameters chosen for task,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,61,"number of clusters, ignoring noise if present",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,86,Ensure it is sparse not merely on diagonals:,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,99,test that precomputed neighbors graph is filtered if computed with,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,100,a radius larger than DBSCAN's eps.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,118,test that the input is not modified by dbscan,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,143,Tests the DBSCAN algorithm with a callable metric.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,144,Parameters chosen specifically for this task.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,145,"Different eps to other test, because distance is not normalised.",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,148,"metric is the function reference, not the string key.",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,150,Compute DBSCAN,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,151,parameters chosen for task,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,156,"number of clusters, ignoring noise if present",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,169,Tests that DBSCAN works with the metrics_params argument.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,174,Compute DBSCAN with metric_params arg,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,179,Test that sample labels are the same as passing Minkowski 'p' directly,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,187,Minkowski with p=1 should be equivalent to Manhattan distance,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,197,Tests the DBSCAN algorithm with balltree for neighbor calculation.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,205,"number of clusters, ignoring noise if present",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,236,DBSCAN.fit should accept a list of lists.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,238,must not raise exception,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,247,Test bad argument values: these should all raise ValueErrors,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,259,ensure min_samples is inclusive of core point,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,262,ensure eps is inclusive of circumference,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,270,ensure sample_weight is validated,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,276,ensure sample_weight has an effect,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,286,points within eps of each other:,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,289,and effect of non-positive and non-integer sample_weight:,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,299,"for non-negative sample_weight, cores should be identical to repetition",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,313,sample_weight should work with precomputed distance matrix,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,320,sample_weight should work with estimator,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,340,"Degenerate case: every sample is a core sample, either with its own",not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,341,cluster or including other close core samples.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,347,With eps=1 and min_samples=2 only the 3 samples from the denser area,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,348,are core samples. All other points are isolated and considered noise.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,354,Only the sample in the middle of the dense area is core. Its two,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,355,neighbors are edge samples. Remaining samples are noise.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,361,It's no longer possible to extract core samples with eps=1:,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,362,everything is noise.,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,370,see https://github.com/scikit-learn/scikit-learn/issues/4641 for,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,371,more details,not
scikit-learn/sklearn/cluster/tests/test_dbscan.py,382,sample matrix with initial two row all zero,not
scikit-learn/sklearn/cluster/tests/common.py,9,,not
scikit-learn/sklearn/cluster/tests/common.py,10,Generate sample data,not
scikit-learn/sklearn/cluster/tests/common.py,16,the data is voluntary shifted away from zero to check clustering,not
scikit-learn/sklearn/cluster/tests/common.py,17,algorithm robustness with regards to non centered data,not
scikit-learn/sklearn/neural_network/_stochastic_optimizers.py,4,Authors: Jiyuan Qian <jq401@nyu.edu>,not
scikit-learn/sklearn/neural_network/_stochastic_optimizers.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,4,Authors: Issam H. Laradji <issam.laradji@gmail.com>,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,5,Andreas Mueller,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,6,Jiyuan Qian,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,102,Iterate over the hidden layers,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,108,For the hidden layers,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,112,For the last layer,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,221,Forward propagate,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,224,Get loss,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,229,Add L2 regularization term to loss,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,234,Backward propagate,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,237,The calculation of delta[last] here works with following,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,238,combinations of output activation and loss function:,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,239,"sigmoid and binary cross entropy, softmax and categorical cross",not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,240,"entropy, and identity with squared loss",not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,243,Compute gradient for the last layer,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,247,Iterate over the hidden layers,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,260,"set all attributes, allocate weights etc for first call",not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,261,Initialize parameters,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,266,Compute the number of layers,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,269,Output for regression,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,272,Output for multi class,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,275,Output for binary class and multi-label,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,279,Initialize coefficient and intercept layers,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,299,Use the initialization method recommended by,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,300,Glorot et al.,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,306,Generate weights and bias:,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,314,Make sure self.hidden_layer_sizes is a list,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,320,Validate input parameters.,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,329,Ensure y is 2D,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,338,check random state,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,343,First time training the model,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,346,lbfgs does not support mini-batches,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,357,Initialize lists,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,368,Run the Stochastic optimization solver,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,373,Run the LBFGS solver,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,417,raise ValueError if not registered,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,433,Store meta information for the parameters,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,438,Save sizes and indices of coefficients for faster unpacking,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,446,Save sizes and indices of intercepts for faster unpacking,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,452,Run LBFGS,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,490,early_stopping in partial_fit doesn't make sense,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,493,don't stratify in multilabel classification,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,517,Only shuffle the sample indices instead of X and y to,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,518,reduce the memory footprint. These indices will be used,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,519,to slice the X and y.,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,539,update weights,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,552,update no_improvement_count based on training loss or,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,553,validation score according to early_stopping,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,556,for learning rate that needs to be updated at iteration end,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,560,not better than last `n_iter_no_change` iterations by tol,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,561,stop or decrease learning rate,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,590,restore best weights,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,596,"compute validation score, use that for stopping",not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,601,update best parameters,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,602,"use validation_scores_, not loss_curve_",not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,603,let's hope no-one overloads .score with mse,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,683,Make sure self.hidden_layer_sizes is a list,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,692,Initialize layers,not
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py,698,forward propagate,not
scikit-learn/sklearn/neural_network/_rbm.py,4,Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>,not
scikit-learn/sklearn/neural_network/_rbm.py,5,Vlad Niculae,not
scikit-learn/sklearn/neural_network/_rbm.py,6,Gabriel Synnaeve,not
scikit-learn/sklearn/neural_network/_rbm.py,7,Lars Buitinck,not
scikit-learn/sklearn/neural_network/_rbm.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/neural_network/_rbm.py,14,logistic function,not
scikit-learn/sklearn/neural_network/_rbm.py,293,sample binomial,not
scikit-learn/sklearn/neural_network/_rbm.py,320,Randomly corrupt one feature in each sample in v.,not
scikit-learn/sklearn/neural_network/__init__.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/neural_network/_base.py,4,Author: Issam H. Laradji <issam.laradji@gmail.com>,not
scikit-learn/sklearn/neural_network/_base.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/neural_network/_base.py,115,Nothing to do,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,27,in-place tricks shouldn't have modified X,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,60,BernoulliRBM should work on small sparse matrices.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,62,no exception,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,97,"Gibbs on the RBM hidden layer should be able to recreate [[0], [1]]",not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,98,from the same input,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,103,you need that much iters,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,112,"Gibbs on the RBM hidden layer should be able to recreate [[0], [1]] from",not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,113,"the same input even when the input is sparse, and test against non-sparse",not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,128,Check if we don't get NaNs sampling the full digits dataset.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,129,Also check that sampling again will yield different results.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,141,Test score_samples (pseudo-likelihood) method.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,142,Assert that pseudo-likelihood is computed without clipping.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,143,"See Fabian's blog, http://bit.ly/1iYefRk",not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,151,Sparse vs. dense should not affect the output. Also test sparse input,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,152,validation.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,159,Test numerical stability (#2785): would previously generate infinities,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,160,and crash with an exception.,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,176,Make sure RBM works with sparse input when verbose=True,not
scikit-learn/sklearn/neural_network/tests/test_rbm.py,186,make sure output is sound,not
scikit-learn/sklearn/neural_network/tests/test_base.py,9,y_proba is equal to one should result in a finite logloss,not
scikit-learn/sklearn/neural_network/tests/test_base.py,24,y_proba is equal to 1 should result in a finite logloss,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,5,Author: Issam H. Laradji,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,59,Test that larger alpha yields weights closer to zero,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,79,Test that the algorithm solution is equal to a worked out example.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,85,set weights,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,96,Initialize parameters,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,100,Compute the number of layers,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,103,Pre-allocate gradient matrices,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,119,Manually worked out example,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,120,h1 = g(X1 * W_i1 + b11) = g(0.6 * 0.1 + 0.8 * 0.3 + 0.7 * 0.5 + 0.1),not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,121,=  0.679178699175393,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,122,h2 = g(X2 * W_i2 + b12) = g(0.6 * 0.2 + 0.8 * 0.1 + 0.7 * 0 + 0.1),not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,123,= 0.574442516811659,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,124,o1 = g(h * W2 + b21) = g(0.679 * 0.1 + 0.574 * 0.2 + 1),not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,125,= 0.7654329236196236,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,126,d21 = -(0 - 0.765) = 0.765,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,127,d11 = (1 - 0.679) * 0.679 * 0.765 * 0.1 = 0.01667,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,128,d12 = (1 - 0.574) * 0.574 * 0.765 * 0.2 = 0.0374,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,129,W1grad11 = X1 * d11 + alpha * W11 = 0.6 * 0.01667 + 0.1 * 0.1 = 0.0200,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,130,W1grad11 = X1 * d12 + alpha * W12 = 0.6 * 0.0374 + 0.1 * 0.2 = 0.04244,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,131,W1grad21 = X2 * d11 + alpha * W13 = 0.8 * 0.01667 + 0.1 * 0.3 = 0.043336,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,132,W1grad22 = X2 * d12 + alpha * W14 = 0.8 * 0.0374 + 0.1 * 0.1 = 0.03992,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,133,W1grad31 = X3 * d11 + alpha * W15 = 0.6 * 0.01667 + 0.1 * 0.5 = 0.060002,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,134,W1grad32 = X3 * d12 + alpha * W16 = 0.6 * 0.0374 + 0.1 * 0 = 0.02244,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,135,W2grad1 = h1 * d21 + alpha * W21 = 0.679 * 0.765 + 0.1 * 0.1 = 0.5294,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,136,W2grad2 = h2 * d21 + alpha * W22 = 0.574 * 0.765 + 0.1 * 0.2 = 0.45911,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,137,b1grad1 = d11 = 0.01667,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,138,b1grad2 = d12 = 0.0374,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,139,b2grad = d21 = 0.765,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,140,"W1 = W1 - eta * [W1grad11, .., W1grad32] = [[0.1, 0.2], [0.3, 0.1],",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,141,"[0.5, 0]] - 0.1 * [[0.0200, 0.04244], [0.043336, 0.03992],",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,142,"[0.060002, 0.02244]] = [[0.098, 0.195756], [0.2956664,",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,143,"0.096008], [0.4939998, -0.002244]]",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,144,"W2 = W2 - eta * [W2grad1, W2grad2] = [[0.1], [0.2]] - 0.1 *",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,145,"[[0.5294], [0.45911]] = [[0.04706], [0.154089]]",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,146,"b1 = b1 - eta * [b1grad1, b1grad2] = 0.1 - 0.1 * [0.01667, 0.0374]",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,147,"= [0.098333, 0.09626]",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,148,b2 = b2 - eta * b2grad = 1.0 - 0.1 * 0.765 = 0.9235,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,158,Testing output,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,159,h1 = g(X1 * W_i1 + b11) = g(0.6 * 0.098 + 0.8 * 0.2956664 +,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,160,0.7 * 0.4939998 + 0.098333) = 0.677,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,161,h2 = g(X2 * W_i2 + b12) = g(0.6 * 0.195756 + 0.8 * 0.096008 +,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,162,0.7 * -0.002244 + 0.09626) = 0.572,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,163,o1 = h * W2 + b21 = 0.677 * 0.04706 +,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,164,0.572 * 0.154089 + 0.9235 = 1.043,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,165,prob = sigmoid(o1) = 0.739,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,170,Test gradient.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,172,This makes sure that the activation functions and their derivatives,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,173,are correct. The numerical and analytical computation of the gradient,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,174,should be close.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,213,analytically compute the gradients,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,223,numerically compute the gradients,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,234,Test lbfgs on classification.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,235,It should achieve a score higher than 0.95 for the binary and multi-class,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,236,versions of the digits dataset.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,255,"Test lbfgs on the boston dataset, a regression problems.",not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,264,Non linear models perform much better than linear bottleneck:,SATD
scikit-learn/sklearn/neural_network/tests/test_mlp.py,270,Test lbfgs parameter max_fun.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,271,It should independently limit the number of iterations for lbfgs.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,273,classification tests,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,285,Test lbfgs parameter max_fun.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,286,It should independently limit the number of iterations for lbfgs.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,288,regression tests,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,303,Tests that warm_start reuse past solutions.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,324,Test that multi-label classification works as expected.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,325,test fit method,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,334,test partial fit method,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,342,Make sure early stopping still work now that spliting is stratified by,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,343,default (it is disabled for multilabel classification),not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,349,Test that multi-output regression works as expected,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,358,Tests that passing different classes to partial_fit raises an error,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,368,Test partial_fit on classification.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,369,`partial_fit` should yield the same results as 'fit' for binary and,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,370,multi-class classification.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,390,Non regression test for bug 6994,SATD
scikit-learn/sklearn/neural_network/tests/test_mlp.py,391,Tests for labeling errors in partial fit,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,401,Test partial_fit on regression.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,402,`partial_fit` should yield the same results as 'fit' for regression.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,411,catch convergence warning,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,427,Test partial_fit error handling.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,431,no classes passed,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,435,lbfgs doesn't support partial_fit,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,463,Test that invalid parameters raise value error,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,473,Test that predict_proba works as expected for binary class.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,497,Test that predict_proba works as expected for multi class.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,518,Test that predict_proba works as expected for multilabel.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,519,Multilabel should not use softmax which makes probabilities sum to 1,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,542,Test that the shuffle parameter affects the training process (it should),not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,546,The coefficients will be identical if both do or do not shuffle,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,557,The coefficients will be slightly different if shuffle=True,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,569,Test that sparse and dense input matrices output the same results.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,586,Test tolerance.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,587,It should force the solver to exit the loop when it converges.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,596,Test verbose.,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,649,No error raised,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,666,test n_iter_no_change using binary data set,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,667,the classifying fitting process is not prone to loss curve fluctuations,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,673,test multiple n_iter_no_change,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,679,validate n_iter_no_change,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,686,test n_iter_no_change using binary data set,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,687,the fitting process should go to max_iter iterations,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,691,set a ridiculous tolerance,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,692,this should always trigger _update_no_improvement_count(),not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,695,fit,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,702,validate n_iter_no_change doesn't cause early stopping,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,705,validate _update_no_improvement_count() was always triggered,not
scikit-learn/sklearn/neural_network/tests/test_mlp.py,710,Make sure data splitting for early stopping is stratified,not
scikit-learn/sklearn/tree/_reingold_tilford.py,1,Authors: William Mill (bill@billmill.org),not
scikit-learn/sklearn/tree/_reingold_tilford.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/tree/_reingold_tilford.py,21,this is the number of the node in its group of siblings 1..n,not
scikit-learn/sklearn/tree/_reingold_tilford.py,84,"print(""finished v ="", v.tree, ""children"")",not
scikit-learn/sklearn/tree/_reingold_tilford.py,101,in buchheim notation:,not
scikit-learn/sklearn/tree/_reingold_tilford.py,102,i == inner; o == outer; r == right; l == left; r = +; l = -,not
scikit-learn/sklearn/tree/_reingold_tilford.py,137,"print(wl.tree, ""is conflicted with"", wr.tree, 'moving', subtrees,",not
scikit-learn/sklearn/tree/_reingold_tilford.py,138,"'shift', shift)",not
scikit-learn/sklearn/tree/_reingold_tilford.py,139,"print wl, wr, wr.number, wl.number, shift, subtrees, shift/subtrees",not
scikit-learn/sklearn/tree/_reingold_tilford.py,150,"print(""shift:"", w, shift, w.change)",not
scikit-learn/sklearn/tree/_reingold_tilford.py,158,the relevant text is at the bottom of page 7 of,not
scikit-learn/sklearn/tree/_reingold_tilford.py,159,"""Improving Walker's Algorithm to Run in Linear Time"" by Buchheim et al,",not
scikit-learn/sklearn/tree/_reingold_tilford.py,160,(2002),not
scikit-learn/sklearn/tree/_reingold_tilford.py,161,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.8757&rep=rep1&type=pdf,not
scikit-learn/sklearn/tree/_classes.py,6,Authors: Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/sklearn/tree/_classes.py,7,Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/sklearn/tree/_classes.py,8,Brian Holt <bdholt1@gmail.com>,not
scikit-learn/sklearn/tree/_classes.py,9,Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/tree/_classes.py,10,Satrajit Gosh <satrajit.ghosh@gmail.com>,not
scikit-learn/sklearn/tree/_classes.py,11,Joly Arnaud <arnaud.v.joly@gmail.com>,not
scikit-learn/sklearn/tree/_classes.py,12,Fares Hedayati <fares.hedayati@gmail.com>,not
scikit-learn/sklearn/tree/_classes.py,13,Nelson Liu <nelson@nelsonliu.me>,not
scikit-learn/sklearn/tree/_classes.py,14,,not
scikit-learn/sklearn/tree/_classes.py,15,License: BSD 3 clause,not
scikit-learn/sklearn/tree/_classes.py,56,=============================================================================,not
scikit-learn/sklearn/tree/_classes.py,57,Types and constants,not
scikit-learn/sklearn/tree/_classes.py,58,=============================================================================,not
scikit-learn/sklearn/tree/_classes.py,73,=============================================================================,not
scikit-learn/sklearn/tree/_classes.py,74,Base decision tree,not
scikit-learn/sklearn/tree/_classes.py,75,=============================================================================,not
scikit-learn/sklearn/tree/_classes.py,151,Need to validate separately here.,not
scikit-learn/sklearn/tree/_classes.py,152,We can't pass multi_ouput=True because that would allow y to be,not
scikit-learn/sklearn/tree/_classes.py,153,csr.,not
scikit-learn/sklearn/tree/_classes.py,166,Determine output settings,not
scikit-learn/sklearn/tree/_classes.py,174,reshape is necessary to preserve the data contiguity against vs,not
scikit-learn/sklearn/tree/_classes.py,175,"[:, np.newaxis] that does not.",not
scikit-learn/sklearn/tree/_classes.py,207,Check parameters,not
scikit-learn/sklearn/tree/_classes.py,219,float,not
scikit-learn/sklearn/tree/_classes.py,233,float,not
scikit-learn/sklearn/tree/_classes.py,262,float,not
scikit-learn/sklearn/tree/_classes.py,296,Set min_weight_leaf from min_weight_fraction_leaf,not
scikit-learn/sklearn/tree/_classes.py,329,Build tree,not
scikit-learn/sklearn/tree/_classes.py,354,TODO: tree should't need this in this case,SATD
scikit-learn/sklearn/tree/_classes.py,358,Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise,not
scikit-learn/sklearn/tree/_classes.py,431,Classification,not
scikit-learn/sklearn/tree/_classes.py,447,Regression,not
scikit-learn/sklearn/tree/_classes.py,518,build pruned tree,not
scikit-learn/sklearn/tree/_classes.py,524,TODO: the tree shouldn't need this param,SATD
scikit-learn/sklearn/tree/_classes.py,593,=============================================================================,not
scikit-learn/sklearn/tree/_classes.py,594,Public estimators,not
scikit-learn/sklearn/tree/_classes.py,595,=============================================================================,not
scikit-learn/sklearn/tree/_classes.py,1251,TODO: Remove method in 0.24,SATD
scikit-learn/sklearn/tree/_classes.py,1259,TODO: Remove method in 0.24,SATD
scikit-learn/sklearn/tree/_export.py,5,Authors: Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,6,Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,7,Brian Holt <bdholt1@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,8,Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/tree/_export.py,9,Satrajit Gosh <satrajit.ghosh@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,10,Trevor Stephens <trev.stephens@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,11,Li Li <aiki.nogard@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,12,Giuseppe Vettigli <vettigli@gmail.com>,not
scikit-learn/sklearn/tree/_export.py,13,License: BSD 3 clause,not
scikit-learn/sklearn/tree/_export.py,46,Initialize saturation & value; calculate chroma & value shift,not
scikit-learn/sklearn/tree/_export.py,52,Calculate some intermediate values,not
scikit-learn/sklearn/tree/_export.py,55,Initialize RGB with same hue & chroma as our color,not
scikit-learn/sklearn/tree/_export.py,64,Shift the initial RGB values to match value and store,not
scikit-learn/sklearn/tree/_export.py,216,Find the appropriate color & intensity for a node,not
scikit-learn/sklearn/tree/_export.py,218,Classification tree,not
scikit-learn/sklearn/tree/_export.py,227,Regression tree or multi-output,not
scikit-learn/sklearn/tree/_export.py,231,unpack numpy scalars,not
scikit-learn/sklearn/tree/_export.py,233,compute the color as alpha against white,not
scikit-learn/sklearn/tree/_export.py,235,Return html color code in #RRGGBB format,not
scikit-learn/sklearn/tree/_export.py,239,Fetch appropriate color for node,not
scikit-learn/sklearn/tree/_export.py,241,Initialize colors and bounds if required,not
scikit-learn/sklearn/tree/_export.py,244,Find max and min impurities for multi-output,not
scikit-learn/sklearn/tree/_export.py,249,Find max and min values in leaf nodes for regression,not
scikit-learn/sklearn/tree/_export.py,256,Regression,not
scikit-learn/sklearn/tree/_export.py,259,If multi-output color node by impurity,not
scikit-learn/sklearn/tree/_export.py,264,Generate the node content string,not
scikit-learn/sklearn/tree/_export.py,270,Should labels be shown?,not
scikit-learn/sklearn/tree/_export.py,276,Write node ID,not
scikit-learn/sklearn/tree/_export.py,282,Write decision criteria,not
scikit-learn/sklearn/tree/_export.py,284,"Always write node decision criteria, except for leaves",not
scikit-learn/sklearn/tree/_export.py,297,Write impurity,not
scikit-learn/sklearn/tree/_export.py,308,Write node sample count,not
scikit-learn/sklearn/tree/_export.py,320,Write node class distribution / regression value,not
scikit-learn/sklearn/tree/_export.py,322,For classification this will show the proportion of samples,not
scikit-learn/sklearn/tree/_export.py,327,Regression,not
scikit-learn/sklearn/tree/_export.py,330,Classification,not
scikit-learn/sklearn/tree/_export.py,333,Classification without floating-point weights,not
scikit-learn/sklearn/tree/_export.py,336,Classification with floating-point weights,not
scikit-learn/sklearn/tree/_export.py,338,Strip whitespace,not
scikit-learn/sklearn/tree/_export.py,346,Write node majority class,not
scikit-learn/sklearn/tree/_export.py,350,Only done for single-output classification trees,not
scikit-learn/sklearn/tree/_export.py,361,Clean up any trailing newlines,not
scikit-learn/sklearn/tree/_export.py,386,PostScript compatibility for special characters,not
scikit-learn/sklearn/tree/_export.py,393,validate,not
scikit-learn/sklearn/tree/_export.py,402,The depth of each node for plotting with 'leaf' option,not
scikit-learn/sklearn/tree/_export.py,404,The colors to render each node with,not
scikit-learn/sklearn/tree/_export.py,408,Check length of feature_names before getting into the tree node,not
scikit-learn/sklearn/tree/_export.py,409,Raise error if length of feature_names does not match,not
scikit-learn/sklearn/tree/_export.py,410,n_features_ in the decision_tree,not
scikit-learn/sklearn/tree/_export.py,417,each part writes to out_file,not
scikit-learn/sklearn/tree/_export.py,419,Now recurse the tree and add node & edge attributes,not
scikit-learn/sklearn/tree/_export.py,429,"If required, draw leaf nodes at same depth as each other",not
scikit-learn/sklearn/tree/_export.py,440,Specify node aesthetics,not
scikit-learn/sklearn/tree/_export.py,455,Specify graph & edge aesthetics,not
scikit-learn/sklearn/tree/_export.py,471,Add node with description,not
scikit-learn/sklearn/tree/_export.py,474,Collect ranks for 'leaf' option in plot_options,not
scikit-learn/sklearn/tree/_export.py,492,Add edge to parent,not
scikit-learn/sklearn/tree/_export.py,495,Draw True/False labels if parent is root node,not
scikit-learn/sklearn/tree/_export.py,517,color cropped nodes grey,not
scikit-learn/sklearn/tree/_export.py,522,Add edge to parent,not
scikit-learn/sklearn/tree/_export.py,540,validate,not
scikit-learn/sklearn/tree/_export.py,549,The depth of each node for plotting with 'leaf' option,not
scikit-learn/sklearn/tree/_export.py,551,The colors to render each node with,not
scikit-learn/sklearn/tree/_export.py,563,"traverses _tree.Tree recursively, builds intermediate",not
scikit-learn/sklearn/tree/_export.py,564,"""_reingold_tilford.Tree"" object",not
scikit-learn/sklearn/tree/_export.py,588,important to make sure we're still,not
scikit-learn/sklearn/tree/_export.py,589,inside the axis after drawing the box,not
scikit-learn/sklearn/tree/_export.py,590,this makes sense because the width of a box,not
scikit-learn/sklearn/tree/_export.py,591,is about the same as the distance between boxes,not
scikit-learn/sklearn/tree/_export.py,605,update sizes of all bboxes,not
scikit-learn/sklearn/tree/_export.py,612,get figure to data transform,not
scikit-learn/sklearn/tree/_export.py,613,adjust fontsize to avoid overlap,not
scikit-learn/sklearn/tree/_export.py,614,get max box width and height,not
scikit-learn/sklearn/tree/_export.py,619,width should be around scale_x in axis coordinates,not
scikit-learn/sklearn/tree/_export.py,634,offset things by .5 to center them in plot,not
scikit-learn/sklearn/tree/_export.py,642,root,not
scikit-learn/sklearn/tree/_export.py,955,leaf,not
scikit-learn/sklearn/tree/tests/test_export.py,17,toy sample,not
scikit-learn/sklearn/tree/tests/test_export.py,26,Check correctness of export_graphviz,not
scikit-learn/sklearn/tree/tests/test_export.py,33,Test export code,not
scikit-learn/sklearn/tree/tests/test_export.py,49,Test with feature_names,not
scikit-learn/sklearn/tree/tests/test_export.py,66,Test with class_names,not
scikit-learn/sklearn/tree/tests/test_export.py,84,Test plot_options,not
scikit-learn/sklearn/tree/tests/test_export.py,106,Test max_depth,not
scikit-learn/sklearn/tree/tests/test_export.py,121,Test max_depth with plot_options,not
scikit-learn/sklearn/tree/tests/test_export.py,136,Test multi-output with weighted samples,not
scikit-learn/sklearn/tree/tests/test_export.py,169,Test regression output with plot_options,not
scikit-learn/sklearn/tree/tests/test_export.py,200,Test classifier with degraded learning set,not
scikit-learn/sklearn/tree/tests/test_export.py,213,Check for errors of export_graphviz,not
scikit-learn/sklearn/tree/tests/test_export.py,216,Check not-fitted decision tree error,not
scikit-learn/sklearn/tree/tests/test_export.py,223,Check if it errors when length of feature_names,not
scikit-learn/sklearn/tree/tests/test_export.py,224,mismatches with number of features,not
scikit-learn/sklearn/tree/tests/test_export.py,235,Check error when argument is not an estimator,not
scikit-learn/sklearn/tree/tests/test_export.py,240,Check class_names error,not
scikit-learn/sklearn/tree/tests/test_export.py,245,Check precision error,not
scikit-learn/sklearn/tree/tests/test_export.py,286,"With the current random state, the impurity and the threshold",not
scikit-learn/sklearn/tree/tests/test_export.py,287,will have the number of precision set in the export_graphviz,not
scikit-learn/sklearn/tree/tests/test_export.py,288,function. We will check the number of precision with a strict,not
scikit-learn/sklearn/tree/tests/test_export.py,289,equality. The value reported will have only 2 precision and,not
scikit-learn/sklearn/tree/tests/test_export.py,290,"therefore, only a less equal comparison will be done.",not
scikit-learn/sklearn/tree/tests/test_export.py,292,check value,not
scikit-learn/sklearn/tree/tests/test_export.py,297,check impurity,not
scikit-learn/sklearn/tree/tests/test_export.py,303,check impurity,not
scikit-learn/sklearn/tree/tests/test_export.py,307,check threshold,not
scikit-learn/sklearn/tree/tests/test_export.py,343,testing that leaves at level 1 are not truncated,not
scikit-learn/sklearn/tree/tests/test_export.py,345,testing that the rest of the tree is truncated,not
scikit-learn/sklearn/tree/tests/test_export.py,416,mostly smoke tests,not
scikit-learn/sklearn/tree/tests/test_export.py,417,Check correctness of export_graphviz for criterion = entropy,not
scikit-learn/sklearn/tree/tests/test_export.py,424,Test export code,not
scikit-learn/sklearn/tree/tests/test_export.py,435,mostly smoke tests,not
scikit-learn/sklearn/tree/tests/test_export.py,436,Check correctness of export_graphviz for criterion = gini,not
scikit-learn/sklearn/tree/tests/test_export.py,443,Test export code,not
scikit-learn/sklearn/tree/tests/test_export.py,453,FIXME: to be removed in 0.25,SATD
scikit-learn/sklearn/tree/tests/test_export.py,457,test that a warning is raised when rotate is used.,not
scikit-learn/sklearn/tree/tests/test_export.py,466,Testing if not fitted tree throws the correct error,not
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,29,parents higher than children:,not
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,33,these trees are always binary,not
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,34,parents are centered above children,not
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,42,test that x values are unique per depth / level,not
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,43,we could also do it quicker using defaultdicts..,not
scikit-learn/sklearn/tree/tests/test_reingold_tilford.py,49,reached all leafs,not
scikit-learn/sklearn/tree/tests/test_tree.py,97,toy sample,not
scikit-learn/sklearn/tree/tests/test_tree.py,103,also load the iris dataset,not
scikit-learn/sklearn/tree/tests/test_tree.py,104,and randomly permute it,not
scikit-learn/sklearn/tree/tests/test_tree.py,111,also load the boston dataset,not
scikit-learn/sklearn/tree/tests/test_tree.py,112,and randomly permute it,not
scikit-learn/sklearn/tree/tests/test_tree.py,127,NB: despite their names X_sparse_* are numpy arrays (and not sparse matrices),not
scikit-learn/sklearn/tree/tests/test_tree.py,183,Check classification on a toy dataset.,not
scikit-learn/sklearn/tree/tests/test_tree.py,197,Check classification on a weighted toy dataset.,not
scikit-learn/sklearn/tree/tests/test_tree.py,211,Check regression on a toy dataset.,not
scikit-learn/sklearn/tree/tests/test_tree.py,225,Check on a XOR problem,not
scikit-learn/sklearn/tree/tests/test_tree.py,246,Check consistency on dataset iris.,not
scikit-learn/sklearn/tree/tests/test_tree.py,264,Check consistency on dataset boston house prices.,not
scikit-learn/sklearn/tree/tests/test_tree.py,274,"using fewer features reduces the learning ability of this tree,",not
scikit-learn/sklearn/tree/tests/test_tree.py,275,but reduces training time.,not
scikit-learn/sklearn/tree/tests/test_tree.py,285,Predict probabilities using DecisionTreeClassifier.,not
scikit-learn/sklearn/tree/tests/test_tree.py,304,Check the array representation.,not
scikit-learn/sklearn/tree/tests/test_tree.py,305,Check resize,not
scikit-learn/sklearn/tree/tests/test_tree.py,315,Check when y is pure.,not
scikit-learn/sklearn/tree/tests/test_tree.py,333,Check numerical stability.,not
scikit-learn/sklearn/tree/tests/test_tree.py,356,Check variable importances.,not
scikit-learn/sklearn/tree/tests/test_tree.py,375,Check on iris that importances are the same for all builders,not
scikit-learn/sklearn/tree/tests/test_tree.py,387,Check if variable importance before fit raises ValueError.,not
scikit-learn/sklearn/tree/tests/test_tree.py,394,Check that gini is equivalent to mse for binary output variable,not
scikit-learn/sklearn/tree/tests/test_tree.py,404,The gini index and the mean square error (variance) might differ due,not
scikit-learn/sklearn/tree/tests/test_tree.py,405,to numerical instability. Since those instabilities mainly occurs at,not
scikit-learn/sklearn/tree/tests/test_tree.py,406,"high tree depth, we restrict this maximal depth.",not
scikit-learn/sklearn/tree/tests/test_tree.py,420,Check max_features.,not
scikit-learn/sklearn/tree/tests/test_tree.py,467,use values of max_features that are invalid,not
scikit-learn/sklearn/tree/tests/test_tree.py,490,Test that it gives proper exception on deficient input.,not
scikit-learn/sklearn/tree/tests/test_tree.py,492,predict before fit,not
scikit-learn/sklearn/tree/tests/test_tree.py,498,wrong feature shape for sample,not
scikit-learn/sklearn/tree/tests/test_tree.py,527,min_impurity_split warning,not
scikit-learn/sklearn/tree/tests/test_tree.py,534,Wrong dimensions,not
scikit-learn/sklearn/tree/tests/test_tree.py,540,Test with arrays that are non-contiguous.,not
scikit-learn/sklearn/tree/tests/test_tree.py,546,predict before fitting,not
scikit-learn/sklearn/tree/tests/test_tree.py,551,predict on vector with different dims,not
scikit-learn/sklearn/tree/tests/test_tree.py,557,wrong sample shape,not
scikit-learn/sklearn/tree/tests/test_tree.py,574,apply before fitting,not
scikit-learn/sklearn/tree/tests/test_tree.py,585,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,not
scikit-learn/sklearn/tree/tests/test_tree.py,586,by setting max_leaf_nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,590,test for integer parameter,not
scikit-learn/sklearn/tree/tests/test_tree.py,595,"count samples on nodes, -1 means it is a leaf",not
scikit-learn/sklearn/tree/tests/test_tree.py,600,test for float parameter,not
scikit-learn/sklearn/tree/tests/test_tree.py,605,"count samples on nodes, -1 means it is a leaf",not
scikit-learn/sklearn/tree/tests/test_tree.py,612,Test if leaves contain more than leaf_count training examples,not
scikit-learn/sklearn/tree/tests/test_tree.py,616,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,not
scikit-learn/sklearn/tree/tests/test_tree.py,617,by setting max_leaf_nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,621,test integer parameter,not
scikit-learn/sklearn/tree/tests/test_tree.py,628,drop inner nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,632,test float parameter,not
scikit-learn/sklearn/tree/tests/test_tree.py,639,drop inner nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,658,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,not
scikit-learn/sklearn/tree/tests/test_tree.py,659,by setting max_leaf_nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,673,drop inner nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,681,test case with no weights passed in,not
scikit-learn/sklearn/tree/tests/test_tree.py,696,drop inner nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,728,test integer min_samples_leaf,not
scikit-learn/sklearn/tree/tests/test_tree.py,741,drop inner nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,752,test float min_samples_leaf,not
scikit-learn/sklearn/tree/tests/test_tree.py,765,drop inner nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,789,test if min_impurity_split creates leaves with impurity,not
scikit-learn/sklearn/tree/tests/test_tree.py,790,"[0, min_impurity_split) when min_samples_leaf = 1 and",not
scikit-learn/sklearn/tree/tests/test_tree.py,791,min_samples_split = 2.,not
scikit-learn/sklearn/tree/tests/test_tree.py,795,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,not
scikit-learn/sklearn/tree/tests/test_tree.py,796,by setting max_leaf_nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,801,verify leaf nodes without min_impurity_split less than,not
scikit-learn/sklearn/tree/tests/test_tree.py,802,impurity 1e-7,not
scikit-learn/sklearn/tree/tests/test_tree.py,820,"verify leaf nodes have impurity [0,min_impurity_split] when using",not
scikit-learn/sklearn/tree/tests/test_tree.py,821,min_impurity_split,not
scikit-learn/sklearn/tree/tests/test_tree.py,842,test if min_impurity_decrease ensure that a split is made only if,not
scikit-learn/sklearn/tree/tests/test_tree.py,843,if the impurity decrease is atleast that value,not
scikit-learn/sklearn/tree/tests/test_tree.py,846,test both DepthFirstTreeBuilder and BestFirstTreeBuilder,not
scikit-learn/sklearn/tree/tests/test_tree.py,847,by setting max_leaf_nodes,not
scikit-learn/sklearn/tree/tests/test_tree.py,851,"Check default value of min_impurity_decrease, 1e-7",not
scikit-learn/sklearn/tree/tests/test_tree.py,853,Check with explicit value of 0.05,not
scikit-learn/sklearn/tree/tests/test_tree.py,856,Check with a much lower value of 0.0001,not
scikit-learn/sklearn/tree/tests/test_tree.py,859,Check with a much lower value of 0.1,not
scikit-learn/sklearn/tree/tests/test_tree.py,871,"If current node is a not leaf node, check if the split was",not
scikit-learn/sklearn/tree/tests/test_tree.py,872,justified w.r.t the min_impurity_decrease,not
scikit-learn/sklearn/tree/tests/test_tree.py,930,Check estimators on multi-output problems.,not
scikit-learn/sklearn/tree/tests/test_tree.py,960,toy classification problem,not
scikit-learn/sklearn/tree/tests/test_tree.py,977,toy regression problem,not
scikit-learn/sklearn/tree/tests/test_tree.py,986,Test that n_classes_ and classes_ have proper shape.,not
scikit-learn/sklearn/tree/tests/test_tree.py,988,"Classification, single output",not
scikit-learn/sklearn/tree/tests/test_tree.py,995,"Classification, multi-output",not
scikit-learn/sklearn/tree/tests/test_tree.py,1006,Check class rebalancing.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1018,Check that it works no matter the memory layout,not
scikit-learn/sklearn/tree/tests/test_tree.py,1023,Nothing,not
scikit-learn/sklearn/tree/tests/test_tree.py,1028,C-order,not
scikit-learn/sklearn/tree/tests/test_tree.py,1033,F-order,not
scikit-learn/sklearn/tree/tests/test_tree.py,1038,Contiguous,not
scikit-learn/sklearn/tree/tests/test_tree.py,1043,csr matrix,not
scikit-learn/sklearn/tree/tests/test_tree.py,1048,csc_matrix,not
scikit-learn/sklearn/tree/tests/test_tree.py,1053,Strided,not
scikit-learn/sklearn/tree/tests/test_tree.py,1060,Check sample weighting.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1061,Test that zero-weighted samples are not taken into account,not
scikit-learn/sklearn/tree/tests/test_tree.py,1073,Test that low weighted samples are not taken into account at low depth,not
scikit-learn/sklearn/tree/tests/test_tree.py,1082,Samples of class '2' are still weightier,not
scikit-learn/sklearn/tree/tests/test_tree.py,1087,Samples of class '2' are no longer weightier,not
scikit-learn/sklearn/tree/tests/test_tree.py,1090,Threshold should have moved,not
scikit-learn/sklearn/tree/tests/test_tree.py,1092,Test that sample weighting is the same as having duplicates,not
scikit-learn/sklearn/tree/tests/test_tree.py,1111,Check sample weighting raises errors.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1132,"Iris is balanced, so no effect expected for using 'balanced' weights",not
scikit-learn/sklearn/tree/tests/test_tree.py,1139,Make a multi-output problem with three copies of Iris,not
scikit-learn/sklearn/tree/tests/test_tree.py,1141,Create user-defined weights that should balance over the outputs,not
scikit-learn/sklearn/tree/tests/test_tree.py,1148,"Check against multi-output ""auto"" which should also have no effect",not
scikit-learn/sklearn/tree/tests/test_tree.py,1153,"Inflate importance of class 1, check against user-defined weights",not
scikit-learn/sklearn/tree/tests/test_tree.py,1163,Check that sample_weight and class_weight are multiplicative,not
scikit-learn/sklearn/tree/tests/test_tree.py,1177,Test if class_weight raises errors and warnings when expected.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1181,Invalid preset string,not
scikit-learn/sklearn/tree/tests/test_tree.py,1188,Not a list or preset for multi-output,not
scikit-learn/sklearn/tree/tests/test_tree.py,1193,Incorrect length list for multi-output,not
scikit-learn/sklearn/tree/tests/test_tree.py,1205,Test greedy trees with max_depth + 1 leafs.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1212,"max_leaf_nodes in (0, 1) should raise ValueError",not
scikit-learn/sklearn/tree/tests/test_tree.py,1225,Test precedence of max_leaf_nodes over max_depth.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1234,Ensure property arrays' memory stays alive when tree disappears,not
scikit-learn/sklearn/tree/tests/test_tree.py,1235,non-regression for #2726,not
scikit-learn/sklearn/tree/tests/test_tree.py,1240,"if pointing to freed memory, contents may be arbitrary",not
scikit-learn/sklearn/tree/tests/test_tree.py,1260,do not check extra random trees,not
scikit-learn/sklearn/tree/tests/test_tree.py,1287,Test if the warning for too large inputs is appropriate.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1308,Sanity check: we cannot request more memory than the size of the address,not
scikit-learn/sklearn/tree/tests/test_tree.py,1309,space. Currently raises OverflowError.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1315,Non-regression test: MemoryError used to be dropped by Cython,not
scikit-learn/sklearn/tree/tests/test_tree.py,1316,"because of missing ""except *"".",not
scikit-learn/sklearn/tree/tests/test_tree.py,1329,Gain testing time,not
scikit-learn/sklearn/tree/tests/test_tree.py,1339,Check the default (depth first search),not
scikit-learn/sklearn/tree/tests/test_tree.py,1380,"Due to numerical instability of MSE and too strict test, we limit the",not
scikit-learn/sklearn/tree/tests/test_tree.py,1381,maximal depth,not
scikit-learn/sklearn/tree/tests/test_tree.py,1391,Check max_features,not
scikit-learn/sklearn/tree/tests/test_tree.py,1400,Check min_samples_split,not
scikit-learn/sklearn/tree/tests/test_tree.py,1410,Check min_samples_leaf,not
scikit-learn/sklearn/tree/tests/test_tree.py,1420,Check best-first search,not
scikit-learn/sklearn/tree/tests/test_tree.py,1435,Check various criterion,not
scikit-learn/sklearn/tree/tests/test_tree.py,1462,n_samples set n_feature to ease construction of a simultaneous,not
scikit-learn/sklearn/tree/tests/test_tree.py,1463,construction of a csr and csc matrix,not
scikit-learn/sklearn/tree/tests/test_tree.py,1467,"Generate X, y",not
scikit-learn/sklearn/tree/tests/test_tree.py,1492,"Ensure that X_sparse_test owns its data, indices and indptr array",not
scikit-learn/sklearn/tree/tests/test_tree.py,1495,Ensure that we have explicit zeros,not
scikit-learn/sklearn/tree/tests/test_tree.py,1499,Perform the comparison,not
scikit-learn/sklearn/tree/tests/test_tree.py,1614,TODO: remove in v0.24,SATD
scikit-learn/sklearn/tree/tests/test_tree.py,1644,Assert that leaves index are correct,not
scikit-learn/sklearn/tree/tests/test_tree.py,1649,Ensure only one leave node per sample,not
scikit-learn/sklearn/tree/tests/test_tree.py,1654,Ensure max depth is consistent with sum of indicator,not
scikit-learn/sklearn/tree/tests/test_tree.py,1673,Currently we don't support sparse y,not
scikit-learn/sklearn/tree/tests/test_tree.py,1754,Test MAE where sample weights are non-uniform (as illustrated above):,not
scikit-learn/sklearn/tree/tests/test_tree.py,1760,Test MAE where all sample weights are uniform:,not
scikit-learn/sklearn/tree/tests/test_tree.py,1766,Test MAE where a `sample_weight` is not explicitly provided.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1767,"This is equivalent to providing uniform sample weights, though",not
scikit-learn/sklearn/tree/tests/test_tree.py,1768,the internal logic is different:,not
scikit-learn/sklearn/tree/tests/test_tree.py,1775,Let's check whether copy of our criterion has the same type,not
scikit-learn/sklearn/tree/tests/test_tree.py,1776,and properties as original,not
scikit-learn/sklearn/tree/tests/test_tree.py,1802,try to make empty leaf by using near infinite value.,not
scikit-learn/sklearn/tree/tests/test_tree.py,1857,single node tree,not
scikit-learn/sklearn/tree/tests/test_tree.py,1861,pruned single node tree,not
scikit-learn/sklearn/tree/tests/test_tree.py,1869,generate trees with increasing alphas,not
scikit-learn/sklearn/tree/tests/test_tree.py,1876,A pruned tree must be a subtree of the previous tree (which had a,not
scikit-learn/sklearn/tree/tests/test_tree.py,1877,smaller ccp_alpha),not
scikit-learn/sklearn/tree/tests/test_tree.py,1905,is a leaf,not
scikit-learn/sklearn/tree/tests/test_tree.py,1909,not a leaf,not
scikit-learn/sklearn/preprocessing/_discretization.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/preprocessing/_discretization.py,3,Author: Henry Lin <hlin117@gmail.com>,not
scikit-learn/sklearn/preprocessing/_discretization.py,4,Tom Dupré la Tour,not
scikit-learn/sklearn/preprocessing/_discretization.py,6,License: BSD,not
scikit-learn/sklearn/preprocessing/_discretization.py,181,fixes import loops,not
scikit-learn/sklearn/preprocessing/_discretization.py,183,Deterministic initialization with uniform spacing,not
scikit-learn/sklearn/preprocessing/_discretization.py,187,1D k-means procedure,not
scikit-learn/sklearn/preprocessing/_discretization.py,190,"Must sort, centers may be unsorted even with sorted init",not
scikit-learn/sklearn/preprocessing/_discretization.py,195,"Remove bins whose width are too small (i.e., <= 1e-8)",not
scikit-learn/sklearn/preprocessing/_discretization.py,212,Fit the OneHotEncoder with toy datasets,not
scikit-learn/sklearn/preprocessing/_discretization.py,213,so that it's ready for use after the KBinsDiscretizer is fitted,not
scikit-learn/sklearn/preprocessing/_discretization.py,276,Values which are close to a bin edge are susceptible to numeric,not
scikit-learn/sklearn/preprocessing/_discretization.py,277,instability. Add eps to X so these values are binned correctly,not
scikit-learn/sklearn/preprocessing/_discretization.py,278,with respect to their decimal truncation. See documentation of,not
scikit-learn/sklearn/preprocessing/_discretization.py,279,numpy.isclose for an explanation of ``rtol`` and ``atol``.,not
scikit-learn/sklearn/preprocessing/_encoders.py,1,Authors: Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/preprocessing/_encoders.py,2,Joris Van den Bossche <jorisvandenbossche@gmail.com>,not
scikit-learn/sklearn/preprocessing/_encoders.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/preprocessing/_encoders.py,42,"if not a dataframe, do normal check_array validation",not
scikit-learn/sklearn/preprocessing/_encoders.py,51,"pandas dataframe, do validation later column by column, in order",not
scikit-learn/sklearn/preprocessing/_encoders.py,52,to keep the dtype information to be used in the encoder.,not
scikit-learn/sklearn/preprocessing/_encoders.py,68,pandas dataframes,not
scikit-learn/sklearn/preprocessing/_encoders.py,70,"numpy arrays, sparse arrays",not
scikit-learn/sklearn/preprocessing/_encoders.py,126,Set the problematic rows to an acceptable value and,not
scikit-learn/sklearn/preprocessing/_encoders.py,127,continue `The rows are marked `X_mask` and will be,not
scikit-learn/sklearn/preprocessing/_encoders.py,128,removed later.,not
scikit-learn/sklearn/preprocessing/_encoders.py,130,cast Xi into the largest string type necessary,not
scikit-learn/sklearn/preprocessing/_encoders.py,131,to handle different lengths of numpy strings,not
scikit-learn/sklearn/preprocessing/_encoders.py,139,"We use check_unknown=False, since _encode_check_unknown was",not
scikit-learn/sklearn/preprocessing/_encoders.py,140,already called above.,not
scikit-learn/sklearn/preprocessing/_encoders.py,312,If we have both dropped columns and ignored unknown,not
scikit-learn/sklearn/preprocessing/_encoders.py,313,"values, there will be ambiguous cells. This creates difficulties",not
scikit-learn/sklearn/preprocessing/_encoders.py,314,in interpreting the model.,not
scikit-learn/sklearn/preprocessing/_encoders.py,427,validation of X happens in _check_X called by _transform,not
scikit-learn/sklearn/preprocessing/_encoders.py,434,"We remove all the dropped categories from mask, and decrement all",not
scikit-learn/sklearn/preprocessing/_encoders.py,435,categories that occur after them to avoid an empty column.,not
scikit-learn/sklearn/preprocessing/_encoders.py,441,drop='if_binary' but feature isn't binary,not
scikit-learn/sklearn/preprocessing/_encoders.py,443,set to cardinality to not drop from X_int,not
scikit-learn/sklearn/preprocessing/_encoders.py,446,dropped,not
scikit-learn/sklearn/preprocessing/_encoders.py,504,validate shape of passed X,not
scikit-learn/sklearn/preprocessing/_encoders.py,510,create resulting array of appropriate dtype,not
scikit-learn/sklearn/preprocessing/_encoders.py,524,Only happens if there was a column with a unique,not
scikit-learn/sklearn/preprocessing/_encoders.py,525,category. In this case we just fill the column with this,not
scikit-learn/sklearn/preprocessing/_encoders.py,526,unique category value.,not
scikit-learn/sklearn/preprocessing/_encoders.py,532,"for sparse X argmax returns 2D matrix, ensure 1D array",not
scikit-learn/sklearn/preprocessing/_encoders.py,537,ignored unknown categories: we have a row of all zero,not
scikit-learn/sklearn/preprocessing/_encoders.py,540,drop will either be None or handle_unknown will be error. If,not
scikit-learn/sklearn/preprocessing/_encoders.py,541,"self.drop_idx_ is not None, then we can safely assume that all of",not
scikit-learn/sklearn/preprocessing/_encoders.py,542,the nulls in each column are the dropped value,not
scikit-learn/sklearn/preprocessing/_encoders.py,550,if ignored are found: potentially need to upcast result to,not
scikit-learn/sklearn/preprocessing/_encoders.py,551,insert None values,not
scikit-learn/sklearn/preprocessing/_encoders.py,723,validate shape of passed X,not
scikit-learn/sklearn/preprocessing/_encoders.py,729,create resulting array of appropriate dtype,not
scikit-learn/sklearn/preprocessing/_data.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/preprocessing/_data.py,2,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/preprocessing/_data.py,3,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/preprocessing/_data.py,4,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/preprocessing/_data.py,5,Eric Martin <eric@ericmart.in>,not
scikit-learn/sklearn/preprocessing/_data.py,6,Giorgio Patrini <giorgio.patrini@anu.edu.au>,not
scikit-learn/sklearn/preprocessing/_data.py,7,Eric Chang <ericchang2017@u.northwestern.edu>,not
scikit-learn/sklearn/preprocessing/_data.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/preprocessing/_data.py,68,"if we are fitting on 1D arrays, scale might be a scalar",not
scikit-learn/sklearn/preprocessing/_data.py,75,New array to avoid side-effects,not
scikit-learn/sklearn/preprocessing/_data.py,140,noqa,not
scikit-learn/sklearn/preprocessing/_data.py,162,Xr is a view on the original array that enables easy use of,not
scikit-learn/sklearn/preprocessing/_data.py,163,broadcasting on the axis in which we are interested in,not
scikit-learn/sklearn/preprocessing/_data.py,168,Verify that mean_1 is 'close to zero'. If X contains very,not
scikit-learn/sklearn/preprocessing/_data.py,169,"large values, mean_1 can also be very large, due to a lack of",not
scikit-learn/sklearn/preprocessing/_data.py,170,"precision of mean_. In this case, a pre-scaling of the",not
scikit-learn/sklearn/preprocessing/_data.py,171,"concerned feature is efficient, for instance by its mean or",not
scikit-learn/sklearn/preprocessing/_data.py,172,maximum.,not
scikit-learn/sklearn/preprocessing/_data.py,185,"If mean_2 is not 'close to zero', it comes from the fact that",not
scikit-learn/sklearn/preprocessing/_data.py,186,"scale_ is very small so that mean_2 = mean_1/scale_ > 0, even",not
scikit-learn/sklearn/preprocessing/_data.py,187,if mean_1 was close to zero. The problem is thus essentially,not
scikit-learn/sklearn/preprocessing/_data.py,188,due to the lack of precision of mean_. A solution is then to,not
scikit-learn/sklearn/preprocessing/_data.py,189,subtract the mean again:,not
scikit-learn/sklearn/preprocessing/_data.py,306,"Checking one attribute is enough, becase they are all set together",not
scikit-learn/sklearn/preprocessing/_data.py,307,in partial_fit,not
scikit-learn/sklearn/preprocessing/_data.py,334,Reset internal state before fitting,not
scikit-learn/sklearn/preprocessing/_data.py,495,noqa,not
scikit-learn/sklearn/preprocessing/_data.py,496,"Unlike the scaler object, this function allows 1d input.",not
scikit-learn/sklearn/preprocessing/_data.py,497,"If copy is required, it will be done inside the scaler object.",not
scikit-learn/sklearn/preprocessing/_data.py,630,noqa,not
scikit-learn/sklearn/preprocessing/_data.py,644,"Checking one attribute is enough, becase they are all set together",not
scikit-learn/sklearn/preprocessing/_data.py,645,in partial_fit,not
scikit-learn/sklearn/preprocessing/_data.py,665,Reset internal state before fitting,not
scikit-learn/sklearn/preprocessing/_data.py,700,"Even in the case of `with_mean=False`, we update the mean anyway",not
scikit-learn/sklearn/preprocessing/_data.py,701,This is needed for the incremental computation of the var,not
scikit-learn/sklearn/preprocessing/_data.py,702,See incr_mean_variance_axis and _incremental_mean_variance_axis,not
scikit-learn/sklearn/preprocessing/_data.py,704,"if n_samples_seen_ is an integer (i.e. no missing values), we need to",not
scikit-learn/sklearn/preprocessing/_data.py,705,"transform it to a NumPy array of shape (n_features,) required by",not
scikit-learn/sklearn/preprocessing/_data.py,706,incr_mean_variance_axis and _incremental_variance_axis,not
scikit-learn/sklearn/preprocessing/_data.py,729,First pass,not
scikit-learn/sklearn/preprocessing/_data.py,732,Next passes,not
scikit-learn/sklearn/preprocessing/_data.py,748,First pass,not
scikit-learn/sklearn/preprocessing/_data.py,765,"for backward-compatibility, reduce n_samples_seen_ to an integer",not
scikit-learn/sklearn/preprocessing/_data.py,766,if the number of samples is the same for each feature (i.e. no,not
scikit-learn/sklearn/preprocessing/_data.py,767,missing values),not
scikit-learn/sklearn/preprocessing/_data.py,925,"Checking one attribute is enough, becase they are all set together",not
scikit-learn/sklearn/preprocessing/_data.py,926,in partial_fit,not
scikit-learn/sklearn/preprocessing/_data.py,942,Reset internal state before fitting,not
scikit-learn/sklearn/preprocessing/_data.py,1068,noqa,not
scikit-learn/sklearn/preprocessing/_data.py,1069,"Unlike the scaler object, this function allows 1d input.",not
scikit-learn/sklearn/preprocessing/_data.py,1071,"If copy is required, it will be done inside the scaler object.",not
scikit-learn/sklearn/preprocessing/_data.py,1198,"at fit, convert sparse matrices to csc for optimized computation of",not
scikit-learn/sklearn/preprocessing/_data.py,1199,the quantiles,not
scikit-learn/sklearn/preprocessing/_data.py,1596,What follows is a faster implementation of:,not
scikit-learn/sklearn/preprocessing/_data.py,1597,"for i, comb in enumerate(combinations):",not
scikit-learn/sklearn/preprocessing/_data.py,1598,"XP[:, i] = X[:, comb].prod(1)",not
scikit-learn/sklearn/preprocessing/_data.py,1599,This implementation uses two optimisations.,not
scikit-learn/sklearn/preprocessing/_data.py,1600,"First one is broadcasting,",not
scikit-learn/sklearn/preprocessing/_data.py,1601,"multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]",not
scikit-learn/sklearn/preprocessing/_data.py,1602,"multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]",not
scikit-learn/sklearn/preprocessing/_data.py,1603,...,not
scikit-learn/sklearn/preprocessing/_data.py,1604,"multiply ([X[:, start:end], X[:, start]) -> ...",not
scikit-learn/sklearn/preprocessing/_data.py,1605,Second optimisation happens for degrees >= 3.,not
scikit-learn/sklearn/preprocessing/_data.py,1606,Xi^3 is computed reusing previous computation:,not
scikit-learn/sklearn/preprocessing/_data.py,1607,Xi^3 = Xi^2 * Xi.,not
scikit-learn/sklearn/preprocessing/_data.py,1615,d = 0,not
scikit-learn/sklearn/preprocessing/_data.py,1622,d >= 1,not
scikit-learn/sklearn/preprocessing/_data.py,1635,"XP[:, start:end] are terms of degree d - 1",not
scikit-learn/sklearn/preprocessing/_data.py,1636,that exclude feature #feature_idx.,not
scikit-learn/sklearn/preprocessing/_data.py,2024,Needed for backported inspect.signature compatibility with PyPy,not
scikit-learn/sklearn/preprocessing/_data.py,2118,Shift columns to the right.,not
scikit-learn/sklearn/preprocessing/_data.py,2120,Column indices of dummy feature are 0 everywhere.,not
scikit-learn/sklearn/preprocessing/_data.py,2122,"Row indices of dummy feature are 0, ..., n_samples-1.",not
scikit-learn/sklearn/preprocessing/_data.py,2124,Prepend the dummy feature n_samples times.,not
scikit-learn/sklearn/preprocessing/_data.py,2128,Shift index pointers since we need to add n_samples elements.,not
scikit-learn/sklearn/preprocessing/_data.py,2130,indptr[0] must be 0.,not
scikit-learn/sklearn/preprocessing/_data.py,2132,"Row indices of dummy feature are 0, ..., n_samples-1.",not
scikit-learn/sklearn/preprocessing/_data.py,2134,Prepend the dummy feature n_samples times.,not
scikit-learn/sklearn/preprocessing/_data.py,2278,"Due to floating-point precision error in `np.nanpercentile`,",not
scikit-learn/sklearn/preprocessing/_data.py,2279,make sure that quantiles are monotonically increasing.,not
scikit-learn/sklearn/preprocessing/_data.py,2280,Upstream issue in numpy:,not
scikit-learn/sklearn/preprocessing/_data.py,2281,https://github.com/numpy/numpy/issues/14685,not
scikit-learn/sklearn/preprocessing/_data.py,2319,"if no nnz, an error will be raised for computing the",not
scikit-learn/sklearn/preprocessing/_data.py,2320,quantiles. Force the quantiles to be zeros.,not
scikit-learn/sklearn/preprocessing/_data.py,2326,"due to floating-point precision error in `np.nanpercentile`,",not
scikit-learn/sklearn/preprocessing/_data.py,2327,make sure the quantiles are monotonically increasing,not
scikit-learn/sklearn/preprocessing/_data.py,2328,Upstream issue in numpy:,not
scikit-learn/sklearn/preprocessing/_data.py,2329,https://github.com/numpy/numpy/issues/14685,not
scikit-learn/sklearn/preprocessing/_data.py,2375,Create the quantiles of reference,not
scikit-learn/sklearn/preprocessing/_data.py,2400,"for inverse transform, match a uniform distribution",not
scikit-learn/sklearn/preprocessing/_data.py,2401,hide NaN comparison warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2404,else output distribution is already a uniform distribution,not
scikit-learn/sklearn/preprocessing/_data.py,2406,find index for lower and higher bounds,not
scikit-learn/sklearn/preprocessing/_data.py,2407,hide NaN comparison warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2420,Interpolate in one direction and in the other and take the,not
scikit-learn/sklearn/preprocessing/_data.py,2421,mean. This is in case of repeated values in the features,not
scikit-learn/sklearn/preprocessing/_data.py,2422,and hence repeated quantiles,not
scikit-learn/sklearn/preprocessing/_data.py,2423,,not
scikit-learn/sklearn/preprocessing/_data.py,2424,"If we don't do this, only one extreme of the duplicated is",not
scikit-learn/sklearn/preprocessing/_data.py,2425,"used (the upper when we do ascending, and the",not
scikit-learn/sklearn/preprocessing/_data.py,2426,lower for descending). We take the mean of these two,not
scikit-learn/sklearn/preprocessing/_data.py,2437,"for forward transform, match the output distribution",not
scikit-learn/sklearn/preprocessing/_data.py,2439,hide NaN comparison warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2442,find the value to clip the data to avoid mapping to,not
scikit-learn/sklearn/preprocessing/_data.py,2443,infinity. Clip such that the inverse transform will be,not
scikit-learn/sklearn/preprocessing/_data.py,2444,consistent,not
scikit-learn/sklearn/preprocessing/_data.py,2449,else output distribution is uniform and the ppf is the,not
scikit-learn/sklearn/preprocessing/_data.py,2450,identity function so we let X_col unchanged,not
scikit-learn/sklearn/preprocessing/_data.py,2457,"In theory reset should be equal to `in_fit`, but there are tests",not
scikit-learn/sklearn/preprocessing/_data.py,2458,checking the input number of feature and they expect a specific,not
scikit-learn/sklearn/preprocessing/_data.py,2459,"string, which is not the same one raised by check_n_features. So we",not
scikit-learn/sklearn/preprocessing/_data.py,2460,don't check n_features_in_ here for now (it's done with adhoc code in,not
scikit-learn/sklearn/preprocessing/_data.py,2461,the estimator anyway).,not
scikit-learn/sklearn/preprocessing/_data.py,2462,TODO: set reset=in_fit when addressing reset in,SATD
scikit-learn/sklearn/preprocessing/_data.py,2463,predict/transform/etc.,not
scikit-learn/sklearn/preprocessing/_data.py,2470,we only accept positive sparse matrix when ignore_implicit_zeros is,not
scikit-learn/sklearn/preprocessing/_data.py,2471,false and that we call fit or transform.,not
scikit-learn/sklearn/preprocessing/_data.py,2472,hide NaN comparison warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2478,check the output distribution,not
scikit-learn/sklearn/preprocessing/_data.py,2489,check that the dimension of X are adequate with the fitted data,not
scikit-learn/sklearn/preprocessing/_data.py,2814,if call from fit(),not
scikit-learn/sklearn/preprocessing/_data.py,2815,force copy so that fit does not change X inplace,not
scikit-learn/sklearn/preprocessing/_data.py,2820,hide NaN warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2828,hide NaN warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2861,hide NaN warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2910,hide NaN warnings,not
scikit-learn/sklearn/preprocessing/_data.py,2933,when x >= 0,not
scikit-learn/sklearn/preprocessing/_data.py,2936,lmbda != 0,not
scikit-learn/sklearn/preprocessing/_data.py,2939,when x < 0,not
scikit-learn/sklearn/preprocessing/_data.py,2943,lmbda == 2,not
scikit-learn/sklearn/preprocessing/_data.py,2954,binary mask,not
scikit-learn/sklearn/preprocessing/_data.py,2956,when x >= 0,not
scikit-learn/sklearn/preprocessing/_data.py,2959,lmbda != 0,not
scikit-learn/sklearn/preprocessing/_data.py,2962,when x < 0,not
scikit-learn/sklearn/preprocessing/_data.py,2965,lmbda == 2,not
scikit-learn/sklearn/preprocessing/_data.py,2976,the computation of lambda is influenced by NaNs so we need to,not
scikit-learn/sklearn/preprocessing/_data.py,2977,get rid of them,not
scikit-learn/sklearn/preprocessing/_data.py,3000,the computation of lambda is influenced by NaNs so we need to,not
scikit-learn/sklearn/preprocessing/_data.py,3001,get rid of them,not
scikit-learn/sklearn/preprocessing/_data.py,3003,"choosing bracket -2, 2 like for boxcox",not
scikit-learn/sklearn/preprocessing/_label.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/preprocessing/_label.py,2,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/preprocessing/_label.py,3,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/preprocessing/_label.py,4,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/preprocessing/_label.py,5,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/preprocessing/_label.py,6,Hamzeh Alsalhi <ha258@cornell.edu>,not
scikit-learn/sklearn/preprocessing/_label.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/preprocessing/_label.py,38,"only used in _encode below, see docstring there for details",not
scikit-learn/sklearn/preprocessing/_label.py,44,unique sorts,not
scikit-learn/sklearn/preprocessing/_label.py,59,"only used in _encode below, see docstring there for details",not
scikit-learn/sklearn/preprocessing/_label.py,273,transform of empty array is empty array,not
scikit-learn/sklearn/preprocessing/_label.py,294,inverse transform of empty array is empty array,not
scikit-learn/sklearn/preprocessing/_label.py,608,XXX Workaround that will be removed when list of list format is,SATD
scikit-learn/sklearn/preprocessing/_label.py,609,dropped,not
scikit-learn/sklearn/preprocessing/_label.py,624,To account for pos_label == 0 in the dense case,not
scikit-learn/sklearn/preprocessing/_label.py,662,pick out the known labels from y,not
scikit-learn/sklearn/preprocessing/_label.py,694,preserve label ordering,not
scikit-learn/sklearn/preprocessing/_label.py,716,Find the argmax for each row in y where y is a CSR matrix,not
scikit-learn/sklearn/preprocessing/_label.py,725,picks out all indices obtaining the maximum per row,not
scikit-learn/sklearn/preprocessing/_label.py,728,For corner case where last row has a max of 0,not
scikit-learn/sklearn/preprocessing/_label.py,732,Gets the index of the first argmax in each row from y_i_all_argmax,not
scikit-learn/sklearn/preprocessing/_label.py,734,first argmax of each row,not
scikit-learn/sklearn/preprocessing/_label.py,737,Handle rows of all 0,not
scikit-learn/sklearn/preprocessing/_label.py,740,Handles rows with max of 0 that contain negative numbers,not
scikit-learn/sklearn/preprocessing/_label.py,765,Perform thresholding,not
scikit-learn/sklearn/preprocessing/_label.py,777,Inverse transform data,not
scikit-learn/sklearn/preprocessing/_label.py,912,Automatically increment on new class,not
scikit-learn/sklearn/preprocessing/_label.py,917,sort classes and reorder columns,not
scikit-learn/sklearn/preprocessing/_label.py,920,(make safe for tuples),not
scikit-learn/sklearn/preprocessing/_label.py,925,ensure yt.indices keeps its current dtype,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1,Authors:,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2,,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,3,Giorgio Patrini,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,4,,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,63,Make some data to be used many times,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,96,Test Polynomial Features,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,142,test some unicode,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,248,This degree should always be one more than the highest degree supported by,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,249,_csr_expansion.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,292,Test scaling of dataset along single axis,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,299,cast only after scaling done,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,317,check inverse transform,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,321,Constant feature,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,333,Ensure scaling does not affect dtype,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,347,1-d inputs,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,360,Test numerical stability of scaling,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,361,np.log(1e-5) is taken because of its floating point representation,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,362,was empirically found to cause numerical problems with np.mean & np.std.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,365,This does not raise a warning as the number of samples is too low,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,366,to trigger the problem in recent numpy,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,370,"with 2 more samples, the std computation run into numerical issues:",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,380,Large values can cause (often recoverable) numerical stability issues:,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,394,Test scaling of 2d array along first axis,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,399,first feature is always of zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,408,Check that X has been copied,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,411,check inverse transform,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,424,Check that the data hasn't been modified,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,431,Check that X has not been copied,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,435,"first feature is a constant, non zero feature",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,441,Check that X has not been copied,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,446,Test if the scaler will not overflow on float16 numpy arrays,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,448,float16 has a maximum of 65500.0. On the worst case 5 * 200000 is 100000,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,449,which is enough to overflow the data type,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,456,Calculate the float64 equivalent to verify result,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,459,"Overflow calculations may cause -inf, inf, or nan. Since there is no nan",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,460,"input, all of the outputs should be finite. This may be redundant since a",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,461,FloatingPointError exception will be thrown on overflow above.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,464,The normal distribution is very unlikely to go above 4. At 4.0-8.0 the,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,465,float16 precision is 2^-8 which is around 0.004. Thus only 2 decimals are,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,466,checked to account for precision differences.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,480,Test if partial_fit run over many batches of size 1 and 50,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,481,gives the same results as fit,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,486,Test mean at the end of the process,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,503,Test std after 1 step,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,518,"Test std until the end of partial fits, and",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,520,Clean estimator,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,530,Test if partial_fit run over many batches of size 1 and 50,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,531,gives the same results as fit,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,536,Test mean at the end of the process,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,544,Nones,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,547,Test std after 1 step,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,559,no constants,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,561,"Test std until the end of partial fits, and",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,563,Clean estimator,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,576,Test if the incremental computation introduces significative errors,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,577,for large datasets with values of large magniture,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,590,"Regardless of abs values, they must not be more diff 6 significant digits",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,595,NOTE Be aware that for much larger offsets std is very unstable (last,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,596,assert) while mean is OK.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,598,Sparse input,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,606,with_mean=False is required with sparse input,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,611,chunk = sparse.csr_matrix(data_chunks),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,614,"Regardless of magnitude, they must not differ more than of 6 digits",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,622,Check that sparsity is not destroyed,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,638,Check some postconditions after applying partial_fit and transform,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,652,No change,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,658,as less or equal,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,660,(i+1) because the Scaler has been already fitted,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,667,default params,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,674,"not default params: min=1, max=2",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,682,"min=-.5, max=.6",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,690,raises on invalid range,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,697,Check min max scaler on toy data with zero variance features,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,706,default params,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,722,not default params,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,730,function interface,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,745,Test scaling of dataset along single axis,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,752,cast only after scaling done,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,764,check inverse transform,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,768,Constant feature,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,776,Function interface,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,787,first feature is always of zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,830,Check that X has not been modified (copy),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,880,test that the scaler return identity when with_mean and with_std are,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,881,False,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,930,test that scaler converts integer input to floating,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,931,for both sparse and dense matrices,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,934,first feature is always of zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,978,Check that X has not been modified (copy),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,999,Check that StandardScaler.fit does not change input,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1002,first feature is always of zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1025,check scaling and fit with direct calls on sparse data,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1036,check transform and inverse_transform after a fit on a dense array,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1053,Check if non finite inputs raise ValueError,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1073,check consistent type of attributes,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1092,check that the scaler is working when there is not data materialized in a,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1093,column of a sparse matrix,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1107,Test robust scaling of 2d array along first axis,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1110,first feature is always of zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1123,Check the equivalence of the fitting with dense and sparse matrices,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1143,Check RobustScaler on transforming csr matrix with one row,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1182,uniform output distribution,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1187,normal output distribution,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1193,make sure it is possible to take the inverse of a sparse matrix,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1194,which contain negative value; this is the case in the iris dataset,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1245,check that an error is raised at fit time,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1250,check that an error is raised at transform time,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1259,check that an error is raised at inverse_transform time,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1264,check that an error is raised if input is scalar,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1268,check that a warning is raised is n_quantiles > n_samples,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1287,dense case -> warning raise,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1300,consider the case where sparse entries are missing values and user-given,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1301,zeros are to be considered,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1335,check in conjunction with subsampling,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1355,"using the a uniform output, each entry of X should be map between 0 and 1",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1356,and equally spaced,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1376,Test that subsampling the input yield to a consistent results We check,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1377,"that the computed quantiles are almost mapped to a [0, 1] vector where",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1378,values are equally spaced. The infinite norm is checked to be smaller,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1379,than a given threshold. This is repeated 5 times.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1381,dense support,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1397,each random subsampling yield a unique approximation to the expected,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1398,linspace CDF,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1401,sparse support,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1415,each random subsampling yield a unique approximation to the expected,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1416,linspace CDF,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1466,Lower and upper bounds are manually mapped. We checked that in the case,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1467,"of a constant feature and binary feature, the bounds are properly mapped.",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1473,check sparse and dense are consistent,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1482,check the consistency of the bounds by learning on 1 matrix,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1483,and transforming another,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1494,check that values outside of the range learned will be mapped properly.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1528,check that the quantile of the first column is all NaN,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1530,all other column should not contain NaN,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1536,Non-regression test for:,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1537,https://github.com/scikit-learn/scikit-learn/issues/15733,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1538,Taken from upstream bug report:,SATD
scikit-learn/sklearn/preprocessing/tests/test_data.py,1539,https://github.com/numpy/numpy/issues/14685,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1547,Check that the estimated quantile threasholds are monotically,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1548,increasing:,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1571,first feature is always of zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1580,test csc has same outcome,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1584,raises value error on axis != 0,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1591,Check that X has not been copied,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1598,null scale,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1622,Check RobustScaler on toy data with zero variance features,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1630,"NOTE: for such a small sample size, what we expect in the third column",SATD
scikit-learn/sklearn/preprocessing/tests/test_data.py,1631,depends HEAVILY on the method used to calculate quantiles. The values,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1632,here were calculated to fit the quantiles produces by np.percentile,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1633,using numpy 1.9 Calculating quantiles with,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1634,scipy.stats.mstats.scoreatquantile or scipy.stats.mstats.mquantiles,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1635,would yield very different results!,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1643,make sure new data gets transformed correctly,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1655,Check MaxAbsScaler on toy data with zero variance features,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1671,make sure new data gets transformed correctly,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1682,function interface,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1686,sparse data,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1704,Check MaxAbsScaler on toy data with a large negative value,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1720,Check MaxAbsScaler on transforming csr matrix with one row,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1732,Test scaling of dataset along single axis,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1739,cast only after scaling done,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1748,check inverse transform,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1752,Constant feature,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1759,function interface,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1766,Test if partial_fit run over many batches of size 1 and 50,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1767,gives the same results as fit,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1772,Test mean at the end of the process,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1801,Test std after 1 step,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1812,"Test std until the end of partial fits, and",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1814,Clean estimator,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1828,set the row number 3 to zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1831,set the row number 3 to zero without pruning (can happen in real life),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1836,build the pruned variant using the regular constructor,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1839,check inputs that support the no-copy optim,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1858,check input for which copy=False won't prevent a copy,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1877,set the row number 3 to zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1880,set the row number 3 to zero without pruning (can happen in real life),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1885,build the pruned variant using the regular constructor,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1888,check inputs that support the no-copy optim,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1906,check input for which copy=False won't prevent a copy,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1925,set the row number 3 to zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1928,set the row number 3 to zero without pruning (can happen in real life),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1933,build the pruned variant using the regular constructor,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1936,check inputs that support the no-copy optim,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1955,check input for which copy=False won't prevent a copy,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1970,check that we normalize by a positive number even for negative data,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1973,set the row number 3 to zero,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1975,check for mixed data where the value with,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1976,largest magnitude is negative,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1991,Test normalize function,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,1992,Only tests functionality not used by the tests for Normalizer.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2021,Test return_norm,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2091,Cannot use threshold < 0 for sparse,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2097,Test that KernelCenterer is equivalent to StandardScaler,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2098,in feature space,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2106,center fit time matrix,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2112,center predict time matrix,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2122,Cross-validate a regression on four coplanar points with the same,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2123,value. Use precomputed kernel to ensure Pipeline with KernelCenterer,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2124,is treated as a _pairwise operation.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2131,did the pipeline set the _pairwise attribute?,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2134,"test cross-validation, score should be almost perfect",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2135,NB: this test is pretty vacuous -- it's mainly to test integration,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2136,of Pipeline and KernelCenterer,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2181,Scalers that have a partial_fit method,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2188,"with a different shape, this may break the scaler unless the internal",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2189,state is reset,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2217,Make sure we get the original input when applying transform and then,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2218,inverse transform,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2274,Test inverse transformation,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2283,Exceptions should be raised for negative arrays and zero arrays when,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2284,method is boxcox,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2313,Yeo-Johnson method should support any kind of input,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2323,Exceptions should be raised for arrays with different num_columns,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2324,than during fitting,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2338,An exception should be raised if PowerTransformer.method isn't valid,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2348,Test the lambda = 0 case,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2355,Make sure lambda = 1 corresponds to the identity for yeo-johnson,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2371,Test the optimization procedure:,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2372,- set a predefined value for lambda,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2373,- apply inverse_transform to a normal dist (we get X_inv),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2374,- apply fit_transform to X_inv (we get X_inv_trans),not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2375,- check that X_inv_trans is roughly equal to X,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2395,"test from original paper ""A new family of power transformations to",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2396,"improve normality or symmetry"" by Yeo and Johnson.",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2406,Make sure lambda estimation is not influenced by NaN values,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2407,and that transform() supports NaN silently,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2414,concat nans at the end and check lambda stays the same,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2430,check that fit_transform() and fit().transform() return the same values,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2442,"Check that neither fit, transform, fit_transform nor inverse_transform",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2443,modify X inplace when copy=True,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2449,sanity checks,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2470,"check that when copy=False fit doesn't change X inplace but transform,",not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2471,fit_transform and inverse_transform do.,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2477,sanity checks,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2483,fit didn't change X,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2503,non-regression test for:,not
scikit-learn/sklearn/preprocessing/tests/test_data.py,2504,https://github.com/scikit-learn/scikit-learn/issues/16448,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,51,check that the preprocessing method let pass nan,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,60,sanity check,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,64,make sure this boundary case is tested,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,68,ensure no warnings are raised,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,70,"missing values should still be missing, and only them",not
scikit-learn/sklearn/preprocessing/tests/test_common.py,73,check that the function leads to the same results as the class,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,81,check that the inverse transform keep NaN,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,84,FIXME: we can introduce equal_nan=True in recent version of numpy.,SATD
scikit-learn/sklearn/preprocessing/tests/test_common.py,85,For the moment which just check that non-NaN values are almost equal.,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,89,train only on non-NaN,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,91,check transforming with NaN works even when training without NaN,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,96,check non-NaN is handled as before - the 1st column is all nan,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,115,check that the dense and sparse inputs lead to the same results,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,116,precompute the matrix to avoid catching side warnings,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,144,Test pandas IntegerArray with pd.NA,not
scikit-learn/sklearn/preprocessing/tests/test_common.py,151,Creates dataframe with IntegerArrays with pd.NA,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,53,Bad shape,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,60,Incorrect number of features,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,67,Bad bin values,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,76,Float bin values,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,96,test the shape of bin_edges_,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,123,replace the feature with zeros,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,145,Test up to discretizing nano units,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,201,with 2 bins,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,206,with 3 bins,not
scikit-learn/sklearn/preprocessing/tests/test_discretization.py,211,with 5 bins,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,22,(args|kwargs)_store will hold the positional and keyword arguments,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,23,passed to the function inside the FunctionTransformer.,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,32,The function should only have received X.,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,39,reset the argument stores.,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,49,The function should have received X,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,60,Test that the numpy.log example still works.,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,72,Test that rounding is correct,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,84,Test that rounding is correct,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,95,Test that rounding is correct,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,102,Test that inverse_transform works correctly,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,145,check that we don't check inverse when one of the func or inverse is not,not
scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py,146,provided.,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,40,one-class case defaults to negative label,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,41,For dense case:,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,50,For sparse case:,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,59,two-class case,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,72,multi-class case,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,107,two-class case with pos_label=0,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,116,multi-class case,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,130,Check that invalid arguments yield ValueError,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,152,Fail on y_type,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,158,Sequence of seq type should raise ValueError,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,163,Fail on the number of classes,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,170,Fail on the dimension of 'binary',not
scikit-learn/sklearn/preprocessing/tests/test_label.py,177,Fail on multioutput data,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,195,"Test LabelEncoder's transform, fit_transform and",not
scikit-learn/sklearn/preprocessing/tests/test_label.py,196,inverse_transform methods,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,232,Check that invalid arguments yield ValueError,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,239,Fail on unseen labels,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,248,"Fail on inverse_transform("""")",not
scikit-learn/sklearn/preprocessing/tests/test_label.py,263,test empty transform,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,266,test empty inverse transform,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,272,test input as iterable of iterables,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,285,With fit_transform,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,290,verify CSR assumption that indices and indptr have same dtype,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,297,With fit,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,302,verify CSR assumption that indices and indptr have same dtype,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,316,test input as iterable of iterables,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,327,With fit_transform,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,334,With fit,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,372,fit_transform(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,377,fit().transform(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,382,ensure works with extra class,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,388,ensure fit is no-op as iterable is not consumed,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,393,ensure a ValueError is thrown if given duplicate classes,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,411,first call,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,414,second call change class,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,420,Ensure sequences of the same length are not interpreted as a 2-d array,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,425,fit_transform(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,430,fit().transform(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,447,fit_transform(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,453,fit().transform(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,475,Not binary,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,478,"The following binary cases are fine, however",not
scikit-learn/sklearn/preprocessing/tests/test_label.py,483,Wrong shape,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,495,Modified class order,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,517,check label_binarize,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,524,check inverse,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,539,Check label binarizer,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,559,Binary case where sparse_output = True will not result in a ValueError,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,639,test for the check_unknown parameter of _encode(),not
scikit-learn/sklearn/preprocessing/tests/test_label.py,643,"Default is True, raise error",not
scikit-learn/sklearn/preprocessing/tests/test_label.py,648,dont raise error if False,not
scikit-learn/sklearn/preprocessing/tests/test_label.py,651,parameter is ignored for object dtype,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,18,check that sparse and dense will give the same results,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,33,check outcome,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,54,Test that one hot encoder raises error for unknown features,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,55,present during transform.,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,61,"Test the ignore option, ignores unknown features (giving all 0's)",not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,68,ensure transformed data was not modified in place,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,71,Raise error if handle_unknown is neither ignore or error.,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,90,Non Regression test for the issue #12470,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,91,"Test the ignore option, when categories are numpy string dtype",not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,92,particularly when the known category strings are larger,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,93,than the unknown category strings,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,100,ensure transformed data was not modified in place,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,179,set params on not yet fitted object,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,183,set params on already fitted object,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,234,with unknown categories,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,235,drop is incompatible with handle_unknown=ignore,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,245,"with an otherwise numerical output, still object if unknown",not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,255,incorrect shape raises,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,271,check that resetting drop option without refitting does not throw an error,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,321,order of categories should not depend on order of samples,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,325,assert enc.categories == 'auto',not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,350,manually specified categories should have same dtype as,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,351,the data when coerced from lists,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,354,"when specifying categories manually, unknown categories should already",not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,355,raise when fitting,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,375,unsorted passed categories still raise for numerical values,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,384,multiple columns,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,393,integer categories but from object dtype data,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,423,Canonical case,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,437,"with only one cat, the behaviour is equivalent to drop=None",not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,513,manually specified categories should have same dtype as,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,514,the data when coerced from lists,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,517,"when specifying categories manually, unknown categories should already",not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,518,raise when fitting,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,531,incorrect shape raises,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,568,check that dtypes are preserved when determining categories,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,574,string dtype,not
scikit-learn/sklearn/preprocessing/tests/test_encoders.py,593,check dtype (similar to test_categorical_encoder_dtypes for dataframes),not
scikit-learn/sklearn/metrics/_regression.py,10,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/metrics/_regression.py,11,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/metrics/_regression.py,12,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/metrics/_regression.py,13,Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/metrics/_regression.py,14,Jochen Wersdorfer <jochen@wersdoerfer.de>,not
scikit-learn/sklearn/metrics/_regression.py,15,Lars Buitinck,not
scikit-learn/sklearn/metrics/_regression.py,16,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/metrics/_regression.py,17,Karan Desai <karandesai281196@gmail.com>,not
scikit-learn/sklearn/metrics/_regression.py,18,Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/metrics/_regression.py,19,Manoj Kumar <manojkumarsivaraj334@gmail.com>,not
scikit-learn/sklearn/metrics/_regression.py,20,Michael Eickenberg <michael.eickenberg@gmail.com>,not
scikit-learn/sklearn/metrics/_regression.py,21,Konstantin Shmelkov <konstantin.shmelkov@polytechnique.edu>,not
scikit-learn/sklearn/metrics/_regression.py,22,Christian Lorentzen <lorentzen.ch@googlemail.com>,not
scikit-learn/sklearn/metrics/_regression.py,23,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/_regression.py,187,pass None as weights to np.average: uniform mean,not
scikit-learn/sklearn/metrics/_regression.py,262,pass None as weights to np.average: uniform mean,not
scikit-learn/sklearn/metrics/_regression.py,395,pass None as weights to np.average: uniform mean,not
scikit-learn/sklearn/metrics/_regression.py,481,return scores individually,not
scikit-learn/sklearn/metrics/_regression.py,484,passing to np.average() None as weights results is uniform mean,not
scikit-learn/sklearn/metrics/_regression.py,609,"arbitrary set to zero to avoid -inf scores, having a constant",not
scikit-learn/sklearn/metrics/_regression.py,610,y_true is not interesting for scoring a regression anyway,not
scikit-learn/sklearn/metrics/_regression.py,614,return scores individually,not
scikit-learn/sklearn/metrics/_regression.py,617,passing None as weights results is uniform mean,not
scikit-learn/sklearn/metrics/_regression.py,621,avoid fail on constant y or one-element arrays,not
scikit-learn/sklearn/metrics/_classification.py,10,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/metrics/_classification.py,11,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/metrics/_classification.py,12,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/metrics/_classification.py,13,Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/metrics/_classification.py,14,Jochen Wersdorfer <jochen@wersdoerfer.de>,not
scikit-learn/sklearn/metrics/_classification.py,15,Lars Buitinck,not
scikit-learn/sklearn/metrics/_classification.py,16,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/metrics/_classification.py,17,Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/metrics/_classification.py,18,Jatin Shah <jatindshah@gmail.com>,not
scikit-learn/sklearn/metrics/_classification.py,19,Saurabh Jha <saurabh.jhaa@gmail.com>,not
scikit-learn/sklearn/metrics/_classification.py,20,Bernardo Stein <bernardovstein@gmail.com>,not
scikit-learn/sklearn/metrics/_classification.py,21,Shangwu Yao <shangwuyao@gmail.com>,not
scikit-learn/sklearn/metrics/_classification.py,22,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/_classification.py,93,We can't have more than one value on y_type => The set is no more needed,not
scikit-learn/sklearn/metrics/_classification.py,96,"No metrics support ""multiclass-multioutput"" format",not
scikit-learn/sklearn/metrics/_classification.py,186,Compute accuracy for each possible representation,not
scikit-learn/sklearn/metrics/_classification.py,305,"convert yt, yp into index",not
scikit-learn/sklearn/metrics/_classification.py,309,"intersect y_pred, y_true with labels, eliminate items not in labels",not
scikit-learn/sklearn/metrics/_classification.py,313,also eliminate weights of eliminated items,not
scikit-learn/sklearn/metrics/_classification.py,316,Choose the accumulator dtype to always have high precision,not
scikit-learn/sklearn/metrics/_classification.py,466,labels are now from 0 to len(labels) - 1 -> use bincount,not
scikit-learn/sklearn/metrics/_classification.py,478,Pathological case,not
scikit-learn/sklearn/metrics/_classification.py,487,Retain only selected labels,not
scikit-learn/sklearn/metrics/_classification.py,496,All labels are index integers for multilabel.,not
scikit-learn/sklearn/metrics/_classification.py,497,Select labels:,not
scikit-learn/sklearn/metrics/_classification.py,513,calculate weighted counts,not
scikit-learn/sklearn/metrics/_classification.py,755,"numerator is 0, and warning should have already been issued",not
scikit-learn/sklearn/metrics/_classification.py,1192,avoid infs/nans,not
scikit-learn/sklearn/metrics/_classification.py,1198,"if ``zero_division=1``, set those with denominator == 0 equal to 1",not
scikit-learn/sklearn/metrics/_classification.py,1201,the user will be removing warnings if zero_division is set to something,not
scikit-learn/sklearn/metrics/_classification.py,1202,different than its default value. If we are computing only f-score,not
scikit-learn/sklearn/metrics/_classification.py,1203,the warning will be raised only if precision and recall are ill-defined,not
scikit-learn/sklearn/metrics/_classification.py,1207,build appropriate warning,not
scikit-learn/sklearn/metrics/_classification.py,1208,"E.g. ""Precision and F-score are ill-defined and being set to 0.0 in",not
scikit-learn/sklearn/metrics/_classification.py,1209,labels with no predicted samples. Use ``zero_division`` parameter to,not
scikit-learn/sklearn/metrics/_classification.py,1210,"control this behavior.""",not
scikit-learn/sklearn/metrics/_classification.py,1436,"Calculate tp_sum, pred_sum, true_sum",not
scikit-learn/sklearn/metrics/_classification.py,1450,"Finally, we have all our sufficient statistics. Divide!",not
scikit-learn/sklearn/metrics/_classification.py,1453,"Divide, and on zero-division, set scores and/or warn according to",not
scikit-learn/sklearn/metrics/_classification.py,1454,zero_division:,not
scikit-learn/sklearn/metrics/_classification.py,1460,"warn for f-score only if zero_division is warn, it is in warn_for",not
scikit-learn/sklearn/metrics/_classification.py,1461,and BOTH prec and rec are ill-defined,not
scikit-learn/sklearn/metrics/_classification.py,1468,"if tp == 0 F will be 1 only if all predictions are zero, all labels are",not
scikit-learn/sklearn/metrics/_classification.py,1469,"zero, and zero_division=1. In all other case, 0",not
scikit-learn/sklearn/metrics/_classification.py,1475,avoid division by 0,not
scikit-learn/sklearn/metrics/_classification.py,1478,Average the results,not
scikit-learn/sklearn/metrics/_classification.py,1483,precision is zero_division if there are no positive predictions,not
scikit-learn/sklearn/metrics/_classification.py,1484,recall is zero_division if there are no positive labels,not
scikit-learn/sklearn/metrics/_classification.py,1485,fscore is zero_division if all labels AND predictions are,not
scikit-learn/sklearn/metrics/_classification.py,1486,negative,not
scikit-learn/sklearn/metrics/_classification.py,1502,return no support,not
scikit-learn/sklearn/metrics/_classification.py,1938,labelled micro average,not
scikit-learn/sklearn/metrics/_classification.py,1959,compute per-class results without averaging,not
scikit-learn/sklearn/metrics/_classification.py,1989,compute all applicable averages,not
scikit-learn/sklearn/metrics/_classification.py,1996,compute averages with specified averaging method,not
scikit-learn/sklearn/metrics/_classification.py,2212,Clipping,not
scikit-learn/sklearn/metrics/_classification.py,2215,"If y_pred is of single dimension, assume y_true to be binary",not
scikit-learn/sklearn/metrics/_classification.py,2216,and then check.,not
scikit-learn/sklearn/metrics/_classification.py,2222,Check if dimensions are consistent.,not
scikit-learn/sklearn/metrics/_classification.py,2238,Renormalize,not
scikit-learn/sklearn/metrics/_classification.py,2347,Handles binary class case,not
scikit-learn/sklearn/metrics/_classification.py,2348,this code assumes that positive and negative labels,not
scikit-learn/sklearn/metrics/_classification.py,2349,are encoded as +1 and -1 respectively,not
scikit-learn/sklearn/metrics/_classification.py,2362,The hinge_loss doesn't penalize good enough predictions.,not
scikit-learn/sklearn/metrics/_classification.py,2447,"if pos_label=None, when y_true is in {-1, 1} or {0, 1},",not
scikit-learn/sklearn/metrics/_classification.py,2448,"pos_label is set to 1 (consistent with precision_recall_curve/roc_curve),",not
scikit-learn/sklearn/metrics/_classification.py,2449,otherwise pos_label is set to the greater label,not
scikit-learn/sklearn/metrics/_classification.py,2450,"(different from precision_recall_curve/roc_curve,",not
scikit-learn/sklearn/metrics/_classification.py,2451,the purpose is to keep backward compatibility).,not
scikit-learn/sklearn/metrics/pairwise.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/metrics/pairwise.py,3,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/metrics/pairwise.py,4,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/metrics/pairwise.py,5,Robert Layton <robertlayton@gmail.com>,not
scikit-learn/sklearn/metrics/pairwise.py,6,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/metrics/pairwise.py,7,Philippe Gervais <philippe.gervais@inria.fr>,not
scikit-learn/sklearn/metrics/pairwise.py,8,Lars Buitinck,not
scikit-learn/sklearn/metrics/pairwise.py,9,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/metrics/pairwise.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/pairwise.py,37,Utility Functions,not
scikit-learn/sklearn/metrics/pairwise.py,199,Pairwise distances,not
scikit-learn/sklearn/metrics/pairwise.py,270,"If norms are passed as float32, they are unused. If arrays are passed as",not
scikit-learn/sklearn/metrics/pairwise.py,271,"float32, norms needs to be recomputed on upcast chunks.",not
scikit-learn/sklearn/metrics/pairwise.py,272,TODO: use a float64 accumulator in row_norms to avoid the latter.,SATD
scikit-learn/sklearn/metrics/pairwise.py,288,"shortcut in the common case euclidean_distances(X, X)",not
scikit-learn/sklearn/metrics/pairwise.py,304,"To minimize precision issues with float32, we compute the distance",not
scikit-learn/sklearn/metrics/pairwise.py,305,matrix on chunks of X and Y upcast to float64,not
scikit-learn/sklearn/metrics/pairwise.py,308,"if dtype is already float64, no need to chunk and upcast",not
scikit-learn/sklearn/metrics/pairwise.py,314,Ensure that distances between vectors and themselves are set to 0.0.,not
scikit-learn/sklearn/metrics/pairwise.py,315,This may not be the case due to floating point rounding errors.,not
scikit-learn/sklearn/metrics/pairwise.py,398,Get missing mask for X,not
scikit-learn/sklearn/metrics/pairwise.py,401,Get missing mask for Y,not
scikit-learn/sklearn/metrics/pairwise.py,404,set missing values to zero,not
scikit-learn/sklearn/metrics/pairwise.py,410,Adjust distances for missing values,not
scikit-learn/sklearn/metrics/pairwise.py,419,Ensure that distances between vectors and themselves are set to 0.0.,not
scikit-learn/sklearn/metrics/pairwise.py,420,This may not be the case due to floating point rounding errors.,not
scikit-learn/sklearn/metrics/pairwise.py,427,avoid divide by zero,not
scikit-learn/sklearn/metrics/pairwise.py,457,"Allow 10% more memory than X, Y and the distance matrix take (at",not
scikit-learn/sklearn/metrics/pairwise.py,458,least 10MiB),not
scikit-learn/sklearn/metrics/pairwise.py,464,The increase amount of memory in 8-byte blocks is:,not
scikit-learn/sklearn/metrics/pairwise.py,465,- x_density * batch_size * n_features (copy of chunk of X),not
scikit-learn/sklearn/metrics/pairwise.py,466,- y_density * batch_size * n_features (copy of chunk of Y),not
scikit-learn/sklearn/metrics/pairwise.py,467,- batch_size * batch_size (chunk of distance matrix),not
scikit-learn/sklearn/metrics/pairwise.py,468,"Hence x² + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem",not
scikit-learn/sklearn/metrics/pairwise.py,469,xd=x_density and yd=y_density,not
scikit-learn/sklearn/metrics/pairwise.py,487,when X is Y the distance matrix is symmetric so we only need,not
scikit-learn/sklearn/metrics/pairwise.py,488,to compute half of it.,not
scikit-learn/sklearn/metrics/pairwise.py,789,this also sorts indices in-place,not
scikit-learn/sklearn/metrics/pairwise.py,830,"1.0 - cosine_similarity(X, Y) without copy",not
scikit-learn/sklearn/metrics/pairwise.py,836,Ensure that distances between vectors and themselves are set to 0.0.,not
scikit-learn/sklearn/metrics/pairwise.py,837,This may not be the case due to floating point rounding errors.,not
scikit-learn/sklearn/metrics/pairwise.py,842,Paired distances,not
scikit-learn/sklearn/metrics/pairwise.py,969,Check the matrix first (it is usually done by the metric),not
scikit-learn/sklearn/metrics/pairwise.py,979,Kernels,not
scikit-learn/sklearn/metrics/pairwise.py,1072,compute tanh in-place,not
scikit-learn/sklearn/metrics/pairwise.py,1105,exponentiate K in-place,not
scikit-learn/sklearn/metrics/pairwise.py,1139,exponentiate K in-place,not
scikit-learn/sklearn/metrics/pairwise.py,1176,to avoid recursive import,not
scikit-learn/sklearn/metrics/pairwise.py,1301,Helper functions - distance,not
scikit-learn/sklearn/metrics/pairwise.py,1303,"If updating this dictionary, update the doc in both distance_metrics()",not
scikit-learn/sklearn/metrics/pairwise.py,1304,and also in pairwise_distances()!,not
scikit-learn/sklearn/metrics/pairwise.py,1312,"HACK: precomputed is always allowed, never called",SATD
scikit-learn/sklearn/metrics/pairwise.py,1361,enforce a threading backend to prevent data communication overhead,not
scikit-learn/sklearn/metrics/pairwise.py,1369,zeroing diagonal for euclidean norm.,not
scikit-learn/sklearn/metrics/pairwise.py,1370,TODO: do it also for other norms.,SATD
scikit-learn/sklearn/metrics/pairwise.py,1382,Only calculate metric for upper triangle,not
scikit-learn/sklearn/metrics/pairwise.py,1388,Make symmetric,not
scikit-learn/sklearn/metrics/pairwise.py,1389,NB: out += out.T will produce incorrect results,not
scikit-learn/sklearn/metrics/pairwise.py,1392,Calculate diagonal,not
scikit-learn/sklearn/metrics/pairwise.py,1393,NB: nonzero diagonals are allowed for both metrics and kernels,not
scikit-learn/sklearn/metrics/pairwise.py,1399,Calculate all cells,not
scikit-learn/sklearn/metrics/pairwise.py,1589,We get as many rows as possible within our working_memory budget to,not
scikit-learn/sklearn/metrics/pairwise.py,1590,store len(Y) distances in each row of output.,not
scikit-learn/sklearn/metrics/pairwise.py,1591,,not
scikit-learn/sklearn/metrics/pairwise.py,1592,Note:,not
scikit-learn/sklearn/metrics/pairwise.py,1593,"- this will get at least 1 row, even if 1 row of distances will",not
scikit-learn/sklearn/metrics/pairwise.py,1594,exceed working_memory.,not
scikit-learn/sklearn/metrics/pairwise.py,1595,- this does not account for any temporary memory usage while,not
scikit-learn/sklearn/metrics/pairwise.py,1596,calculating distances (e.g. difference of vectors in manhattan,not
scikit-learn/sklearn/metrics/pairwise.py,1597,distance.,not
scikit-learn/sklearn/metrics/pairwise.py,1603,precompute data-derived metric params,not
scikit-learn/sklearn/metrics/pairwise.py,1609,enable optimised paths for X is Y,not
scikit-learn/sklearn/metrics/pairwise.py,1617,"zeroing diagonal, taking care of aliases of ""euclidean"",",not
scikit-learn/sklearn/metrics/pairwise.py,1618,"i.e. ""l2""",not
scikit-learn/sklearn/metrics/pairwise.py,1766,precompute data-derived metric params,not
scikit-learn/sklearn/metrics/pairwise.py,1778,"These distances require boolean arrays, when using scipy.spatial.distance",not
scikit-learn/sklearn/metrics/pairwise.py,1791,Helper functions - distance,not
scikit-learn/sklearn/metrics/pairwise.py,1793,"If updating this dictionary, update the doc in both distance_metrics()",not
scikit-learn/sklearn/metrics/pairwise.py,1794,and also in pairwise_distances()!,not
scikit-learn/sklearn/metrics/pairwise.py,1918,import GPKernel locally to prevent circular imports,not
scikit-learn/sklearn/metrics/_base.py,5,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/metrics/_base.py,6,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/metrics/_base.py,7,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/metrics/_base.py,8,Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/metrics/_base.py,9,Jochen Wersdorfer <jochen@wersdoerfer.de>,not
scikit-learn/sklearn/metrics/_base.py,10,Lars Buitinck,not
scikit-learn/sklearn/metrics/_base.py,11,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/metrics/_base.py,12,Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/metrics/_base.py,13,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/_base.py,103,swap average_weight <-> score_weight,not
scikit-learn/sklearn/metrics/_base.py,122,Average the results,not
scikit-learn/sklearn/metrics/_base.py,125,"Scores with 0 weights are forced to be 0, preventing the average",not
scikit-learn/sklearn/metrics/_base.py,126,score from being affected by 0-weighted NaN elements.,not
scikit-learn/sklearn/metrics/_base.py,185,"Compute scores treating a as positive class and b as negative class,",not
scikit-learn/sklearn/metrics/_base.py,186,then b as positive class and a as negative class,not
scikit-learn/sklearn/metrics/_ranking.py,10,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/metrics/_ranking.py,11,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/metrics/_ranking.py,12,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/metrics/_ranking.py,13,Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/metrics/_ranking.py,14,Jochen Wersdorfer <jochen@wersdoerfer.de>,not
scikit-learn/sklearn/metrics/_ranking.py,15,Lars Buitinck,not
scikit-learn/sklearn/metrics/_ranking.py,16,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/metrics/_ranking.py,17,Noel Dawe <noel@dawe.me>,not
scikit-learn/sklearn/metrics/_ranking.py,18,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/_ranking.py,98,Reductions such as .sum used internally in np.trapz do not return a,not
scikit-learn/sklearn/metrics/_ranking.py,99,scalar by default for numpy.memmap instances contrary to,not
scikit-learn/sklearn/metrics/_ranking.py,100,regular numpy.ndarray instances.,not
scikit-learn/sklearn/metrics/_ranking.py,199,Return the step function integral,not
scikit-learn/sklearn/metrics/_ranking.py,200,The following works because the last entry of precision is,not
scikit-learn/sklearn/metrics/_ranking.py,201,"guaranteed to be 1, as returned by precision_recall_curve",not
scikit-learn/sklearn/metrics/_ranking.py,233,Add a single point at max_fpr by linear interpolation,not
scikit-learn/sklearn/metrics/_ranking.py,241,McClish correction: standardize result to be 0.5 if non-discriminant,not
scikit-learn/sklearn/metrics/_ranking.py,242,and 1 if maximal,not
scikit-learn/sklearn/metrics/_ranking.py,377,do not support partial ROC computation for multiclass,not
scikit-learn/sklearn/metrics/_ranking.py,394,multilabel-indicator,not
scikit-learn/sklearn/metrics/_ranking.py,442,validation of the input y_score,not
scikit-learn/sklearn/metrics/_ranking.py,448,validation for multiclass parameter specifications,not
scikit-learn/sklearn/metrics/_ranking.py,489,Hand & Till (2001) implementation (ovo),not
scikit-learn/sklearn/metrics/_ranking.py,494,ovr is same as multi-label,not
scikit-learn/sklearn/metrics/_ranking.py,535,Check to make sure y_true is valid,not
scikit-learn/sklearn/metrics/_ranking.py,550,ensure binary classification if pos_label is not specified,not
scikit-learn/sklearn/metrics/_ranking.py,551,"classes.dtype.kind in ('O', 'U', 'S') is required to avoid",not
scikit-learn/sklearn/metrics/_ranking.py,552,"triggering a FutureWarning by calling np.array_equal(a, b)",not
scikit-learn/sklearn/metrics/_ranking.py,553,when elements in the two arrays are not comparable.,not
scikit-learn/sklearn/metrics/_ranking.py,571,make y_true a boolean vector,not
scikit-learn/sklearn/metrics/_ranking.py,574,sort scores and corresponding truth values,not
scikit-learn/sklearn/metrics/_ranking.py,583,y_score typically has many tied values. Here we extract,not
scikit-learn/sklearn/metrics/_ranking.py,584,the indices associated with the distinct values. We also,not
scikit-learn/sklearn/metrics/_ranking.py,585,concatenate a value for the end of the curve.,not
scikit-learn/sklearn/metrics/_ranking.py,589,accumulate the true positives with decreasing threshold,not
scikit-learn/sklearn/metrics/_ranking.py,592,express fps as a cumsum to ensure fps is increasing even in,not
scikit-learn/sklearn/metrics/_ranking.py,593,the presence of floating point errors,not
scikit-learn/sklearn/metrics/_ranking.py,683,stop when full recall attained,not
scikit-learn/sklearn/metrics/_ranking.py,684,and reverse the outputs so recall is decreasing,not
scikit-learn/sklearn/metrics/_ranking.py,778,Attempt to drop thresholds corresponding to points in between and,not
scikit-learn/sklearn/metrics/_ranking.py,779,collinear with other points. These are always suboptimal and do not,not
scikit-learn/sklearn/metrics/_ranking.py,780,appear on a plotted ROC curve (and thus do not affect the AUC).,not
scikit-learn/sklearn/metrics/_ranking.py,781,"Here np.diff(_, 2) is used as a ""second derivative"" to tell if there",not
scikit-learn/sklearn/metrics/_ranking.py,782,is a corner at the point. Both fps and tps must be tested to handle,not
scikit-learn/sklearn/metrics/_ranking.py,783,thresholds with multiple data points (which are combined in,not
scikit-learn/sklearn/metrics/_ranking.py,784,"_binary_clf_curve). This keeps all cases where the point should be kept,",not
scikit-learn/sklearn/metrics/_ranking.py,785,"but does not drop more complicated cases like fps = [1, 3, 7],",not
scikit-learn/sklearn/metrics/_ranking.py,786,"tps = [1, 2, 4]; there is no harm in keeping too many thresholds.",not
scikit-learn/sklearn/metrics/_ranking.py,796,Add an extra threshold position,not
scikit-learn/sklearn/metrics/_ranking.py,797,"to make sure that the curve starts at (0, 0)",not
scikit-learn/sklearn/metrics/_ranking.py,874,Handle badly formatted array and the degenerate case with one label,not
scikit-learn/sklearn/metrics/_ranking.py,890,"If all labels are relevant or unrelevant, the score is also",not
scikit-learn/sklearn/metrics/_ranking.py,891,equal to 1. The label ranking has no meaning.,not
scikit-learn/sklearn/metrics/_ranking.py,1029,Sort and bin the label scores,not
scikit-learn/sklearn/metrics/_ranking.py,1039,"if the scores are ordered, it's possible to count the number of",not
scikit-learn/sklearn/metrics/_ranking.py,1040,incorrectly ordered paires in linear time by cumulatively counting,not
scikit-learn/sklearn/metrics/_ranking.py,1041,how many false labels of a given score have a score higher than the,not
scikit-learn/sklearn/metrics/_ranking.py,1042,accumulated true labels with lower score.,not
scikit-learn/sklearn/metrics/_ranking.py,1050,"When there is no positive or no negative labels, those values should",not
scikit-learn/sklearn/metrics/_ranking.py,1051,"be consider as correct, i.e. the ranking doesn't matter.",not
scikit-learn/sklearn/metrics/_ranking.py,1325,Here we use the order induced by y_true so we can ignore ties since,not
scikit-learn/sklearn/metrics/_ranking.py,1326,the gain associated to tied indices is the same (permuting ties doesn't,not
scikit-learn/sklearn/metrics/_ranking.py,1327,change the value of the re-ordered y_true),not
scikit-learn/sklearn/metrics/_scorer.py,16,Authors: Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/metrics/_scorer.py,17,Lars Buitinck,not
scikit-learn/sklearn/metrics/_scorer.py,18,Arnaud Joly <arnaud.v.joly@gmail.com>,not
scikit-learn/sklearn/metrics/_scorer.py,19,License: Simplified BSD,not
scikit-learn/sklearn/metrics/_scorer.py,107,Only one scorer,not
scikit-learn/sklearn/metrics/_scorer.py,130,XXX After removing the deprecated scorers (v0.24) remove the,SATD
scikit-learn/sklearn/metrics/_scorer.py,131,XXX deprecation_msg property again and remove __call__'s body again,SATD
scikit-learn/sklearn/metrics/_scorer.py,251,not multiclass,not
scikit-learn/sklearn/metrics/_scorer.py,309,For multi-output multi-class estimator,not
scikit-learn/sklearn/metrics/_scorer.py,357,deprecated,not
scikit-learn/sklearn/metrics/_scorer.py,407,Heuristic to ensure user has not passed a metric,not
scikit-learn/sklearn/metrics/_scorer.py,618,Standard regression scores,not
scikit-learn/sklearn/metrics/_scorer.py,642,Standard Classification Scores,not
scikit-learn/sklearn/metrics/_scorer.py,646,Score functions that need decision values,not
scikit-learn/sklearn/metrics/_scorer.py,662,Score function for probabilistic classification,not
scikit-learn/sklearn/metrics/_scorer.py,677,Clustering scores,not
scikit-learn/sklearn/metrics/_scorer.py,707,Cluster metrics that use supervised evaluation,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,3,Authors: Robert Layton <robertlayton@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,4,Arnaud Fouchet <foucheta@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,5,Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,134,accumulate distances from each sample to each cluster,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,141,intra_index selects intra-cluster distances within clust_dists,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,143,intra_clust_dists are averaged over cluster size outside this function,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,145,of the remaining distances we normalise and extract the minimum,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,216,Check for non-zero diagonal entries in precomputed distance matrix,not
scikit-learn/sklearn/metrics/cluster/_unsupervised.py,247,"nan values are for clusters of size 1, and should be 0",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,7,Authors: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,8,Wei LI <kuantkid@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,9,Diego Molla <dmolla-aliod@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,10,Arnaud Fouchet <foucheta@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,11,Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,12,Gregory Stupp <stuppie@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,13,Joel Nothman <joel.nothman@gmail.com>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,14,Arya McCarthy <arya@jhu.edu>,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,15,License: BSD 3 clause,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,31,the exact version is faster for k == 2: use it by default globally in,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,32,this module instead of the float approximate variant,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,54,input checks,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,121,"Using coo_matrix to accelerate simple histogram calculation,",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,122,i.e. bins are consecutive integers,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,123,"Currently, coo_matrix is faster than histogram2d for simple cases",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,134,don't use += as contingency is integer,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,139,clustering measures,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,227,Special limit cases: no clustering since the data is not split;,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,228,or trivial clustering where each document is assigned a unique cluster.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,229,These are perfect matches hence return 1.0.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,235,Compute the ARI using the contingency data,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,633,For an array,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,637,For a sparse matrix,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,648,"Don't need to calculate the full outer product, just for non-zeroes",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,750,Special limit cases: no clustering since the data is not split.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,751,This is a perfect match hence return 1.0.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,758,Calculate the MI for the two clusterings,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,761,Calculate the expected value for the mutual information,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,763,Calculate entropy for each labeling,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,767,"Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,768,"normalizer should always be >= emi, but because of floating-point",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,769,"representation, sometimes emi is slightly larger. Correct this",not
scikit-learn/sklearn/metrics/cluster/_supervised.py,770,by preserving the sign.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,859,Special limit cases: no clustering since the data is not split.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,860,This is a perfect match hence return 1.0.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,867,Calculate the MI for the two clusterings,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,870,Calculate the expected value for the mutual information,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,871,Calculate entropy for each labeling,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,874,Avoid 0.0 / 0.0 when either entropy is zero.,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,978,log(a / b) should be calculated as log(a) - log(b) for,not
scikit-learn/sklearn/metrics/cluster/_supervised.py,979,possible loss of precision,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,21,Dictionaries of metrics,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,22,------------------------,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,23,The goal of having those dictionaries is to have an easy way to call a,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,24,particular metric and associate a name to each function:,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,25,- SUPERVISED_METRICS: all supervised cluster metrics - (when given a,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,26,ground truth value),not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,27,- UNSUPERVISED_METRICS: all unsupervised cluster metrics,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,28,,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,29,Those dictionaries will be used to test systematically some invariance,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,30,"properties, e.g. invariance toward several input layout.",not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,31,,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,51,Lists of metrics with common properties,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,52,---------------------------------------,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,53,Lists of metrics with common properties are used to test systematically some,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,54,"functionalities and invariance, e.g. SYMMETRIC_METRICS lists all metrics",not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,55,that are symmetric with respect to their input argument y_true and y_pred.,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,56,,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,57,--------------------------------------------------------------------,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,58,Symmetric with respect to their input arguments y_true and y_pred.,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,59,Symmetric metrics only apply to supervised clusters.,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,68,Metrics whose upper bound is 1,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,86,0.22 AMI and NMI changes,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,106,0.22 AMI and NMI changes,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,126,0.22 AMI and NMI changes,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,132,All clustering metrics do not change score due to permutations of labels,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,133,that is when 0 and 1 exchanged.,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,149,0.22 AMI and NMI changes,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,154,For all clustering metrics Input parameters can be both,not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,155,"in the form of arrays lists, positive, negative or string",not
scikit-learn/sklearn/metrics/cluster/tests/test_common.py,190,only the supervised metrics support single sample,not
scikit-learn/sklearn/metrics/cluster/tests/test_bicluster.py,49,"B contains 2 of the 3 biclusters in A, so score should be 2/3",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,16,Tests the Silhouette Coefficient.,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,26,"Given that the actual labels are used, we can assume that S would be",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,27,positive.,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,30,Test without calculating D,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,40,Test with sampling,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,58,Assert Silhouette Coefficient == 0 when there is 1 sample in a cluster,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,59,(cluster 0). We also test the case where there are identical samples,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,60,"as the only members of a cluster (cluster 2). To our knowledge, this case",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,61,"is not discussed in reference material, and we choose for it a sample",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,62,score of 1.,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,66,Cluster 0: 1 sample -> score of 0 by Rousseeuw's convention,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,67,"Cluster 1: intra-cluster = [.5, .5, 1]",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,68,"inter-cluster = [1, 1, 1]",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,69,"silhouette    = [.5, .5, 0]",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,70,"Cluster 2: intra-cluster = [0, 0]",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,71,"inter-cluster = [arbitrary, arbitrary]",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,72,"silhouette    = [1., 1.]",not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,81,Explicitly check per-sample results against Rousseeuw (1987),not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,82,Data from Table 1,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,101,Data from Figure 2,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,108,Data from Figure 3,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,118,we check to 2dp because that's what's in the paper,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,130,Assert 1 < n_labels < n_samples,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,134,n_labels = n_samples,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,141,n_labels = 1,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,169,Make sure silhouette_samples requires diagonal to be zero.,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,170,Non-regression test for #12178,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,172,Construct a zero-diagonal matrix,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,177,small values on the diagonal are OK,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,181,values bigger than eps * 100 are not,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,206,Assert the value is 1. when all samples are equals,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,210,Assert the value is 0. when all the mean cluster are equal,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,214,General case (with non numpy arrays),not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,226,Assert the value is 0. when all samples are equals,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,230,Assert the value is 0. when all the mean cluster are equal,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,234,General case (with non numpy arrays),not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,240,Ensure divide by zero warning is not raised in general case,not
scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py,249,General case - cluster have one sample,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,88,homogeneous but not complete clustering,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,98,complete but not homogeneous clustering,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,108,neither complete nor homogeneous but not so bad either,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,118,test for when beta passed to,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,119,homogeneity_completeness_v_measure,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,120,and v_measure_score,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,143,regression tests for labels with gaps,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,167,Compute score for random uniform cluster labelings,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,180,Check that adjusted scores are almost zero on random labels,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,193,Compute the Adjusted Mutual Information and test against known values,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,196,Mutual information,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,199,with provided sparse contingency,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,203,with provided dense contingency,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,207,Expected mutual information,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,211,Adjusted mutual information,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,216,Test with a very large array,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,224,Test for regression where contingency cell exceeds 2**16,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,225,"leading to overflow in np.outer, resulting in EMI > 1",not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,230,Test overflow in mutual_info_classif and fowlkes_mallows_score,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,271,Check numerical stability when information is exactly zero,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,287,"Check relation between v_measure, entropy and mutual information",not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,303,General case,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,308,Perfect match but where the label names changed,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,313,Worst case,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,320,handcrafted example,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,324,FMI = TP / sqrt((TP + FP) * (TP + FN)),not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,329,symmetric property,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,333,permutation property,not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,337,symmetric and permutation(both together),not
scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py,349,non-regression test for #16355,not
scikit-learn/sklearn/metrics/tests/test_classification.py,54,,not
scikit-learn/sklearn/metrics/tests/test_classification.py,55,Utilities for testing,not
scikit-learn/sklearn/metrics/tests/test_classification.py,66,import some data to play with,not
scikit-learn/sklearn/metrics/tests/test_classification.py,73,restrict to a binary classification task,not
scikit-learn/sklearn/metrics/tests/test_classification.py,84,add noisy features to make the problem harder and avoid perfect results,not
scikit-learn/sklearn/metrics/tests/test_classification.py,88,"run classifier, get class probabilities and label predictions",not
scikit-learn/sklearn/metrics/tests/test_classification.py,93,only interested in probabilities of the positive case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,94,XXX: do we really want a special API for the binary case?,SATD
scikit-learn/sklearn/metrics/tests/test_classification.py,102,,not
scikit-learn/sklearn/metrics/tests/test_classification.py,103,Tests,not
scikit-learn/sklearn/metrics/tests/test_classification.py,107,Test performance report with dictionary output,not
scikit-learn/sklearn/metrics/tests/test_classification.py,111,print classification report with class names,not
scikit-learn/sklearn/metrics/tests/test_classification.py,138,assert the 2 dicts are equal.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,173,Dense label indicator matrix format,not
scikit-learn/sklearn/metrics/tests/test_classification.py,187,Test Precision Recall and F1 Score for binary classification task,not
scikit-learn/sklearn/metrics/tests/test_classification.py,190,detailed measures for each class,not
scikit-learn/sklearn/metrics/tests/test_classification.py,197,individual scoring function that can be used for grid search: in the,not
scikit-learn/sklearn/metrics/tests/test_classification.py,198,binary class case the score is the value of the measure for the positive,not
scikit-learn/sklearn/metrics/tests/test_classification.py,199,class (e.g. label == 1). This is deprecated for average != 'binary'.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,218,"Test precision, recall and F-scores behave with a single positive or",not
scikit-learn/sklearn/metrics/tests/test_classification.py,219,negative class,not
scikit-learn/sklearn/metrics/tests/test_classification.py,220,Such a case may occur with non-stratified cross-validation,not
scikit-learn/sklearn/metrics/tests/test_classification.py,236,Test handling of explicit additional (not in input) labels to PRF,not
scikit-learn/sklearn/metrics/tests/test_classification.py,245,No average: zeros in array,not
scikit-learn/sklearn/metrics/tests/test_classification.py,250,Macro average is changed,not
scikit-learn/sklearn/metrics/tests/test_classification.py,255,No effect otheriwse,not
scikit-learn/sklearn/metrics/tests/test_classification.py,265,Error when introducing invalid label in multilabel case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,266,(although it would only affect performance if average='macro'/None),not
scikit-learn/sklearn/metrics/tests/test_classification.py,275,tests non-regression on issue #10307,not
scikit-learn/sklearn/metrics/tests/test_classification.py,286,Test a subset of labels may be requested for PRF,not
scikit-learn/sklearn/metrics/tests/test_classification.py,304,ensure the above were meaningful tests:,not
scikit-learn/sklearn/metrics/tests/test_classification.py,311,Test that average_precision_score function returns an error when trying,not
scikit-learn/sklearn/metrics/tests/test_classification.py,312,to compute average_precision_score for multiclass task.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,316,y_true contains three different class values,not
scikit-learn/sklearn/metrics/tests/test_classification.py,324,Duplicate values with precision-recall require a different,not
scikit-learn/sklearn/metrics/tests/test_classification.py,325,"processing than when computing the AUC of a ROC, because the",not
scikit-learn/sklearn/metrics/tests/test_classification.py,326,precision-recall curve is a decreasing curve,not
scikit-learn/sklearn/metrics/tests/test_classification.py,327,The following situation corresponds to a perfect,not
scikit-learn/sklearn/metrics/tests/test_classification.py,328,"test statistic, the average_precision_score should be 1",not
scikit-learn/sklearn/metrics/tests/test_classification.py,335,"Here if we go from left to right in y_true, the 0 values are",not
scikit-learn/sklearn/metrics/tests/test_classification.py,336,"are separated from the 1 values, so it appears that we've",not
scikit-learn/sklearn/metrics/tests/test_classification.py,337,Correctly sorted our classifications. But in fact the first two,not
scikit-learn/sklearn/metrics/tests/test_classification.py,338,values have the same score (0.5) and so the first two values,not
scikit-learn/sklearn/metrics/tests/test_classification.py,339,"could be swapped around, creating an imperfect sorting. This",SATD
scikit-learn/sklearn/metrics/tests/test_classification.py,340,"imperfection should come through in the end score, making it less",not
scikit-learn/sklearn/metrics/tests/test_classification.py,341,than one.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,351,Bad beta,not
scikit-learn/sklearn/metrics/tests/test_classification.py,355,Bad pos_label,not
scikit-learn/sklearn/metrics/tests/test_classification.py,361,Bad average option,not
scikit-learn/sklearn/metrics/tests/test_classification.py,368,Check warning that pos_label unused when set to non-default value,not
scikit-learn/sklearn/metrics/tests/test_classification.py,369,but average != 'binary'; even if data is binary.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,379,Test confusion matrix - binary classification case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,401,Test multilabel confusion matrix - binary classification case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,415,Test multilabel confusion matrix - multi-class case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,419,compute confusion matrix with default labels introspection,not
scikit-learn/sklearn/metrics/tests/test_classification.py,425,compute confusion matrix with explicit label ordering,not
scikit-learn/sklearn/metrics/tests/test_classification.py,432,compute confusion matrix with super set of present labels,not
scikit-learn/sklearn/metrics/tests/test_classification.py,447,Test multilabel confusion matrix - multilabel-indicator case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,457,cross test different types,not
scikit-learn/sklearn/metrics/tests/test_classification.py,470,test support for samplewise,not
scikit-learn/sklearn/metrics/tests/test_classification.py,476,test support for labels,not
scikit-learn/sklearn/metrics/tests/test_classification.py,481,test support for labels with samplewise,not
scikit-learn/sklearn/metrics/tests/test_classification.py,488,test support for sample_weight with sample_wise,not
scikit-learn/sklearn/metrics/tests/test_classification.py,501,Bad sample_weight,not
scikit-learn/sklearn/metrics/tests/test_classification.py,510,Bad labels,not
scikit-learn/sklearn/metrics/tests/test_classification.py,518,Using samplewise outside multilabel,not
scikit-learn/sklearn/metrics/tests/test_classification.py,522,Bad y_type,not
scikit-learn/sklearn/metrics/tests/test_classification.py,558,additionally check that no warnings are raised due to a division by zero,not
scikit-learn/sklearn/metrics/tests/test_classification.py,570,These label vectors reproduce the contingency matrix from Artstein and,not
scikit-learn/sklearn/metrics/tests/test_classification.py,571,"Poesio (2008), Table 1: np.array([[20, 20], [10, 50]]).",not
scikit-learn/sklearn/metrics/tests/test_classification.py,578,Add spurious labels and ignore them.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,585,"Multiclass example: Artstein and Poesio, Table 4.",not
scikit-learn/sklearn/metrics/tests/test_classification.py,590,"Weighting example: none, linear, quadratic.",not
scikit-learn/sklearn/metrics/tests/test_classification.py,616,Check that the multiclass matthews_corrcoef agrees with the definition,not
scikit-learn/sklearn/metrics/tests/test_classification.py,617,"presented in Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC",not
scikit-learn/sklearn/metrics/tests/test_classification.py,618,and CEN Error Measures in MultiClass Prediction,not
scikit-learn/sklearn/metrics/tests/test_classification.py,650,corrcoef of same vectors must be 1,not
scikit-learn/sklearn/metrics/tests/test_classification.py,653,"corrcoef, when the two vectors are opposites of each other, should be -1",not
scikit-learn/sklearn/metrics/tests/test_classification.py,661,"For the zero vector case, the corrcoef cannot be calculated and should",not
scikit-learn/sklearn/metrics/tests/test_classification.py,662,result in a RuntimeWarning,not
scikit-learn/sklearn/metrics/tests/test_classification.py,665,But will output 0,not
scikit-learn/sklearn/metrics/tests/test_classification.py,668,And also for any other vector with 0 variance,not
scikit-learn/sklearn/metrics/tests/test_classification.py,671,But will output 0,not
scikit-learn/sklearn/metrics/tests/test_classification.py,674,These two vectors have 0 correlation and hence mcc should be 0,not
scikit-learn/sklearn/metrics/tests/test_classification.py,679,Check that sample weight is able to selectively exclude,not
scikit-learn/sklearn/metrics/tests/test_classification.py,681,Now the first half of the vector elements are alone given a weight of 1,not
scikit-learn/sklearn/metrics/tests/test_classification.py,682,and hence the mcc will not be a perfect 0 as in the previous case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,694,corrcoef of same vectors must be 1,not
scikit-learn/sklearn/metrics/tests/test_classification.py,697,with multiclass > 2 it is not possible to achieve -1,not
scikit-learn/sklearn/metrics/tests/test_classification.py,702,Maximizing false positives and negatives minimizes the MCC,not
scikit-learn/sklearn/metrics/tests/test_classification.py,703,The minimum will be different for depending on the input,not
scikit-learn/sklearn/metrics/tests/test_classification.py,709,Zero variance will result in an mcc of zero and a Runtime Warning,not
scikit-learn/sklearn/metrics/tests/test_classification.py,716,These two vectors have 0 correlation and hence mcc should be 0,not
scikit-learn/sklearn/metrics/tests/test_classification.py,721,We can test that binary assumptions hold using the multiclass computation,not
scikit-learn/sklearn/metrics/tests/test_classification.py,722,by masking the weight of samples not in the first two classes,not
scikit-learn/sklearn/metrics/tests/test_classification.py,724,Masking the last label should let us get an MCC of -1,not
scikit-learn/sklearn/metrics/tests/test_classification.py,731,"For the zero vector case, the corrcoef cannot be calculated and should",not
scikit-learn/sklearn/metrics/tests/test_classification.py,732,result in a RuntimeWarning,not
scikit-learn/sklearn/metrics/tests/test_classification.py,740,But will output 0,not
scikit-learn/sklearn/metrics/tests/test_classification.py,746,https://github.com/scikit-learn/scikit-learn/issues/9622,not
scikit-learn/sklearn/metrics/tests/test_classification.py,761,binary,not
scikit-learn/sklearn/metrics/tests/test_classification.py,768,binary,not
scikit-learn/sklearn/metrics/tests/test_classification.py,770,multiclass,not
scikit-learn/sklearn/metrics/tests/test_classification.py,780,Test Precision Recall and F1 Score for multiclass classification task,not
scikit-learn/sklearn/metrics/tests/test_classification.py,783,compute scores with default labels introspection,not
scikit-learn/sklearn/metrics/tests/test_classification.py,790,averaging tests,not
scikit-learn/sklearn/metrics/tests/test_classification.py,827,same prediction but with and explicit label ordering,not
scikit-learn/sklearn/metrics/tests/test_classification.py,839,test that labels need not be sorted in the multilabel case,not
scikit-learn/sklearn/metrics/tests/test_classification.py,855,compute scores with default labels introspection,not
scikit-learn/sklearn/metrics/tests/test_classification.py,872,Check that pathological cases do not bring NaNs,not
scikit-learn/sklearn/metrics/tests/test_classification.py,892,Test confusion matrix - multi-class case with subset of labels,not
scikit-learn/sklearn/metrics/tests/test_classification.py,895,compute confusion matrix with only first two labels considered,not
scikit-learn/sklearn/metrics/tests/test_classification.py,900,compute confusion matrix with explicit label ordering for only subset,not
scikit-learn/sklearn/metrics/tests/test_classification.py,901,of labels,not
scikit-learn/sklearn/metrics/tests/test_classification.py,906,a label not in y_true should result in zeros for that row/column,not
scikit-learn/sklearn/metrics/tests/test_classification.py,939,confusion_matrix returns int64 by default,not
scikit-learn/sklearn/metrics/tests/test_classification.py,942,The dtype of confusion_matrix is always 64 bit,not
scikit-learn/sklearn/metrics/tests/test_classification.py,952,np.iinfo(np.uint32).max should be accumulated correctly,not
scikit-learn/sklearn/metrics/tests/test_classification.py,958,np.iinfo(np.int64).max should cause an overflow,not
scikit-learn/sklearn/metrics/tests/test_classification.py,966,Test performance report,not
scikit-learn/sklearn/metrics/tests/test_classification.py,970,print classification report with class names,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1010,print classification report with label detection,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1027,Test performance report with added digits in floating point values,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1031,print classification report with class names,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1189,Dense label indicator matrix format,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1203,Dense label indicator matrix format,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1218,sp_hamming only works with 1-D arrays,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1258,Dense label indicator matrix format,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1262,"size(y1 \inter y2) = [1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1263,"size(y1 \union y2) = [2, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1275,average='macro',not
scikit-learn/sklearn/metrics/tests/test_classification.py,1278,average='micro',not
scikit-learn/sklearn/metrics/tests/test_classification.py,1281,average='samples',not
scikit-learn/sklearn/metrics/tests/test_classification.py,1290,average=None,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1298,average='weighted',not
scikit-learn/sklearn/metrics/tests/test_classification.py,1344,"other than average='samples'/'none-samples', test everything else here",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1362,"tp=0, fp=0, fn=1, tn=0",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1364,"tp=0, fp=0, fn=0, tn=1",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1372,"tp=1, fp=0, fn=0, tn=0 (pos_label=0)",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1388,Test precision_recall_f1_score on a crafted multilabel example,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1389,First crafted example,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1396,"tp = [0, 1, 1, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1397,"fn = [1, 0, 0, 1]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1398,"fp = [1, 1, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1399,Check per class,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1410,Check macro,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1420,Check micro,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1431,Check weighted,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1441,Check samples,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1442,"|h(x_i) inter y_i | = [0, 1, 1]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1443,"|y_i| = [1, 1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1444,"|h(x_i)| = [1, 1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1457,Test precision_recall_f1_score on a crafted multilabel example 2,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1458,Second crafted example,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1462,tp = [ 0.  1.  0.  0.],not
scikit-learn/sklearn/metrics/tests/test_classification.py,1463,fp = [ 1.  0.  0.  2.],not
scikit-learn/sklearn/metrics/tests/test_classification.py,1464,fn = [ 1.  1.  1.  0.],not
scikit-learn/sklearn/metrics/tests/test_classification.py,1509,Check samples,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1510,"|h(x_i) inter y_i | = [0, 0, 1]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1511,"|y_i| = [1, 1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1512,"|h(x_i)| = [1, 1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1529,true_pos = [ 0.  1.  1.  0.],not
scikit-learn/sklearn/metrics/tests/test_classification.py,1530,false_pos = [ 0.  0.  0.  1.],not
scikit-learn/sklearn/metrics/tests/test_classification.py,1531,false_neg = [ 1.  1.  0.  0.],not
scikit-learn/sklearn/metrics/tests/test_classification.py,1584,"|h(x_i) inter y_i | = [0, 0, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1585,"|y_i| = [1, 1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1586,"|h(x_i)| = [0, 1, 2]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1644,"tp = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1645,"fn = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1646,"fp = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1647,"support = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1648,"|y_hat_i inter y_i | = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1649,"|y_i| = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1650,"|y_hat_i| = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1680,"tp = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1681,"fn = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1682,"fp = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1683,"support = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1684,"|y_hat_i inter y_i | = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1685,"|y_i| = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1686,"|y_hat_i| = [0, 0, 0]",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1705,average of per-label scores,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1721,average of per-sample scores,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1736,single score: micro-average,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1751,single positive label,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1781,average of per-label scores,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1791,average of per-sample scores,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1800,single score: micro-average,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1809,single positive label,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1912,Error if user does not explicitly set non-binary average mode,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1935,"Check that _check_targets correctly merges target types, squeezes",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1936,output and fails if input lengths differ.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1943,all of length 3,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1946,must not be considered binary,not
scikit-learn/sklearn/metrics/tests/test_classification.py,1957,"expected type given input types, or None for error",not
scikit-learn/sklearn/metrics/tests/test_classification.py,1958,(types will be tried in either order),not
scikit-learn/sklearn/metrics/tests/test_classification.py,1968,Disallowed types,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2019,Make sure seq of seq is not supported,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2031,https://github.com/scikit-learn/scikit-learn/issues/8098,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2109,"Currently, invariance of string and integer labels cannot be tested",not
scikit-learn/sklearn/metrics/tests/test_classification.py,2110,in common invariance tests because invariance tests for multiclass,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2111,decision functions is not implemented yet.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2136,"binary case with symbolic labels (""no"" < ""yes"")",not
scikit-learn/sklearn/metrics/tests/test_classification.py,2143,multiclass case; adapted from http://bit.ly/RJJHWA,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2149,check that we got all the shapes and axes right,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2150,by doubling the length of y_true and y_pred,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2156,check eps and handling of absolute zero and one probabilities,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2161,raise error if number of classes are not equal.,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2167,case when y_true is a string array object,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2173,test labels option,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2188,works when the labels argument is used,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2194,ensure labels work when len(np.unique(y_true)) != y_pred.shape[1],not
scikit-learn/sklearn/metrics/tests/test_classification.py,2202,case when input is a pandas series and dataframe gh-5715,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2212,"y_pred dataframe, y_true series",not
scikit-learn/sklearn/metrics/tests/test_classification.py,2219,Check brier_score_loss function,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2237,ensure to raise an error for multiclass y_true,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2245,calculate correctly when there's only one class in y_true,not
scikit-learn/sklearn/metrics/tests/test_classification.py,2270,Warnings are tested in test_balanced_accuracy_score_unseen,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,52,Test the pairwise_distance helper function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,55,Euclidean distance should be equivalent to calling the function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,61,"Euclidean distance, with Y != X.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,66,Check to ensure NaNs work with pairwise_distances.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,74,Test with tuples as X and Y,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,80,Test haversine distance,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,81,The data should be valid latitude and longitude,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,89,"Test haversine distance, with Y != X",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,97,"""cityblock"" uses scikit-learn metric, cityblock (function) is",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,98,scipy.spatial.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,105,The manhattan metric should be equivalent to cityblock.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,112,Test cosine as a string metric versus cosine callable,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,113,"The string ""cosine"" uses sklearn.metric,",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,114,while the function cosine is scipy.spatial,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,121,"Test with sparse X and Y,",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,122,"currently only supported for Euclidean, L1 and cosine.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,137,"Test with scipy.spatial.distance metric, with a kwd",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,143,same with Y = None,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,149,Test that scipy distance metrics throw an error if sparse matrix given,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,155,Test that a value error is raised if the metric is unknown,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,162,test that we convert to boolean arrays for boolean distances,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,168,ignore conversion to boolean in pairwise_distances,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,175,non-boolean arrays are converted to boolean for boolean,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,176,distance metrics with a data conversion warning,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,181,Check that the warning is raised if X is boolean by Y is not boolean:,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,185,Check that no warning is raised if X is already boolean and Y is None:,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,192,No warnings issued if metric is not a boolean distance function,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,202,Test correct shape,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,205,with two args,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,208,even if shape[1] agrees (although thus second arg is spurious),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,212,Test not copied (if appropriate dtype),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,216,with two args,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,221,Test always returns float dtype,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,225,Test converts list to array-like,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,231,Test non-negative values,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,240,Callable version of pairwise.rbf_kernel.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,262,Not all metrics support sparse input,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,263,ValueError may be triggered by bad callable,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,279,"paired_distances should allow callable metric where metric(x, x) != 0",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,280,Knowing that the callable is a strict metric would allow the diagonal to,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,281,be left uncalculated and set to 0.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,285,Test with all metrics that should be in PAIRWISE_KERNEL_FUNCTIONS.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,291,Test the pairwise_kernels helper function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,297,Test with Y=None,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,301,Test with Y=Y,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,305,Test with tuples as X and Y,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,311,Test with sparse X and Y,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,315,these don't support sparse matrices yet,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,325,Test the pairwise_kernels helper function,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,326,"with a callable function, with given keywords.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,337,"callable function, X=Y",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,358,Test the pairwise_distance helper function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,360,Euclidean distance should be equivalent to calling the function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,362,"Euclidean distance, with Y != X.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,371,Check the pairwise_distances implementation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,372,gives the same value,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,379,Test the pairwise_distance helper function,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,380,with the callable implementation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,382,Euclidean distance should be equivalent to calling the function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,384,"Euclidean distance, with Y != X.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,391,Test that a value error is raised when the lengths of X and Y should not,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,392,differ,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,399,Check pairwise minimum distances computation for any metric,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,410,euclidean metric,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,416,sparse matrix case,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,420,We don't want np.matrix here,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,424,euclidean metric squared,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,430,Non-euclidean scikit-learn metric,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,436,sparse matrix case,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,441,Non-euclidean Scipy distance (callable),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,447,Non-euclidean Scipy distance (string),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,453,Compare with naive implementation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,475,Reduced Euclidean distance,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,482,atol is for diagonal where S is explicitly zeroed on the diagonal,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,487,check that the reduce func is allowed to return None,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,577,Test the pairwise_distance helper function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,579,Euclidean distance should be equivalent to calling the function.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,583,Test small amounts of memory,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,587,X as list,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,590,"Euclidean distance, with Y != X.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,596,absurdly large working_memory,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,599,"""cityblock"" uses scikit-learn metric, cityblock (function) is",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,600,scipy.spatial.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,603,Test that a value error is raised if the metric is unknown,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,607,Test precomputed returns all at once,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,623,Check the pairwise Euclidean distances computation on known result,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,634,"check that we still get the right answers with {X,Y}_norm_squared",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,635,"and that we get a wrong answer with wrong {X,Y}_norm_squared",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,640,norms will only be used if their dtype is float64,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,655,"check we get the wrong answer with wrong {X,Y}_norm_squared",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,669,check that euclidean distances gives same result as scipy cdist,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,670,when X and Y != X are provided,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,683,the default rtol=1e-7 is too close to the float32 precision,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,684,and fails due too rounding errors.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,693,check that euclidean distances gives same result as scipy pdist,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,694,when only X is provided,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,704,the default rtol=1e-7 is too close to the float32 precision,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,705,and fails due too rounding errors.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,717,check batches handling when Y != X (#13910),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,731,the default rtol=1e-7 is too close to the float32 precision,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,732,and fails due too rounding errors.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,740,check batches handling when X is Y (#13910),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,751,the default rtol=1e-7 is too close to the float32 precision,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,752,and fails due too rounding errors.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,764,check that euclidean distances is correct with float32 input thanks to,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,765,upcasting. On float64 there are still precision issues.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,777,with no nan values,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,856,Check for symmetry,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,862,Check with explicit formula and squared=True,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,868,Check with explicit formula and squared=False,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,874,Check when Y = X is explicitly passed,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,881,Check copy = True against copy = False,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,889,First feature is the only feature that is non-nan and in both,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,890,samples. The result of `nan_euclidean_distances` with squared=True,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,891,should be non-negative. The non-squared version should all be close to 0.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,905,Check the pairwise Cosine distances computation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,911,"check that all elements are in [0, 2]",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,914,check that diagonal elements are equal to 0,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,919,"check that all elements are in [0, 2]",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,922,check that diagonal elements are equal to 0 and non diagonal to 2,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,925,check large random matrix,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,928,check that diagonal elements are equal to 0,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,935,Check haversine distance with distances computation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,950,Test haversine distance does not accept X where n_feature != 2,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,957,Paired distances,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,960,Check the paired Euclidean distances computation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,968,Check the paired manhattan distances computation,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,990,check diagonal is ones for data with itself,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,993,check off-diagonal is < 1 but > 0:,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,996,check that float32 is preserved,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1002,"check integer type gets converted,",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1003,check that zeros are handled,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1009,check that kernel of similar things is greater than dissimilar ones,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1016,test negative input,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1024,different n_features in X and Y,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1028,sparse matrices,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1040,Valid kernels should be symmetric,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1064,the diagonal elements of a linear kernel are their squared norm,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1072,the diagonal elements of a rbf kernel are 1,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1080,the diagonal elements of a laplacian kernel are 1,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1083,off-diagonal elements are < 1 but > 0:,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1098,should be sparse,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1102,"should be dense, and equal to K1",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1107,show the kernel output equal to the sparse.todense(),not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1113,Test the cosine_similarity.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1123,Test that the cosine is kernel is equal to a linear kernel when data,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1124,has been previously normalized by L2-norm.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1134,Ensure that pairwise array check works for dense matrices.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1135,"Check that if XB is None, XB is returned as reference to XA",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1143,"Ensure that if XA and XB are given correctly, they return as equal.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1144,"Check that if XB is not None, it is returned equal.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1145,Note that the second dimension of XB is the same as XA.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1159,Ensure an error is raised if the dimensions are different.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1171,Ensure an error is raised on 1D input arrays.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1172,"The modified tests are not 1D. In the old test, the array was internally",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1173,converted to 2D anyways,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1185,Ensures that checks return valid sparse matrices.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1192,compare their difference because testing csr matrices for,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1193,equality with '==' does not work as expected.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1207,Turns a numpy matrix (any n-dimensional array) into tuples.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1210,Tuplify each sub-array in the input.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1213,"Single dimension input, just return tuple of contents.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1218,Ensures that checks return valid tuples.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1230,Ensures that type float32 is preserved.,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1237,both float32,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1242,mismatched A,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1248,mismatched B,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1262,check that pairwise_distances give the same result in sequential and,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1263,"parallel, when metric has data-derived parameters.",not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1264,to have more than 1 chunk,not
scikit-learn/sklearn/metrics/tests/test_pairwise.py,1284,TODO: Remove warn_checker in 0.25,SATD
scikit-learn/sklearn/metrics/tests/test_regression.py,41,"Tweedie deviance needs positive y_pred, except for p=0,",not
scikit-learn/sklearn/metrics/tests/test_regression.py,42,p>=2 needs positive y_true,not
scikit-learn/sklearn/metrics/tests/test_regression.py,43,results evaluated by sympy,not
scikit-learn/sklearn/metrics/tests/test_regression.py,60,non-regression test for,not
scikit-learn/sklearn/metrics/tests/test_regression.py,61,https://github.com/scikit-learn/scikit-learn/pull/16323,not
scikit-learn/sklearn/metrics/tests/test_regression.py,84,mean_absolute_error and mean_squared_error are equal because,not
scikit-learn/sklearn/metrics/tests/test_regression.py,85,it is a binary problem.,not
scikit-learn/sklearn/metrics/tests/test_regression.py,120,Tweedie deviance error,not
scikit-learn/sklearn/metrics/tests/test_regression.py,159,All of length 3,not
scikit-learn/sklearn/metrics/tests/test_regression.py,209,mean_absolute_error and mean_squared_error are equal because,not
scikit-learn/sklearn/metrics/tests/test_regression.py,210,it is a binary problem.,not
scikit-learn/sklearn/metrics/tests/test_regression.py,228,Checking for the condition in which both numerator and denominator is,not
scikit-learn/sklearn/metrics/tests/test_regression.py,229,zero.,not
scikit-learn/sklearn/metrics/tests/test_regression.py,240,Handling msle separately as it does not accept negative inputs.,not
scikit-learn/sklearn/metrics/tests/test_regression.py,266,Handling msle separately as it does not accept negative inputs.,not
scikit-learn/sklearn/metrics/tests/test_regression.py,281,Trigger the warning,not
scikit-learn/sklearn/metrics/tests/test_regression.py,296,"Ws we get closer to the limit, with 1e-12 difference the absolute",not
scikit-learn/sklearn/metrics/tests/test_regression.py,297,tolerance to pass the below check increases. There are likely,not
scikit-learn/sklearn/metrics/tests/test_regression.py,298,numerical precision issues on the edges of different definition,not
scikit-learn/sklearn/metrics/tests/test_regression.py,299,regions.,not
scikit-learn/sklearn/metrics/tests/test_common.py,63,Note toward developers about metric testing,not
scikit-learn/sklearn/metrics/tests/test_common.py,64,-------------------------------------------,not
scikit-learn/sklearn/metrics/tests/test_common.py,65,It is often possible to write one general test for several metrics:,not
scikit-learn/sklearn/metrics/tests/test_common.py,66,,not
scikit-learn/sklearn/metrics/tests/test_common.py,67,"- invariance properties, e.g. invariance to sample order",not
scikit-learn/sklearn/metrics/tests/test_common.py,68,"- common behavior for an argument, e.g. the ""normalize"" with value True",not
scikit-learn/sklearn/metrics/tests/test_common.py,69,will return the mean of the metrics and with value False will return,not
scikit-learn/sklearn/metrics/tests/test_common.py,70,the sum of the metrics.,not
scikit-learn/sklearn/metrics/tests/test_common.py,71,,not
scikit-learn/sklearn/metrics/tests/test_common.py,72,"In order to improve the overall metric testing, it is a good idea to write",not
scikit-learn/sklearn/metrics/tests/test_common.py,73,first a specific test for the given metric and then add a general test for,not
scikit-learn/sklearn/metrics/tests/test_common.py,74,all metrics that have the same behavior.,not
scikit-learn/sklearn/metrics/tests/test_common.py,75,,not
scikit-learn/sklearn/metrics/tests/test_common.py,76,Two types of datastructures are used in order to implement this system:,not
scikit-learn/sklearn/metrics/tests/test_common.py,77,dictionaries of metrics and lists of metrics wit common properties.,not
scikit-learn/sklearn/metrics/tests/test_common.py,78,,not
scikit-learn/sklearn/metrics/tests/test_common.py,79,Dictionaries of metrics,not
scikit-learn/sklearn/metrics/tests/test_common.py,80,------------------------,not
scikit-learn/sklearn/metrics/tests/test_common.py,81,The goal of having those dictionaries is to have an easy way to call a,not
scikit-learn/sklearn/metrics/tests/test_common.py,82,particular metric and associate a name to each function:,not
scikit-learn/sklearn/metrics/tests/test_common.py,83,,not
scikit-learn/sklearn/metrics/tests/test_common.py,84,- REGRESSION_METRICS: all regression metrics.,not
scikit-learn/sklearn/metrics/tests/test_common.py,85,- CLASSIFICATION_METRICS: all classification metrics,not
scikit-learn/sklearn/metrics/tests/test_common.py,86,which compare a ground truth and the estimated targets as returned by a,not
scikit-learn/sklearn/metrics/tests/test_common.py,87,classifier.,not
scikit-learn/sklearn/metrics/tests/test_common.py,88,- THRESHOLDED_METRICS: all classification metrics which,not
scikit-learn/sklearn/metrics/tests/test_common.py,89,"compare a ground truth and a score, e.g. estimated probabilities or",not
scikit-learn/sklearn/metrics/tests/test_common.py,90,decision function (format might vary),not
scikit-learn/sklearn/metrics/tests/test_common.py,91,,not
scikit-learn/sklearn/metrics/tests/test_common.py,92,Those dictionaries will be used to test systematically some invariance,not
scikit-learn/sklearn/metrics/tests/test_common.py,93,"properties, e.g. invariance toward several input layout.",not
scikit-learn/sklearn/metrics/tests/test_common.py,94,,not
scikit-learn/sklearn/metrics/tests/test_common.py,117,`confusion_matrix` returns absolute values and hence behaves unnormalized,not
scikit-learn/sklearn/metrics/tests/test_common.py,118,. Naming it with an unnormalized_ prefix is necessary for this module to,not
scikit-learn/sklearn/metrics/tests/test_common.py,119,skip sample_weight scaling checks which will fail for unnormalized,not
scikit-learn/sklearn/metrics/tests/test_common.py,120,metrics.,not
scikit-learn/sklearn/metrics/tests/test_common.py,135,These are needed to test averaging,not
scikit-learn/sklearn/metrics/tests/test_common.py,218,"default: average=""macro""",not
scikit-learn/sklearn/metrics/tests/test_common.py,231,"default: average=""macro""",not
scikit-learn/sklearn/metrics/tests/test_common.py,250,Lists of metrics with common properties,not
scikit-learn/sklearn/metrics/tests/test_common.py,251,---------------------------------------,not
scikit-learn/sklearn/metrics/tests/test_common.py,252,Lists of metrics with common properties are used to test systematically some,not
scikit-learn/sklearn/metrics/tests/test_common.py,253,"functionalities and invariance, e.g. SYMMETRIC_METRICS lists all metrics that",not
scikit-learn/sklearn/metrics/tests/test_common.py,254,are symmetric with respect to their input argument y_true and y_pred.,not
scikit-learn/sklearn/metrics/tests/test_common.py,255,,not
scikit-learn/sklearn/metrics/tests/test_common.py,256,"When you add a new metric or functionality, check if a general test",not
scikit-learn/sklearn/metrics/tests/test_common.py,257,is already written.,not
scikit-learn/sklearn/metrics/tests/test_common.py,259,Those metrics don't support binary inputs,not
scikit-learn/sklearn/metrics/tests/test_common.py,275,Those metrics don't support multiclass inputs,not
scikit-learn/sklearn/metrics/tests/test_common.py,292,"with default average='binary', multiclass is prohibited",not
scikit-learn/sklearn/metrics/tests/test_common.py,299,curves,not
scikit-learn/sklearn/metrics/tests/test_common.py,304,"Metric undefined with ""binary"" or ""multiclass"" input",not
scikit-learn/sklearn/metrics/tests/test_common.py,308,"Metrics with an ""average"" argument",not
scikit-learn/sklearn/metrics/tests/test_common.py,314,"Threshold-based metrics with an ""average"" argument",not
scikit-learn/sklearn/metrics/tests/test_common.py,319,"Metrics with a ""pos_label"" argument",not
scikit-learn/sklearn/metrics/tests/test_common.py,334,pos_label support deprecated; to be removed in 0.18:,not
scikit-learn/sklearn/metrics/tests/test_common.py,345,"Metrics with a ""labels"" argument",not
scikit-learn/sklearn/metrics/tests/test_common.py,346,TODO: Handle multi_class metrics that has a labels argument as well as a,SATD
scikit-learn/sklearn/metrics/tests/test_common.py,347,decision function argument. e.g hinge_loss,not
scikit-learn/sklearn/metrics/tests/test_common.py,375,"Metrics with a ""normalize"" option",not
scikit-learn/sklearn/metrics/tests/test_common.py,381,"Threshold-based metrics with ""multilabel-indicator"" format support",not
scikit-learn/sklearn/metrics/tests/test_common.py,400,"Classification metrics with  ""multilabel-indicator"" format",not
scikit-learn/sklearn/metrics/tests/test_common.py,425,"Regression metrics with ""multioutput-continuous"" format support",not
scikit-learn/sklearn/metrics/tests/test_common.py,431,Symmetric with respect to their input arguments y_true and y_pred,not
scikit-learn/sklearn/metrics/tests/test_common.py,432,"metric(y_true, y_pred) == metric(y_pred, y_true).",not
scikit-learn/sklearn/metrics/tests/test_common.py,444,P = R = F = accuracy in multiclass case,not
scikit-learn/sklearn/metrics/tests/test_common.py,454,Asymmetric with respect to their input arguments y_true and y_pred,not
scikit-learn/sklearn/metrics/tests/test_common.py,455,"metric(y_true, y_pred) != metric(y_pred, y_true).",not
scikit-learn/sklearn/metrics/tests/test_common.py,479,No Sample weight support,not
scikit-learn/sklearn/metrics/tests/test_common.py,504,We shouldn't forget any metrics,not
scikit-learn/sklearn/metrics/tests/test_common.py,517,Test the symmetry of score and loss functions,not
scikit-learn/sklearn/metrics/tests/test_common.py,544,Test the symmetry of score and loss functions,not
scikit-learn/sklearn/metrics/tests/test_common.py,554,use context manager to supply custom error message,not
scikit-learn/sklearn/metrics/tests/test_common.py,583,Generate some data,not
scikit-learn/sklearn/metrics/tests/test_common.py,654,Mix format support,not
scikit-learn/sklearn/metrics/tests/test_common.py,679,These mix representations aren't allowed,not
scikit-learn/sklearn/metrics/tests/test_common.py,693,"NB: We do not test for y1_row, y2_row as these may be",not
scikit-learn/sklearn/metrics/tests/test_common.py,694,interpreted as multilabel or multioutput data.,not
scikit-learn/sklearn/metrics/tests/test_common.py,705,Ensure that classification metrics with string labels are invariant,not
scikit-learn/sklearn/metrics/tests/test_common.py,720,"Ugly, but handle case with a pos_label and label",SATD
scikit-learn/sklearn/metrics/tests/test_common.py,753,Ensure that thresholded metrics with string labels are invariant,not
scikit-learn/sklearn/metrics/tests/test_common.py,765,"Ugly, but handle case with a pos_label and label",SATD
scikit-learn/sklearn/metrics/tests/test_common.py,781,TODO those metrics doesn't support string label yet,SATD
scikit-learn/sklearn/metrics/tests/test_common.py,805,Classification metrics all raise a mixed input exception,not
scikit-learn/sklearn/metrics/tests/test_common.py,814,Non-regression test: scores should work with a single sample.,not
scikit-learn/sklearn/metrics/tests/test_common.py,815,This is important for leave-one-out cross validation.,not
scikit-learn/sklearn/metrics/tests/test_common.py,816,"Score functions tested are those that formerly called np.squeeze,",not
scikit-learn/sklearn/metrics/tests/test_common.py,817,which turns an array of size 1 into a 0-d array (!).,not
scikit-learn/sklearn/metrics/tests/test_common.py,820,assert that no exception is thrown,not
scikit-learn/sklearn/metrics/tests/test_common.py,840,Those metrics are not always defined with one sample,not
scikit-learn/sklearn/metrics/tests/test_common.py,841,or in multiclass classification,not
scikit-learn/sklearn/metrics/tests/test_common.py,865,test invariance to dimension shuffling,not
scikit-learn/sklearn/metrics/tests/test_common.py,883,Generate some data,not
scikit-learn/sklearn/metrics/tests/test_common.py,894,To make sure at least one empty label is present,not
scikit-learn/sklearn/metrics/tests/test_common.py,910,XXX cruel hack to work with partial functions,SATD
scikit-learn/sklearn/metrics/tests/test_common.py,917,Check representation invariance,not
scikit-learn/sklearn/metrics/tests/test_common.py,938,make sure the multilabel-sequence format raises ValueError,not
scikit-learn/sklearn/metrics/tests/test_common.py,954,Test in the binary case,not
scikit-learn/sklearn/metrics/tests/test_common.py,971,Test in the multiclass case,not
scikit-learn/sklearn/metrics/tests/test_common.py,987,Test in the multilabel case,not
scikit-learn/sklearn/metrics/tests/test_common.py,991,"for both random_state 0 and 1, y_true and y_pred has at least one",not
scikit-learn/sklearn/metrics/tests/test_common.py,992,unlabelled entry,not
scikit-learn/sklearn/metrics/tests/test_common.py,1004,To make sure at least one empty label is present,not
scikit-learn/sklearn/metrics/tests/test_common.py,1023,No averaging,not
scikit-learn/sklearn/metrics/tests/test_common.py,1029,Micro measure,not
scikit-learn/sklearn/metrics/tests/test_common.py,1034,Macro measure,not
scikit-learn/sklearn/metrics/tests/test_common.py,1038,Weighted measure,not
scikit-learn/sklearn/metrics/tests/test_common.py,1049,Sample measure,not
scikit-learn/sklearn/metrics/tests/test_common.py,1129,Test _average_binary_score for weight.sum() == 0,not
scikit-learn/sklearn/metrics/tests/test_common.py,1154,check that unit weights gives the same score as no weight,not
scikit-learn/sklearn/metrics/tests/test_common.py,1163,check that the weighted and unweighted scores are unequal,not
scikit-learn/sklearn/metrics/tests/test_common.py,1166,use context manager to supply custom error message,not
scikit-learn/sklearn/metrics/tests/test_common.py,1173,check that sample_weight can be a list,not
scikit-learn/sklearn/metrics/tests/test_common.py,1182,check that integer weights is the same as repeated samples,not
scikit-learn/sklearn/metrics/tests/test_common.py,1190,check that ignoring a fraction of the samples is equivalent to setting,not
scikit-learn/sklearn/metrics/tests/test_common.py,1191,the corresponding weights to zero,not
scikit-learn/sklearn/metrics/tests/test_common.py,1208,check that the score is invariant under scaling of the weights by a,not
scikit-learn/sklearn/metrics/tests/test_common.py,1209,common factor,not
scikit-learn/sklearn/metrics/tests/test_common.py,1217,Check that if number of samples in y_true and sample_weight are not,not
scikit-learn/sklearn/metrics/tests/test_common.py,1218,"equal, meaningful error is raised.",not
scikit-learn/sklearn/metrics/tests/test_common.py,1236,regression,not
scikit-learn/sklearn/metrics/tests/test_common.py,1249,binary,not
scikit-learn/sklearn/metrics/tests/test_common.py,1268,multiclass,not
scikit-learn/sklearn/metrics/tests/test_common.py,1276,softmax,not
scikit-learn/sklearn/metrics/tests/test_common.py,1289,multilabel indicator,not
scikit-learn/sklearn/metrics/tests/test_common.py,1310,test labels argument when not using averaging,not
scikit-learn/sklearn/metrics/tests/test_common.py,1311,in multi-class and multi-label cases,not
scikit-learn/sklearn/metrics/tests/test_common.py,1361,Makes sure all samples have at least one label. This works around errors,not
scikit-learn/sklearn/metrics/tests/test_common.py,1362,"when running metrics where average=""sample""",not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,64,All supervised cluster scorers (They behave like classification metric),not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,89,Make estimators that make sense to test various scoring methods,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,91,some of the regressions scorers require strictly positive input.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,111,Create some memory mapped data,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,125,GC closes the mmap file descriptors,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,167,Test that all scorers have a working repr,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,173,Test all branches of single metric usecases,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,200,Test the allow_none parameter for check_scoring alone,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,208,This wraps the _check_multimetric_scoring to take in,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,209,single metric scoring parameter so we can run the tests,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,210,"that we will run for check_scoring, for check_multimetric_scoring",not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,211,too for single-metric usecases,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,214,"For all single metric use cases, it should register as not multimetric",not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,227,To make sure the check_scoring is correctly applied to the constituent,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,228,scorers,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,232,For multiple metric use cases,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,233,Make sure it works for the valid cases,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,262,Make sure it raises errors when scoring parameter is not valid.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,263,More weird corner cases are tested at test_validation.py,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,265,Tuple of callables,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,273,test that check_scoring works on GridSearchCV and pipeline.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,274,slightly redundant non-regression test.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,284,check that cross_val_score definitely calls the scorer,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,285,and doesn't make any assumptions about the estimator apart from having a,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,286,fit.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,293,Sanity check on the make_scorer factory function.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,300,Test classification scorers.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,329,test fbeta score that takes an argument,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,335,test that custom scorer can be pickled,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,340,smoke test the repr:,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,345,Test regression scorers.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,357,Test scorers that take thresholds.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,372,same for an estimator without decision_function,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,379,test with a regressor (no decision_function),not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,386,Test that an exception is raised on more than two classes,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,393,test error is raised with a single class present in model,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,394,(predict_proba shape is not suitable for binary auc),not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,402,for proba scorers,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,408,Test that the scorer work with multilabel-indicator format,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,409,for multilabel and multi-output multi-class classifier,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,414,Multi-output multi-class predict_proba,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,422,Multi-output multi-class decision_function,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,423,TODO Is there any yet?,SATD
scikit-learn/sklearn/metrics/tests/test_score_objects.py,435,Multilabel predict_proba,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,442,Multilabel decision function,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,451,Test clustering scorers against gold standard labeling.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,464,"Test that when a list of scores is returned, we raise proper errors.",not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,478,Test that scorers support sample_weight or raise sensible errors,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,480,"Unlike the metrics invariance test, in the scorer case it's harder",not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,481,"to ensure that, on the classifier output, weighted and unweighted",not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,482,scores really should be unequal.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,492,get sensible estimators for each metric,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,525,Non-regression test for #6147: some score functions would,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,526,return singleton memmap when computed on memmap data instead of scalar,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,527,float values.,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,535,UndefinedMetricWarning for P / R scores,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,598,compare dict keys,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,616,no decision function,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,639,no decision function,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,652,scoring dictionary returned is the same as calling each scorer separately,not
scikit-learn/sklearn/metrics/tests/test_score_objects.py,714,Perceptron has no predict_proba,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,35,,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,36,Utilities for testing,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,46,import some data to play with,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,53,restrict to a binary classification task,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,64,add noisy features to make the problem harder and avoid perfect results,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,68,"run classifier, get class probabilities and label predictions",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,73,only interested in probabilities of the positive case,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,74,XXX: do we really want a special API for the binary case?,SATD
scikit-learn/sklearn/metrics/tests/test_ranking.py,82,,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,83,Tests,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,90,Count the number of times positive samples are correctly ranked above,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,91,negative samples.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,118,Compute precision up to document i,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,119,"i.e, percentage of relevant documents up to document i.",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,169,Formula (5) from McClish 1989,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,179,Test Area under Receiver Operating Characteristic (ROC) curve,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,193,Make sure that roc_curve returns a curve start at 0 and ending and,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,194,1 even in corner cases,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,206,Test whether the returned threshold matches up with tpr,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,207,make small toy dataset,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,211,use the given thresholds to determine the tpr,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,218,compare tpr and tpr_correct to see if the thresholds' order was correct,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,225,roc_curve not applicable for multi-class problems,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,233,roc_curve for confidence scores,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,244,roc_curve for hard decisions,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,247,always predict one,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,255,always predict zero,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,263,hard decisions,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,274,assert there are warnings,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,277,"all true labels, all fpr should be nan",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,282,assert there are warnings,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,286,"all negative labels, all tpr should be nan",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,293,Binary classification,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,336,assert UndefinedMetricWarning because of no positive sample in y_true,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,346,assert UndefinedMetricWarning because of no negative sample in y_true,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,354,Multi-label classification task,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,389,Test that drop_intermediate drops the correct thresholds,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,395,Test dropping thresholds with repeating scores,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,406,Ensure that fpr and tpr returned by roc_curve are increasing.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,407,Construct an edge case with float y_score and sample_weight,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,408,when some adjacent values of fpr and tpr are actually the same.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,418,Test Area Under Curve (AUC) computation,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,437,Incompatible shapes,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,441,Too few x values,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,445,x is not in order,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,462,Tests the one-vs-one multiclass ROC AUC algorithm,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,463,"on a small example, representative of an expected use case.",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,467,Used to compute the expected output.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,468,Consider labels 0 and 1:,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,469,"positive label is 0, negative label is 1",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,471,"positive label is 1, negative label is 0",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,475,Consider labels 0 and 2:,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,480,Consider labels 1 and 2:,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,485,"Unweighted, one-vs-one multiclass ROC AUC algorithm",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,492,"Weighted, one-vs-one multiclass ROC AUC algorithm",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,493,Each term is weighted by the prevalence for the positive label.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,510,Tests the one-vs-one multiclass ROC AUC algorithm for binary y_true,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,511,,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,512,"on a small example, representative of an expected use case.",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,516,Used to compute the expected output.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,517,Consider labels 0 and 1:,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,518,"positive label is 0, negative label is 1",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,520,"positive label is 1, negative label is 0",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,528,"Weighted, one-vs-one multiclass ROC AUC algorithm",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,541,"Tests the unweighted, one-vs-rest multiclass ROC AUC algorithm",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,542,"on a small example, representative of an expected use case.",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,545,Compute the expected result by individually computing the 'one-vs-rest',not
scikit-learn/sklearn/metrics/tests/test_ranking.py,546,"ROC AUC scores for classes 0, 1, and 2.",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,556,"Tests the weighted, one-vs-rest multiclass ROC AUC algorithm",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,557,"on the same input (Provost & Domingos, 2000)",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,622,Test that roc_auc_score function returns an error when trying,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,623,to compute multiclass AUC for parameters where an output,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,624,is not defined.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,634,Test that roc_auc_score function returns an error when trying,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,635,to compute AUC for non-binary class values.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,638,y_true contains only one class value,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,653,y_true contains only one class value,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,683,Check that using string class labels raises an informative,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,684,error for any supported string dtype:,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,695,The error message is slightly different for bytes-encoded,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,696,"class labels, but otherwise the behavior is the same:",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,704,Check that it is possible to use floating point class labels,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,705,that are interpreted similarly to integer class labels:,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,717,"Use {-1, 1} for labels; make sure original labels aren't modified",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,734,Test Precision-Recall and aread under PR curve,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,740,`_average_precision` is not very precise in case of 0.5 ties: be tolerant,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,745,Smoke test in the case of proba having only one value,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,753,Contains non-binary labels,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,760,Binary classification,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,775,Here we are doing a terrible prediction: we are always getting,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,776,"it wrong, hence the average_precision_score is the accuracy at",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,777,chance: 50%,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,818,Multi-label classification task,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,864,if one class is never present weighted should not be NaN,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,872,Check the average_precision_score of a constant predictor is,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,873,the TPR,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,875,Generate a dataset with 25% of positives,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,878,And a constant score,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,880,The precision is then the fraction of positive whatever the recall,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,881,"is, as there is only one threshold:",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,886,Raise an error when pos_label is not in binary y_true,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,892,Raise an error for multilabel-indicator y_true with,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,893,pos_label other than 1,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,904,Test that average_precision_score and roc_auc_score are invariant by,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,905,the scaling or shifting of probabilities,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,906,This test was expanded (added scaled_down) in response to github,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,907,"issue #3864 (and others), where overly aggressive rounding was causing",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,908,problems for users with very small y_score values,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,929,Check on several small example that it works,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,963,Tie handling,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,991,No relevant labels,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,996,Only relevant labels,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1001,Degenerate case: only one label,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1007,Raise value error if not appropriate format,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1017,Check that y_true.shape != y_score.shape raise the proper exception,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1033,Check tie handling in score,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1034,Basic check with only ties and increasing label space,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1038,Check for growing number of consecutive relevant,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1040,Check for a bunch of positions,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1049,Check that Label ranking average precision works for various,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1050,Basic check with increasing label space size and decreasing score,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1054,First and last,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1061,Check for growing number of consecutive relevant label,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1063,Check for a bunch of position,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1080,The best rank correspond to 1. Rank higher than 1 are worse.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1081,The best inverse ranking correspond to n_labels.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1086,Rank need to be corrected to take into account ties,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1087,ex: rank 1 ex aequo means that both label are rank 2.,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1098,Let's count the number of relevant label with better rank,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1099,(smaller rank).,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1102,Weight by the rank of the actual label,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1118,Score with ties,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1129,Uniform score,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1165,"Degenerate sample labeling (e.g., zero labels for a sample) is a valid",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1166,special case for lrap (the sample is considered to achieve perfect,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1167,"precision), but this case is not tested in test_common.",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1168,"For these test samples, the APs are 0.5, 0.75, and 1.0 (default for zero",not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1169,labels).,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1184,Toy case,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1217,Non trival case,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1264,Undefined metrics -  the ranking doesn't matter,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1278,Non trival case,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1293,Sparse csr matrices,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1301,Check that y_true.shape != y_score.shape raise the proper exception,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1317,Tie handling,not
scikit-learn/sklearn/metrics/tests/test_ranking.py,1450,Check `roc_auc_score` for max_fpr != `None`,not
scikit-learn/sklearn/metrics/_plot/confusion_matrix.py,94,print text with appropriate color depending on background,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,19,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,SATD
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,194,make sure text color is appropriate depending on background,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,200,diagonal text is black,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,204,off-diagonal text is white,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,209,diagonal text is white,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,213,off-diagonal text is black,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,217,Regression test for #15920,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,252,Make sure plot text is formatted with 'values_format'.,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,272,"Values should be shown as whole numbers 'd',",not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,273,except the first number which should be shown as 1e+07 (longer length),not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,274,and the last number will be shown as 1.2e+07 (longer length),not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,281,"Values should now formatted as '.2g', since there's a float in",not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py,282,"Values are have two dec places max, (e.g 100 becomes 1e+02)",not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,18,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,SATD
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,111,cannot fail thanks to pyplot fixture,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,112,noqal,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,140,non-regression test checking that the `name` used when calling,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_roc_curve.py,141,`plot_roc_curve` is used as well when calling `disp.plot()`,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,20,TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved,SATD
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,32,Unfitted classifer,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,40,Fitted multiclass classifier with binary data,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,108,cannot fail thanks to pyplot fixture,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,109,noqa,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,120,draw again with another label,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,140,regression test #15738,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,160,non-regression test checking that the `name` used when calling,not
scikit-learn/sklearn/metrics/_plot/tests/test_plot_precision_recall.py,161,`plot_roc_curve` is used as well when calling `disp.plot()`,not
scikit-learn/sklearn/manifold/_mds.py,5,author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,not
scikit-learn/sklearn/manifold/_mds.py,6,License: BSD,not
scikit-learn/sklearn/manifold/_mds.py,77,Randomly choose initial configuration,not
scikit-learn/sklearn/manifold/_mds.py,81,overrides the parameter p,not
scikit-learn/sklearn/manifold/_mds.py,91,Compute distance and monotonic regression,not
scikit-learn/sklearn/manifold/_mds.py,98,dissimilarities with 0 are considered as missing values,not
scikit-learn/sklearn/manifold/_mds.py,101,Compute the disparities using a monotonic regression,not
scikit-learn/sklearn/manifold/_mds.py,109,Compute stress,not
scikit-learn/sklearn/manifold/_mds.py,112,Update X using the Guttman transform,not
scikit-learn/sklearn/manifold/_locally_linear.py,3,Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/manifold/_locally_linear.py,4,Jake Vanderplas  -- <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/manifold/_locally_linear.py,5,License: BSD 3 clause (C) INRIA 2011,not
scikit-learn/sklearn/manifold/_locally_linear.py,52,this might raise a LinalgError if G is singular and has trace,not
scikit-learn/sklearn/manifold/_locally_linear.py,53,zero,not
scikit-learn/sklearn/manifold/_locally_linear.py,55,broadcasting,not
scikit-learn/sklearn/manifold/_locally_linear.py,160,"initialize with [-1,1] as in ARPACK",not
scikit-learn/sklearn/manifold/_locally_linear.py,318,we'll compute M = (I-W)'(I-W),not
scikit-learn/sklearn/manifold/_locally_linear.py,319,"depending on the solver, we'll do this differently",not
scikit-learn/sklearn/manifold/_locally_linear.py,325,W = W - I = W - I,not
scikit-learn/sklearn/manifold/_locally_linear.py,350,build Hessian estimator,not
scikit-learn/sklearn/manifold/_locally_linear.py,388,find the eigenvectors and eigenvalues of each local covariance,not
scikit-learn/sklearn/manifold/_locally_linear.py,389,"matrix. We want V[i] to be a [n_neighbors x n_neighbors] matrix,",not
scikit-learn/sklearn/manifold/_locally_linear.py,390,where the columns are eigenvectors,not
scikit-learn/sklearn/manifold/_locally_linear.py,395,choose the most efficient way to find the eigenvectors,not
scikit-learn/sklearn/manifold/_locally_linear.py,412,find regularized weights: this is like normal LLE.,not
scikit-learn/sklearn/manifold/_locally_linear.py,413,"because we've already computed the SVD of each covariance matrix,",not
scikit-learn/sklearn/manifold/_locally_linear.py,414,it's faster to use this rather than np.linalg.solve,not
scikit-learn/sklearn/manifold/_locally_linear.py,426,calculate eta: the median of the ratio of small to large eigenvalues,not
scikit-learn/sklearn/manifold/_locally_linear.py,427,"across the points.  This is used to determine s_i, below",not
scikit-learn/sklearn/manifold/_locally_linear.py,431,"find s_i, the size of the ""almost null space"" for each point:",not
scikit-learn/sklearn/manifold/_locally_linear.py,432,this is the size of the largest set of eigenvalues,not
scikit-learn/sklearn/manifold/_locally_linear.py,433,such that Sum[v; v in set]/Sum[v; v not in set] < eta,not
scikit-learn/sklearn/manifold/_locally_linear.py,439,number of zero eigenvalues,not
scikit-learn/sklearn/manifold/_locally_linear.py,441,Now calculate M.,not
scikit-learn/sklearn/manifold/_locally_linear.py,442,This is the [N x N] matrix whose null space is the desired embedding,not
scikit-learn/sklearn/manifold/_locally_linear.py,447,select bottom s_i eigenvectors and calculate alpha,not
scikit-learn/sklearn/manifold/_locally_linear.py,451,compute Householder matrix which satisfies,not
scikit-learn/sklearn/manifold/_locally_linear.py,452,Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s),not
scikit-learn/sklearn/manifold/_locally_linear.py,453,using prescription from paper,not
scikit-learn/sklearn/manifold/_locally_linear.py,462,Householder matrix is,not
scikit-learn/sklearn/manifold/_locally_linear.py,463,">> Hi = np.identity(s_i) - 2*np.outer(h,h)",not
scikit-learn/sklearn/manifold/_locally_linear.py,464,Then the weight matrix is,not
scikit-learn/sklearn/manifold/_locally_linear.py,465,">> Wi = np.dot(Vi,Hi) + (1-alpha_i) * w_reg[i,:,None]",not
scikit-learn/sklearn/manifold/_locally_linear.py,466,We do this much more efficiently:,not
scikit-learn/sklearn/manifold/_locally_linear.py,470,Update M as follows:,not
scikit-learn/sklearn/manifold/_locally_linear.py,471,">> W_hat = np.zeros( (N,s_i) )",not
scikit-learn/sklearn/manifold/_locally_linear.py,472,">> W_hat[neighbors[i],:] = Wi",not
scikit-learn/sklearn/manifold/_locally_linear.py,473,>> W_hat[i] -= 1,not
scikit-learn/sklearn/manifold/_locally_linear.py,474,">> M += np.dot(W_hat,W_hat.T)",not
scikit-learn/sklearn/manifold/_locally_linear.py,475,We can do this much more efficiently:,not
scikit-learn/sklearn/manifold/_locally_linear.py,499,compute n_components largest eigenvalues of Xi * Xi^T,not
scikit-learn/sklearn/manifold/_isomap.py,3,Author: Jake Vanderplas  -- <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/manifold/_isomap.py,4,License: BSD 3 clause (C) 2011,not
scikit-learn/sklearn/manifold/_isomap.py,171,mypy error: Decorated property not supported,not
scikit-learn/sklearn/manifold/_isomap.py,172,type: ignore,not
scikit-learn/sklearn/manifold/_isomap.py,267,Create the graph of shortest distances from X to,not
scikit-learn/sklearn/manifold/_isomap.py,268,training data via the nearest neighbors of X.,not
scikit-learn/sklearn/manifold/_isomap.py,269,"This can be done as a single array operation, but it potentially",not
scikit-learn/sklearn/manifold/_isomap.py,270,"takes a lot of memory.  To avoid that, use a loop:",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,3,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,4,Wei LI <kuantkid@gmail.com>,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,48,speed up row-wise access to boolean connection mask,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,84,"sparse graph, find all the connected components",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,88,"dense graph, find all connected components start from node 0",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,113,We need all entries in the diagonal to values,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,122,If the matrix has a small number of diagonals (as in the,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,123,"case of structured matrices coming from images), the",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,124,dia format might be best suited for matvec products:,SATD
scikit-learn/sklearn/manifold/_spectral_embedding.py,127,3 or less outer diagonals on each side,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,130,csr has the fastest matvec and is thus best suited to,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,131,arpack,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,231,Whether to drop the first eigenvector,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,243,lobpcg used with eigen_solver='amg' has bugs for low number of nodes,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,244,for details see the source code in scipy:,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,245,https://github.com/scipy/scipy/blob/v0.11.0/scipy/sparse/linalg/eigen,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,246,/lobpcg/lobpcg.py#L237,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,247,or matlab:,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,248,https://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,251,Here we'll use shift-invert mode for fast eigenvalues,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,252,(see https://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,253,for a short explanation of what this means),not
scikit-learn/sklearn/manifold/_spectral_embedding.py,254,"Because the normalized Laplacian has eigenvalues between 0 and 2,",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,255,I - L has eigenvalues between -1 and 1.  ARPACK is most efficient,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,256,when finding eigenvalues of largest magnitude (keyword which='LM'),not
scikit-learn/sklearn/manifold/_spectral_embedding.py,257,and when these eigenvalues are very large compared to the rest.,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,258,"For very large, very sparse graphs, I - L can have many, many",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,259,eigenvalues very near 1.0.  This leads to slow convergence.  So,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,260,"instead, we'll use ARPACK's shift-invert mode, asking for the",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,261,eigenvalues near 1.0.  This effectively spreads-out the spectrum,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,262,near 1.0 and leads to much faster convergence: potentially an,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,263,orders-of-magnitude speedup over simply using keyword which='LA',not
scikit-learn/sklearn/manifold/_spectral_embedding.py,264,in standard mode.,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,266,We are computing the opposite of the laplacian inplace so as,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,267,to spare a memory allocation of a possibly very large array,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,277,"When submatrices are exactly singular, an LU decomposition",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,278,in arpack fails. We fallback to lobpcg,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,280,Revert the laplacian to its opposite to have lobpcg work,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,284,Use AMG to get a preconditioner and speed up the eigenvalue,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,285,problem.,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,288,lobpcg needs double precision floats,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,293,"The Laplacian matrix is always singular, having at least one zero",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,294,"eigenvalue, corresponding to the trivial eigenvector, which is a",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,295,constant. Using a singular matrix for preconditioning may result in,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,296,random failures in LOBPCG and is not supported by the existing,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,297,theory:,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,298,see https://doi.org/10.1007/s10208-015-9297-1,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,299,Shift the Laplacian so its diagononal is not all ones. The shift,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,300,"does change the eigenpairs however, so we'll feed the shifted",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,301,matrix to the solver and afterward set it back to the original.,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,320,lobpcg needs double precision floats,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,324,see note above under arpack why lobpcg has problems with small,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,325,number of nodes,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,326,"lobpcg will fallback to eigh, so we short circuit it",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,335,"We increase the number of eigenvectors requested, as lobpcg",not
scikit-learn/sklearn/manifold/_spectral_embedding.py,336,doesn't behave well in low dimension,not
scikit-learn/sklearn/manifold/_spectral_embedding.py,505,currently only symmetric affinity_matrix supported,not
scikit-learn/sklearn/manifold/_t_sne.py,1,Author: Alexander Fabisch  -- <afabisch@informatik.uni-bremen.de>,not
scikit-learn/sklearn/manifold/_t_sne.py,2,Author: Christopher Moody <chrisemoody@gmail.com>,not
scikit-learn/sklearn/manifold/_t_sne.py,3,Author: Nick Travers <nickt@squareup.com>,not
scikit-learn/sklearn/manifold/_t_sne.py,4,License: BSD 3 clause (C) 2014,not
scikit-learn/sklearn/manifold/_t_sne.py,6,This is the exact and Barnes-Hut t-SNE implementation. There are other,not
scikit-learn/sklearn/manifold/_t_sne.py,7,modifications of the algorithm:,not
scikit-learn/sklearn/manifold/_t_sne.py,8,* Fast Optimization for t-SNE:,not
scikit-learn/sklearn/manifold/_t_sne.py,9,https://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/papers/vandermaaten.pdf,not
scikit-learn/sklearn/manifold/_t_sne.py,25,mypy error: Module 'sklearn.manifold' has no attribute '_utils',not
scikit-learn/sklearn/manifold/_t_sne.py,26,type: ignore,not
scikit-learn/sklearn/manifold/_t_sne.py,27,mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne',not
scikit-learn/sklearn/manifold/_t_sne.py,28,type: ignore,not
scikit-learn/sklearn/manifold/_t_sne.py,55,Compute conditional probabilities such that they approximately match,not
scikit-learn/sklearn/manifold/_t_sne.py,56,the desired perplexity,not
scikit-learn/sklearn/manifold/_t_sne.py,92,Compute conditional probabilities such that they approximately match,not
scikit-learn/sklearn/manifold/_t_sne.py,93,the desired perplexity,not
scikit-learn/sklearn/manifold/_t_sne.py,103,Symmetrize the joint probability distribution using sparse operations,not
scikit-learn/sklearn/manifold/_t_sne.py,109,Normalize the joint probability distribution,not
scikit-learn/sklearn/manifold/_t_sne.py,162,Q is a heavy-tailed distribution: Student's t-distribution,not
scikit-learn/sklearn/manifold/_t_sne.py,169,"Optimization trick below: np.dot(x, y) is faster than",not
scikit-learn/sklearn/manifold/_t_sne.py,170,np.sum(x * y) because it calls BLAS,not
scikit-learn/sklearn/manifold/_t_sne.py,172,Objective: C (Kullback-Leibler divergence of P and Q),not
scikit-learn/sklearn/manifold/_t_sne.py,179,Gradient: dC/dY,not
scikit-learn/sklearn/manifold/_t_sne.py,180,pdist always returns double precision distances. Thus we need to take,not
scikit-learn/sklearn/manifold/_t_sne.py,357,only compute the error when needed,not
scikit-learn/sklearn/manifold/_t_sne.py,453,we set the diagonal to np.inf to exclude the points themselves from,not
scikit-learn/sklearn/manifold/_t_sne.py,454,their own neighborhood,not
scikit-learn/sklearn/manifold/_t_sne.py,457,`ind_X[i]` is the index of sorted distances between i and other samples,not
scikit-learn/sklearn/manifold/_t_sne.py,461,"We build an inverted index of neighbors in the input space: For sample i,",not
scikit-learn/sklearn/manifold/_t_sne.py,462,we define `inverted_index[i]` as the inverted index of sorted distances:,not
scikit-learn/sklearn/manifold/_t_sne.py,463,"inverted_index[i][ind_X[i]] = np.arange(1, n_sample + 1)",not
scikit-learn/sklearn/manifold/_t_sne.py,634,Control the number of exploration iterations with early_exaggeration on,not
scikit-learn/sklearn/manifold/_t_sne.py,637,Control the number of iterations between progress checks,not
scikit-learn/sklearn/manifold/_t_sne.py,709,"Retrieve the distance matrix, either using the precomputed one or",not
scikit-learn/sklearn/manifold/_t_sne.py,710,computing it.,not
scikit-learn/sklearn/manifold/_t_sne.py,728,compute the joint probability distribution for the input space,not
scikit-learn/sklearn/manifold/_t_sne.py,736,Compute the number of nearest neighbors to find.,not
scikit-learn/sklearn/manifold/_t_sne.py,737,LvdM uses 3 * perplexity as the number of neighbors.,not
scikit-learn/sklearn/manifold/_t_sne.py,738,In the event that we have very small # of points,not
scikit-learn/sklearn/manifold/_t_sne.py,739,set the neighbors to n - 1.,not
scikit-learn/sklearn/manifold/_t_sne.py,746,Find the nearest neighbors for every point,not
scikit-learn/sklearn/manifold/_t_sne.py,765,Free the memory used by the ball_tree,not
scikit-learn/sklearn/manifold/_t_sne.py,769,knn return the euclidean distance but we need it squared,not
scikit-learn/sklearn/manifold/_t_sne.py,770,to be consistent with the 'exact' method. Note that the,not
scikit-learn/sklearn/manifold/_t_sne.py,771,the method was derived using the euclidean method as in the,not
scikit-learn/sklearn/manifold/_t_sne.py,772,input space. Not sure of the implication of using a different,not
scikit-learn/sklearn/manifold/_t_sne.py,773,metric.,not
scikit-learn/sklearn/manifold/_t_sne.py,776,compute the joint probability distribution for the input space,not
scikit-learn/sklearn/manifold/_t_sne.py,787,The embedding is initialized with iid samples from Gaussians with,not
scikit-learn/sklearn/manifold/_t_sne.py,788,standard deviation 1e-4.,not
scikit-learn/sklearn/manifold/_t_sne.py,795,Degrees of freedom of the Student's t-distribution. The suggestion,not
scikit-learn/sklearn/manifold/_t_sne.py,796,degrees_of_freedom = n_components - 1 comes from,not
scikit-learn/sklearn/manifold/_t_sne.py,797,"""Learning a Parametric Embedding by Preserving Local Structure""",not
scikit-learn/sklearn/manifold/_t_sne.py,798,"Laurens van der Maaten, 2009.",not
scikit-learn/sklearn/manifold/_t_sne.py,809,t-SNE minimizes the Kullback-Leiber divergence of the Gaussians P,not
scikit-learn/sklearn/manifold/_t_sne.py,810,and the Student's t-distributions Q. The optimization algorithm that,not
scikit-learn/sklearn/manifold/_t_sne.py,811,we use is batch gradient descent with two stages:,not
scikit-learn/sklearn/manifold/_t_sne.py,812,* initial optimization with early exaggeration and momentum at 0.5,not
scikit-learn/sklearn/manifold/_t_sne.py,813,* final optimization with momentum at 0.8,not
scikit-learn/sklearn/manifold/_t_sne.py,831,Repeat verbose argument for _kl_divergence_bh,not
scikit-learn/sklearn/manifold/_t_sne.py,833,Get the number of threads for gradient computation here to,not
scikit-learn/sklearn/manifold/_t_sne.py,834,avoid recomputing it at each iteration.,not
scikit-learn/sklearn/manifold/_t_sne.py,839,Learning schedule (part 1): do 250 iteration with lower momentum but,not
scikit-learn/sklearn/manifold/_t_sne.py,840,higher learning rate controlled via the early exaggeration parameter,not
scikit-learn/sklearn/manifold/_t_sne.py,848,Learning schedule (part 2): disable early exaggeration and finish,not
scikit-learn/sklearn/manifold/_t_sne.py,849,optimization with a higher momentum at 0.8,not
scikit-learn/sklearn/manifold/_t_sne.py,860,Save the final number of iterations,not
scikit-learn/sklearn/manifold/tests/test_mds.py,9,"test metric smacof using the data of ""Modern Multidimensional Scaling"",",not
scikit-learn/sklearn/manifold/tests/test_mds.py,10,"Borg & Groenen, p 154",not
scikit-learn/sklearn/manifold/tests/test_mds.py,28,Not symmetric similarity matrix:,not
scikit-learn/sklearn/manifold/tests/test_mds.py,37,Not squared similarity matrix:,not
scikit-learn/sklearn/manifold/tests/test_mds.py,45,init not None and not correct format:,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,24,mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne',not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,25,type: ignore,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,45,Test stopping conditions of gradient descent.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,57,Gradient norm,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,73,Maximum number of iterations without improvement,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,89,Maximum number of iterations,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,107,Test if the binary search finds Gaussians with desired perplexity.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,120,Binary perplexity search approximation.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,121,Should be approximately equal to the slow method when we use,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,122,all points as neighbors.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,130,Test that when we use all the neighbors the results are identical,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,144,Test that the highest P_ij are the same when fewer neighbors are used,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,147,check the top 10 * k entries out of k * k entries,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,162,Binary perplexity search should be stable.,SATD
scikit-learn/sklearn/manifold/tests/test_t_sne.py,163,The binary_search_perplexity had a bug wherein the P array,SATD
scikit-learn/sklearn/manifold/tests/test_t_sne.py,164,"was uninitialized, leading to sporadically failing tests.",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,181,Convert the sparse matrix to a dense one for testing,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,192,Test gradient of Kullback-Leibler divergence.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,219,Test trustworthiness score.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,222,Affine transformation,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,226,Randomly shuffled,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,232,Completely different,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,241,Nearest neighbors should be preserved approximately.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,268,X can be a sparse matrix.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,281,Nearest neighbors should be preserved approximately.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,295,Test trustworthiness with a metric different from 'euclidean' and,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,296,'precomputed',not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,305,Early exaggeration factor must be >= 1.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,312,Number of gradient descent iterations must be at least 200.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,340,Perplexity should be less than 50,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,370,Computed distance matrices must be positive.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,381,"'init' must be 'pca', 'random', or numpy array.",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,389,Initialize TSNE with ndarray and test fit,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,396,Initialize TSNE with ndarray and metric 'precomputed',not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,397,Make sure no FutureWarning is thrown from _fit,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,403,'metric' must be valid.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,414,'nethod' must be 'barnes_hut' or 'exact',not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,421,check the angle parameter range,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,430,Precomputed distance matrices must be square matrices.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,439,barnes_hut method should only be used with n_components <= 3,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,446,check that the ``early_exaggeration`` parameter has an effect,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,465,check that the ``n_iter`` parameter has an effect,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,481,Test the tree with only a single set of children.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,482,,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,483,These tests & answers have been checked against the reference,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,484,implementation by LvdM.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,496,Four points tests the tree with multiple levels of children.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,497,,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,498,These tests & answers have been checked against the reference,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,499,implementation by LvdM.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,518,Test the kwargs option skip_num_points.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,519,,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,520,Skip num points should make it such that the Barnes_hut gradient,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,521,is not calculated for indices below skip_num_point.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,522,Aside from skip_num_points=2 and the first two gradient rows,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,523,"being set to zero, these data points are the same as in",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,524,test_answer_gradient_four_points(),not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,565,Verbose options write to stdout.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,587,t-SNE should allow metrics that cannot be squared (issue #3526).,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,595,t-SNE should allow reduction to one component (issue #4154).,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,606,Ensure 64bit arrays are handled correctly.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,616,"tsne cython code is only single precision, so the output will",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,617,"always be single precision, irrespectively of the input dtype",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,623,Ensure kl_divergence_ is computed at last iteration,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,624,"even though n_iter % n_iter_check != 0, i.e. 1003 % 50 != 0",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,636,When Barnes-Hut's angle=0 this corresponds to the exact method.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,669,Use a dummy negative n_iter_without_progress and check output on stdout,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,687,The output needs to contain the value of n_iter_without_progress,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,693,Make sure that the parameter min_grad_norm is used correctly,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,711,extract the gradient norm from the verbose output,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,714,When the computation is Finished just an old gradient norm value,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,715,is repeated that we do not need to store,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,725,Compute how often the gradient norm is smaller than min_grad_norm,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,730,"The gradient norm can be smaller than min_grad_norm at most once,",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,731,because in the moment it becomes smaller the optimization stops,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,736,Ensures that the accessible kl_divergence matches the computed value,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,752,The output needs to contain the accessible kl_divergence as the error at,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,753,the last iteration,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,787,"If the test fails a first time, re-run with init=Y to see if",not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,788,this was caused by a bad initialization. Note that this will,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,789,also run an early_exaggeration step.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,797,Ensure that the resulting embedding leads to approximately,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,798,uniformly spaced points: the distance to the closest neighbors,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,799,should be non-zero and approximately constant.,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,812,check that the ``barnes_hut`` method match the exact one when,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,813,``angle = 0`` and ``perplexity > n_samples / 3``,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,823,Kill the early_exaggeration,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,833,check that the bh gradient with different num_threads gives the same,not
scikit-learn/sklearn/manifold/tests/test_t_sne.py,834,results,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,23,"non centered, sparse centers to check the",not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,53,Connect all elements within the group at least once via an,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,54,arbitrary path that spans the group.,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,58,Add some more random connections within the group,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,65,Build a symmetric affinity matrix,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,76,We should retrieve the same component mask by starting by both ends,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,77,of the group,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,84,Test spectral embedding with two components,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,88,first component,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,91,second component,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,95,Test of internal _graph_connected_component before connection,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,103,connection,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,115,Some numpy versions are touchy with types,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,118,thresholding on the first components using 0.,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,126,Test spectral embedding with precomputed kernel,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,141,Test precomputed graph filtering when containing too many neighbors,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,160,Test spectral embedding with callable affinity,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,179,TODO: Remove when pyamg does replaces sp.rand call with np.random.rand,SATD
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,180,https://github.com/scikit-learn/scikit-learn/issues/15913,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,184,Test spectral embedding with amg solver,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,197,same with special case in which amg is not actually used,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,198,regression test for #10715,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,199,affinity between nodes,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,213,TODO: Remove filterwarnings when pyamg does replaces sp.rand call with,SATD
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,214,np.random.rand:,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,215,https://github.com/scikit-learn/scikit-learn/issues/15913,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,219,Non-regression test for amg solver failure (issue #13393 on github),not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,231,Check that the learned embedding is stable w.r.t. random solver init:,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,243,Test using pipeline to do spectral clustering,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,262,Test that SpectralClustering fails with an unknown eigensolver,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,271,Test that SpectralClustering fails with an unknown affinity type,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,279,Test that graph connectivity test works as expected,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,299,Test that Spectral Embedding is deterministic,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,309,Test that spectral_embedding is also processing unnormalized laplacian,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,310,correctly,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,320,Verify using manual computation with dense eigh,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,331,Test that the first eigenvector of spectral_embedding,not
scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py,332,is constant and that the second is not (for a connected graph),not
scikit-learn/sklearn/manifold/tests/test_isomap.py,19,Isomap should preserve distances when all neighbors are used,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,24,"grid of equidistant points in 2D, n_components = n_dim",not
scikit-learn/sklearn/manifold/tests/test_isomap.py,27,distances from each point to all others,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,45,"Same setup as in test_isomap_simple_grid, with an added dimension",not
scikit-learn/sklearn/manifold/tests/test_isomap.py,50,"grid of equidistant points in 2D, n_components = n_dim",not
scikit-learn/sklearn/manifold/tests/test_isomap.py,53,add noise in a third dimension,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,58,compute input kernel,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,72,compute output kernel,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,79,make sure error agrees,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,90,Create S-curve dataset,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,93,Compute isomap embedding,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,97,Re-embed a noisy version of the points,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,102,Make sure the rms error on re-embedding is comparable to noise_scale,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,107,check that Isomap works fine as a transformer in a Pipeline,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,108,only checks that no error is raised.,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,109,TODO check that it actually does something useful,SATD
scikit-learn/sklearn/manifold/tests/test_isomap.py,119,Test chaining NearestNeighborsTransformer and Isomap with,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,120,neighbors_algorithm='precomputed',not
scikit-learn/sklearn/manifold/tests/test_isomap.py,127,compare the chained version and the compact version,not
scikit-learn/sklearn/manifold/tests/test_isomap.py,145,"Test that the metric parameters work correctly, and default to euclidean",not
scikit-learn/sklearn/manifold/tests/test_isomap.py,149,"metric, p, is_euclidean",not
scikit-learn/sklearn/manifold/tests/test_isomap.py,170,regression test for bug reported in #6062,SATD
scikit-learn/sklearn/manifold/tests/test_isomap.py,182,Should not error,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,16,----------------------------------------------------------------------,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,17,Test utility routines,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,29,check that columns sum to one,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,35,----------------------------------------------------------------------,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,36,Test LLE by computing the reconstruction error on some manifolds.,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,39,"note: ARPACK is numerically unstable, so this test will fail for",not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,40,some random seeds.  We choose 2 because the tests pass.,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,43,"grid of equidistant points in 2D, n_components = n_dim",not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,67,re-embed a noisy version of X using the transform method,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,75,similar test on a slightly more complex manifold,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,103,Test the error raised when parameter passed to lle is invalid,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,117,check that LocallyLinearEmbedding works fine as a Pipeline,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,118,only checks that no error is raised.,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,119,TODO check that it actually does something useful,SATD
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,129,Test the error raised when the weight matrix is singular,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,139,regression test for #6033,not
scikit-learn/sklearn/manifold/tests/test_locally_linear.py,146,this previously raised a TypeError,not
scikit-learn/sklearn/model_selection/_search.py,6,"Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,",not
scikit-learn/sklearn/model_selection/_search.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/model_selection/_search.py,8,Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/model_selection/_search.py,9,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/model_selection/_search.py,10,Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/sklearn/model_selection/_search.py,11,License: BSD 3 clause,not
scikit-learn/sklearn/model_selection/_search.py,97,wrap dictionary in a singleton list to support either dict,not
scikit-learn/sklearn/model_selection/_search.py,98,or list of dicts,not
scikit-learn/sklearn/model_selection/_search.py,101,check if all entries are dictionaries of lists,not
scikit-learn/sklearn/model_selection/_search.py,124,"Always sort the keys of a dictionary, for reproducibility",not
scikit-learn/sklearn/model_selection/_search.py,136,Product function that can handle iterables (np.product can't).,not
scikit-learn/sklearn/model_selection/_search.py,154,This is used to make discrete sampling without replacement memory,not
scikit-learn/sklearn/model_selection/_search.py,155,efficient.,not
scikit-learn/sklearn/model_selection/_search.py,157,XXX: could memoize information used here,SATD
scikit-learn/sklearn/model_selection/_search.py,165,Reverse so most frequent cycling parameter comes first,not
scikit-learn/sklearn/model_selection/_search.py,171,Try the next grid,not
scikit-learn/sklearn/model_selection/_search.py,245,wrap dictionary in a singleton list to support either dict,not
scikit-learn/sklearn/model_selection/_search.py,246,or list of dicts,not
scikit-learn/sklearn/model_selection/_search.py,264,check if all distributions are given as lists,not
scikit-learn/sklearn/model_selection/_search.py,265,in this case we want to sample without replacement,not
scikit-learn/sklearn/model_selection/_search.py,272,look up sampled parameter settings in parameter grid,not
scikit-learn/sklearn/model_selection/_search.py,291,"Always sort the keys of a dictionary, for reproducibility",not
scikit-learn/sklearn/model_selection/_search.py,306,FIXME Remove fit_grid_point in 0.25,SATD
scikit-learn/sklearn/model_selection/_search.py,367,NOTE we are not using the return value as the scorer by itself should be,not
scikit-learn/sklearn/model_selection/_search.py,368,validated before. We use check_scoring only to reject multimetric scorer,not
scikit-learn/sklearn/model_selection/_search.py,428,allows cross-validation to see 'precomputed' metrics,not
scikit-learn/sklearn/model_selection/_search.py,576,For consistency with other estimators we raise a AttributeError so,not
scikit-learn/sklearn/model_selection/_search.py,577,that hasattr() fails if the search estimator isn't fitted.,not
scikit-learn/sklearn/model_selection/_search.py,660,This will work for both dict / list (tuple),not
scikit-learn/sklearn/model_selection/_search.py,738,"For multi-metric evaluation, store the best_index_, best_params_ and",not
scikit-learn/sklearn/model_selection/_search.py,739,best_score_ iff refit is one of the scorer names,not
scikit-learn/sklearn/model_selection/_search.py,740,"In single metric evaluation, refit_metric is ""score""",not
scikit-learn/sklearn/model_selection/_search.py,742,"If callable, refit is expected to return the index of the best",not
scikit-learn/sklearn/model_selection/_search.py,743,parameter set.,not
scikit-learn/sklearn/model_selection/_search.py,759,we clone again after setting params in case some,not
scikit-learn/sklearn/model_selection/_search.py,760,of the params are estimators as well.,not
scikit-learn/sklearn/model_selection/_search.py,771,Store the only scorer not as a dict for single metric evaluation,not
scikit-learn/sklearn/model_selection/_search.py,782,"if one choose to see train score, ""out"" will contain train score info",not
scikit-learn/sklearn/model_selection/_search.py,790,test_score_dicts and train_score dicts are lists of dictionaries and,not
scikit-learn/sklearn/model_selection/_search.py,791,we make them into dict of lists,not
scikit-learn/sklearn/model_selection/_search.py,800,"When iterated first by splits, then by parameters",not
scikit-learn/sklearn/model_selection/_search.py,801,We want `array` to have `n_candidates` rows and `n_splits` cols.,not
scikit-learn/sklearn/model_selection/_search.py,806,Uses closure to alter the results,not
scikit-learn/sklearn/model_selection/_search.py,812,Weighted std is not directly available in numpy,not
scikit-learn/sklearn/model_selection/_search.py,824,Use one MaskedArray and mask all the places where the param is not,not
scikit-learn/sklearn/model_selection/_search.py,825,applicable for that candidate. Use defaultdict as each candidate may,not
scikit-learn/sklearn/model_selection/_search.py,826,not contain all the params,not
scikit-learn/sklearn/model_selection/_search.py,833,An all masked empty array gets created for the key,not
scikit-learn/sklearn/model_selection/_search.py,834,"`""param_%s"" % name` at the first occurrence of `name`.",not
scikit-learn/sklearn/model_selection/_search.py,835,Setting the value at an index also unmasks that index,not
scikit-learn/sklearn/model_selection/_search.py,839,Store a list of param dicts at the key 'params',not
scikit-learn/sklearn/model_selection/_search.py,842,NOTE test_sample counts (weights) remain the same for all candidates,not
scikit-learn/sklearn/model_selection/_search.py,856,Computed the (weighted) mean and std for test scores alone,not
scikit-learn/sklearn/model_selection/_validation.py,6,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/model_selection/_validation.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/model_selection/_validation.py,8,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/model_selection/_validation.py,9,Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/sklearn/model_selection/_validation.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/model_selection/_validation.py,238,We clone the estimator to make sure that all the folds are,not
scikit-learn/sklearn/model_selection/_validation.py,239,"independent, and that it is pickle-able.",not
scikit-learn/sklearn/model_selection/_validation.py,398,To ensure multimetric format is not supported,not
scikit-learn/sklearn/model_selection/_validation.py,507,Adjust length of sample weights,not
scikit-learn/sklearn/model_selection/_validation.py,513,clone after setting parameters in case any parameters,not
scikit-learn/sklearn/model_selection/_validation.py,514,are estimators (like pipeline steps),not
scikit-learn/sklearn/model_selection/_validation.py,515,because pipeline doesn't clone steps in fit,not
scikit-learn/sklearn/model_selection/_validation.py,534,Note fit time as time until error,not
scikit-learn/sklearn/model_selection/_validation.py,602,will cache method calls if needed. scorer() returns a dict,not
scikit-learn/sklearn/model_selection/_validation.py,615,e.g. unwrap memmapped scalars,not
scikit-learn/sklearn/model_selection/_validation.py,620,scalar,not
scikit-learn/sklearn/model_selection/_validation.py,623,e.g. unwrap memmapped scalars,not
scikit-learn/sklearn/model_selection/_validation.py,752,"If classification methods produce multiple columns of output,",not
scikit-learn/sklearn/model_selection/_validation.py,753,we need to manually encode classes to ensure consistent column ordering.,not
scikit-learn/sklearn/model_selection/_validation.py,767,We clone the estimator to make sure that all the folds are,not
scikit-learn/sklearn/model_selection/_validation.py,768,"independent, and that it is pickle-able.",not
scikit-learn/sklearn/model_selection/_validation.py,775,Concatenate the predictions,not
scikit-learn/sklearn/model_selection/_validation.py,789,`predictions` is a list of method outputs from each fold.,not
scikit-learn/sklearn/model_selection/_validation.py,790,"If each of those is also a list, then treat this as a",not
scikit-learn/sklearn/model_selection/_validation.py,791,multioutput-multiclass task. We need to separately concatenate,not
scikit-learn/sklearn/model_selection/_validation.py,792,the method outputs for each label into an `n_labels` long list.,not
scikit-learn/sklearn/model_selection/_validation.py,852,Adjust length of sample weights,not
scikit-learn/sklearn/model_selection/_validation.py,876,A 2D y array should be a binary label indicator matrix,not
scikit-learn/sklearn/model_selection/_validation.py,909,This handles the case when the shape of predictions,not
scikit-learn/sklearn/model_selection/_validation.py,910,does not match the number of classes used to train,not
scikit-learn/sklearn/model_selection/_validation.py,911,it with. This case is found when sklearn.svm.SVC is,not
scikit-learn/sklearn/model_selection/_validation.py,912,set to `decision_function_shape='ovo'`.,not
scikit-learn/sklearn/model_selection/_validation.py,920,"In this special case, `predictions` contains a 1D array.",not
scikit-learn/sklearn/model_selection/_validation.py,1069,We clone the estimator to make sure that all the folds are,not
scikit-learn/sklearn/model_selection/_validation.py,1070,"independent, and that it is pickle-able.",not
scikit-learn/sklearn/model_selection/_validation.py,1246,Store it as list as we will be iterating over the list multiple times,not
scikit-learn/sklearn/model_selection/_validation.py,1252,"Because the lengths of folds can be significantly different, it is",not
scikit-learn/sklearn/model_selection/_validation.py,1253,not guaranteed that we use all of the available training data when we,not
scikit-learn/sklearn/model_selection/_validation.py,1254,use the first 'n_max_training_samples' samples.,not
scikit-learn/sklearn/model_selection/_validation.py,1501,NOTE do not change order of iteration to allow one time cv splitters,not
scikit-learn/sklearn/model_selection/_split.py,6,"Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,",not
scikit-learn/sklearn/model_selection/_split.py,7,"Gael Varoquaux <gael.varoquaux@normalesup.org>,",not
scikit-learn/sklearn/model_selection/_split.py,8,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/model_selection/_split.py,9,Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/sklearn/model_selection/_split.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/model_selection/_split.py,85,Since subclasses must implement either _iter_test_masks or,not
scikit-learn/sklearn/model_selection/_split.py,86,"_iter_test_indices, neither can be abstract.",not
scikit-learn/sklearn/model_selection/_split.py,291,None is the default,not
scikit-learn/sklearn/model_selection/_split.py,292,TODO 0.24: raise a ValueError instead of a warning,SATD
scikit-learn/sklearn/model_selection/_split.py,518,Weight groups by their number of occurrences,not
scikit-learn/sklearn/model_selection/_split.py,521,Distribute the most frequent groups first,not
scikit-learn/sklearn/model_selection/_split.py,525,Total weight of each fold,not
scikit-learn/sklearn/model_selection/_split.py,528,Mapping from group index to fold index,not
scikit-learn/sklearn/model_selection/_split.py,531,Distribute samples by adding the largest weight to the lightest fold,not
scikit-learn/sklearn/model_selection/_split.py,656,y_inv encodes y according to lexicographic order. We invert y_idx to,not
scikit-learn/sklearn/model_selection/_split.py,657,map the classes so that they are encoded by order of appearance:,not
scikit-learn/sklearn/model_selection/_split.py,658,"0 represents the first label appearing in y, 1 the second, etc.",not
scikit-learn/sklearn/model_selection/_split.py,674,"Determine the optimal number of samples from each class in each fold,",not
scikit-learn/sklearn/model_selection/_split.py,675,using round robin over the sorted y. (This can be done direct from,not
scikit-learn/sklearn/model_selection/_split.py,676,"counts, but that code is unreadable.)",not
scikit-learn/sklearn/model_selection/_split.py,682,To maintain the data order dependencies as best as possible within,not
scikit-learn/sklearn/model_selection/_split.py,683,"the stratification constraint, we assign samples from each class in",not
scikit-learn/sklearn/model_selection/_split.py,684,blocks (and then mess that up when shuffle=True).,not
scikit-learn/sklearn/model_selection/_split.py,687,since the kth column of allocation stores the number of samples,not
scikit-learn/sklearn/model_selection/_split.py,688,"of class k in each test set, this generates blocks of fold",not
scikit-learn/sklearn/model_selection/_split.py,689,indices corresponding to the allocation for class k.,not
scikit-learn/sklearn/model_selection/_split.py,867,Make sure we have enough samples for the given split parameters,not
scikit-learn/sklearn/model_selection/_split.py,936,We make a copy of groups to avoid side-effects during iteration,not
scikit-learn/sklearn/model_selection/_split.py,1499,random partition,not
scikit-learn/sklearn/model_selection/_split.py,1587,these are the indices of classes in the partition,not
scikit-learn/sklearn/model_selection/_split.py,1588,invert them into data indices,not
scikit-learn/sklearn/model_selection/_split.py,1705,"for multi-label y, map each distinct row to a string repr",not
scikit-learn/sklearn/model_selection/_split.py,1706,using join because str(row) uses an ellipsis if len(row) > 1000,not
scikit-learn/sklearn/model_selection/_split.py,1728,Find the sorted list of instances for each class:,not
scikit-learn/sklearn/model_selection/_split.py,1729,"(np.unique above performs a sort, so code is O(n logn) already)",not
scikit-learn/sklearn/model_selection/_split.py,1736,"if there are ties in the class-counts, we want",not
scikit-learn/sklearn/model_selection/_split.py,1737,to make sure to break them anew in each iteration,not
scikit-learn/sklearn/model_selection/_split.py,2072,New style cv objects are passed without any modification,not
scikit-learn/sklearn/model_selection/_split.py,2204,Tell nose that train_test_split is not a test.,not
scikit-learn/sklearn/model_selection/_split.py,2205,(Needed for external libraries that may use nose.),not
scikit-learn/sklearn/model_selection/_split.py,2206,Use setattr to avoid mypy errors when monkeypatching.,not
scikit-learn/sklearn/model_selection/_split.py,2211,XXX This is copied from BaseEstimator's get_params,SATD
scikit-learn/sklearn/model_selection/_split.py,2214,"Ignore varargs, kw and default values and pop self",not
scikit-learn/sklearn/model_selection/_split.py,2216,Consider the constructor parameters excluding 'self',not
scikit-learn/sklearn/model_selection/_split.py,2225,We need deprecation warnings to always be on in order to,not
scikit-learn/sklearn/model_selection/_split.py,2226,catch deprecated param values.,not
scikit-learn/sklearn/model_selection/_split.py,2227,This is set in utils/__init__.py but it gets overwritten,not
scikit-learn/sklearn/model_selection/_split.py,2228,when running under python3 somehow.,not
scikit-learn/sklearn/model_selection/_split.py,2236,"if the parameter is deprecated, don't show it",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,103,"training score becomes worse (2 -> 1), test error better (0 -> 1)",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,227,"XXX: use 2D array, since 1D X is being detected as a single sample in",SATD
scikit-learn/sklearn/model_selection/tests/test_validation.py,228,check_consistent_length,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,232,"The number of samples per class needs to be > n_splits,",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,233,for StratifiedKFold(n_splits=3),not
scikit-learn/sklearn/model_selection/tests/test_validation.py,243,Smoke test,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,247,test with multioutput y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,255,test with multioutput y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,259,test with X and y as list,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,269,test with 3d X and,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,280,regression test for #12154: cv='warn' with n_jobs>1 trigger a copy of,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,281,the parameters leading to a failure in check_cv due to cv is 'warn',not
scikit-learn/sklearn/model_selection/tests/test_validation.py,282,instead of cv == 'warn'.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,293,Test the errors,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,296,List/tuple of callables should raise a message advising users to use,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,297,dict of names to callables mapping,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,306,So should empty lists/tuples,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,310,So should duplicated entries,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,315,Nested Lists should raise a generic error message,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,323,Empty dict should raise invalid scoring error,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,327,And so should any other invalid entry,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,333,Multiclass Scorers that return multiple values are not supported yet,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,347,Multiclass Scorers that return multiple values are not supported yet,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,360,Compute train and test mse/r2 scores,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,363,Regression,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,367,Classification,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,372,It's okay to evaluate regression metrics on classification too,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,404,Test single metric evaluation when scoring is string or singleton list,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,406,Single metric passed as a string,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,422,Single metric passed as a list,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,424,It must be True by default - deprecated,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,436,Test return_estimator option,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,446,Test multimetric evaluation when scoring is a list / dict,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,461,return_train_score must be True by default - deprecated,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,480,Make sure all the arrays are of np.ndarray type,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,487,Ensure all the times are within sane limits,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,495,Check if ValueError (when groups is None) propagates to cross_val_score,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,496,and cross_val_predict,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,497,And also check if groups is correctly passed to the cv object,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,515,check cross_val_score doesn't destroy pandas dataframe,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,523,"X dataframe, y series",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,524,3 fold cross val is used so we need atleast 3 samples per class,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,533,test that cross_val_score works with boolean masks,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,552,test for svm with precomputed kernel,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,562,test with callable,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,567,Error raised for non-square X,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,571,test error is raised when the precomputed kernel is not array-like,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,572,or sparse,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,591,Function to test that the values are passed correctly to the,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,592,classifier arguments for non-array type,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,621,Test that score function is called only 3 times (for cv=3),not
scikit-learn/sklearn/model_selection/tests/test_validation.py,636,Default score (should be the accuracy score),not
scikit-learn/sklearn/model_selection/tests/test_validation.py,640,Correct classification score (aka. zero / one score) - should be the,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,641,same as the default estimator score,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,646,F1 score (class are balanced so f1_score should be equal to zero/one,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,647,score,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,658,Default score of the Ridge regression estimator,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,662,R2 score (aka. determination coefficient) - should be the,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,663,same as the default estimator score,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,667,"Mean squared error; this is a loss function, so ""scores"" are negative",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,673,Explained variance,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,698,check that we obtain the same results with a sparse representation,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,708,test with custom scoring object,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,719,set random y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,730,Check that permutation_test_score allows input data with NaNs,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,742,Check that cross_val_score allows input data with NaNs,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,776,Naive loop (should be same as cross_val_predict):,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,832,This specifically tests imbalanced splits for binary,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,833,classification with decision_function. This is only,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,834,applicable to classifiers that can be fit on a single,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,835,class.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,901,3 fold cv is used --> atleast 3 samples per class,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,902,Smoke test,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,906,test with multioutput y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,913,test with multioutput y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,917,test with X and y as list,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,925,test with X and y as list and non empty method,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,933,test with 3d X and,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,942,python3.7 deprecation warnings in pandas via matplotlib :-/,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,944,check cross_val_score doesn't destroy pandas dataframe,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,952,"X dataframe, y series",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,964,Change the first sample to a new class,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,970,sanity check for further assertions,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,979,ensure that cross_val_predict works when y is None,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1025,Cannot use assert_array_almost_equal for fit and score times because,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1026,the values are hardware-dependant,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1030,Test a custom cv splitter that can iterate only once,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1080,The mockup does not have partial_fit(),not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1185,Following test case was designed this way to verify the code,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1186,changes made in pull request: #7506.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1192,Splits on these groups fail without shuffle as the first iteration,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1193,of the learning curve doesn't contain label 4 in the training set.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1258,"The OneTimeSplitter is a non-re-entrant cv splitter. Unless, the",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1259,"`split` is called for each parameter, the following should produce",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1260,identical results for param setting 1 and param setting 2 as both have,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1261,the same C value.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1270,"For scores2, compare the 1st and 2nd parameter's scores",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1271,"(Since the C value for 1st two param setting is 0.1, they must be",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1272,consistent unless the train test folds differ between the param settings),not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1281,OneTimeSplitter is basically unshuffled KFold(n_splits=5). Sanity check.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1295,Check if the additional duplicate indices are caught,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1300,check that cross_val_predict gives same result for sparse and dense input,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1318,Generate expected outputs,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1328,Check actual outputs for several representations of y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1338,Generate expected outputs,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1353,Check actual outputs for several representations of y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1366,Create empty arrays of the correct size to hold outputs,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1382,Generate expected outputs,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1392,Decision function with <=2 classes,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1398,Check actual outputs for several representations of y,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1407,This test includes the decision_function with two classes.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1408,This is a special case: it has only one column of output.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1430,Regression test for issue #9639. Tests that cross_val_predict does not,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1431,check estimator methods (e.g. predict_proba) before fitting,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1452,"OVR does multilabel predictions, but only arrays of",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1453,binary indicator columns. The output of predict_proba,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1454,"is a 2D array with shape (n_samples, n_classes).",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1467,None of the current multioutput-multiclass estimators have,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1468,decision function methods. Create a mock decision function,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1469,to test the cross_val_predict function's handling of this case.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1479,The RandomForest allows multiple classes in each label.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1480,Output of predict_proba is a list of outputs of predict_proba,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1481,for each individual label.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1486,Put three classes in the first column,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1490,"Suppress ""RuntimeWarning: divide by zero encountered in log""",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1496,Test a multiclass problem where one class will be missing from,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1497,one of the CV training sets.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1504,Suppress warning about too few examples of a class,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1510,The RandomForest allows anything for the contents of the labels.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1511,Output of predict_proba is a list of outputs of predict_proba,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1512,for each individual label.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1513,"In this test, the first label has a class with a single example.",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1514,We'll have one CV fold where the training data don't include it.,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1521,"Suppress ""RuntimeWarning: divide by zero encountered in log""",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1534,To avoid 2 dimensional indexing,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1561,Test with n_splits=3,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1565,Runs a naive loop (should be same as cross_val_predict):,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1570,Test with n_splits=4,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1577,Testing unordered labels,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1588,Ensure a scalar score of memmap type is accepted,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1599,non-scalar should still fail,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1603,Best effort to release the mmap file handles before deleting the,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1604,backing file under Windows,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1616,check permutation_test_score doesn't destroy pandas dataframe,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1624,"X dataframe, y series",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1635,Create a failing classifier to deliberately fail,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1637,dummy X data,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1642,passing error score to trigger the warning message,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1644,check if the warning message type is as expected,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1647,"since we're using FailingClassfier, our error will be the following",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1649,the warning message we're expecting to see,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1657,note: handles more than '\n',not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1660,check traceback is included,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1665,"check if exception was raised, with default error_score='raise'",not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1670,check that functions upstream pass error_score param to _fit_and_score,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1689,FailingClassifier coverage,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1696,Test return_parameters option,not
scikit-learn/sklearn/model_selection/tests/test_validation.py,1724,test print without train score,not
scikit-learn/sklearn/model_selection/tests/test_search.py,70,noqa,not
scikit-learn/sklearn/model_selection/tests/test_search.py,76,"Neither of the following two estimators inherit from BaseEstimator,",not
scikit-learn/sklearn/model_selection/tests/test_search.py,77,to test hyperparameter search on user-defined classifiers.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,146,Test basic properties of ParameterGrid.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,159,loop to assert we can iterate over the grid multiple times,not
scikit-learn/sklearn/model_selection/tests/test_search.py,161,"tuple + chain transforms {""a"": 1, ""b"": 2} to (""a"", 1, ""b"", 2)",not
scikit-learn/sklearn/model_selection/tests/test_search.py,168,Special case: empty grid (useful to get default estimator settings),not
scikit-learn/sklearn/model_selection/tests/test_search.py,182,Test that the best estimator contains the right value for foo_param,not
scikit-learn/sklearn/model_selection/tests/test_search.py,185,make sure it selects the smallest parameter in case of ties,not
scikit-learn/sklearn/model_selection/tests/test_search.py,195,Smoke test the score etc:,not
scikit-learn/sklearn/model_selection/tests/test_search.py,201,Test exception handling on scoring,not
scikit-learn/sklearn/model_selection/tests/test_search.py,207,check that parameters that are estimators are cloned before fitting,not
scikit-learn/sklearn/model_selection/tests/test_search.py,219,check that we didn't modify the parameter grid that was passed,not
scikit-learn/sklearn/model_selection/tests/test_search.py,233,The CheckingClassifier generates an assertion error if,not
scikit-learn/sklearn/model_selection/tests/test_search.py,234,a parameter is missing or has length != len(X).,not
scikit-learn/sklearn/model_selection/tests/test_search.py,247,Test grid-search on classifier that has no score function.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,257,smoketest grid search,not
scikit-learn/sklearn/model_selection/tests/test_search.py,260,check that best params are equal,not
scikit-learn/sklearn/model_selection/tests/test_search.py,262,check that we can call score and that it gives the correct result,not
scikit-learn/sklearn/model_selection/tests/test_search.py,265,giving no scoring function raises an error,not
scikit-learn/sklearn/model_selection/tests/test_search.py,284,Check warning only occurs in situation where behavior changed:,not
scikit-learn/sklearn/model_selection/tests/test_search.py,285,estimator requires score method to compete with scoring parameter,not
scikit-learn/sklearn/model_selection/tests/test_search.py,291,ensure the test is sane,not
scikit-learn/sklearn/model_selection/tests/test_search.py,301,Check if ValueError (when groups is None) propagates to GridSearchCV,not
scikit-learn/sklearn/model_selection/tests/test_search.py,302,And also check if groups is correctly passed to the cv object,not
scikit-learn/sklearn/model_selection/tests/test_search.py,323,Should not raise an error,not
scikit-learn/sklearn/model_selection/tests/test_search.py,328,Test that classes_ property matches best_estimator_.classes_,not
scikit-learn/sklearn/model_selection/tests/test_search.py,338,Test that regressors do not have a classes_ attribute,not
scikit-learn/sklearn/model_selection/tests/test_search.py,343,Test that the grid searcher has no classes_ attribute before it's fit,not
scikit-learn/sklearn/model_selection/tests/test_search.py,347,Test that the grid searcher has no classes_ attribute without a refit,not
scikit-learn/sklearn/model_selection/tests/test_search.py,355,"Test search over a ""grid"" with only one point.",not
scikit-learn/sklearn/model_selection/tests/test_search.py,367,Test that GSCV can be used for model selection alone without refitting,not
scikit-learn/sklearn/model_selection/tests/test_search.py,378,Make sure the functions predict/transform etc raise meaningful,not
scikit-learn/sklearn/model_selection/tests/test_search.py,379,error messages,not
scikit-learn/sklearn/model_selection/tests/test_search.py,387,Test that an invalid refit param raises appropriate error messages,not
scikit-learn/sklearn/model_selection/tests/test_search.py,399,Test that grid search will capture errors on data with different length,not
scikit-learn/sklearn/model_selection/tests/test_search.py,422,Test that the best estimator contains the right value for foo_param,not
scikit-learn/sklearn/model_selection/tests/test_search.py,464,Test that grid search works with both dense and sparse matrices,not
scikit-learn/sklearn/model_selection/tests/test_search.py,502,Smoke test the score,not
scikit-learn/sklearn/model_selection/tests/test_search.py,503,"np.testing.assert_allclose(f1_score(cv.predict(X_[:180]), y[:180]),",not
scikit-learn/sklearn/model_selection/tests/test_search.py,504,"cv.score(X_[:180], y[:180]))",not
scikit-learn/sklearn/model_selection/tests/test_search.py,506,test loss where greater is worse,not
scikit-learn/sklearn/model_selection/tests/test_search.py,520,Test that grid search works when the input features are given in the,not
scikit-learn/sklearn/model_selection/tests/test_search.py,521,form of a precomputed kernel matrix,not
scikit-learn/sklearn/model_selection/tests/test_search.py,524,compute the training kernel matrix corresponding to the linear kernel,not
scikit-learn/sklearn/model_selection/tests/test_search.py,534,compute the test kernel matrix,not
scikit-learn/sklearn/model_selection/tests/test_search.py,542,test error is raised when the precomputed kernel is not array-like,not
scikit-learn/sklearn/model_selection/tests/test_search.py,543,or sparse,not
scikit-learn/sklearn/model_selection/tests/test_search.py,548,Test that grid search returns an error with a non-square precomputed,not
scikit-learn/sklearn/model_selection/tests/test_search.py,549,training kernel matrix,not
scikit-learn/sklearn/model_selection/tests/test_search.py,573,Regression test for bug in refitting,SATD
scikit-learn/sklearn/model_selection/tests/test_search.py,574,Simulates re-fitting a broken estimator; this used to break with,not
scikit-learn/sklearn/model_selection/tests/test_search.py,575,sparse SVMs.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,595,Fit a dummy clf with `refit=True` to get a list of keys in,not
scikit-learn/sklearn/model_selection/tests/test_search.py,596,clf.cv_results_.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,602,Ensure that `best_index_ != 0` for this dummy clf,not
scikit-learn/sklearn/model_selection/tests/test_search.py,605,Assert every key matches those in `cv_results`,not
scikit-learn/sklearn/model_selection/tests/test_search.py,618,Ensure `best_score_` is disabled when using `refit=callable`,not
scikit-learn/sklearn/model_selection/tests/test_search.py,686,Ensure `best_score_` is disabled when using `refit=callable`,not
scikit-learn/sklearn/model_selection/tests/test_search.py,691,Pass X as list in GridSearchCV,not
scikit-learn/sklearn/model_selection/tests/test_search.py,703,Pass X as list in GridSearchCV,not
scikit-learn/sklearn/model_selection/tests/test_search.py,715,Pass y as list in GridSearchCV,not
scikit-learn/sklearn/model_selection/tests/test_search.py,728,check cross_val_score doesn't destroy pandas dataframe,not
scikit-learn/sklearn/model_selection/tests/test_search.py,740,"X dataframe, y series",not
scikit-learn/sklearn/model_selection/tests/test_search.py,758,test grid-search with unsupervised estimator,not
scikit-learn/sklearn/model_selection/tests/test_search.py,762,Multi-metric evaluation unsupervised,not
scikit-learn/sklearn/model_selection/tests/test_search.py,768,Both ARI and FMS can find the right number :),not
scikit-learn/sklearn/model_selection/tests/test_search.py,771,Single metric evaluation unsupervised,not
scikit-learn/sklearn/model_selection/tests/test_search.py,777,"Now without a score, and without y",not
scikit-learn/sklearn/model_selection/tests/test_search.py,784,test grid-search with an estimator without predict.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,785,slight duplication of a test from KDE,not
scikit-learn/sklearn/model_selection/tests/test_search.py,799,test basic properties of param sampler,not
scikit-learn/sklearn/model_selection/tests/test_search.py,810,test that repeated calls yield identical parameters,not
scikit-learn/sklearn/model_selection/tests/test_search.py,824,Check if the search `cv_results`'s array are of correct types,not
scikit-learn/sklearn/model_selection/tests/test_search.py,841,Test the search.cv_results_ contains all the required results,not
scikit-learn/sklearn/model_selection/tests/test_search.py,848,0.24,not
scikit-learn/sklearn/model_selection/tests/test_search.py,876,Check if score and timing are reasonable,not
scikit-learn/sklearn/model_selection/tests/test_search.py,883,Check cv_results structure,not
scikit-learn/sklearn/model_selection/tests/test_search.py,886,Check masking,not
scikit-learn/sklearn/model_selection/tests/test_search.py,901,0.24,not
scikit-learn/sklearn/model_selection/tests/test_search.py,931,Check results structure,not
scikit-learn/sklearn/model_selection/tests/test_search.py,954,Test the IID parameter,not
scikit-learn/sklearn/model_selection/tests/test_search.py,955,noise-free simple 2d-data,not
scikit-learn/sklearn/model_selection/tests/test_search.py,958,split dataset into two folds that are not iid,not
scikit-learn/sklearn/model_selection/tests/test_search.py,959,"first one contains data of all 4 blobs, second only from two.",not
scikit-learn/sklearn/model_selection/tests/test_search.py,963,this leads to perfect classification on one fold and a score of 1/3 on,not
scikit-learn/sklearn/model_selection/tests/test_search.py,964,the other,not
scikit-learn/sklearn/model_selection/tests/test_search.py,965,"create ""cv"" for splits",not
scikit-learn/sklearn/model_selection/tests/test_search.py,988,scores are the same as above,not
scikit-learn/sklearn/model_selection/tests/test_search.py,991,Unweighted mean/std is used,not
scikit-learn/sklearn/model_selection/tests/test_search.py,995,"For the train scores, we do not take a weighted mean irrespective of",not
scikit-learn/sklearn/model_selection/tests/test_search.py,996,i.i.d. or not,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1001,0.24,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1003,Test the IID parameter,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1004,noise-free simple 2d-data,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1007,split dataset into two folds that are not iid,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1008,"first one contains data of all 4 blobs, second only from two.",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1012,this leads to perfect classification on one fold and a score of 1/3 on,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1013,the other,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1014,"create ""cv"" for splits",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1016,once with iid=True (default),not
scikit-learn/sklearn/model_selection/tests/test_search.py,1039,Test the first candidate,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1044,"for first split, 1/4 of dataset is in test, for second 3/4.",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1045,take weighted average and weighted std,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1056,"For the train scores, we do not take a weighted mean irrespective of",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1057,i.i.d. or not,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1061,once with iid=False,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1087,scores are the same as above,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1089,Unweighted mean/std is used,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1093,"For the train scores, we do not take a weighted mean irrespective of",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1094,i.i.d. or not,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1099,0.24,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1122,0.24,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1129,"Scipy 0.12's stats dists do not accept seed, hence we use param grid",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1136,"If True, for multi-metric pass refit='accuracy'",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1157,0.24,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1174,"Check if score and timing are reasonable, also checks if the keys",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1175,are present,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1180,"Compare the keys, other than time keys, among multi-metric and",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1181,single metric grid search results. np.testing.assert_equal performs a,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1182,deep nested comparison of the two cv_results dicts,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1196,search cannot predict/score without refit,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1210,The two C values are close enough to give similar models,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1211,which would result in a tie of their mean cv-scores,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1223,Check tie breaking strategy -,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1224,Check that there is a tie in the mean scores between,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1225,candidates 1 and 2 alone,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1234,'min' rank should be assigned to the tied candidates,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1264,NOTE The precision of time.time in windows is not high,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1265,enough for the fit/score times to be non-zero for trivial X and y,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1280,test that correct scores are used,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1289,Test scorer names,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1314,FIXME remove test_fit_grid_point as the function will be removed on 0.25,SATD
scikit-learn/sklearn/model_selection/tests/test_search.py,1332,Test the return values of fit_grid_point,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1337,Should raise an error upon multimetric scorer,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1344,FIXME remove test_fit_grid_point_deprecated as,SATD
scikit-learn/sklearn/model_selection/tests/test_search.py,1345,fit_grid_point will be removed on 0.25,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1360,Test that a fit search can be pickled,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1377,Test search with multi-output estimator,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1388,Test with grid search cv,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1403,Test with a randomized search,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1422,Test predict_proba when disabled on estimator.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1431,Test GridSearchCV with SimpleImputer,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1462,GridSearchCV with on_error != 'raise',not
scikit-learn/sklearn/model_selection/tests/test_search.py,1463,Ensures that a warning is raised and score reset where appropriate.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1469,refit=False because we only want to check that errors caused by fits,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1470,to individual folds will be caught and warnings raised instead. If,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1471,"refit was done, then an exception would be raised on refit and not",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1472,"caught by grid_search (expected behavior), and this would cause an",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1473,error in this test.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1479,Ensure that grid scores were set to zero as required for those fits,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1480,that are expected to fail.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1501,Check that succeeded estimators have lower ranks,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1503,Check that failed estimator has the highest rank,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1509,GridSearchCV with on_error == 'raise' raises the error,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1515,refit=False because we want to test the behaviour of the grid search part,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1519,FailingClassifier issues a ValueError so this is what we look for.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1524,raise warning if n_iter is bigger than total parameter space,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1537,degenerates to GridSearchCV if n_iter the same as grid_size,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1544,test sampling without replacement in a large grid,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1553,doesn't go into infinite loops,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1561,Make sure the predict_proba works when loss is specified,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1562,as one of the parameters in the param_grid.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1571,"When the estimator is not fitted, `predict_proba` is not available as the",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1572,loss is 'hinge'.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1578,Make sure `predict_proba` is not available when setting loss=['hinge'],not
scikit-learn/sklearn/model_selection/tests/test_search.py,1579,in param_grid,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1600,Check if a one time iterable is accepted as a cv parameter.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1617,Give generator as a cv parameter,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1640,Check if generators are supported as cv and,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1641,that the splits are consistent,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1645,OneTimeSplitter is a non-re-entrant cv where split can be called only,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1646,once if ``cv.split`` is called once per param setting in GridSearchCV.fit,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1647,the 2nd and 3rd parameter will not be evaluated as no train/test indices,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1648,will be generated for the 2nd and subsequent cv.split calls.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1649,This is a check to make sure cv.split is not called once per param,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1650,setting.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1656,Check consistency of folds across the parameters,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1663,As the first two param settings (C=0.1) and the next two param,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1664,"settings (C=0.2) are same, the test and train scores must also be",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1665,same as long as the same train/test indices are generated for all,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1666,"the cv splits, for both param setting",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1695,XXX: results['params'] is a list :|,SATD
scikit-learn/sklearn/model_selection/tests/test_search.py,1720,Using regressor to make sure each score differs,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1730,"TODO: remove in v0.24, the deprecation goes away then.",SATD
scikit-learn/sklearn/model_selection/tests/test_search.py,1750,this should not raise any exceptions,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1759,this should raise a NotImplementedError,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1765,FIXME: remove in 0.24,SATD
scikit-learn/sklearn/model_selection/tests/test_search.py,1776,"Use global X, y",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1778,create cv,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1781,"pop all of it, this should cause the expected ValueError",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1783,cv is empty now,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1789,assert that this raises an error,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1798,"Use global X, y",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1804,create bad cv,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1811,assert that this raises an error,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1820,make sure grid search and random search delegate n_features_in to the,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1821,best estimator,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1866,defaults to euclidean metric (minkowski p = 2),not
scikit-learn/sklearn/model_selection/tests/test_search.py,1872,precompute euclidean metric to validate _pairwise is working,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1889,unofficially sanctioned tolerance for scalar values in fit_params,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1890,non-regression test for:,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1891,https://github.com/scikit-learn/scikit-learn/issues/15805,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1914,"check support for scalar values in fit_params, for instance in LightGBM",not
scikit-learn/sklearn/model_selection/tests/test_search.py,1915,that do not exactly respect the scikit-learn API contract but that we do,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1916,not want to break without an explicit deprecation cycle and API,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1917,recommendations for implementing early stopping with a user provided,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1918,validation set. non-regression test for:,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1919,https://github.com/scikit-learn/scikit-learn/issues/15805,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1932,The tuple of arrays should be preserved as tuple.,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1945,NOTE: `fit_params` should be data dependent (e.g. `sample_weight`) which,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1946,is not the case for the following parameters. But this abuse is common in,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1947,popular third-party libraries and we should tolerate this behavior for,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1948,now and be careful not to break support for those without following,not
scikit-learn/sklearn/model_selection/tests/test_search.py,1949,proper deprecation cycle.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,71,(the default value),not
scikit-learn/sklearn/model_selection/tests/test_split.py,84,n_splits = np of unique folds = 2,not
scikit-learn/sklearn/model_selection/tests/test_split.py,104,Test if get_n_splits works correctly,not
scikit-learn/sklearn/model_selection/tests/test_split.py,107,Test if the cross-validator works as expected even if,not
scikit-learn/sklearn/model_selection/tests/test_split.py,108,the data is 1d,not
scikit-learn/sklearn/model_selection/tests/test_split.py,111,"Test that train, test indices returned are integers",not
scikit-learn/sklearn/model_selection/tests/test_split.py,116,Test if the repr works without any errors,not
scikit-learn/sklearn/model_selection/tests/test_split.py,119,ValueError for get_n_splits methods,not
scikit-learn/sklearn/model_selection/tests/test_split.py,128,smoke test for 2d y and multi-label,not
scikit-learn/sklearn/model_selection/tests/test_split.py,155,Use python sets to get more informative assertion failure messages,not
scikit-learn/sklearn/model_selection/tests/test_split.py,158,Train and test split should not overlap,not
scikit-learn/sklearn/model_selection/tests/test_split.py,162,Check that the union of train an test split cover all the indices,not
scikit-learn/sklearn/model_selection/tests/test_split.py,168,Check that a all the samples appear at least once in a test fold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,178,Check that the accumulated test samples cover the whole dataset,not
scikit-learn/sklearn/model_selection/tests/test_split.py,187,Check that errors are raised if there is not enough samples,not
scikit-learn/sklearn/model_selection/tests/test_split.py,190,Check that a warning is raised if the least populated class has too few,not
scikit-learn/sklearn/model_selection/tests/test_split.py,191,members.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,198,Check that despite the warning the folds are still computed even,not
scikit-learn/sklearn/model_selection/tests/test_split.py,199,though all the classes are not necessarily represented at on each,not
scikit-learn/sklearn/model_selection/tests/test_split.py,200,side of the split at each split,SATD
scikit-learn/sklearn/model_selection/tests/test_split.py,205,Check that errors are raised if all n_groups for individual,not
scikit-learn/sklearn/model_selection/tests/test_split.py,206,classes are less than n_splits.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,211,Error when number of folds is <= 1,not
scikit-learn/sklearn/model_selection/tests/test_split.py,221,When n_splits is not integer:,not
scikit-learn/sklearn/model_selection/tests/test_split.py,227,When shuffle is not  a bool:,not
scikit-learn/sklearn/model_selection/tests/test_split.py,232,Check all indices are returned in the test folds,not
scikit-learn/sklearn/model_selection/tests/test_split.py,237,Check all indices are returned in the test folds even when equal-sized,not
scikit-learn/sklearn/model_selection/tests/test_split.py,238,folds are not possible,not
scikit-learn/sklearn/model_selection/tests/test_split.py,243,Check if get_n_splits returns the number of folds,not
scikit-learn/sklearn/model_selection/tests/test_split.py,248,Manually check that KFold preserves the data ordering on toy datasets,not
scikit-learn/sklearn/model_selection/tests/test_split.py,271,Manually check that StratifiedKFold preserves the data ordering as much,not
scikit-learn/sklearn/model_selection/tests/test_split.py,272,as possible on toy datasets in order to avoid hiding sample dependencies,not
scikit-learn/sklearn/model_selection/tests/test_split.py,273,when possible,not
scikit-learn/sklearn/model_selection/tests/test_split.py,294,Check if get_n_splits returns the number of folds,not
scikit-learn/sklearn/model_selection/tests/test_split.py,297,Make sure string labels are also supported,not
scikit-learn/sklearn/model_selection/tests/test_split.py,305,Check equivalence to KFold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,316,Check that stratified kfold preserves class ratios in individual splits,not
scikit-learn/sklearn/model_selection/tests/test_split.py,317,Repeat with shuffling turned off and on,not
scikit-learn/sklearn/model_selection/tests/test_split.py,338,Check that stratified kfold gives the same indices regardless of labels,not
scikit-learn/sklearn/model_selection/tests/test_split.py,360,Check that KFold returns folds with balanced sizes,not
scikit-learn/sklearn/model_selection/tests/test_split.py,370,Check that KFold returns folds with balanced sizes (only when,not
scikit-learn/sklearn/model_selection/tests/test_split.py,371,stratification is possible),not
scikit-learn/sklearn/model_selection/tests/test_split.py,372,Repeat with shuffling turned off and on,not
scikit-learn/sklearn/model_selection/tests/test_split.py,387,Check the indices are shuffled properly,not
scikit-learn/sklearn/model_selection/tests/test_split.py,398,Assert that there is no complete overlap,not
scikit-learn/sklearn/model_selection/tests/test_split.py,401,Set all test indices in successive iterations of kf2 to 1,not
scikit-learn/sklearn/model_selection/tests/test_split.py,404,Check that all indices are returned in the different test folds,not
scikit-learn/sklearn/model_selection/tests/test_split.py,409,Divisible by 3,not
scikit-learn/sklearn/model_selection/tests/test_split.py,411,Not divisible by 3,not
scikit-learn/sklearn/model_selection/tests/test_split.py,414,"Check that when the shuffle is True, multiple split calls produce the",not
scikit-learn/sklearn/model_selection/tests/test_split.py,415,same split when random_state is int,not
scikit-learn/sklearn/model_selection/tests/test_split.py,423,"Check that when the shuffle is True, multiple split calls often",not
scikit-learn/sklearn/model_selection/tests/test_split.py,424,(not always) produce different splits when random_state is,not
scikit-learn/sklearn/model_selection/tests/test_split.py,425,RandomState instance or None,not
scikit-learn/sklearn/model_selection/tests/test_split.py,432,Test if the two splits are different cv,not
scikit-learn/sklearn/model_selection/tests/test_split.py,435,"cv.split(...) returns an array of tuples, each tuple",not
scikit-learn/sklearn/model_selection/tests/test_split.py,436,consisting of an array with train indices and test indices,not
scikit-learn/sklearn/model_selection/tests/test_split.py,437,Ensure that the splits for data are not same,not
scikit-learn/sklearn/model_selection/tests/test_split.py,438,when random state is not set,not
scikit-learn/sklearn/model_selection/tests/test_split.py,444,"Check that shuffling is happening when requested, and for proper",not
scikit-learn/sklearn/model_selection/tests/test_split.py,445,sample coverage,not
scikit-learn/sklearn/model_selection/tests/test_split.py,455,Ensure that we shuffle each class's samples with different,not
scikit-learn/sklearn/model_selection/tests/test_split.py,456,random_state in StratifiedKFold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,457,See https://github.com/scikit-learn/scikit-learn/pull/13124,not
scikit-learn/sklearn/model_selection/tests/test_split.py,467,see #2372,not
scikit-learn/sklearn/model_selection/tests/test_split.py,468,The digits samples are dependent: they are apparently grouped by authors,not
scikit-learn/sklearn/model_selection/tests/test_split.py,469,although we don't have any information on the groups segment locations,not
scikit-learn/sklearn/model_selection/tests/test_split.py,470,for this data. We can highlight this fact by computing k-fold cross-,not
scikit-learn/sklearn/model_selection/tests/test_split.py,471,validation with and without shuffling: we observe that the shuffling case,not
scikit-learn/sklearn/model_selection/tests/test_split.py,472,wrongly makes the IID assumption and is therefore too optimistic: it,not
scikit-learn/sklearn/model_selection/tests/test_split.py,473,estimates a much higher accuracy (around 0.93) than that the non,SATD
scikit-learn/sklearn/model_selection/tests/test_split.py,474,shuffling variant (around 0.81).,not
scikit-learn/sklearn/model_selection/tests/test_split.py,486,Shuffling the data artificially breaks the dependency and hides the,not
scikit-learn/sklearn/model_selection/tests/test_split.py,487,overfitting of the model with regards to the writing style of the authors,not
scikit-learn/sklearn/model_selection/tests/test_split.py,488,by yielding a seriously overestimated score:,not
scikit-learn/sklearn/model_selection/tests/test_split.py,498,"Similarly, StratifiedKFold should try to shuffle the data as little",not
scikit-learn/sklearn/model_selection/tests/test_split.py,499,as possible (while respecting the balanced class constraints),not
scikit-learn/sklearn/model_selection/tests/test_split.py,500,and thus be able to detect the dependency by not overestimating,not
scikit-learn/sklearn/model_selection/tests/test_split.py,501,the CV score either. As the digits dataset is approximately balanced,not
scikit-learn/sklearn/model_selection/tests/test_split.py,502,the estimated mean score is close to the score measured with,not
scikit-learn/sklearn/model_selection/tests/test_split.py,503,non-shuffled KFold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,533,"Check that the default value has the expected behavior, i.e. 0.1 if both",not
scikit-learn/sklearn/model_selection/tests/test_split.py,534,unspecified or complement train_size unless both are specified.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,550,"Check that the default value has the expected behavior, i.e. 0.2 if both",not
scikit-learn/sklearn/model_selection/tests/test_split.py,551,unspecified or complement train_size unless both are specified.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,567,Check that error is raised if there is a class with only one sample,not
scikit-learn/sklearn/model_selection/tests/test_split.py,571,Check that error is raised if the test set size is smaller than n_classes,not
scikit-learn/sklearn/model_selection/tests/test_split.py,573,Check that error is raised if the train set size is smaller than,not
scikit-learn/sklearn/model_selection/tests/test_split.py,574,n_classes,not
scikit-learn/sklearn/model_selection/tests/test_split.py,581,Train size or test size too small,not
scikit-learn/sklearn/model_selection/tests/test_split.py,613,To make it indexable for y[train],not
scikit-learn/sklearn/model_selection/tests/test_split.py,614,this is how test-size is computed internally,not
scikit-learn/sklearn/model_selection/tests/test_split.py,615,in _validate_shuffle_split,not
scikit-learn/sklearn/model_selection/tests/test_split.py,620,Checks if folds keep classes proportions,not
scikit-learn/sklearn/model_selection/tests/test_split.py,635,"Test the StratifiedShuffleSplit, indices are drawn with a",not
scikit-learn/sklearn/model_selection/tests/test_split.py,636,equal chance,not
scikit-learn/sklearn/model_selection/tests/test_split.py,641,Here we test that the distribution of the counts,not
scikit-learn/sklearn/model_selection/tests/test_split.py,642,per index is close enough to a binomial,not
scikit-learn/sklearn/model_selection/tests/test_split.py,685,See https://github.com/scikit-learn/scikit-learn/issues/6121 for,not
scikit-learn/sklearn/model_selection/tests/test_split.py,686,the original bug report,SATD
scikit-learn/sklearn/model_selection/tests/test_split.py,695,no overlap,not
scikit-learn/sklearn/model_selection/tests/test_split.py,698,complete partition,not
scikit-learn/sklearn/model_selection/tests/test_split.py,703,fix for issue 9037,not
scikit-learn/sklearn/model_selection/tests/test_split.py,712,no overlap,not
scikit-learn/sklearn/model_selection/tests/test_split.py,715,complete partition,not
scikit-learn/sklearn/model_selection/tests/test_split.py,718,correct stratification of entire rows,not
scikit-learn/sklearn/model_selection/tests/test_split.py,719,"(by design, here y[:, 0] uniquely determines the entire row of y)",not
scikit-learn/sklearn/model_selection/tests/test_split.py,726,"fix in PR #9922: for multilabel data with > 1000 labels, str(row)",not
scikit-learn/sklearn/model_selection/tests/test_split.py,727,truncates with an ellipsis for elements in positions 4 through,not
scikit-learn/sklearn/model_selection/tests/test_split.py,728,"len(row) - 4, so labels were not being correctly split using the powerset",not
scikit-learn/sklearn/model_selection/tests/test_split.py,729,method for transforming a multilabel problem to a multiclass one; this,not
scikit-learn/sklearn/model_selection/tests/test_split.py,730,test checks that this problem is fixed.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,741,correct stratification of entire rows,not
scikit-learn/sklearn/model_selection/tests/test_split.py,742,"(by design, here y[:, 4] uniquely determines the entire row of y)",not
scikit-learn/sklearn/model_selection/tests/test_split.py,749,Check that PredefinedSplit can reproduce a split generated by Kfold.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,758,n_splits is simply the no of unique folds,not
scikit-learn/sklearn/model_selection/tests/test_split.py,772,Make sure the repr works,not
scikit-learn/sklearn/model_selection/tests/test_split.py,775,Test that the length is correct,not
scikit-learn/sklearn/model_selection/tests/test_split.py,782,First test: no train group is in the test set and vice versa,not
scikit-learn/sklearn/model_selection/tests/test_split.py,788,Second test: train and test add up to all the data,not
scikit-learn/sklearn/model_selection/tests/test_split.py,791,Third test: train and test are disjoint,not
scikit-learn/sklearn/model_selection/tests/test_split.py,794,Fourth test:,not
scikit-learn/sklearn/model_selection/tests/test_split.py,795,"unique train and test groups are correct, +- 1 for rounding error",not
scikit-learn/sklearn/model_selection/tests/test_split.py,807,Make sure the repr works,not
scikit-learn/sklearn/model_selection/tests/test_split.py,822,Test that the length is correct,not
scikit-learn/sklearn/model_selection/tests/test_split.py,827,Split using the original list / array / list of string groups_i,not
scikit-learn/sklearn/model_selection/tests/test_split.py,829,First test: no train group is in the test set and vice versa,not
scikit-learn/sklearn/model_selection/tests/test_split.py,834,Second test: train and test add up to all the data,not
scikit-learn/sklearn/model_selection/tests/test_split.py,837,Third test:,not
scikit-learn/sklearn/model_selection/tests/test_split.py,838,The number of groups in test must be equal to p_groups_out,not
scikit-learn/sklearn/model_selection/tests/test_split.py,841,check get_n_splits() with dummy parameters,not
scikit-learn/sklearn/model_selection/tests/test_split.py,847,raise ValueError if a `groups` parameter is illegal,not
scikit-learn/sklearn/model_selection/tests/test_split.py,861,Check that LeaveOneGroupOut and LeavePGroupsOut work normally if,not
scikit-learn/sklearn/model_selection/tests/test_split.py,862,the groups variable is changed before calling split,not
scikit-learn/sklearn/model_selection/tests/test_split.py,876,n_splits = no of 2 (p) group combinations of the unique groups = 3C2 = 3,not
scikit-learn/sklearn/model_selection/tests/test_split.py,880,"n_splits = no of unique groups (C(uniq_lbls, 1) = n_unique_groups)",not
scikit-learn/sklearn/model_selection/tests/test_split.py,912,n_repeats is not integer or <= 0,not
scikit-learn/sklearn/model_selection/tests/test_split.py,937,split should produce same and deterministic splits on,not
scikit-learn/sklearn/model_selection/tests/test_split.py,938,each call,not
scikit-learn/sklearn/model_selection/tests/test_split.py,985,split should produce same and deterministic splits on,not
scikit-learn/sklearn/model_selection/tests/test_split.py,986,each call,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1067,"Check that the default value has the expected behavior, i.e. complement",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1068,train_size unless both are specified.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1080,simple test,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1084,test correspondence of X and y,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1088,don't convert lists to anything else by default,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1094,allow nd-arrays,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1103,test stratification option,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1112,check the 1:1 ratio of ones and twos in the data is preserved,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1115,test unshuffled split,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1125,check train_test_split doesn't destroy pandas dataframe,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1133,X dataframe,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1141,check that train_test_split converts scipy sparse matrices,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1142,"to csr, as stated in the documentation",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1153,X mock dataframe,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1162,"Check that when y is a list / list of string labels, it works.",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1196,Check that iterating twice on the ShuffleSplit gives the same,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1197,sequence of train-test when the random_state is given,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1204,"Check that when y is a list / list of string labels, it works.",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1218,Check that train_test_split allows input data with NaNs,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1228,Use numpy.testing.assert_equal which recursively compares,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1229,lists of lists,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1241,also works with 2d multiclass,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1267,"Since the wrapped iterable is enlisted and stored,",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1268,split can be called any number of times to produce,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1269,consistent results.,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1272,"If the splits are randomized, successive calls to split yields different",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1273,results,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1276,numpy's assert_array_equal properly compares nested lists,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1294,Parameters of the test,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1301,Construct the test data,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1302,5 percent error allowed,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1308,Get the test fold indices from the test set indices of each fold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1314,Check that folds have approximately the same size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1320,Check that each group appears only in 1 fold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1324,Check that no group is on both sides of the split,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1329,Construct the test data,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1341,5 percent error allowed,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1346,Get the test fold indices from the test set indices of each fold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1351,Check that folds have approximately the same size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1357,Check that each group appears only in 1 fold,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1363,Check that no group is on both sides of the split,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1368,groups can also be a list,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1375,Should fail if there are more folds than groups,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1385,Should fail if there are more folds than samples,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1392,Manually check that Time Series CV preserves the data,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1393,ordering on toy datasets,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1413,Check get_n_splits returns the correct number of splits,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1434,Test for the case where the size of a fold is greater than max_train_size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1438,Test for the case where the size of each fold is less than max_train_size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1446,Test alone,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1461,Test with max_train_size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1473,Should fail with not enough data points for configuration,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1482,Test alone,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1493,Test with max_train_size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1508,Test with test_size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1520,Test with additional test_size,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1531,Verify proper error is thrown,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1538,Test if nested cross validation works with different combinations of cv,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1572,1 sample,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1581,1 sample,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1588,"3 samples, ask for more than 2 thirds",not
scikit-learn/sklearn/model_selection/tests/test_split.py,1597,LeaveOneGroup out expect at least 2 groups so no need to check,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1599,1 sample,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1607,No need to check LeavePGroupsOut,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1609,2 samples,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1618,passing a non-default random_state when shuffle=False makes no sense,not
scikit-learn/sklearn/model_selection/tests/test_split.py,1619,TODO 0.24: raise a ValueError instead of a warning,SATD
scikit-learn/sklearn/experimental/enable_iterative_imputer.py,18,use settattr to avoid mypy errors when monkeypatching,not
scikit-learn/sklearn/experimental/enable_hist_gradient_boosting.py,29,use settattr to avoid mypy errors when monkeypatching,not
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,9,Make sure different import strategies work or fail as expected.,not
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,11,"Since Python caches the imported modules, we need to run a child process",not
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,12,"for every test case. Else, the tests would not be independent",not
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,13,(manually removing the imports from the cache (sys.modules) is not,not
scikit-learn/sklearn/experimental/tests/test_enable_iterative_imputer.py,14,recommended and can lead to many complications).,not
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,9,Make sure different import strategies work or fail as expected.,not
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,11,"Since Python caches the imported modules, we need to run a child process",not
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,12,"for every test case. Else, the tests would not be independent",not
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,13,(manually removing the imports from the cache (sys.modules) is not,not
scikit-learn/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py,14,recommended and can lead to many complications).,not
scikit-learn/sklearn/svm/_bounds.py,2,Author: Paolo Losi,not
scikit-learn/sklearn/svm/_bounds.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/svm/_bounds.py,61,maximum absolute value over classes and features,not
scikit-learn/sklearn/svm/_bounds.py,73,loss == 'log':,not
scikit-learn/sklearn/svm/_classes.py,417,SVR only accepts l2 penalty,not
scikit-learn/sklearn/svm/_classes.py,1014,mypy error: Decorated property not supported,not
scikit-learn/sklearn/svm/_classes.py,1015,type: ignore,not
scikit-learn/sklearn/svm/_classes.py,1022,mypy error: Decorated property not supported,not
scikit-learn/sklearn/svm/_classes.py,1023,type: ignore,not
scikit-learn/sklearn/svm/_classes.py,1359,mypy error: Decorated property not supported,not
scikit-learn/sklearn/svm/_classes.py,1360,type: ignore,not
scikit-learn/sklearn/svm/_classes.py,1367,mypy error: Decorated property not supported,not
scikit-learn/sklearn/svm/_classes.py,1368,type: ignore,not
scikit-learn/sklearn/svm/__init__.py,5,See http://scikit-learn.sourceforge.net/modules/svm.html for complete,not
scikit-learn/sklearn/svm/__init__.py,6,documentation.,not
scikit-learn/sklearn/svm/__init__.py,8,Author: Fabian Pedregosa <fabian.pedregosa@inria.fr> with help from,not
scikit-learn/sklearn/svm/__init__.py,9,the scikit-learn community. LibSVM and LibLinear are copyright,not
scikit-learn/sklearn/svm/__init__.py,10,of their respective owners.,not
scikit-learn/sklearn/svm/__init__.py,11,License: BSD 3 clause (C) INRIA 2010,not
scikit-learn/sklearn/svm/setup.py,13,Section LibSVM,not
scikit-learn/sklearn/svm/setup.py,15,we compile both libsvm and libsvm_sparse,not
scikit-learn/sklearn/svm/setup.py,21,Force C++ linking in case gcc is picked up instead,not
scikit-learn/sklearn/svm/setup.py,22,of g++ under windows with some versions of MinGW,not
scikit-learn/sklearn/svm/setup.py,24,Use C++11 to use the random number generator fix,SATD
scikit-learn/sklearn/svm/setup.py,44,liblinear module,not
scikit-learn/sklearn/svm/setup.py,49,precompile liblinear to use C++11 flag,not
scikit-learn/sklearn/svm/setup.py,56,Force C++ linking in case gcc is picked up instead,not
scikit-learn/sklearn/svm/setup.py,57,of g++ under windows with some versions of MinGW,not
scikit-learn/sklearn/svm/setup.py,59,Use C++11 to use the random number generator fix,SATD
scikit-learn/sklearn/svm/setup.py,76,"extra_compile_args=['-O0 -fno-inline'],",not
scikit-learn/sklearn/svm/setup.py,79,end liblinear module,not
scikit-learn/sklearn/svm/setup.py,81,this should go *after* libsvm-skl,not
scikit-learn/sklearn/svm/_base.py,6,mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm',not
scikit-learn/sklearn/svm/_base.py,7,(and same for other imports),not
scikit-learn/sklearn/svm/_base.py,8,type: ignore,not
scikit-learn/sklearn/svm/_base.py,9,type: ignore,not
scikit-learn/sklearn/svm/_base.py,10,type: ignore,not
scikit-learn/sklearn/svm/_base.py,34,get 1vs1 weights for all n*(n-1) classifiers.,not
scikit-learn/sklearn/svm/_base.py,35,this is somewhat messy.,not
scikit-learn/sklearn/svm/_base.py,36,shape of dual_coef_ is nSV * (n_classes -1),not
scikit-learn/sklearn/svm/_base.py,37,see docs for details,not
scikit-learn/sklearn/svm/_base.py,40,XXX we could do preallocation of coef but,SATD
scikit-learn/sklearn/svm/_base.py,41,would have to take care in the sparse case,SATD
scikit-learn/sklearn/svm/_base.py,45,SVs for class1:,not
scikit-learn/sklearn/svm/_base.py,48,SVs for class1:,not
scikit-learn/sklearn/svm/_base.py,51,dual coef for class1 SVs:,not
scikit-learn/sklearn/svm/_base.py,53,dual coef for class2 SVs:,not
scikit-learn/sklearn/svm/_base.py,55,build weight for class1 vs class2,not
scikit-learn/sklearn/svm/_base.py,70,The order of these must match the integer values in LibSVM.,not
scikit-learn/sklearn/svm/_base.py,71,XXX These are actually the same in the dense case. Need to factor,SATD
scikit-learn/sklearn/svm/_base.py,72,this out.,not
scikit-learn/sklearn/svm/_base.py,107,Used by cross_val_score.,not
scikit-learn/sklearn/svm/_base.py,171,input validation,not
scikit-learn/sklearn/svm/_base.py,193,unused but needs to be a float for cython code that ignores,not
scikit-learn/sklearn/svm/_base.py,194,it anyway,not
scikit-learn/sklearn/svm/_base.py,198,var = E[X^2] - E[X]^2 if sparse,not
scikit-learn/sklearn/svm/_base.py,218,see comment on the other call to np.iinfo in this file,not
scikit-learn/sklearn/svm/_base.py,222,"In binary case, we need to flip the sign of coef, intercept and",not
scikit-learn/sklearn/svm/_base.py,223,decision function. Use self._intercept_ and self._dual_coef_,not
scikit-learn/sklearn/svm/_base.py,224,internally.,not
scikit-learn/sklearn/svm/_base.py,238,XXX this is ugly.,SATD
scikit-learn/sklearn/svm/_base.py,239,Regression models should not have a class_weight_ attribute.,not
scikit-learn/sklearn/svm/_base.py,254,you must store a reference to X to compute the kernel in predict,not
scikit-learn/sklearn/svm/_base.py,255,TODO: add keyword copy to copy on demand,SATD
scikit-learn/sklearn/svm/_base.py,264,we don't pass **self.get_params() to allow subclasses to,not
scikit-learn/sklearn/svm/_base.py,265,add other parameters to __init__,not
scikit-learn/sklearn/svm/_base.py,304,regression,not
scikit-learn/sklearn/svm/_base.py,360,Precondition: X is a csr_matrix of dtype np.float64.,not
scikit-learn/sklearn/svm/_base.py,367,C is not useful here,not
scikit-learn/sklearn/svm/_base.py,385,"in the case of precomputed kernel given as a function, we",not
scikit-learn/sklearn/svm/_base.py,386,have to compute explicitly the kernel matrix,not
scikit-learn/sklearn/svm/_base.py,406,NOTE: _validate_for_predict contains check for is_fitted,not
scikit-learn/sklearn/svm/_base.py,407,hence must be placed before any other attributes are used.,not
scikit-learn/sklearn/svm/_base.py,416,"In binary case, we need to flip the sign of coef, intercept and",not
scikit-learn/sklearn/svm/_base.py,417,decision function.,not
scikit-learn/sklearn/svm/_base.py,497,"coef_ being a read-only property, it's better to mark the value as",SATD
scikit-learn/sklearn/svm/_base.py,498,immutable to avoid hiding potential bugs for the unsuspecting user.,not
scikit-learn/sklearn/svm/_base.py,500,sparse matrix do not have global flags,not
scikit-learn/sklearn/svm/_base.py,503,regular dense array,not
scikit-learn/sklearn/svm/_base.py,521,SVR and OneClass,not
scikit-learn/sklearn/svm/_base.py,522,"_n_support has size 2, we make it size 1",not
scikit-learn/sklearn/svm/_base.py,617,Hacky way of getting predict_proba to raise an AttributeError when,SATD
scikit-learn/sklearn/svm/_base.py,618,probability=False using properties. Do not use this in new code; when,not
scikit-learn/sklearn/svm/_base.py,619,"probabilities are not available depending on a setting, introduce two",not
scikit-learn/sklearn/svm/_base.py,620,estimators.,not
scikit-learn/sklearn/svm/_base.py,743,binary classifier,not
scikit-learn/sklearn/svm/_base.py,746,1vs1 classifier,not
scikit-learn/sklearn/svm/_base.py,777,"nested dicts containing level 1: available loss functions,",not
scikit-learn/sklearn/svm/_base.py,778,"level2: available penalties for the given loss function,",not
scikit-learn/sklearn/svm/_base.py,779,level3: whether the dual solver is available for the specified,not
scikit-learn/sklearn/svm/_base.py,780,combination of loss function and penalty,not
scikit-learn/sklearn/svm/_base.py,938,LinearSVC breaks when intercept_scaling is <= 0,not
scikit-learn/sklearn/svm/_base.py,952,Liblinear doesn't support 64bit sparse matrix indices yet,not
scikit-learn/sklearn/svm/_base.py,956,"LibLinear wants targets as doubles, even for classification",not
scikit-learn/sklearn/svm/_base.py,968,Regarding rnd.randint(..) in the above signature:,not
scikit-learn/sklearn/svm/_base.py,969,seed for srand in range [0..INT_MAX); due to limitations in Numpy,not
scikit-learn/sklearn/svm/_base.py,970,"on 32-bit platforms, we can't get to the UINT_MAX limit that",not
scikit-learn/sklearn/svm/_base.py,971,srand supports,not
scikit-learn/sklearn/svm/tests/test_svm.py,31,mypy error: Module 'sklearn.svm' has no attribute '_libsvm',not
scikit-learn/sklearn/svm/tests/test_svm.py,32,type: ignore,not
scikit-learn/sklearn/svm/tests/test_svm.py,34,toy sample,not
scikit-learn/sklearn/svm/tests/test_svm.py,40,also load the iris dataset,not
scikit-learn/sklearn/svm/tests/test_svm.py,49,Test parameters on classes that make use of libsvm.,not
scikit-learn/sklearn/svm/tests/test_svm.py,59,Check consistency on dataset iris.,not
scikit-learn/sklearn/svm/tests/test_svm.py,61,shuffle the dataset so that labels are not ordered,not
scikit-learn/sklearn/svm/tests/test_svm.py,69,check also the low-level API,not
scikit-learn/sklearn/svm/tests/test_svm.py,85,"If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence",not
scikit-learn/sklearn/svm/tests/test_svm.py,86,we should get deterministic results (assuming that there is no other,not
scikit-learn/sklearn/svm/tests/test_svm.py,87,thread calling this wrapper calling `srand` concurrently).,not
scikit-learn/sklearn/svm/tests/test_svm.py,96,SVC with a precomputed kernel.,not
scikit-learn/sklearn/svm/tests/test_svm.py,97,We test it with a toy dataset and with iris.,not
scikit-learn/sklearn/svm/tests/test_svm.py,99,Gram matrix for train data (square matrix),not
scikit-learn/sklearn/svm/tests/test_svm.py,100,(we use just a linear kernel),not
scikit-learn/sklearn/svm/tests/test_svm.py,103,Gram matrix for test data (rectangular matrix),not
scikit-learn/sklearn/svm/tests/test_svm.py,115,"Gram matrix for test data but compute KT[i,j]",not
scikit-learn/sklearn/svm/tests/test_svm.py,116,for support vectors j only.,not
scikit-learn/sklearn/svm/tests/test_svm.py,125,"same as before, but using a callable function instead of the kernel",not
scikit-learn/sklearn/svm/tests/test_svm.py,126,matrix. kernel is just a linear kernel,not
scikit-learn/sklearn/svm/tests/test_svm.py,138,test a precomputed kernel with the iris dataset,not
scikit-learn/sklearn/svm/tests/test_svm.py,139,and check parameters against a linear SVC,not
scikit-learn/sklearn/svm/tests/test_svm.py,151,"Gram matrix for test data but compute KT[i,j]",not
scikit-learn/sklearn/svm/tests/test_svm.py,152,for support vectors j only.,not
scikit-learn/sklearn/svm/tests/test_svm.py,167,Test Support Vector Regression,not
scikit-learn/sklearn/svm/tests/test_svm.py,178,"non-regression test; previously, BaseLibSVM would check that",not
scikit-learn/sklearn/svm/tests/test_svm.py,179,"len(np.unique(y)) < 2, which must only be done for SVC",not
scikit-learn/sklearn/svm/tests/test_svm.py,185,check that SVR(kernel='linear') and LinearSVC() give,not
scikit-learn/sklearn/svm/tests/test_svm.py,186,comparable results,not
scikit-learn/sklearn/svm/tests/test_svm.py,200,check correct result when sample_weight is 1,not
scikit-learn/sklearn/svm/tests/test_svm.py,201,check that SVR(kernel='linear') and LinearSVC() give,not
scikit-learn/sklearn/svm/tests/test_svm.py,202,comparable results,not
scikit-learn/sklearn/svm/tests/test_svm.py,218,"check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",not
scikit-learn/sklearn/svm/tests/test_svm.py,219,"X = X1 repeated n1 times, X2 repeated n2 times and so forth",not
scikit-learn/sklearn/svm/tests/test_svm.py,240,Bad kernel,not
scikit-learn/sklearn/svm/tests/test_svm.py,248,Test OneClassSVM,not
scikit-learn/sklearn/svm/tests/test_svm.py,264,Test OneClassSVM decision function,not
scikit-learn/sklearn/svm/tests/test_svm.py,268,Generate train data,not
scikit-learn/sklearn/svm/tests/test_svm.py,272,Generate some regular novel observations,not
scikit-learn/sklearn/svm/tests/test_svm.py,275,Generate some abnormal novel observations,not
scikit-learn/sklearn/svm/tests/test_svm.py,278,fit the model,not
scikit-learn/sklearn/svm/tests/test_svm.py,282,predict things,not
scikit-learn/sklearn/svm/tests/test_svm.py,301,Make sure some tweaking of parameters works.,not
scikit-learn/sklearn/svm/tests/test_svm.py,302,We change clf.dual_coef_ at run time and expect .predict() to change,not
scikit-learn/sklearn/svm/tests/test_svm.py,303,accordingly. Notice that this is not trivial since it involves a lot,not
scikit-learn/sklearn/svm/tests/test_svm.py,304,of C/Python copying in the libsvm bindings.,not
scikit-learn/sklearn/svm/tests/test_svm.py,305,The success of this test ensures that the mapping between libsvm and,not
scikit-learn/sklearn/svm/tests/test_svm.py,306,the python classifier is complete.,not
scikit-learn/sklearn/svm/tests/test_svm.py,316,Predict probabilities using SVC,not
scikit-learn/sklearn/svm/tests/test_svm.py,317,"This uses cross validation, so we use a slightly bigger testing set.",not
scikit-learn/sklearn/svm/tests/test_svm.py,334,Test decision_function,not
scikit-learn/sklearn/svm/tests/test_svm.py,335,"Sanity check, test that decision_function implemented in python",not
scikit-learn/sklearn/svm/tests/test_svm.py,336,returns the same as the one in libsvm,not
scikit-learn/sklearn/svm/tests/test_svm.py,337,multi class:,not
scikit-learn/sklearn/svm/tests/test_svm.py,345,binary:,not
scikit-learn/sklearn/svm/tests/test_svm.py,356,kernel binary:,not
scikit-learn/sklearn/svm/tests/test_svm.py,367,check that decision_function_shape='ovr' or 'ovo' gives,not
scikit-learn/sklearn/svm/tests/test_svm.py,368,correct shape and is consistent with predict,not
scikit-learn/sklearn/svm/tests/test_svm.py,376,with five classes:,not
scikit-learn/sklearn/svm/tests/test_svm.py,386,check shape of ovo_decition_function=True,not
scikit-learn/sklearn/svm/tests/test_svm.py,397,Test SVR's decision_function,not
scikit-learn/sklearn/svm/tests/test_svm.py,398,"Sanity check, test that predict implemented in python",not
scikit-learn/sklearn/svm/tests/test_svm.py,399,returns the same as the one in libsvm,not
scikit-learn/sklearn/svm/tests/test_svm.py,404,linear kernel,not
scikit-learn/sklearn/svm/tests/test_svm.py,410,rbf kernel,not
scikit-learn/sklearn/svm/tests/test_svm.py,419,Test class weights,not
scikit-learn/sklearn/svm/tests/test_svm.py,421,we give a small weights to class 1,not
scikit-learn/sklearn/svm/tests/test_svm.py,423,so all predicted values belong to class 2,not
scikit-learn/sklearn/svm/tests/test_svm.py,439,fit a linear SVM and check that giving more weight to opposed samples,not
scikit-learn/sklearn/svm/tests/test_svm.py,440,in the space will flip the decision toward these samples.,not
scikit-learn/sklearn/svm/tests/test_svm.py,444,"check that with unit weights, a sample is supposed to be predicted on",not
scikit-learn/sklearn/svm/tests/test_svm.py,445,the boundary,not
scikit-learn/sklearn/svm/tests/test_svm.py,451,give more weights to opposed samples,not
scikit-learn/sklearn/svm/tests/test_svm.py,468,similar test to test_svm_classifier_sided_sample_weight but for,not
scikit-learn/sklearn/svm/tests/test_svm.py,469,SVM regressors,not
scikit-learn/sklearn/svm/tests/test_svm.py,473,"check that with unit weights, a sample is supposed to be predicted on",not
scikit-learn/sklearn/svm/tests/test_svm.py,474,the boundary,not
scikit-learn/sklearn/svm/tests/test_svm.py,480,give more weights to opposed samples,not
scikit-learn/sklearn/svm/tests/test_svm.py,493,test that rescaling all samples is the same as changing C,not
scikit-learn/sklearn/svm/tests/test_svm.py,580,model generates equal coefficients,not
scikit-learn/sklearn/svm/tests/test_svm.py,589,Test class weights for imbalanced data,not
scikit-learn/sklearn/svm/tests/test_svm.py,591,We take as dataset the two-dimensional projection of iris so,not
scikit-learn/sklearn/svm/tests/test_svm.py,592,that it is not separable and remove half of predictors from,not
scikit-learn/sklearn/svm/tests/test_svm.py,593,class 1.,not
scikit-learn/sklearn/svm/tests/test_svm.py,594,We add one to the targets as a non-regression test:,not
scikit-learn/sklearn/svm/tests/test_svm.py,595,"class_weight=""balanced""",not
scikit-learn/sklearn/svm/tests/test_svm.py,596,used to work only when the labels where a range [0..K).,not
scikit-learn/sklearn/svm/tests/test_svm.py,607,check that score is better when class='balanced' is set.,SATD
scikit-learn/sklearn/svm/tests/test_svm.py,617,Test that it gives proper exception on deficient input,not
scikit-learn/sklearn/svm/tests/test_svm.py,618,impossible value of C,not
scikit-learn/sklearn/svm/tests/test_svm.py,622,impossible value of nu,not
scikit-learn/sklearn/svm/tests/test_svm.py,627,wrong dimensions for labels,not
scikit-learn/sklearn/svm/tests/test_svm.py,631,Test with arrays that are non-contiguous.,not
scikit-learn/sklearn/svm/tests/test_svm.py,642,error for precomputed kernelsx,not
scikit-learn/sklearn/svm/tests/test_svm.py,647,predict with sparse input when trained with dense,not
scikit-learn/sklearn/svm/tests/test_svm.py,680,Test that a unicode kernel name does not cause a TypeError,not
scikit-learn/sklearn/svm/tests/test_svm.py,698,Regression test for #14893,not
scikit-learn/sklearn/svm/tests/test_svm.py,711,Test possible parameter combinations in LinearSVC,not
scikit-learn/sklearn/svm/tests/test_svm.py,712,Generate list of possible parameter combinations,not
scikit-learn/sklearn/svm/tests/test_svm.py,732,Incorrect loss value - test if explicit error message is raised,not
scikit-learn/sklearn/svm/tests/test_svm.py,738,Check if Upper case notation raises error at _fit_liblinear,not
scikit-learn/sklearn/svm/tests/test_svm.py,739,which is called by fit,not
scikit-learn/sklearn/svm/tests/test_svm.py,753,Test basic routines using LinearSVC,not
scikit-learn/sklearn/svm/tests/test_svm.py,756,by default should have intercept,not
scikit-learn/sklearn/svm/tests/test_svm.py,762,the same with l1 penalty,not
scikit-learn/sklearn/svm/tests/test_svm.py,767,l2 penalty with dual formulation,not
scikit-learn/sklearn/svm/tests/test_svm.py,771,"l2 penalty, l1 loss",not
scikit-learn/sklearn/svm/tests/test_svm.py,776,test also decision function,not
scikit-learn/sklearn/svm/tests/test_svm.py,783,Test LinearSVC with crammer_singer multi-class svm,not
scikit-learn/sklearn/svm/tests/test_svm.py,788,similar prediction for ovr and crammer-singer:,not
scikit-learn/sklearn/svm/tests/test_svm.py,792,classifiers shouldn't be the same,not
scikit-learn/sklearn/svm/tests/test_svm.py,795,test decision function,not
scikit-learn/sklearn/svm/tests/test_svm.py,803,check correct result when sample_weight is 1,not
scikit-learn/sklearn/svm/tests/test_svm.py,810,check if same as sample_weight=None,not
scikit-learn/sklearn/svm/tests/test_svm.py,814,"check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",not
scikit-learn/sklearn/svm/tests/test_svm.py,815,"X = X1 repeated n1 times, X2 repeated n2 times and so forth",not
scikit-learn/sklearn/svm/tests/test_svm.py,834,Test Crammer-Singer formulation in the binary case,not
scikit-learn/sklearn/svm/tests/test_svm.py,845,Test that LinearSVC gives plausible predictions on the iris dataset,not
scikit-learn/sklearn/svm/tests/test_svm.py,846,"Also, test symbolic class names (classes_).",not
scikit-learn/sklearn/svm/tests/test_svm.py,858,Test that dense liblinear honours intercept_scaling param,not
scikit-learn/sklearn/svm/tests/test_svm.py,869,"when intercept_scaling is low the intercept value is highly ""penalized""",not
scikit-learn/sklearn/svm/tests/test_svm.py,870,by regularization,not
scikit-learn/sklearn/svm/tests/test_svm.py,875,"when intercept_scaling is sufficiently high, the intercept value",not
scikit-learn/sklearn/svm/tests/test_svm.py,876,is not affected by regularization,not
scikit-learn/sklearn/svm/tests/test_svm.py,882,"when intercept_scaling is sufficiently high, the intercept value",not
scikit-learn/sklearn/svm/tests/test_svm.py,883,doesn't depend on intercept_scaling value,not
scikit-learn/sklearn/svm/tests/test_svm.py,891,multi-class case,not
scikit-learn/sklearn/svm/tests/test_svm.py,899,binary-class case,not
scikit-learn/sklearn/svm/tests/test_svm.py,915,Check that primal coef modification are not silently ignored,not
scikit-learn/sklearn/svm/tests/test_svm.py,931,stdout: redirect,not
scikit-learn/sklearn/svm/tests/test_svm.py,933,save original stdout,not
scikit-learn/sklearn/svm/tests/test_svm.py,934,replace it,not
scikit-learn/sklearn/svm/tests/test_svm.py,936,actual call,not
scikit-learn/sklearn/svm/tests/test_svm.py,940,stdout: restore,not
scikit-learn/sklearn/svm/tests/test_svm.py,941,restore original stdout,not
scikit-learn/sklearn/svm/tests/test_svm.py,945,"create SVM with callable linear kernel, check that results are the same",not
scikit-learn/sklearn/svm/tests/test_svm.py,946,as with built-in linear kernel,not
scikit-learn/sklearn/svm/tests/test_svm.py,950,clone for checking clonability with lambda functions..,not
scikit-learn/sklearn/svm/tests/test_svm.py,985,input validation not required when SVM not fitted,not
scikit-learn/sklearn/svm/tests/test_svm.py,996,ignore convergence warnings from max_iter=1,not
scikit-learn/sklearn/svm/tests/test_svm.py,1007,Test that warnings are raised if model does not converge,not
scikit-learn/sklearn/svm/tests/test_svm.py,1019,"Test that SVR(kernel=""linear"") has coef_ with the right sign.",not
scikit-learn/sklearn/svm/tests/test_svm.py,1020,Non-regression test for #2933.,not
scikit-learn/sklearn/svm/tests/test_svm.py,1033,Test that the right error message is thrown when intercept_scaling <= 0,not
scikit-learn/sklearn/svm/tests/test_svm.py,1044,Test that intercept_scaling is ignored when fit_intercept is False,not
scikit-learn/sklearn/svm/tests/test_svm.py,1052,"Method must be (un)available before or after fit, switched by",not
scikit-learn/sklearn/svm/tests/test_svm.py,1053,`probability` param,not
scikit-learn/sklearn/svm/tests/test_svm.py,1065,Switching to `probability=True` after fitting should make,not
scikit-learn/sklearn/svm/tests/test_svm.py,1066,"predict_proba available, but calling it must not work:",not
scikit-learn/sklearn/svm/tests/test_svm.py,1083,One point from each quadrant represents one class,not
scikit-learn/sklearn/svm/tests/test_svm.py,1087,First point is closer to the decision boundaries than the second point,not
scikit-learn/sklearn/svm/tests/test_svm.py,1090,For all the quadrants (classes),not
scikit-learn/sklearn/svm/tests/test_svm.py,1092,Q1,not
scikit-learn/sklearn/svm/tests/test_svm.py,1093,Q2,not
scikit-learn/sklearn/svm/tests/test_svm.py,1094,Q3,not
scikit-learn/sklearn/svm/tests/test_svm.py,1095,Q4,not
scikit-learn/sklearn/svm/tests/test_svm.py,1105,Test if the prediction is the same as y,not
scikit-learn/sklearn/svm/tests/test_svm.py,1110,Assert that the predicted class has the maximum value,not
scikit-learn/sklearn/svm/tests/test_svm.py,1113,Get decision value at test points for the predicted class,not
scikit-learn/sklearn/svm/tests/test_svm.py,1116,Assert pred_class_deci_val > 0 here,not
scikit-learn/sklearn/svm/tests/test_svm.py,1119,Test if the first point has lower decision value on every quadrant,not
scikit-learn/sklearn/svm/tests/test_svm.py,1120,compared to the second point,not
scikit-learn/sklearn/svm/tests/test_svm.py,1173,"X_var ~= 1 shouldn't raise warning, for when",not
scikit-learn/sklearn/svm/tests/test_svm.py,1174,gamma is not explicitly set.,not
scikit-learn/sklearn/svm/tests/test_svm.py,1219,Make n_support is correct for oneclass and SVR (used to be,not
scikit-learn/sklearn/svm/tests/test_svm.py,1220,non-initialized),not
scikit-learn/sklearn/svm/tests/test_svm.py,1221,this is a non regression test for issue #14774,not
scikit-learn/sklearn/svm/tests/test_svm.py,1237,TODO: Remove in 0.25 when probA_ and probB_ are deprecated,SATD
scikit-learn/sklearn/svm/tests/test_svm.py,1256,count encoding,not
scikit-learn/sklearn/svm/tests/test_svm.py,1280,classifier,not
scikit-learn/sklearn/svm/tests/test_svm.py,1287,regressor,not
scikit-learn/sklearn/svm/tests/test_bounds.py,38,loss='l2' should raise ValueError,not
scikit-learn/sklearn/svm/tests/test_sparse.py,17,test sample 1,not
scikit-learn/sklearn/svm/tests/test_sparse.py,24,test sample 2,not
scikit-learn/sklearn/svm/tests/test_sparse.py,34,permute,not
scikit-learn/sklearn/svm/tests/test_sparse.py,39,sparsify,not
scikit-learn/sklearn/svm/tests/test_sparse.py,79,many class dataset:,not
scikit-learn/sklearn/svm/tests/test_sparse.py,97,test that the result with sorted and unsorted indices in csr is the same,not
scikit-learn/sklearn/svm/tests/test_sparse.py,98,"we use a subset of digits as iris, blobs or make_classification didn't",not
scikit-learn/sklearn/svm/tests/test_sparse.py,99,show the problem,not
scikit-learn/sklearn/svm/tests/test_sparse.py,110,make sure dense and sparse SVM give the same result,not
scikit-learn/sklearn/svm/tests/test_sparse.py,113,reverse each row's indices,not
scikit-learn/sklearn/svm/tests/test_sparse.py,133,make sure unsorted indices give same result,not
scikit-learn/sklearn/svm/tests/test_sparse.py,148,Test the sparse SVC with the iris dataset,not
scikit-learn/sklearn/svm/tests/test_sparse.py,164,Test decision_function,not
scikit-learn/sklearn/svm/tests/test_sparse.py,166,"Sanity check, test that decision_function implemented in python",not
scikit-learn/sklearn/svm/tests/test_sparse.py,167,returns the same as the one in libsvm,not
scikit-learn/sklearn/svm/tests/test_sparse.py,169,multi class:,not
scikit-learn/sklearn/svm/tests/test_sparse.py,177,binary:,not
scikit-learn/sklearn/svm/tests/test_sparse.py,190,Test that it gives proper exception on deficient input,not
scikit-learn/sklearn/svm/tests/test_sparse.py,191,impossible value of C,not
scikit-learn/sklearn/svm/tests/test_sparse.py,195,impossible value of nu,not
scikit-learn/sklearn/svm/tests/test_sparse.py,200,wrong dimensions for labels,not
scikit-learn/sklearn/svm/tests/test_sparse.py,210,Similar to test_SVC,not
scikit-learn/sklearn/svm/tests/test_sparse.py,229,Test the sparse LinearSVC with the iris dataset,not
scikit-learn/sklearn/svm/tests/test_sparse.py,241,check decision_function,not
scikit-learn/sklearn/svm/tests/test_sparse.py,245,sparsify the coefficients on both models and check that they still,not
scikit-learn/sklearn/svm/tests/test_sparse.py,246,produce the same results,not
scikit-learn/sklearn/svm/tests/test_sparse.py,254,Test class weights,not
scikit-learn/sklearn/svm/tests/test_sparse.py,269,Test weights on individual samples,not
scikit-learn/sklearn/svm/tests/test_sparse.py,280,Test that sparse liblinear honours intercept_scaling param,not
scikit-learn/sklearn/svm/tests/test_sparse.py,288,Check that sparse OneClassSVM gives the same result as dense OneClassSVM,not
scikit-learn/sklearn/svm/tests/test_sparse.py,289,many class dataset:,not
scikit-learn/sklearn/svm/tests/test_sparse.py,302,Test on a subset from the 20newsgroups dataset.,not
scikit-learn/sklearn/svm/tests/test_sparse.py,303,This catches some bugs if input is not correctly converted into,not
scikit-learn/sklearn/svm/tests/test_sparse.py,304,sparse format or weights are not correctly initialized.,not
scikit-learn/sklearn/svm/tests/test_sparse.py,331,"Test that the ""dense_fit"" is called even though we use sparse input",not
scikit-learn/sklearn/svm/tests/test_sparse.py,332,meaning that everything works fine.,not
scikit-learn/sklearn/svm/tests/test_sparse.py,345,b.decision_function(X_sp)  # XXX : should be supported,SATD
scikit-learn/sklearn/mixture/_bayesian_mixture.py,2,Author: Wei Xue <xuewei4d@gmail.com>,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,3,Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,58,To simplify the computation we have removed the np.log(np.pi) term,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,449,spherical case,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,481,For dirichlet process weight_concentration will be a tuple,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,482,containing the two parameters of the beta distribution,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,488,case Variationnal Gaussian mixture with dirichlet distribution,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,545,"Warning : in some Bishop book, there is a typo on the formula 10.63",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,546,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk` is,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,547,the correct formula,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,560,"Contrary to the original bishop book, we normalize the covariances",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,579,"Warning : in some Bishop book, there is a typo on the formula 10.63",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,580,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk`,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,581,is the correct formula,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,591,"Contrary to the original bishop book, we normalize the covariances",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,609,"Warning : in some Bishop book, there is a typo on the formula 10.63",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,610,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk`,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,611,is the correct formula,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,620,"Contrary to the original bishop book, we normalize the covariances",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,638,"Warning : in some Bishop book, there is a typo on the formula 10.63",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,639,`degrees_of_freedom_k = degrees_of_freedom_0 + Nk`,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,640,is the correct formula,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,649,"Contrary to the original bishop book, we normalize the covariances",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,680,case Variationnal Gaussian mixture with dirichlet distribution,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,686,We remove `n_features * np.log(self.degrees_of_freedom_)` because,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,687,the precision matrix is normalized,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,721,"Contrary to the original formula, we have done some simplification",not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,722,and removed all the constant terms.,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,725,We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,726,because the precision matrix is normalized.,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,759,Weights computation,not
scikit-learn/sklearn/mixture/_bayesian_mixture.py,772,Precisions matrices computation,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,3,Author: Wei Xue <xuewei4d@gmail.com>,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,4,Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,17,,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,18,Gaussian mixture shape checkers used by the GaussianMixture class,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,39,check range,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,46,check normalization,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,139,,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,140,Gaussian mixture parameters estimators (used by the M-Step),not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,338,,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,339,Gaussian mixture probability estimators,not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,404,det(precision_chol) is half of det(precision),not
scikit-learn/sklearn/mixture/_gaussian_mixture.py,699,Attributes computation,not
scikit-learn/sklearn/mixture/_base.py,3,Author: Wei Xue <xuewei4d@gmail.com>,not
scikit-learn/sklearn/mixture/_base.py,4,Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/mixture/_base.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/mixture/_base.py,119,Check all the parameters values of the derived class,not
scikit-learn/sklearn/mixture/_base.py,224,"if we enable warm_start, we will have a unique initialisation",not
scikit-learn/sklearn/mixture/_base.py,275,Always do a final e-step to guarantee that the labels returned by,not
scikit-learn/sklearn/mixture/_base.py,276,fit_predict(X) are always consistent with fit(X).predict(X),not
scikit-learn/sklearn/mixture/_base.py,277,for any value of max_iter and tol (and any random_state).,not
scikit-learn/sklearn/mixture/_base.py,506,ignore underflow,not
scikit-learn/sklearn/mixture/tests/test_mixture.py,1,Author: Guillaume Lemaitre <g.lemaitre58@gmail.com>,not
scikit-learn/sklearn/mixture/tests/test_mixture.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/mixture/tests/test_mixture.py,17,check that n_iter is the number of iteration performed.,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1,Author: Wei Xue <xuewei4d@gmail.com>,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,2,Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,102,test bad parameters,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,113,"covariance_type should be in [spherical, diag, tied, full]",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,158,test good parameters,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,208,Check bad shape,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,217,Check bad range,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,227,Check bad normalization,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,237,Check good weights matrix,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,253,Check means bad shape,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,260,Check good means matrix,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,273,Define the bad precisions for each covariance_type,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,280,Define not positive-definite precisions,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,303,Check precisions with bad shapes,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,310,Check not positive precisions,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,317,Check the correct init of precisions_init,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,324,compare the precision matrix compute from the,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,325,EmpiricalCovariance.covariance fitted on X*sqrt(resp),not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,326,"with _sufficient_sk_full, n_components=1",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,330,"special case 1, assuming data is ""centered""",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,342,check the precision computation,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,348,"special case 2, assuming resp are all ones",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,358,check the precision computation,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,366,use equation Nk * Sk / N = S_tied,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,387,check the precision computation,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,395,test against 'full' case,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,414,check the precision computation,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,420,computing spherical covariance equals to the variance of one-dimension,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,421,"data after flattening, n_components=1",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,436,check the precision computation,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,458,We compute the cholesky decomposition of the covariance matrix,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,475,test against with _naive_lmvnpdf_diag,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,487,full covariances,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,493,diag covariances,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,498,tied,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,508,spherical,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,518,"skip tests on weighted_log_probabilities, log_weights",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,522,test whether responsibilities are normalized,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,558,Check a warning message arrive if we don't do fit,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,573,strict non-convergence,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,574,loose non-convergence,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,575,strict convergence,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,576,loose convergence,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,591,check if fit_predict(X) is equivalent to fit(X).predict(X),not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,600,"Check that fit_predict is equivalent to fit.predict, when n_init > 1",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,609,recover the ground truth,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,622,needs more data to pass the test with rtol=1e-7,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,652,"the accuracy depends on the number of data and randomness, rng",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,696,Test that multiple inits does not much worse than a single one,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,711,Test that the right number of parameters is estimated,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,724,Test all of the covariance_types return the same BIC score for,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,725,"1-dimensional, 1 component fits.",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,740,Test the aic and bic criteria,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,744,standard gaussian entropy,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,789,Assert the warm_start give the same result for the same number of iter,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,806,Assert that by using warm_start we can converge to a good solution,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,818,depending on the data there is large variability in the number of,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,819,refit necessary to converge due to the complete randomness of the,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,820,data,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,830,We check that convergence is detected when warm_start=True,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,854,Check the error message if we don't call fit,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,863,Check score value,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,871,Check if the score increase,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,885,Check the error message if we don't call fit,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,898,We check that each step of the EM without regularization improve,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,899,monotonically the training set likelihood,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,913,Do one training iteration at a time so we can make sure that the,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,914,training log likelihood increases after each iteration.,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,927,We train the GaussianMixture on degenerate data by defining two clusters,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,928,of a 0 covariance.,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,984,To sample we need that GaussianMixture is fitted,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,992,Just to make sure the class samples correctly,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1016,"Check shapes of sampled data, see",not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1017,https://github.com/scikit-learn/scikit-learn/issues/7701,not
scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py,1027,We check that by increasing the n_init number we have a better solution,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,1,Author: Wei Xue <xuewei4d@gmail.com>,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,2,Thierry Guillemot <thierry.guillemot.work@gmail.com>,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,96,Check raise message for a bad value of weight_concentration_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,107,Check correct init for a given value of weight_concentration_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,115,Check correct init for the default value of weight_concentration_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,126,Check raise message for a bad value of mean_precision_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,137,Check correct init for a given value of mean_precision_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,144,Check correct init for the default value of mean_precision_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,148,Check raise message for a bad shape of mean_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,157,Check correct init for a given value of mean_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,164,Check correct init for the default value of bemean_priorta,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,175,Check raise message for a bad value of degrees_of_freedom_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,186,Check correct init for a given value of degrees_of_freedom_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,194,Check correct init for the default value of degrees_of_freedom_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,202,Check correct init for a given value of covariance_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,217,Check raise message for a bad spherical value of covariance_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,228,Check correct init for the default value of covariance_prior,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,247,Check raise message,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,261,Case Dirichlet distribution for the weight concentration prior type,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,271,Case Dirichlet process for the weight concentration prior type,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,287,We check that each step of the each step of variational inference without,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,288,regularization improve monotonically the training set of the bound,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,301,Do one training iteration at a time so we can make sure that the,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,302,training log likelihood increases after each iteration.,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,314,We can compare the 'full' precision with the other cov_type if we apply,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,315,1 iter of the M-step (done during _initialize_parameters).,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,322,Computation of the full_covariance,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,333,"Check tied_covariance = mean(full_covariances, 0)",not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,344,Check diag_covariance = diag(full_covariances),not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,358,"Check spherical_covariance = np.mean(diag_covariances, 0)",not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,373,We check that the dot product of the covariance and the precision,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,374,matrices is identity.,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,379,Computation of the full_covariance,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,406,We check here that adding a constant in the data change correctly the,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,407,parameters of the mixture,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,431,strict non-convergence,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,432,loose non-convergence,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,433,strict convergence,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,434,loose convergence,not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,455,"Check that fit_predict is equivalent to fit.predict, when n_init > 1",not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,464,this is the same test as test_gaussian_mixture_predict_predict_proba(),not
scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py,477,Check a warning message arrive if we don't do fit,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,1,coding=utf8,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,55,Authors: Clay Woolam <clay@woolam.org>,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,56,Utkarsh Upadhyay <mail@musicallyut.in>,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,57,License: BSD,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,116,kernel parameters,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,121,clamping factor,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,232,actual graph construction (implementations should override this),not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,235,label construction,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,236,construct a categorical distribution for classification only,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,251,initialize distributions,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,258,LabelPropagation,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,261,LabelSpreading,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,286,clamp,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,300,set the transduction item,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,501,this one has different base parameters,not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,508,compute affinity matrix (or gram matrix),not
scikit-learn/sklearn/semi_supervised/_label_propagation.py,519,set diag to 0.0,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,46,unstable test; changes in k-NN ordering break it,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,77,adopting notation from Zhou et al (2004):,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,103,adopting notation from Zhu et al 2002,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,130,This is a non-regression test for #5774,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,136,this should converge quickly:,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,142,This is a non-regression test for #5774,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,161,check that we don't divide by zero in case of null normalizer,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,162,non-regression test for,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,163,https://github.com/scikit-learn/scikit-learn/pull/15946,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,173,This is a non-regression test for #15866,not
scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py,175,Custom sparse kernel (top-K RBF),not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,2,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,3,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,4,Gael Varoquaux <gael.varoquaux@inria.fr>,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,5,,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,29,mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast',not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,30,type: ignore,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,65,"As of scipy 1.1.0, new argument copy=False by default.",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,66,This is what we want.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,77,,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,78,Paths functions,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,137,X can be touched inplace thanks to the above line,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,143,Workaround to find alpha_max for sparse matrices.,SATD
scikit-learn/sklearn/linear_model/_coordinate_descent.py,144,since we should not destroy the sparsity of such matrices.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,435,We expect X and y to be already Fortran ordered when bypassing,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,436,checks,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,443,Xy should be a 1d contiguous array or a 2D C ordered array,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,458,MultiTaskElasticNet does not support sparse matrices,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,461,As sparse matrices are not actually centered we need this,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,462,to be passed to the CD solver.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,468,X should be normalized and fit already if function is called,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,469,from ElasticNet.fit,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,475,No need to normalize of fit_intercept: it has been done,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,476,above,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,481,make sure alphas are properly ordered,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,518,We expect precompute to be already Fortran ordered when bypassing,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,519,checks,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,551,,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,552,ElasticNet model,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,751,Remember if X is copied,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,753,We expect X and y to be float64 or float32 Fortran ordered arrays,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,754,when bypassing checks,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,777,simplify things by rescaling sw to sum up to n_samples,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,778,"=> np.average(x, weights=sw) = np.mean(sw * x)",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,780,Objective function is:,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,781,"1/2 * np.average(squared error, weights=sw) + alpha * penalty",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,782,but coordinate descent minimizes:,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,783,1/2 * sum(squared error) + alpha * penalty,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,784,enet_path therefore sets alpha = n_samples * alpha,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,785,"With sw, enet_path should set alpha = sum(sw) * alpha",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,786,"Therefore, we rescale alpha = sum(sw) / n_samples * alpha",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,787,"Note: As we rescaled sample_weights to sum up to n_samples,",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,788,we don't need this,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,789,alpha *= np.sum(sample_weight) / n_samples,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,791,"Ensure copying happens only once, don't do it again if done above.",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,792,"X and y will be rescaled if sample_weight is not None, order='F'",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,793,ensures that the returned X and y are still F-contiguous.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,799,coordinate descent needs F-ordered arrays and _pre_fit might have,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,800,called _rescale_data,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,856,workaround since _set_intercept will cast self.coef_ into X.dtype,SATD
scikit-learn/sklearn/linear_model/_coordinate_descent.py,859,return self for chaining fit and predict calls,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,887,,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,888,Lasso model,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1021,,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1022,Functions for CV with paths functions,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1076,fancy indexing should create a writable copy but it doesn't,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1077,for read-only memmaps (cf. numpy#14132).,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1086,No Gram variant of multi-task exists right now.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1087,Fall back to default enet_multitask,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1105,"Do the ordering and type casting here, as if it is done in the path,",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1106,X is copied and a reference is kept here,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1113,Doing this so that it becomes coherent with multioutput.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1178,This makes sure that there is no duplication in memory.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1179,Dealing right with copy_X is important in the following:,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1180,Multiple functions touch X and subsamples of X and can induce a,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1181,lot of duplication of memory,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1187,Keep a reference to X,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1189,Let us not impose fortran ordering so far: it is,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1190,not useful for the cross-validation loop and will be done,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1191,by the model fitting itself,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1193,Need to validate separately here.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1194,We can't pass multi_ouput=True because that would allow y to be,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1195,csr. We also want to allow y to be 64 or 32 but check_X_y only,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1196,allows to convert for 64.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1205,X is a sparse matrix and has been copied,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1208,X has been copied,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1212,Need to validate separately here.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1213,We can't pass multi_ouput=True because that would allow y to be,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1214,csr. We also want to allow y to be 64 or 32 but check_X_y only,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1215,allows to convert for 64.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1249,All LinearModelCV parameters except 'cv' are acceptable,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1253,"For the first path, we need to set l1_ratio",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1269,Making sure alphas is properly ordered.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1271,We want n_alphas to be the number of alphas used for each l1_ratio.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1276,"We are not computing in parallel, we can modify X",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1277,inplace in the folds,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1281,init cross-validation generator,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1284,Compute path for all folds and compute MSE to get the best alpha,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1288,"We do a double for loop folded in one, in order to be able to",not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1289,iterate in parallel on l1_ratio and folds,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1316,Remove duplicate alphas in case alphas is provided.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1320,Refit the model with the parameters selected,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1730,,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1731,Multi Task ElasticNet and Lasso models (with joint feature selection),not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1879,Need to validate separately here.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1880,We can't pass multi_ouput=True because that would allow y to be csr.,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1912,coef contiguous in memory,not
scikit-learn/sklearn/linear_model/_coordinate_descent.py,1925,return self for chaining fit and predict calls,not
scikit-learn/sklearn/linear_model/_ridge.py,5,Author: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/linear_model/_ridge.py,6,Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>,not
scikit-learn/sklearn/linear_model/_ridge.py,7,Fabian Pedregosa <fabian@fseoane.net>,not
scikit-learn/sklearn/linear_model/_ridge.py,8,Michael Eickenberg <michael.eickenberg@nsup.org>,not
scikit-learn/sklearn/linear_model/_ridge.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_ridge.py,81,kernel ridge,not
scikit-learn/sklearn/linear_model/_ridge.py,82,w = X.T * inv(X X^t + alpha*Id) y,not
scikit-learn/sklearn/linear_model/_ridge.py,85,FIXME atol,SATD
scikit-learn/sklearn/linear_model/_ridge.py,89,old scipy,not
scikit-learn/sklearn/linear_model/_ridge.py,93,linear ridge,not
scikit-learn/sklearn/linear_model/_ridge.py,94,w = inv(X^t X + alpha*Id) * X.T y,not
scikit-learn/sklearn/linear_model/_ridge.py,98,FIXME atol,SATD
scikit-learn/sklearn/linear_model/_ridge.py,103,old scipy,not
scikit-learn/sklearn/linear_model/_ridge.py,122,"According to the lsqr documentation, alpha = damp^2.",not
scikit-learn/sklearn/linear_model/_ridge.py,136,w = inv(X^t X + alpha*Id) * X.T y,not
scikit-learn/sklearn/linear_model/_ridge.py,160,dual_coef = inv(X X^t + alpha*Id) y,not
scikit-learn/sklearn/linear_model/_ridge.py,173,"Unlike other solvers, we need to support sample_weight directly",not
scikit-learn/sklearn/linear_model/_ridge.py,174,because K might be a pre-computed kernel.,not
scikit-learn/sklearn/linear_model/_ridge.py,180,"Only one penalty, we can solve multi-target problems in one time.",not
scikit-learn/sklearn/linear_model/_ridge.py,184,Note: we must use overwrite_a=False in order to be able to,not
scikit-learn/sklearn/linear_model/_ridge.py,185,use the fall-back solution below in case a LinAlgError,not
scikit-learn/sklearn/linear_model/_ridge.py,186,is raised,not
scikit-learn/sklearn/linear_model/_ridge.py,194,K is expensive to compute and store in memory so change it back in,not
scikit-learn/sklearn/linear_model/_ridge.py,195,case it was user-given.,not
scikit-learn/sklearn/linear_model/_ridge.py,203,One penalty per target. We need to solve each target separately.,not
scikit-learn/sklearn/linear_model/_ridge.py,222,same default value as scipy.linalg.pinv,not
scikit-learn/sklearn/linear_model/_ridge.py,389,only sag supports fitting intercept directly,not
scikit-learn/sklearn/linear_model/_ridge.py,433,"SAG supports sample_weight directly. For other solvers,",not
scikit-learn/sklearn/linear_model/_ridge.py,434,we implement sample_weight via a simple rescaling.,not
scikit-learn/sklearn/linear_model/_ridge.py,437,There should be either 1 or n_targets penalties,not
scikit-learn/sklearn/linear_model/_ridge.py,467,use SVD solver if matrix is singular,not
scikit-learn/sklearn/linear_model/_ridge.py,473,use SVD solver if matrix is singular,not
scikit-learn/sklearn/linear_model/_ridge.py,477,precompute max_squared_sum for all targets,not
scikit-learn/sklearn/linear_model/_ridge.py,508,"When y was passed as a 1d-array, we flatten the coefficients.",not
scikit-learn/sklearn/linear_model/_ridge.py,538,all other solvers work at both float precision levels,not
scikit-learn/sklearn/linear_model/_ridge.py,571,when X is sparse we only remove offset from y,not
scikit-learn/sklearn/linear_model/_ridge.py,582,add the offset which was subtracted by _preprocess_data,not
scikit-learn/sklearn/linear_model/_ridge.py,587,required to fit intercept with sparse_cg solver,not
scikit-learn/sklearn/linear_model/_ridge.py,590,for dense matrices or when intercept is set to 0,not
scikit-learn/sklearn/linear_model/_ridge.py,935,we don't (yet) support multi-label classification in Ridge,not
scikit-learn/sklearn/linear_model/_ridge.py,941,modify the sample weights with the corresponding class weight,not
scikit-learn/sklearn/linear_model/_ridge.py,962,"if X has more rows than columns, use decomposition of X^T.X,",not
scikit-learn/sklearn/linear_model/_ridge.py,963,otherwise X.X^T,not
scikit-learn/sklearn/linear_model/_ridge.py,1135,"compute diagonal of the matrix: dot(Q, dot(diag(v_prime), Q^T))",not
scikit-learn/sklearn/linear_model/_ridge.py,1140,"compute dot(diag(D), B)",not
scikit-learn/sklearn/linear_model/_ridge.py,1142,handle case where B is > 1-d,not
scikit-learn/sklearn/linear_model/_ridge.py,1179,in this case centering has been done in preprocessing,not
scikit-learn/sklearn/linear_model/_ridge.py,1180,or we are not fitting an intercept.,not
scikit-learn/sklearn/linear_model/_ridge.py,1183,X is sparse,not
scikit-learn/sklearn/linear_model/_ridge.py,1225,in this case centering has been done in preprocessing,not
scikit-learn/sklearn/linear_model/_ridge.py,1226,or we are not fitting an intercept.,not
scikit-learn/sklearn/linear_model/_ridge.py,1229,this function only gets called for sparse X,not
scikit-learn/sklearn/linear_model/_ridge.py,1281,if X is dense it has already been centered in preprocessing,not
scikit-learn/sklearn/linear_model/_ridge.py,1284,"to emulate centering X with sample weights,",not
scikit-learn/sklearn/linear_model/_ridge.py,1285,"ie removing the weighted average, we add a column",not
scikit-learn/sklearn/linear_model/_ridge.py,1286,containing the square roots of the sample weights.,not
scikit-learn/sklearn/linear_model/_ridge.py,1287,"by centering, it is orthogonal to the other columns",not
scikit-learn/sklearn/linear_model/_ridge.py,1300,the vector containing the square roots of the sample weights (1,not
scikit-learn/sklearn/linear_model/_ridge.py,1301,when no sample weights) is the eigenvector of XX^T which,not
scikit-learn/sklearn/linear_model/_ridge.py,1302,corresponds to the intercept; we cancel the regularization on,not
scikit-learn/sklearn/linear_model/_ridge.py,1303,this dimension. the corresponding eigenvalue is,not
scikit-learn/sklearn/linear_model/_ridge.py,1304,sum(sample_weight).,not
scikit-learn/sklearn/linear_model/_ridge.py,1307,cancel regularization for the intercept,not
scikit-learn/sklearn/linear_model/_ridge.py,1311,handle case where y is 2-d,not
scikit-learn/sklearn/linear_model/_ridge.py,1325,"to emulate centering X with sample weights,",not
scikit-learn/sklearn/linear_model/_ridge.py,1326,"ie removing the weighted average, we add a column",not
scikit-learn/sklearn/linear_model/_ridge.py,1327,containing the square roots of the sample weights.,not
scikit-learn/sklearn/linear_model/_ridge.py,1328,"by centering, it is orthogonal to the other columns",not
scikit-learn/sklearn/linear_model/_ridge.py,1329,when all samples have the same weight we add a column of 1,not
scikit-learn/sklearn/linear_model/_ridge.py,1336,remove eigenvalues and vectors in the null space of X^T.X,not
scikit-learn/sklearn/linear_model/_ridge.py,1354,handle case where y is 2-d,not
scikit-learn/sklearn/linear_model/_ridge.py,1366,"the vector [0, 0, ..., 0, 1]",not
scikit-learn/sklearn/linear_model/_ridge.py,1367,is the eigenvector of X^TX which,not
scikit-learn/sklearn/linear_model/_ridge.py,1368,corresponds to the intercept; we cancel the regularization on,not
scikit-learn/sklearn/linear_model/_ridge.py,1369,this dimension. the corresponding eigenvalue is,not
scikit-learn/sklearn/linear_model/_ridge.py,1370,"sum(sample_weight), e.g. n when uniform sample weights.",not
scikit-learn/sklearn/linear_model/_ridge.py,1377,add a column to X containing the square roots of sample weights,not
scikit-learn/sklearn/linear_model/_ridge.py,1382,"return (1 - hat_diag), (y - y_hat)",not
scikit-learn/sklearn/linear_model/_ridge.py,1384,handle case where y is 2-d,not
scikit-learn/sklearn/linear_model/_ridge.py,1402,X already centered,not
scikit-learn/sklearn/linear_model/_ridge.py,1405,"to emulate fit_intercept=True situation, add a column",not
scikit-learn/sklearn/linear_model/_ridge.py,1406,containing the square roots of the sample weights,not
scikit-learn/sklearn/linear_model/_ridge.py,1407,"by centering, the other columns are orthogonal to that one",not
scikit-learn/sklearn/linear_model/_ridge.py,1424,detect intercept column,not
scikit-learn/sklearn/linear_model/_ridge.py,1427,cancel the regularization for the intercept,not
scikit-learn/sklearn/linear_model/_ridge.py,1432,handle case where y is 2-d,not
scikit-learn/sklearn/linear_model/_ridge.py,1901,modify the sample weights with the corresponding class weight,not
scikit-learn/sklearn/linear_model/_logistic.py,5,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/linear_model/_logistic.py,6,Fabian Pedregosa <f@bianp.net>,not
scikit-learn/sklearn/linear_model/_logistic.py,7,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/linear_model/_logistic.py,8,Manoj Kumar <manojkumarsivaraj334@gmail.com>,not
scikit-learn/sklearn/linear_model/_logistic.py,9,Lars Buitinck,not
scikit-learn/sklearn/linear_model/_logistic.py,10,Simon Wu <s8wu@uwaterloo.ca>,not
scikit-learn/sklearn/linear_model/_logistic.py,11,Arthur Mensch <arthur.mensch@m4x.org,not
scikit-learn/sklearn/linear_model/_logistic.py,45,.. some helper functions for logistic_regression_path ..,not
scikit-learn/sklearn/linear_model/_logistic.py,121,Logistic loss is the negative of the log of the logistic function.,not
scikit-learn/sklearn/linear_model/_logistic.py,129,Case where we fit the intercept.,not
scikit-learn/sklearn/linear_model/_logistic.py,166,Logistic loss is the negative of the log of the logistic function.,not
scikit-learn/sklearn/linear_model/_logistic.py,215,Case where we fit the intercept.,not
scikit-learn/sklearn/linear_model/_logistic.py,219,The mat-vec product of the Hessian,not
scikit-learn/sklearn/linear_model/_logistic.py,225,Precompute as much as possible,not
scikit-learn/sklearn/linear_model/_logistic.py,229,Calculate the double derivative with respect to intercept,not
scikit-learn/sklearn/linear_model/_logistic.py,230,In the case of sparse matrices this returns a matrix object.,not
scikit-learn/sklearn/linear_model/_logistic.py,238,For the fit intercept case.,not
scikit-learn/sklearn/linear_model/_logistic.py,399,`loss` is unused. Refactoring to avoid computing it does not,not
scikit-learn/sklearn/linear_model/_logistic.py,400,significantly speed up the computation and decreases readability,not
scikit-learn/sklearn/linear_model/_logistic.py,404,Hessian-vector product derived by applying the R-operator on the gradient,not
scikit-learn/sklearn/linear_model/_logistic.py,405,of the multinomial loss function.,not
scikit-learn/sklearn/linear_model/_logistic.py,413,r_yhat holds the result of applying the R-operator on the multinomial,not
scikit-learn/sklearn/linear_model/_logistic.py,414,estimator.,not
scikit-learn/sklearn/linear_model/_logistic.py,636,Preprocessing.,not
scikit-learn/sklearn/linear_model/_logistic.py,651,np.unique(y) gives labels in sorted order.,not
scikit-learn/sklearn/linear_model/_logistic.py,654,"If sample weights exist, convert them to array (support for lists)",not
scikit-learn/sklearn/linear_model/_logistic.py,655,and check length,not
scikit-learn/sklearn/linear_model/_logistic.py,656,Otherwise set them to 1 for all examples,not
scikit-learn/sklearn/linear_model/_logistic.py,660,"If class_weights is a dict (provided by the user), the weights",not
scikit-learn/sklearn/linear_model/_logistic.py,661,"are assigned to the original labels. If it is ""balanced"", then",not
scikit-learn/sklearn/linear_model/_logistic.py,662,the class_weights are assigned after masking the labels with a OvR.,not
scikit-learn/sklearn/linear_model/_logistic.py,668,"For doing a ovr, we need to mask the labels first. for the",not
scikit-learn/sklearn/linear_model/_logistic.py,669,multinomial case this is not necessary.,not
scikit-learn/sklearn/linear_model/_logistic.py,676,for compute_class_weight,not
scikit-learn/sklearn/linear_model/_logistic.py,690,"SAG multinomial solver needs LabelEncoder, not LabelBinarizer",not
scikit-learn/sklearn/linear_model/_logistic.py,698,it must work both giving the bias term and not,not
scikit-learn/sklearn/linear_model/_logistic.py,706,"For binary problems coef.shape[0] should be 1, otherwise it",not
scikit-learn/sklearn/linear_model/_logistic.py,707,should be classes.size.,not
scikit-learn/sklearn/linear_model/_logistic.py,727,scipy.optimize.minimize and newton-cg accepts only,not
scikit-learn/sklearn/linear_model/_logistic.py,728,ravelled parameters.,not
scikit-learn/sklearn/linear_model/_logistic.py,784,"alpha is for L2-norm, beta is for L1-norm",not
scikit-learn/sklearn/linear_model/_logistic.py,791,Elastic-Net penalty,not
scikit-learn/sklearn/linear_model/_logistic.py,819,helper function for LogisticCV,not
scikit-learn/sklearn/linear_model/_logistic.py,974,The score method of Logistic Regression has a classes_ attribute.,not
scikit-learn/sklearn/linear_model/_logistic.py,1317,default values,not
scikit-learn/sklearn/linear_model/_logistic.py,1322,Note that check for l1_ratio is done right above,not
scikit-learn/sklearn/linear_model/_logistic.py,1390,Hack so that we iterate only once for the multinomial case.,SATD
scikit-learn/sklearn/linear_model/_logistic.py,1399,The SAG solver releases the GIL so it's more efficient to use,not
scikit-learn/sklearn/linear_model/_logistic.py,1400,threads for this solver.,not
scikit-learn/sklearn/linear_model/_logistic.py,1470,"Workaround for multi_class=""multinomial"" and binary outcomes",SATD
scikit-learn/sklearn/linear_model/_logistic.py,1471,which requires softmax prediction with only a 1D decision.,not
scikit-learn/sklearn/linear_model/_logistic.py,1822,Encode for string labels,not
scikit-learn/sklearn/linear_model/_logistic.py,1829,The original class labels,not
scikit-learn/sklearn/linear_model/_logistic.py,1841,init cross-validation generator,not
scikit-learn/sklearn/linear_model/_logistic.py,1845,Use the label encoded classes,not
scikit-learn/sklearn/linear_model/_logistic.py,1854,OvR in case of binary problems is as good as fitting,not
scikit-learn/sklearn/linear_model/_logistic.py,1855,the higher label,not
scikit-learn/sklearn/linear_model/_logistic.py,1860,"We need this hack to iterate only once over labels, in the case of",SATD
scikit-learn/sklearn/linear_model/_logistic.py,1861,"multi_class = multinomial, without changing the value of the labels.",not
scikit-learn/sklearn/linear_model/_logistic.py,1868,compute the class weights for the entire dataset y,not
scikit-learn/sklearn/linear_model/_logistic.py,1877,The SAG solver releases the GIL so it's more efficient to use,not
scikit-learn/sklearn/linear_model/_logistic.py,1878,threads for this solver.,not
scikit-learn/sklearn/linear_model/_logistic.py,1902,_log_reg_scoring_path will output different shapes depending on the,not
scikit-learn/sklearn/linear_model/_logistic.py,1903,"multi_class param, so we need to reshape the outputs accordingly.",not
scikit-learn/sklearn/linear_model/_logistic.py,1904,"Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the",not
scikit-learn/sklearn/linear_model/_logistic.py,1905,"rows are equal, so we just take the first one.",not
scikit-learn/sklearn/linear_model/_logistic.py,1906,"After reshaping,",not
scikit-learn/sklearn/linear_model/_logistic.py,1907,"- scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)",not
scikit-learn/sklearn/linear_model/_logistic.py,1908,- coefs_paths is of shape,not
scikit-learn/sklearn/linear_model/_logistic.py,1909,"(n_classes, n_folds, n_Cs . n_l1_ratios, n_features)",not
scikit-learn/sklearn/linear_model/_logistic.py,1910,- n_iter is of shape,not
scikit-learn/sklearn/linear_model/_logistic.py,1911,"(n_classes, n_folds, n_Cs . n_l1_ratios) or",not
scikit-learn/sklearn/linear_model/_logistic.py,1912,"(1, n_folds, n_Cs . n_l1_ratios)",not
scikit-learn/sklearn/linear_model/_logistic.py,1920,"equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),",not
scikit-learn/sklearn/linear_model/_logistic.py,1921,"(1, 2, 0, 3))",not
scikit-learn/sklearn/linear_model/_logistic.py,1928,repeat same scores across all classes,not
scikit-learn/sklearn/linear_model/_logistic.py,1955,"For multinomial, all scores are the same across classes",not
scikit-learn/sklearn/linear_model/_logistic.py,1957,coefs_paths will keep its original shape because,not
scikit-learn/sklearn/linear_model/_logistic.py,1958,logistic_regression_path expects it this way,not
scikit-learn/sklearn/linear_model/_logistic.py,1961,best_index is between 0 and (n_Cs . n_l1_ratios - 1),not
scikit-learn/sklearn/linear_model/_logistic.py,1962,"for example, with n_cs=2 and n_l1_ratios=3",not
scikit-learn/sklearn/linear_model/_logistic.py,1963,the layout of scores is,not
scikit-learn/sklearn/linear_model/_logistic.py,1964,"[c1, c2, c1, c2, c1, c2]",not
scikit-learn/sklearn/linear_model/_logistic.py,1965,"l1_1 ,  l1_2 ,  l1_3",not
scikit-learn/sklearn/linear_model/_logistic.py,1982,Note that y is label encoded and hence pos_class must be,not
scikit-learn/sklearn/linear_model/_logistic.py,1983,the encoded label / None (for 'multinomial'),not
scikit-learn/sklearn/linear_model/_logistic.py,1999,Take the best scores across every fold and the average of,not
scikit-learn/sklearn/linear_model/_logistic.py,2000,all coefficients corresponding to the best scores.,not
scikit-learn/sklearn/linear_model/_logistic.py,2032,"if elasticnet was used, add the l1_ratios dimension to some",not
scikit-learn/sklearn/linear_model/_logistic.py,2033,attributes,not
scikit-learn/sklearn/linear_model/_logistic.py,2035,with n_cs=2 and n_l1_ratios=3,not
scikit-learn/sklearn/linear_model/_logistic.py,2036,the layout of scores is,not
scikit-learn/sklearn/linear_model/_logistic.py,2037,"[c1, c2, c1, c2, c1, c2]",not
scikit-learn/sklearn/linear_model/_logistic.py,2038,"l1_1 ,  l1_2 ,  l1_3",not
scikit-learn/sklearn/linear_model/_logistic.py,2039,To get a 2d array with the following layout,not
scikit-learn/sklearn/linear_model/_logistic.py,2040,"l1_1, l1_2, l1_3",not
scikit-learn/sklearn/linear_model/_logistic.py,2041,"c1 [[ .  ,  .  ,  .  ],",not
scikit-learn/sklearn/linear_model/_logistic.py,2042,"c2  [ .  ,  .  ,  .  ]]",not
scikit-learn/sklearn/linear_model/_logistic.py,2043,We need to first reshape and then transpose.,not
scikit-learn/sklearn/linear_model/_logistic.py,2044,The same goes for the other arrays,not
scikit-learn/sklearn/linear_model/_ransac.py,1,coding: utf-8,not
scikit-learn/sklearn/linear_model/_ransac.py,3,Author: Johannes Schönberger,not
scikit-learn/sklearn/linear_model/_ransac.py,4,,not
scikit-learn/sklearn/linear_model/_ransac.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_ransac.py,254,Need to validate separately here.,not
scikit-learn/sklearn/linear_model/_ransac.py,255,We can't pass multi_ouput=True because that would allow y to be csr.,not
scikit-learn/sklearn/linear_model/_ransac.py,268,assume linear model by default,not
scikit-learn/sklearn/linear_model/_ransac.py,288,MAD (median absolute deviation),not
scikit-learn/sklearn/linear_model/_ransac.py,318,Not all estimator accept a random_state,not
scikit-learn/sklearn/linear_model/_ransac.py,344,number of data samples,not
scikit-learn/sklearn/linear_model/_ransac.py,357,choose random sample set,not
scikit-learn/sklearn/linear_model/_ransac.py,363,check if random sample set is valid,not
scikit-learn/sklearn/linear_model/_ransac.py,369,fit model for current random sample set,not
scikit-learn/sklearn/linear_model/_ransac.py,376,check if estimated model is valid,not
scikit-learn/sklearn/linear_model/_ransac.py,382,residuals of all data for current random sample model,not
scikit-learn/sklearn/linear_model/_ransac.py,386,classify data into inliers and outliers,not
scikit-learn/sklearn/linear_model/_ransac.py,390,less inliers -> skip current random sample,not
scikit-learn/sklearn/linear_model/_ransac.py,395,extract inlier data set,not
scikit-learn/sklearn/linear_model/_ransac.py,400,score of inlier data set,not
scikit-learn/sklearn/linear_model/_ransac.py,404,same number of inliers but worse score -> skip current random,not
scikit-learn/sklearn/linear_model/_ransac.py,405,sample,not
scikit-learn/sklearn/linear_model/_ransac.py,410,save current random sample as best sample,SATD
scikit-learn/sklearn/linear_model/_ransac.py,423,break if sufficient number of inliers or score is reached,not
scikit-learn/sklearn/linear_model/_ransac.py,428,if none of the iterations met the required criteria,not
scikit-learn/sklearn/linear_model/_ransac.py,453,estimate final model using all inliers,not
scikit-learn/sklearn/linear_model/_sag.py,3,Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,not
scikit-learn/sklearn/linear_model/_sag.py,4,,not
scikit-learn/sklearn/linear_model/_sag.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_sag.py,70,inverse Lipschitz constant for squared loss,not
scikit-learn/sklearn/linear_model/_sag.py,76,SAGA theoretical step size is 1/3L or 1 / (2 * (L + mu n)),not
scikit-learn/sklearn/linear_model/_sag.py,77,See Defazio et al. 2014,not
scikit-learn/sklearn/linear_model/_sag.py,81,SAG theoretical step size is 1/16L but it is recommended to use 1 / L,not
scikit-learn/sklearn/linear_model/_sag.py,82,"see http://www.birs.ca//workshops//2014/14w5003/files/schmidt.pdf,",not
scikit-learn/sklearn/linear_model/_sag.py,83,slide 65,not
scikit-learn/sklearn/linear_model/_sag.py,237,Ridge default max_iter is None,not
scikit-learn/sklearn/linear_model/_sag.py,247,"As in SGD, the alpha is scaled by n_samples.",not
scikit-learn/sklearn/linear_model/_sag.py,251,"if loss == 'multinomial', y should be label encoded.",not
scikit-learn/sklearn/linear_model/_sag.py,254,initialization,not
scikit-learn/sklearn/linear_model/_sag.py,260,assume fit_intercept is False,not
scikit-learn/sklearn/linear_model/_sag.py,264,coef_init contains possibly the intercept_init at the end.,not
scikit-learn/sklearn/linear_model/_sag.py,265,"Note that Ridge centers the data before fitting, so fit_intercept=False.",not
scikit-learn/sklearn/linear_model/_least_angle.py,5,Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/linear_model/_least_angle.py,6,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/linear_model/_least_angle.py,7,Gael Varoquaux,not
scikit-learn/sklearn/linear_model/_least_angle.py,8,,not
scikit-learn/sklearn/linear_model/_least_angle.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_least_angle.py,22,mypy error: Module 'sklearn.utils' has no attribute 'arrayfuncs',not
scikit-learn/sklearn/linear_model/_least_angle.py,23,type: ignore,not
scikit-learn/sklearn/linear_model/_least_angle.py,415,force copy. setting the array to be fortran-ordered,not
scikit-learn/sklearn/linear_model/_least_angle.py,416,speeds up the calculation of the (partial) Gram matrix,not
scikit-learn/sklearn/linear_model/_least_angle.py,417,and allows to easily swap columns,not
scikit-learn/sklearn/linear_model/_least_angle.py,442,better ideas?,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,446,holds the sign of covariance,not
scikit-learn/sklearn/linear_model/_least_angle.py,450,will hold the cholesky factorization. Only lower part is,not
scikit-learn/sklearn/linear_model/_least_angle.py,451,referenced.,not
scikit-learn/sklearn/linear_model/_least_angle.py,467,to avoid division by 0 warning,not
scikit-learn/sklearn/linear_model/_least_angle.py,497,early stopping,not
scikit-learn/sklearn/linear_model/_least_angle.py,499,interpolation factor 0 <= ss < 1,not
scikit-learn/sklearn/linear_model/_least_angle.py,501,"In the first iteration, all alphas are zero, the formula",not
scikit-learn/sklearn/linear_model/_least_angle.py,502,below would make ss a NaN,not
scikit-learn/sklearn/linear_model/_least_angle.py,515,,not
scikit-learn/sklearn/linear_model/_least_angle.py,516,Append x_j to the Cholesky factorization of (Xa * Xa'),not
scikit-learn/sklearn/linear_model/_least_angle.py,517,,not
scikit-learn/sklearn/linear_model/_least_angle.py,518,( L   0 ),not
scikit-learn/sklearn/linear_model/_least_angle.py,519,"L  ->  (       )  , where L * w = Xa' x_j",not
scikit-learn/sklearn/linear_model/_least_angle.py,520,( w   z )    and z = ||x_j||,not
scikit-learn/sklearn/linear_model/_least_angle.py,521,,not
scikit-learn/sklearn/linear_model/_least_angle.py,522,,not
scikit-learn/sklearn/linear_model/_least_angle.py,533,remove Cov[0],not
scikit-learn/sklearn/linear_model/_least_angle.py,541,swap does only work inplace if matrix is fortran,not
scikit-learn/sklearn/linear_model/_least_angle.py,542,contiguous ...,not
scikit-learn/sklearn/linear_model/_least_angle.py,548,Update the cholesky decomposition for the Gram matrix,not
scikit-learn/sklearn/linear_model/_least_angle.py,561,The system is becoming too ill-conditioned.,not
scikit-learn/sklearn/linear_model/_least_angle.py,562,We have degenerate vectors in our active set.,not
scikit-learn/sklearn/linear_model/_least_angle.py,563,We'll 'drop for good' the last regressor added.,not
scikit-learn/sklearn/linear_model/_least_angle.py,565,Note: this case is very rare. It is no longer triggered by,not
scikit-learn/sklearn/linear_model/_least_angle.py,566,the test suite. The `equality_tolerance` margin added in 0.16,not
scikit-learn/sklearn/linear_model/_least_angle.py,567,to get early stopping to work consistently on all versions of,not
scikit-learn/sklearn/linear_model/_least_angle.py,568,Python including 32 bit Python under Windows seems to make it,not
scikit-learn/sklearn/linear_model/_least_angle.py,569,very difficult to trigger the 'drop for good' strategy.,not
scikit-learn/sklearn/linear_model/_least_angle.py,579,XXX: need to figure a 'drop for good' way,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,593,alpha is increasing. This is because the updates of Cov are,not
scikit-learn/sklearn/linear_model/_least_angle.py,594,bringing in too much numerical error that is greater than,not
scikit-learn/sklearn/linear_model/_least_angle.py,595,than the remaining correlation with the,not
scikit-learn/sklearn/linear_model/_least_angle.py,596,regressors. Time to bail out,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,606,least squares solution,not
scikit-learn/sklearn/linear_model/_least_angle.py,612,This happens because sign_active[:n_active] = 0,not
scikit-learn/sklearn/linear_model/_least_angle.py,616,is this really needed ?,not
scikit-learn/sklearn/linear_model/_least_angle.py,620,L is too ill-conditioned,not
scikit-learn/sklearn/linear_model/_least_angle.py,634,equiangular direction of variables in the active set,not
scikit-learn/sklearn/linear_model/_least_angle.py,636,correlation between each unactive variables and,not
scikit-learn/sklearn/linear_model/_least_angle.py,637,eqiangular vector,not
scikit-learn/sklearn/linear_model/_least_angle.py,640,"if huge number of features, this takes 50% of time, I",not
scikit-learn/sklearn/linear_model/_least_angle.py,641,think could be avoided if we just update it using an,not
scikit-learn/sklearn/linear_model/_least_angle.py,642,orthogonal (QR) decomposition of X,not
scikit-learn/sklearn/linear_model/_least_angle.py,653,TODO: better names for these variables: z,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,658,some coefficients have changed sign,not
scikit-learn/sklearn/linear_model/_least_angle.py,661,"update the sign, important for LAR",not
scikit-learn/sklearn/linear_model/_least_angle.py,673,resize the coefs and alphas array,not
scikit-learn/sklearn/linear_model/_least_angle.py,682,mimic the effect of incrementing n_iter on the array references,not
scikit-learn/sklearn/linear_model/_least_angle.py,689,update correlations,not
scikit-learn/sklearn/linear_model/_least_angle.py,692,See if any coefficient has changed sign,not
scikit-learn/sklearn/linear_model/_least_angle.py,695,handle the case when idx is not length of 1,not
scikit-learn/sklearn/linear_model/_least_angle.py,700,handle the case when idx is not length of 1,not
scikit-learn/sklearn/linear_model/_least_angle.py,704,propagate dropped variable,not
scikit-learn/sklearn/linear_model/_least_angle.py,708,yeah this is stupid,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,711,TODO: this could be updated,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,724,Cov_n = Cov_j + x_j * X + increment(betas) TODO:,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,725,will this still work with multiple drops ?,not
scikit-learn/sklearn/linear_model/_least_angle.py,727,recompute covariance. Probably could be done better,not
scikit-learn/sklearn/linear_model/_least_angle.py,728,wrong as Xy is not swapped with the rest of variables,not
scikit-learn/sklearn/linear_model/_least_angle.py,730,TODO: this could be updated,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,735,just to maintain size,not
scikit-learn/sklearn/linear_model/_least_angle.py,741,resize coefs in case of early stop,not
scikit-learn/sklearn/linear_model/_least_angle.py,756,,not
scikit-learn/sklearn/linear_model/_least_angle.py,757,Estimator classes,not
scikit-learn/sklearn/linear_model/_least_angle.py,970,n_nonzero_coefs parametrization takes priority,not
scikit-learn/sklearn/linear_model/_least_angle.py,1136,,not
scikit-learn/sklearn/linear_model/_least_angle.py,1137,Cross-validated estimator classes,not
scikit-learn/sklearn/linear_model/_least_angle.py,1413,init cross-validation generator,not
scikit-learn/sklearn/linear_model/_least_angle.py,1416,"As we use cross-validation, the Gram matrix is not precomputed here",not
scikit-learn/sklearn/linear_model/_least_angle.py,1432,Unique also sorts,not
scikit-learn/sklearn/linear_model/_least_angle.py,1434,Take at most max_n_alphas values,not
scikit-learn/sklearn/linear_model/_least_angle.py,1457,Select the alpha that minimizes left-out error,not
scikit-learn/sklearn/linear_model/_least_angle.py,1461,Store our parameters,not
scikit-learn/sklearn/linear_model/_least_angle.py,1466,Now compute the full model,not
scikit-learn/sklearn/linear_model/_least_angle.py,1467,it will call a lasso internally when self if LassoLarsCV,not
scikit-learn/sklearn/linear_model/_least_angle.py,1468,as self.method == 'lasso',not
scikit-learn/sklearn/linear_model/_least_angle.py,1633,XXX : we don't use super().__init__,SATD
scikit-learn/sklearn/linear_model/_least_angle.py,1634,to avoid setting n_nonzero_coefs,not
scikit-learn/sklearn/linear_model/_least_angle.py,1808,AIC,not
scikit-learn/sklearn/linear_model/_least_angle.py,1810,BIC,not
scikit-learn/sklearn/linear_model/_least_angle.py,1814,residuals,not
scikit-learn/sklearn/linear_model/_least_angle.py,1818,Degrees of freedom,not
scikit-learn/sklearn/linear_model/_least_angle.py,1823,get the number of degrees of freedom equal to:,not
scikit-learn/sklearn/linear_model/_least_angle.py,1824,"Xc = X[:, mask]",not
scikit-learn/sklearn/linear_model/_least_angle.py,1825,"Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs",not
scikit-learn/sklearn/linear_model/_least_angle.py,1831,"Eqns. 2.15--16 in (Zou et al, 2007)",not
scikit-learn/sklearn/linear_model/_perceptron.py,1,Author: Mathieu Blondel,not
scikit-learn/sklearn/linear_model/_perceptron.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1,Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com> (main author),not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,2,Mathieu Blondel (partial_fit support),not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,3,,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,45,Default value of ``epsilon`` parameter.,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,56,to pass check_is_fitted,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,99,current tests expect init to do parameter validation,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,100,but we are not allowed to set attributes,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,150,raises ValueError if not registered,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,187,allocate coef_ for multi-class,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,198,allocate intercept_ for multi-class,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,209,allocate coef_ for binary problem,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,223,allocate intercept_ for binary problem,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,233,initialize average parameters,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,260,"use the full set for training, with an empty validation set",not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,291,mypy error: Decorated property not supported,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,292,type: ignore,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,298,mypy error: Decorated property not supported,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,299,type: ignore,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,307,mypy error: Decorated property not supported,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,308,type: ignore,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,314,mypy error: Decorated property not supported,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,315,type: ignore,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,407,"if average is not true, average_coef, and average_intercept will be",not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,408,unused,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,426,numpy mtrand expects a C long which is a signed 32 bit integer under,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,427,Windows,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,499,Allocate datastructures from input arguments,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,515,delegate to concrete training procedure,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,543,"labels can be encoded as float, int, or string literals",not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,544,np.unique sorts in asc order; largest class id is positive class,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,562,Clear iteration count for multiple call to fit.,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,589,need to be 2d,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,600,"intercept is a float, need to convert it to an array of length 1",not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,610,Precompute the validation split using the multiclass labels,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,611,to ensure proper balancing of the classes.,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,614,Use joblib to fit OvA in parallel.,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,615,Pick the random seed for each job outside of fit_binary to avoid,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,616,sharing the estimator random state between threads which could lead,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,617,to non-deterministic behavior,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,629,take the maximum of n_iter_ over every binary fit,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1049,"the above might assign zero to all classes, which doesn't",not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1050,normalize neatly; work around this to produce uniform,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1051,probabilities,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1058,normalize,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1137,Allocate datastructures from input arguments,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1198,Clear iteration count for multiple call to fit.,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1295,numpy mtrand expects a C long which is a signed 32 bit integer under,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1296,Windows,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1309,Not used,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1310,Not used,not
scikit-learn/sklearn/linear_model/_stochastic_gradient.py,1342,made enough updates for averaging to be taken into account,not
scikit-learn/sklearn/linear_model/_bayes.py,5,"Authors: V. Michel, F. Pedregosa, A. Gramfort",not
scikit-learn/sklearn/linear_model/_bayes.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_bayes.py,20,,not
scikit-learn/sklearn/linear_model/_bayes.py,21,BayesianRidge regression,not
scikit-learn/sklearn/linear_model/_bayes.py,204,Sample weight can be implemented via a simple rescaling.,not
scikit-learn/sklearn/linear_model/_bayes.py,211,Initialization of the values of the parameters,not
scikit-learn/sklearn/linear_model/_bayes.py,213,Add `eps` in the denominator to omit division by zero if `np.var(y)`,not
scikit-learn/sklearn/linear_model/_bayes.py,214,is zero,not
scikit-learn/sklearn/linear_model/_bayes.py,235,Convergence loop of the bayesian ridge regression,not
scikit-learn/sklearn/linear_model/_bayes.py,238,update posterior mean coef_ based on alpha_ and lambda_ and,not
scikit-learn/sklearn/linear_model/_bayes.py,239,compute corresponding rmse,not
scikit-learn/sklearn/linear_model/_bayes.py,244,compute the log marginal likelihood,not
scikit-learn/sklearn/linear_model/_bayes.py,251,"Update alpha and lambda according to (MacKay, 1992)",not
scikit-learn/sklearn/linear_model/_bayes.py,259,Check for convergence,not
scikit-learn/sklearn/linear_model/_bayes.py,268,"return regularization parameters and corresponding posterior mean,",not
scikit-learn/sklearn/linear_model/_bayes.py,269,log marginal likelihood and posterior covariance,not
scikit-learn/sklearn/linear_model/_bayes.py,276,compute the log marginal likelihood,not
scikit-learn/sklearn/linear_model/_bayes.py,284,posterior covariance is given by 1/alpha_ * scaled_sigma_,not
scikit-learn/sklearn/linear_model/_bayes.py,357,compute the log of the determinant of the posterior covariance.,not
scikit-learn/sklearn/linear_model/_bayes.py,358,posterior covariance is given by,not
scikit-learn/sklearn/linear_model/_bayes.py,359,"sigma = (lambda_ * np.eye(n_features) + alpha_ * np.dot(X.T, X))^-1",not
scikit-learn/sklearn/linear_model/_bayes.py,380,,not
scikit-learn/sklearn/linear_model/_bayes.py,381,ARD (Automatic Relevance Determination) regression,not
scikit-learn/sklearn/linear_model/_bayes.py,538,Launch the convergence loop,not
scikit-learn/sklearn/linear_model/_bayes.py,547,Initialization of the values of the parameters,not
scikit-learn/sklearn/linear_model/_bayes.py,549,Add `eps` in the denominator to omit division by zero if `np.var(y)`,not
scikit-learn/sklearn/linear_model/_bayes.py,550,is zero,not
scikit-learn/sklearn/linear_model/_bayes.py,564,Iterative procedure of ARDRegression,not
scikit-learn/sklearn/linear_model/_bayes.py,569,Update alpha and lambda,not
scikit-learn/sklearn/linear_model/_bayes.py,578,Prune the weights with a precision over a threshold,not
scikit-learn/sklearn/linear_model/_bayes.py,582,Compute the objective function,not
scikit-learn/sklearn/linear_model/_bayes.py,591,Check for convergence,not
scikit-learn/sklearn/linear_model/_bayes.py,602,update sigma and mu using updated params from the last iteration,not
scikit-learn/sklearn/linear_model/_bayes.py,616,See slides as referenced in the docstring note,not
scikit-learn/sklearn/linear_model/_bayes.py,617,this function is used when n_samples < n_features and will invert,not
scikit-learn/sklearn/linear_model/_bayes.py,618,"a matrix of shape (n_samples, n_samples) making use of the",not
scikit-learn/sklearn/linear_model/_bayes.py,619,woodbury formula:,not
scikit-learn/sklearn/linear_model/_bayes.py,620,https://en.wikipedia.org/wiki/Woodbury_matrix_identity,not
scikit-learn/sklearn/linear_model/_bayes.py,633,See slides as referenced in the docstring note,not
scikit-learn/sklearn/linear_model/_bayes.py,634,this function is used when n_samples >= n_features and will,not
scikit-learn/sklearn/linear_model/_bayes.py,635,"invert a matrix of shape (n_features, n_features)",not
scikit-learn/sklearn/linear_model/_passive_aggressive.py,1,"Authors: Rob Zinkov, Mathieu Blondel",not
scikit-learn/sklearn/linear_model/_passive_aggressive.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_theil_sen.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/linear_model/_theil_sen.py,6,Author: Florian Wilhelm <florian.wilhelm@gmail.com>,not
scikit-learn/sklearn/linear_model/_theil_sen.py,7,,not
scikit-learn/sklearn/linear_model/_theil_sen.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_theil_sen.py,59,x_old equals one of our samples,not
scikit-learn/sklearn/linear_model/_theil_sen.py,66,to avoid division by zero,not
scikit-learn/sklearn/linear_model/_theil_sen.py,114,We are computing the tol on the squared norm,not
scikit-learn/sklearn/linear_model/_theil_sen.py,183,gelss need to pad y_subpopulation to be of the max dim of X_subpopulation,not
scikit-learn/sklearn/linear_model/_theil_sen.py,327,if n_samples < n_features,not
scikit-learn/sklearn/linear_model/_theil_sen.py,374,Determine indices of subpopulation,not
scikit-learn/sklearn/linear_model/_huber.py,1,Authors: Manoj Kumar mks542@nyu.edu,not
scikit-learn/sklearn/linear_model/_huber.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_huber.py,60,Calculate the values where |y - X'w -c / sigma| > epsilon,not
scikit-learn/sklearn/linear_model/_huber.py,61,The values above this threshold are outliers.,not
scikit-learn/sklearn/linear_model/_huber.py,68,Calculate the linear loss due to the outliers.,not
scikit-learn/sklearn/linear_model/_huber.py,69,This is equal to (2 * M * |y - X'w -c / sigma| - M**2) * sigma,not
scikit-learn/sklearn/linear_model/_huber.py,74,n_sq_outliers includes the weight give to the outliers while,not
scikit-learn/sklearn/linear_model/_huber.py,75,num_outliers is just the number of outliers.,not
scikit-learn/sklearn/linear_model/_huber.py,81,Calculate the quadratic loss due to the non-outliers.-,not
scikit-learn/sklearn/linear_model/_huber.py,82,This is equal to |(y - X'w - c)**2 / sigma**2| * sigma,not
scikit-learn/sklearn/linear_model/_huber.py,93,Gradient due to the squared loss.,not
scikit-learn/sklearn/linear_model/_huber.py,98,Gradient due to the linear loss.,not
scikit-learn/sklearn/linear_model/_huber.py,107,Gradient due to the penalty.,not
scikit-learn/sklearn/linear_model/_huber.py,110,Gradient due to sigma.,not
scikit-learn/sklearn/linear_model/_huber.py,115,Gradient due to the intercept.,not
scikit-learn/sklearn/linear_model/_huber.py,274,Make sure to initialize the scale parameter to a strictly,not
scikit-learn/sklearn/linear_model/_huber.py,275,positive value:,not
scikit-learn/sklearn/linear_model/_huber.py,278,Sigma or the scale factor should be non-negative.,not
scikit-learn/sklearn/linear_model/_huber.py,279,Setting it to be zero might cause undefined bounds hence we set it,not
scikit-learn/sklearn/linear_model/_huber.py,280,to a value close to zero.,not
scikit-learn/sklearn/linear_model/__init__.py,5,See http://scikit-learn.sourceforge.net/modules/sgd.html and,not
scikit-learn/sklearn/linear_model/__init__.py,6,http://scikit-learn.sourceforge.net/modules/linear_model.html for,not
scikit-learn/sklearn/linear_model/__init__.py,7,complete documentation.,not
scikit-learn/sklearn/linear_model/setup.py,26,generate sag_fast from template,not
scikit-learn/sklearn/linear_model/setup.py,34,add other directories,not
scikit-learn/sklearn/linear_model/_base.py,5,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/linear_model/_base.py,6,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/linear_model/_base.py,7,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/linear_model/_base.py,8,Vincent Michel <vincent.michel@inria.fr>,not
scikit-learn/sklearn/linear_model/_base.py,9,Peter Prettenhofer <peter.prettenhofer@gmail.com>,not
scikit-learn/sklearn/linear_model/_base.py,10,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/linear_model/_base.py,11,Lars Buitinck,not
scikit-learn/sklearn/linear_model/_base.py,12,Maryan Morel <maryan.morel@polytechnique.edu>,not
scikit-learn/sklearn/linear_model/_base.py,13,Giorgio Patrini <giorgio.patrini@anu.edu.au>,not
scikit-learn/sklearn/linear_model/_base.py,14,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_base.py,41,TODO: bayesian_ridge_regression and bayesian_regression_ard,SATD
scikit-learn/sklearn/linear_model/_base.py,42,should be squashed into its respective objects.,not
scikit-learn/sklearn/linear_model/_base.py,45,For sparse data intercept updates are scaled by this decay factor to avoid,not
scikit-learn/sklearn/linear_model/_base.py,46,intercept oscillation.,not
scikit-learn/sklearn/linear_model/_base.py,80,seed should never be 0 in SequentialDataset64,not
scikit-learn/sklearn/linear_model/_base.py,146,TODO: f_normalize could be used here as well but the function,SATD
scikit-learn/sklearn/linear_model/_base.py,147,inplace_csr_row_normalize_l2 must be changed such that it,not
scikit-learn/sklearn/linear_model/_base.py,148,can return also the norms computed internally,not
scikit-learn/sklearn/linear_model/_base.py,150,transform variance to norm in-place,not
scikit-learn/sklearn/linear_model/_base.py,180,TODO: _rescale_data should be factored into _preprocess_data.,SATD
scikit-learn/sklearn/linear_model/_base.py,181,"Currently, the fact that sag implements its own way to deal with",not
scikit-learn/sklearn/linear_model/_base.py,182,sample_weight makes the refactoring tricky.,not
scikit-learn/sklearn/linear_model/_base.py,253,"XXX Should this derive from LinearModel? It should be a mixin, not an ABC.",SATD
scikit-learn/sklearn/linear_model/_base.py,254,Maybe the n_features checking can be moved to LinearModel.,not
scikit-learn/sklearn/linear_model/_base.py,326,"OvR normalization, like LibLinear's predict_probability",not
scikit-learn/sklearn/linear_model/_base.py,518,Sample weight can be implemented via a simple rescaling.,not
scikit-learn/sklearn/linear_model/_base.py,539,"sparse_lstsq cannot handle y with shape (M, K)",not
scikit-learn/sklearn/linear_model/_base.py,569,copy is not needed here as X is not modified inplace when X is sparse,not
scikit-learn/sklearn/linear_model/_base.py,575,copy was done in fit if necessary,not
scikit-learn/sklearn/linear_model/_base.py,588,recompute Gram,not
scikit-learn/sklearn/linear_model/_base.py,592,precompute if n_samples > n_features,not
scikit-learn/sklearn/linear_model/_base.py,597,make sure that the 'precompute' array is contiguous.,not
scikit-learn/sklearn/linear_model/_base.py,603,cannot use Xy if precompute is not Gram,not
scikit-learn/sklearn/linear_model/_base.py,608,"Xy is 1d, make sure it is contiguous.",not
scikit-learn/sklearn/linear_model/_base.py,612,Make sure that Xy is always F contiguous even if X or y are not,not
scikit-learn/sklearn/linear_model/_base.py,613,contiguous: the goal is to make it fast to extract the data for a,not
scikit-learn/sklearn/linear_model/_base.py,614,specific target.,not
scikit-learn/sklearn/linear_model/_omp.py,4,Author: Vlad Niculae,not
scikit-learn/sklearn/linear_model/_omp.py,5,,not
scikit-learn/sklearn/linear_model/_omp.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_omp.py,73,"even if we are allowed to overwrite, still copy it if bad order",not
scikit-learn/sklearn/linear_model/_omp.py,84,keeping track of swapping,not
scikit-learn/sklearn/linear_model/_omp.py,96,atom already selected or inner product too small,not
scikit-learn/sklearn/linear_model/_omp.py,101,Updates the Cholesky decomposition of X' X,not
scikit-learn/sklearn/linear_model/_omp.py,110,selected atoms are dependent,not
scikit-learn/sklearn/linear_model/_omp.py,122,solves LL'x = X'y as a composition of two triangular systems,not
scikit-learn/sklearn/linear_model/_omp.py,202,keeping track of swapping,not
scikit-learn/sklearn/linear_model/_omp.py,220,"selected same atom twice, or inner product too small",not
scikit-learn/sklearn/linear_model/_omp.py,232,selected atoms are dependent,not
scikit-learn/sklearn/linear_model/_omp.py,244,solves LL'x = X'y as a composition of two triangular systems,not
scikit-learn/sklearn/linear_model/_omp.py,353,subsequent targets will be affected,not
scikit-learn/sklearn/linear_model/_omp.py,356,default for n_nonzero_coefs is 0.1 * n_features,not
scikit-learn/sklearn/linear_model/_omp.py,357,but at least one.,not
scikit-learn/sklearn/linear_model/_omp.py,492,or subsequent target will be affected,not
scikit-learn/sklearn/linear_model/_omp.py,499,Make the copy once instead of many times in _gram_omp itself.,not
scikit-learn/sklearn/linear_model/_omp.py,660,default for n_nonzero_coefs is 0.1 * n_features,not
scikit-learn/sklearn/linear_model/_omp.py,661,but at least one.,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,1,Authors: Danny Sullivan <dbsullivan23@gmail.com>,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,2,Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,3,,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,32,this is used for sag classification,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,35,approximately equal and saves the computation of the log,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,47,this is used for sag regression,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,56,function for measuring the log loss,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,81,sparse data has a fixed decay of .01,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,88,idx = k,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,142,sparse data has a fixed decay of .01,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,149,idx = k,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,245,SAGA variance w.r.t. stream order is higher,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,405,TODO: uncomment when sparse Ridge with intercept will be fixed (#4710),SATD
scikit-learn/sklearn/linear_model/tests/test_sag.py,406,"assert_array_almost_equal(clf2.coef_.ravel(),",not
scikit-learn/sklearn/linear_model/tests/test_sag.py,407,"spweights2.ravel(),",not
scikit-learn/sklearn/linear_model/tests/test_sag.py,408,decimal=3),not
scikit-learn/sklearn/linear_model/tests/test_sag.py,409,"assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''",not
scikit-learn/sklearn/linear_model/tests/test_sag.py,416,sum the squares of the second sample because that's the largest,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,466,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,479,simple linear function with noise,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,762,test if the multinomial loss and gradient computations are consistent,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,773,compute loss and gradient like in multinomial SAG,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,778,compute loss and gradient like in multinomial LogisticRegression,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,787,comparison,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,793,"n_samples, n_features, n_classes = 4, 2, 3",not
scikit-learn/sklearn/linear_model/tests/test_sag.py,820,ground truth,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,830,"Following #13316, the error handling behavior changed in cython sag. This",not
scikit-learn/sklearn/linear_model/tests/test_sag.py,831,is simply a non-regression test to make sure numerical errors are,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,832,properly raised.,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,834,Train a classifier on a simple problem,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,840,Trigger a numerical error by:,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,841,- corrupting the fitted coefficients of the classifier,not
scikit-learn/sklearn/linear_model/tests/test_sag.py,842,- fit it again starting from its current state thanks to warm_start,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,106,Classifier can be retrained on different labels and features.,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,137,Test class weights.,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,147,we give a small weights to class 1,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,153,now the hyperplane should rotate clock-wise and,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,154,the prediction on this point should shift,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,159,partial_fit with class_weight='balanced' not supported,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,171,"Already balanced, so ""balanced"" weights should have no effect",not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,180,should be similar up to some epsilon due to learning rate schedule,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,186,ValueError due to wrong class_weight label.,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,196,ValueError due to wrong class_weight argument type.,not
scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py,270,TODO: remove in 0.25,SATD
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,15,Check that the sparse_coef property works,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,24,Check that the normalize option in enet works,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,37,Check that the sparse lasso can handle zero data without crashing,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,49,Test ElasticNet for various values of alpha and l1_ratio with list X,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,53,just a straight line,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,54,test sample,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,56,this should be the same as unregularized least squares,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,58,catch warning about alpha=0.,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,59,this is discouraged but should work.,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,82,Test ElasticNet for various values of alpha and l1_ratio with sparse X,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,84,training samples,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,87,"X[1, 0] = 0",not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,89,just a straight line (the identity function),not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,91,test samples,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,97,this should be the same as lasso,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,124,build an ill-posed linear regression problem with many noisy features and,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,125,comparatively few samples,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,127,generate a ground truth model,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,129,only the top features are impacting the model,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,135,50% of zeros in input signal,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,137,generate training ground truth labels,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,163,check the convergence is the same as the dense version,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,175,check that the coefs are sparse,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,204,check the convergence is the same as the dense version,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,210,check that the coefs are sparse,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,219,XXX: There is a bug when precompute is not None!,SATD
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,238,new params,not
scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py,243,compare with dense data,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,67,Simple sanity check on a 2 classes dataset,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,68,Make sure it predicts the correct result on simple datasets.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,82,Test for appropriate exception on errors,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,123,Cs[2] has the highest score (0.8) from MockScorer,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,126,scorer called 8 times (cv*len(Cs)),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,129,reset mock_scorer,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,165,Test logistic regression with the iris dataset,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,170,Test that both multinomial and OvR solvers handle,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,171,multiclass data correctly and give good accuracy,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,172,score (>0.95) for the training data.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,218,only 'liblinear' solver,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,223,all solvers except 'liblinear' and 'saga',not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,235,only saga supports elasticnet. We only test for liblinear because the,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,236,error is raised before for the other solvers (solver %s supports only l2,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,237,penalties),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,244,liblinear does not support penalty='none',not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,252,Test multinomial LR on a binary problem.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,273,Test multinomial LR gives expected probabilities based on the,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,274,"decision function, for a binary problem.",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,290,Test sparsify and densify members.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,313,Test that an exception is raised on inconsistent input,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,321,Wrong dimensions for training data,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,325,Wrong dimensions for test data,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,331,Test that we can write to coef_ and intercept_,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,340,Test proper NaN handling.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,341,Regression test for Issue #252: fit used to go into an infinite loop.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,349,Test that the path algorithm is consistent,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,356,can't test with fit_intercept=True since LIBLINEAR,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,357,penalizes the intercept,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,371,test for fit_intercept=True,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,392,Check that the convergence message points to both a model agnostic,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,393,advice (scaling the data) and to the logistic regression specific,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,394,documentation that includes hints on the solver configuration.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,408,random_state is relevant for liblinear solver only if dual=True,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,420,same result for same random state,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,422,different results for different random states,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,438,First check that our derivation of the grad is correct,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,445,Second check that our intercept implementation is good,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,471,First check that _logistic_grad_hess is consistent,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,472,with _logistic_loss_and_grad,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,477,Now check our hessian along the second direction of the grad,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,482,Computation of the Hessian is particularly fragile to numerical,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,483,errors when doing simple finite differences. Here we compute the,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,484,grad along a path in the direction of the vector and then use a,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,485,least-square regression to estimate the slope,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,498,Second check that our intercept implementation is good,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,508,test for LogisticRegressionCV object,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,537,no need to test for micro averaging because it,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,538,"is the same as accuracy for f1, precision,",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,539,and recall (see https://github.com/,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,540,scikit-learn/scikit-learn/pull/,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,541,11578#discussion_r203250062),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,546,test that LogisticRegressionCV uses the right score to compute its,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,547,cross-validation scores when using a multinomial scoring,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,548,see https://github.com/scikit-learn/scikit-learn/issues/8720,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,553,we use lbfgs to support multinomial,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,555,we store the params to set them further in _log_reg_scoring_path,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,568,Test with string labels for LogisticRegression(CV),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,574,"For numerical labels, let y values be taken from set (-1, 0, 1)",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,576,Test for string labels,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,593,The predictions should be in original labels,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,597,Make sure class weights can be given with string labels,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,624,Fit intercept case.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,630,Do not fit intercept. This can be considered equivalent to adding,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,631,"a feature vector of ones, i.e column of one vectors.",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,636,"In the fit_intercept=False case, the feature vector of ones is",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,637,penalized. This should be taken care of.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,640,Check gradient.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,653,Test that OvR and multinomial are correct using the iris dataset.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,657,The cv indices from stratified kfold (where stratification is done based,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,658,"on the fine-grained iris classes, i.e, before the classes 0 and 1 are",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,659,conflated) is used for both clf and clf1,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,664,Train clf on the original dataset where classes 0 and 1 are separated,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,668,Conflate classes 0 and 1 and train clf1 on this modified dataset,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,674,Ensure that what OvR learns for class2 is same regardless of whether,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,675,classes 0 and 1 are separated or not,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,680,Test the shape of various attributes.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,689,Test that for the iris data multinomial gives a better accuracy than OvR,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,701,Test attributes of LogisticRegressionCV,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,820,Test that passing sample_weight as ones is the same as,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,821,not passing them at all (default None),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,830,"Test that sample weights work the same with the lbfgs,",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,831,"newton-cg, and 'sag' solvers",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,837,ignore convergence warning due to small dataset,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,849,"Test that passing class_weight as [1,2] is the same as",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,850,"passing class weight = [1,1] but adjusting sample weights",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,851,to be 2 for all instances of class 2,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,860,Test the above for l1 penalty and l2 penalty with dual=True.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,861,since the patched liblinear code is different.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,884,helper for returning a dictionary instead of an array,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,892,Multinomial case: remove 90% of class 0,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,907,Binary case: remove 90% of class 0 and 100% of class 2,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,924,Tests for the multinomial option in logistic regression,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,926,Some basic attributes of Logistic Regression,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,935,'lbfgs' is used as a referenced,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,956,Compare solutions between lbfgs and the other solvers,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,961,Test that the path give almost the same results. However since in this,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,962,case we take the average of the coefs after fitting across all the,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,963,"folds, it need not be exactly the same.",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,984,extract first column of hessian matrix,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,989,Estimate hessian using least squares as done in,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,990,test_logistic_grad_hess,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1004,Test negative prediction when decision_function values are zero.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1005,Liblinear predicts the positive class when decision_function values,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1006,are zero. This is a test to verify that we do not do the same.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1007,See Issue: https://github.com/scikit-learn/scikit-learn/issues/3600,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1008,and the PR https://github.com/scikit-learn/scikit-learn/pull/3623,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1014,Dummy data such that the decision function becomes zero.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1020,Test LogRegCV with solver='liblinear' works for sparse matrices,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1028,Test LogRegCV with solver='liblinear' works for sparse matrices,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1036,Test that the right error message is thrown when intercept_scaling <= 0,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1048,Test that intercept_scaling is ignored when fit_intercept is False,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1056,"Because liblinear penalizes the intercept and saga does not, we do not",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1057,fit the intercept to make it possible to compare the coefficients of,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1058,the two models at convergence.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1077,Noise and constant features should be regularized to zero by the l1,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1078,penalty,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1084,"Because liblinear penalizes the intercept and saga does not, we do not",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1085,fit the intercept to make it possible to compare the coefficients of,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1086,the two models at convergence.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1107,Noise and constant features should be regularized to zero by the l1,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1108,penalty,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1112,Check that solving on the sparse and dense data yield the same results,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1123,"Test that when refit=True, logistic regression cv with the saga solver",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1124,converges to the same solution as logistic regression with a fixed,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1125,regularization parameter.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1126,Internally the LogisticRegressionCV model uses a warm start to refit on,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1127,the full data model with the optimal C found by CV. As the penalized,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1128,"logistic regression loss is convex, we should still recover exactly",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1129,the same solution as long as the stopping criterion is strict enough (and,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1130,that there are no exactly duplicated features when penalty='l1').,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1151,Predicted probabilities using the true-entropy loss should give a,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1152,smaller loss than those using the ovr method.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1161,Predicted probabilities using the soft-max function should give a,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1162,smaller loss than those using the logistic function.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1169,Test that the maximum number of iteration is reached,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1190,Test that self.n_iter_ has the correct format.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1199,OvR case,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1216,multinomial case,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1241,A 1-iteration second fit on same data should give almost same result,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1242,"with warm starting, and quite different result without warm starting.",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1243,Warm starting does not work with liblinear solver.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1284,alpha=1e-3 is time consuming,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1304,Convergence for alpha=1e-3 is very slow,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1312,Test that np.float32 input data is not cast to np.float64 when possible,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1313,and that the output is approximately the same no matter the input format.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1332,Check 32-bit type consistency,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1337,Check 32-bit type consistency with sparsity,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1342,Check 64-bit type consistency,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1347,Check 64-bit type consistency with sparsity,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1352,solver_tol bounds the norm of the loss gradient,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1353,"dw ~= inv(H)*grad ==> |dw| ~= |inv(H)| * solver_tol, where H - hessian",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1354,,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1355,See https://github.com/scikit-learn/scikit-learn/pull/13645,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1356,,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1357,"with  Z = np.hstack((np.ones((3,1)), np.array(X)))",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1358,"In [8]: np.linalg.norm(np.diag([0,2,2]) + np.linalg.inv((Z.T @ Z)/4))",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1359,Out[8]: 1.7193336918135917,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1361,factor of 2 to get the ball diameter,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1364,FIXME,SATD
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1367,Check accuracy consistency,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1371,FIXME: SAGA on sparse data fits the intercept inaccurately with the,SATD
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1372,default tol and max_iter parameters.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1380,"Test to see that the logistic regression converges on warm start,",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1381,with multi_class='multinomial'. Non-regressive test for #10836,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1401,make sure elasticnet penalty gives different coefficients from l1 and l2,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1402,with saga solver (l1_ratio different from 0 or 1),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1415,make sure coeffs differ by at least .1,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1426,Make sure elasticnet is equivalent to l1 when l1_ratio=1 and to l2 when,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1427,l1_ratio=0.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1442,Make sure that elasticnet with grid search on l1_ratio gives same or,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1443,better results than just l1 or just l2.,SATD
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1469,Check that training with a penalty matching the objective leads,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1470,to a lower objective.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1471,Here we train a logistic regression with l2 (a) and elasticnet (b),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1472,"penalties, and compute the elasticnet objective. That of a should be",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1473,greater than that of b (both objectives are convex).,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1499,make sure LogisticRegressionCV gives same best params (l1 and C) as,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1500,GridSearchCV when penalty is elasticnet,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1503,"This is actually binary classification, ovr multiclass is treated in",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1504,test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1531,make sure LogisticRegressionCV gives same best params (l1 and C) as,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1532,GridSearchCV when penalty is elasticnet and multiclass is ovr. We can't,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1533,compare best_params like in the previous test because,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1534,LogisticRegressionCV with multi_class='ovr' will have one C and one,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1535,"l1_param for each class, while LogisticRegression will share the",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1536,parameters over the *n_classes* classifiers.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1557,Check that predictions are 80% the same,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1565,Test LogisticRegressionCV attribute shapes when refit is False,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1589,Make sure the shapes of scores_ and coefs_paths_ attributes are correct,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1590,when using elasticnet (added one dimension for l1_ratios),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1652,Compare elasticnet penalty in LogisticRegression() and SGD(loss='log'),not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1672,Make sure that the returned coefs by logistic_regression_path when,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1673,multi_class='multinomial' don't override each other (used to be a,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1674,bug).,SATD
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1699,check multi_class='auto' => multi_class='ovr' iff binary y or liblinear,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1731,Make sure multi_class='ovr' is distinct from ='multinomial',not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1742,- Make sure warning is raised if penalty='none' and C is set to a,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1743,non-default value.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1744,- Make sure setting penalty='none' is equivalent to setting C=np.inf with,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1745,l2 penalty.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1776,check that we support sample_weight with liblinear in all possible cases:,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1777,"l1-primal, l2-primal, l2-dual",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1803,Non regression test for issue #14955.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1804,when penalty is elastic net the scores_ attribute has shape,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1805,"(n_classes, n_Cs, n_l1_ratios)",not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1806,We here make sure that the second dimension indeed corresponds to Cs and,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1807,the third dimension corresponds to l1_ratios.,not
scikit-learn/sklearn/linear_model/tests/test_logistic.py,1820,average over folds,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,1,Authors: Manoj Kumar mks542@nyu.edu,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,19,Generate data with outliers by replacing 10% of the samples with noise.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,24,Replace 10% of the sample with noise.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,32,Test that Ridge matches LinearRegression for large epsilon,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,50,Test that the gradient calculated by _huber_loss_and_gradient is correct,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,61,Check using optimize.check_grad that the gradients are equal.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,63,Check for both fit_intercept and otherwise.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,73,"Test sample_weights implementation in HuberRegressor""""""",not
scikit-learn/sklearn/linear_model/tests/test_huber.py,81,Rescale coefs before comparing with assert_array_almost_equal to make,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,82,sure that the number of decimal places used is somewhat insensitive to,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,83,the amplitude of the coefficients and therefore to the scale of the,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,84,data and the regularization parameter,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,108,Test sparse implementation with sample weights.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,129,Test that outliers filtering is scaling independent.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,146,Test they should converge to same coefficients for same parameters,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,150,Fit once to find out the scale parameter. Scale down X and y by scale,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,151,so that the scale parameter is optimized to 1.0,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,176,"SciPy performs the tol check after doing the coef updates, so",not
scikit-learn/sklearn/linear_model/tests/test_huber.py,177,these would be almost same but not equal.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,184,"Test that huber returns a better r2 score than non-outliers""""""",SATD
scikit-learn/sklearn/linear_model/tests/test_huber.py,193,The Ridge regressor should be influenced by the outliers and hence,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,194,give a worse score on the non-outliers as compared to the huber,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,195,regressor.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,202,The huber model should also fit poorly on the outliers.,not
scikit-learn/sklearn/linear_model/tests/test_huber.py,207,Test that it does not crash with bool data,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,23,TODO: use another dataset that has multiple drops,SATD
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,32,Principle of Lars is to keep covariances tied and decreasing,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,34,also test verbose output,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,55,no more than max_pred variables can go into the active set,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,62,"The same, with precomputed Gram matrix",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,76,no more than max_pred variables can go into the active set,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,99,Test that lars_path with no X and Gram raises exception,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,106,Test that lars_path with precomputed Gram and Xy gives the right answer,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,118,numpy deprecation,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,120,Test that Lars gives least square solution at the end,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,121,of the path,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,122,use un-normalized dataset,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,125,Avoid FutureWarning about default value change when numpy >= 1.14,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,132,numpy deprecation,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,134,Test that Lars Lasso gives least square solution at the end,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,135,of the path,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,142,Check that lars_path is robust to collinearity in input,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,153,just make sure it's bounded,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,166,Test that the ``return_path=False`` option returns the correct output,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,177,Test that the ``return_path=False`` option with Gram remains correct,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,188,Test that the ``return_path=False`` option with Gram and Xy remains,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,189,correct,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,206,Check for different values of precompute,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,218,Test when input is a singular matrix,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,226,consistency test that checks that LARS Lasso is handling rank,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,227,deficient input data (with n_features < rank) in the same way,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,228,as coordinate descent Lasso,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,238,"To be able to use the coefs to compute the objective function,",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,239,we need to turn off normalization,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,253,Test that LassoLars and Lasso using coordinate descent give the,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,254,same results.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,267,"similar test, with the classifiers",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,275,"same test, with normalized data",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,290,Test that LassoLars and Lasso using coordinate descent give the,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,291,same results when early stopping is used.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,292,"(test : before, in the middle, and in the last part of the path)",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,304,"same test, with normalization",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,316,Test that the path length of the LassoLars is right,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,322,Also check that the sequence of alphas is always decreasing,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,327,"Test lasso lars on a very ill-conditioned design, and check that",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,328,"it does not blow up, and stays somewhat close to a solution given",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,329,by the coordinate descent solver,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,330,Also test that lasso_path (using lars_path output style) gives,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,331,the same result as lars_path and previous lasso output style,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,332,under these conditions.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,335,Generate data,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,359,Create an ill-conditioned situation in which the LARS has to go,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,360,"far in the path to converge, and check that LARS and coordinate",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,361,descent give the same answers,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,362,Note it used to be the case that Lars had to use the drop for good,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,363,strategy for this but this is no longer the case with the,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,364,equality_tolerance checks,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,388,assure that at least some features get added if necessary,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,389,test for 6d2b4c,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,390,Hilbert matrix,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,402,The path should be of length 6 + 1 in a Lars going down to 6,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,403,non-zero coefs,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,409,Assure that estimators receiving multidimensional y do the right thing,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,415,regression test for gh-1615,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,436,Test the LassoLarsCV object by checking that the optimal alpha,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,437,increases as the number of samples increases.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,438,This property is not actually guaranteed in general and is just a,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,439,"property of the given dataset, with the given steps chosen.",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,459,add correlated features,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,462,Check that there is no warning in general and no ConvergenceWarning,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,463,in particular.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,464,Materialize the string representation of the warning to get a more,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,465,informative error message in case of AssertionError.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,471,Test the LassoLarsIC object by checking that,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,472,- some good features are selected.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,473,- alpha_bic > alpha_aic,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,474,- n_nonzero_bic < n_nonzero_aic,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,479,add 5 bad features,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,488,test error on unknown IC,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,494,"When using automated memory mapping on large input, the",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,495,fold data is in read-only mode,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,496,This is a non-regression test for:,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,497,https://github.com/scikit-learn/scikit-learn/issues/4597,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,500,The following should not fail despite copy=False,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,505,this is the main test for the positive parameter on the lars_path method,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,506,the estimator classes just make use of this function,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,508,we do the test on the diabetes dataset,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,510,ensure that we get negative coefficients when positive=False,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,511,and all positive when positive=True,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,512,for method 'lar' (default) and lasso,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,531,now we gonna test the positive option for all estimator classes,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,541,testing the transmissibility for the positive option of all estimator,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,542,classes in this same function here,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,560,Test that LassoLars and Lasso using coordinate descent give the,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,561,same results when using the positive option,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,563,This test is basically a copy of the above with additional positive,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,564,"option. However for the middle part, the comparison of coefficient values",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,565,"for a range of alphas, we had to make an adaptations. See below.",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,567,not normalized data,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,581,The range of alphas chosen for coefficient comparison here is restricted,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,582,as compared with the above test without the positive option. This is due,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,583,to the circumstance that the Lars-Lasso algorithm does not converge to,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,584,"the least-squares-solution for small alphas, see 'Least Angle Regression'",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,585,by Efron et al 2004. The coefficients are typically in congruence up to,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,586,the smallest alpha reached by the Lars-Lasso algorithm and start to,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,587,diverge thereafter.  See,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,588,https://gist.github.com/michigraber/7e7d7c75eca694c7a6ff,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,598,normalized data,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,604,don't include alpha=0,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,612,Test that sklearn LassoLars implementation agrees with the LassoLars,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,613,implementation available in R (lars library) under the following,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,614,scenarios:,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,615,1) fit_intercept=False and normalize=False,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,616,2) fit_intercept=True and normalize=True,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,618,Let's generate the data used in the bug report 7778,SATD
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,630,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,631,Scenario 1: Let's compare R vs sklearn when fit_intercept=False and,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,632,normalize=False,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,633,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,634,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,635,The R result was obtained using the following code:,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,636,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,637,library(lars),not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,638,"model_lasso_lars = lars(X, t(y), type=""lasso"", intercept=FALSE,",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,639,"trace=TRUE, normalize=FALSE)",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,640,r = t(model_lasso_lars$beta),not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,641,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,666,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,668,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,669,Scenario 2: Let's compare R vs sklearn when fit_intercept=True and,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,670,normalize=True,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,671,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,672,"Note: When normalize is equal to True, R returns the coefficients in",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,673,"their original units, that is, they are rescaled back, whereas sklearn",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,674,"does not do that, therefore, we need to do this step before comparing",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,675,their results.,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,676,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,677,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,678,The R result was obtained using the following code:,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,679,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,680,library(lars),not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,681,"model_lasso_lars2 = lars(X, t(y), type=""lasso"", intercept=TRUE,",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,682,"trace=TRUE, normalize=TRUE)",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,683,r2 = t(model_lasso_lars2$beta),not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,697,Let's rescale back the coefficients returned by sklearn before comparing,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,698,against the R result (read the note above),not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,704,,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,740,"Test that a small amount of jitter helps stability,",not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,741,using example provided in issue #2746,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,748,set to fit_intercept to False since target is constant and we want check,not
scikit-learn/sklearn/linear_model/tests/test_least_angle.py,749,the value of coef. coef would be all zeros otherwise.,not
scikit-learn/sklearn/linear_model/tests/test_base.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/linear_model/tests/test_base.py,2,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/linear_model/tests/test_base.py,3,,not
scikit-learn/sklearn/linear_model/tests/test_base.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_base.py,33,Test LinearRegression on a simple dataset.,not
scikit-learn/sklearn/linear_model/tests/test_base.py,34,a simple dataset,not
scikit-learn/sklearn/linear_model/tests/test_base.py,45,test it also for degenerate input,not
scikit-learn/sklearn/linear_model/tests/test_base.py,57,TODO: loop over sparse data as well,SATD
scikit-learn/sklearn/linear_model/tests/test_base.py,61,It would not work with under-determined systems,not
scikit-learn/sklearn/linear_model/tests/test_base.py,70,LinearRegression with explicit sample_weight,not
scikit-learn/sklearn/linear_model/tests/test_base.py,76,sanity checks,not
scikit-learn/sklearn/linear_model/tests/test_base.py,79,Closed form of the weighted least square,not
scikit-learn/sklearn/linear_model/tests/test_base.py,80,theta = (X^T W X)^(-1) * X^T W y,not
scikit-learn/sklearn/linear_model/tests/test_base.py,99,Sample weights must be either scalar or 1D,not
scikit-learn/sklearn/linear_model/tests/test_base.py,113,"make sure the ""OK"" sample weights actually work",not
scikit-learn/sklearn/linear_model/tests/test_base.py,120,Test assertions on betas shape.,not
scikit-learn/sklearn/linear_model/tests/test_base.py,142,Test that linear regression also works with sparse data,not
scikit-learn/sklearn/linear_model/tests/test_base.py,160,Test that linear regression agrees between sparse and dense,not
scikit-learn/sklearn/linear_model/tests/test_base.py,178,Test multiple-outcome linear regressions,not
scikit-learn/sklearn/linear_model/tests/test_base.py,194,Test multiple-outcome linear regressions with sparse data,not
scikit-learn/sklearn/linear_model/tests/test_base.py,212,restrict the pd versions < '0.24.0' as they have a bug in is_sparse func,not
scikit-learn/sklearn/linear_model/tests/test_base.py,216,Warning is raised only when some of the columns is sparse,not
scikit-learn/sklearn/linear_model/tests/test_base.py,221,all columns but the first column is sparse,not
scikit-learn/sklearn/linear_model/tests/test_base.py,231,does not warn when the whole dataframe is sparse,not
scikit-learn/sklearn/linear_model/tests/test_base.py,309,"XXX: if normalize=True, should we expect a weighted standard deviation?",SATD
scikit-learn/sklearn/linear_model/tests/test_base.py,310,"Currently not weighted, but calculated with respect to weighted mean",not
scikit-learn/sklearn/linear_model/tests/test_base.py,336,random_state not supported yet in sparse.rand,not
scikit-learn/sklearn/linear_model/tests/test_base.py,337,", random_state=rng",not
scikit-learn/sklearn/linear_model/tests/test_base.py,372,"Test output format of _preprocess_data, when input is csr",not
scikit-learn/sklearn/linear_model/tests/test_base.py,502,array,not
scikit-learn/sklearn/linear_model/tests/test_base.py,514,csr,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,1,Authors: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,2,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,73,Check that the lasso can handle zero data without crashing,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,84,Test Lasso on a toy example for various values of alpha.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,85,When validating this against glmnet notice that glmnet divides it,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,86,against nobs.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,89,just a straight line,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,90,test sample,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,122,Test ElasticNet for various parameters of alpha and l1_ratio.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,123,"Actually, the parameters alpha = 0 should not be allowed. However,",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,124,we test it as a border case.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,125,ElasticNet is tested with and without precomputed Gram matrix,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,128,just a straight line,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,129,test sample,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,131,this should be the same as lasso,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,148,with Gram,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,155,with Gram,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,199,Check that the lars and the coordinate descent implementation,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,200,select a similar alpha,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,202,for this we check that they don't fall in the grid of,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,203,clf.alphas further than 1,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,206,check that they also give a similar MSE,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,211,test set,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,237,Ensure the unconstrained fit has a negative coefficient,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,243,"On same data, constrained fit has non-negative coefficients",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,251,Test that lasso_path with lars_path style output gives the,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,252,same result,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,254,Some toy data,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,259,Use lars_path and lasso_path(new output) with 1D linear interpolation,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,260,to compute the same path,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,275,We use a large number of samples and of informative features so that,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,276,the l1_ratio selected is more toward ridge than lasso,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,281,"Here we have a small number of iterations, and thus the",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,282,ElasticNet might not converge. This is to speed up tests,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,287,"Well-conditioned settings, we should have selected our",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,288,smallest penalty,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,290,Non-sparse ground truth: we should have selected an elastic-net,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,291,that is closer to ridge than to lasso,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,299,"Well-conditioned settings, we should have selected our",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,300,smallest penalty,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,302,Non-sparse ground truth: we should have selected an elastic-net,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,303,that is closer to ridge than to lasso,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,306,We are in well-conditioned settings with low noise: we should,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,307,have a good test-set performance,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,310,Multi-output/target case,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,315,We are in well-conditioned settings with low noise: we should,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,316,have a good test-set performance,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,320,Mono-output should have same cross-validated alpha_ and l1_ratio_,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,321,in both cases.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,337,new params,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,347,do a second round with 5 iterations,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,356,just a straight line,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,364,just a straight line with negative slope,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,377,just a straight line with negative slope,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,388,Ensure the unconstrained fit has a negative coefficient,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,395,"On same data, constrained fit has non-negative coefficients",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,436,"Y_test = np.c_[y_test, y_test]",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,451,just a straight line,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,452,test sample,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,571,Precompute = 'auto' is not supported for ElasticNet,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,581,This dataset is not trivial enough for the model to converge in one pass.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,584,Check that n_iter_ is invariant to multiple calls to fit,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,585,"when warm_start=False, all else being equal.",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,590,"Fit the same model again, using a warm start: the optimizer just performs",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,591,a single pass before checking that it has already converged,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,601,Train a model to converge on a lightly regularized problem,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,605,Fitting a new model on a more regularized version of the same problem.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,606,Fitting with high regularization is easier it should converge faster,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,607,in general.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,611,"Fit the solution to the original, less regularized version of the",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,612,problem but from the solution of the highly regularized variant of,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,613,the problem as a better starting point. This should also converge,SATD
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,614,faster than the original model that starts from zero.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,622,Test that both random and cyclic selection give the same results.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,623,Ensure that the test models fully converge and check a wide,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,624,range of conditions.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,626,This uses the coordinate descent algo using the gram trick.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,635,This uses the descent algo without the gram trick,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,643,Sparse Case,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,651,Multioutput case.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,661,Raise error when selection is not in cyclic or random.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,667,Test positive parameter,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,671,For mono output,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,672,Test that the coefs returned by positive=True in enet_path are positive,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,677,"For multi output, positive parameter is not allowed",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,678,Test that an error is raised,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,684,Test that dense and sparse input give the same input for descent paths.,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,698,Check that no error is raised if data is provided in the right format,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,700,"With check_input=False, an exhaustive check is not made on y but its",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,701,dtype is still cast in _preprocess_data to X's dtype. So the test should,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,702,pass anyway,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,705,"With no input checking, providing X in C order should result in false",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,706,computation,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,731,"No copying, X is overwritten",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,760,Generate dataset,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,762,"Here we have a small number of iterations, and thus the",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,763,ElasticNet might not converge. This is to speed up tests,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,783,test precompute Gram array,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,794,test multi task enet,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,814,Test that an error message is raised if an estimator that,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,815,uses _alpha_grid is called with l1_ratio=0,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,827,Test that l1_ratio=0 is allowed if we supply a grid manually,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,856,do a second round with 5 iterations,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,885,check that the model fails to converge (a negative dual gap cannot occur),not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,889,check that the model converges w/o warnings,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,903,check that the model converges w/o warnings,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,938,make it explicit that X is int,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,964,sample_weight=np.ones(..) should be equivalent to sample_weight=None,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,971,sample_weight=None should be equivalent to sample_weight = number,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,978,"scaling of sample_weight should have no effect, cf. np.average()",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,985,setting one element of sample_weight to 0 is equivalent to removing,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,986,the corresponding sample,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,998,check that multiplying sample_weight by 2 is equivalent,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,999,to repeating corresponding samples twice,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,1033,LinearModelsCV.fit performs inplace operations on input data which is,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,1034,"memmapped when using loky backend, causing an error due to unexpected",not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,1035,behavior of fancy indexing of read-only memmaps (cf. numpy#14132).,not
scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py,1040,Create a problem sufficiently large to cause memmapping (1MB).,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,65,XXX untested as of v0.22,SATD
scikit-learn/sklearn/linear_model/tests/test_sgd.py,91,Test Data,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,93,test sample 1,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,99,test sample 2; string class labels,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,107,test sample 3,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,114,test sample 4 - two more or less redundant feature groups,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,123,test sample 5 - test sample 1 as binary classification problem,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,129,,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,130,Common Test Case to classification and regression,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,132,a simple implementation of ASGD to use for testing,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,133,uses squared loss to find the gradient,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,145,sparse data has a fixed decay of .01,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,171,Check whether expected ValueError on bad alpha,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,178,Check whether expected ValueError on bad penalty,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,186,Check whether expected ValueError on bad loss,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,191,Test that explicit warm restart...,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,202,... and implicit warm restart are equivalent.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,228,Input format tests.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,240,Test whether clone works ok.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,272,TODO: remove in 0.25,SATD
scikit-learn/sklearn/linear_model/tests/test_sgd.py,335,"Check whether expected ValueError on bad alpha, i.e. 0",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,336,since alpha is used to compute the optimal learning rate,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,393,remove shuffling,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,404,test that n_iter_ increases monotonically with n_iter_no_change,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,417,test an error is raised if the training or validation set is empty,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,423,,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,424,Classification Test Case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,428,Check that SGD gives any results :-),not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,434,"assert_almost_equal(clf.coef_[0], clf.coef_[1], decimal=7)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,440,Check whether expected ValueError on bad l1_ratio,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,446,Check whether expected ValueError on bad learning_rate,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,452,Check whether expected ValueError on bad eta0,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,459,Test parameter validity check,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,465,Test parameter validity check,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,471,Test parameter validity check,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,477,Test parameter validity check,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,483,Test parameter validity check,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,489,Checks coef_init not allowed as model argument (only fit),not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,490,Provided coef_ does not match dataset,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,496,Checks coef_init shape for the warm starts,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,497,Provided coef_ does not match dataset.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,504,Checks intercept_ shape for the warm starts,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,505,Provided intercept_ does not match dataset.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,512,Test parameter validity check,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,519,Checks intercept_ shape for the warm starts in binary case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,525,Checks the SGDClassifier correctly computes the average weights,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,540,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,556,Checks intercept_ shape consistency for the warm starts,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,557,Inconsistent intercept_ shape.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,566,Target must have at least two labels,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,573,"partial_fit with class_weight='balanced' not supported""""""",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,590,Multi-class test case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,603,Multi-class average test case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,626,Multi-class test case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,638,Multi-class test case with multi-core support,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,649,Checks coef_init and intercept_init shape for multi-class,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,650,problems,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,651,Provided coef_ does not match dataset,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,655,Provided coef_ does match dataset,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,658,Provided intercept_ does not match dataset,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,663,Provided intercept_ does match dataset.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,669,Checks that SGDClassifier predict_proba and predict_log_proba methods,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,670,can either be accessed or raise an appropriate error message,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,671,otherwise. See,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,672,https://github.com/scikit-learn/scikit-learn/issues/10938 for more,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,673,details.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,694,Check SGD.predict_proba,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,696,Hinge loss does not allow for conditional prob estimate.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,697,"We cannot use the factory here, because it defines predict_proba",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,698,anyway.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,704,log and modified_huber losses can output probability estimates,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,705,binary case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,719,log loss multiclass probability estimates,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,740,Modified Huber multiclass probability estimates; requires a separate,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,741,test because the hard zero/one probabilities may destroy the,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,742,ordering present in decision_function output.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,749,XXX the sparse test gets a different X2 (?),SATD
scikit-learn/sklearn/linear_model/tests/test_sgd.py,752,"the following sample produces decision_function values < -1,",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,753,which would cause naive normalization to fail (see comment,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,754,in SGDClassifier.predict_proba),not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,757,XXX not true in sparse test case (why?),SATD
scikit-learn/sklearn/linear_model/tests/test_sgd.py,764,Test L1 regularization,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,780,test sparsify with dense inputs,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,786,pickle and unpickle with sparse coef_,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,795,Test class weights.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,805,we give a small weights to class 1,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,810,now the hyperplane should rotate clock-wise and,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,811,the prediction on this point should shift,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,817,Test if equal class weights approx. equals no class weights.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,829,should be similar up to some epsilon due to learning rate schedule,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,835,ValueError due to not existing class label.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,842,ValueError due to wrong class_weight argument type.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,849,Tests that class_weight and sample_weight are multiplicative,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,868,"Test class weights for imbalanced data""""""",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,869,compute reference metrics on iris dataset that is quite balanced by,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,870,default,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,883,make the same prediction using balanced class_weight,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,890,Make sure that in the balanced case it does not change anything,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,891,"to use ""balanced""",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,894,build an very very imbalanced dataset out of iris data,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,901,fit a model on the imbalanced data without class weight info,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,907,fit a model with balanced class_weight enabled,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,917,Test weights on individual samples,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,926,we give a small weights to class 1,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,929,now the hyperplane should rotate clock-wise and,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,930,the prediction on this point should shift,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,936,Test if ValueError is raised if sample_weight has wrong shape,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,938,provided sample_weight too long,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,945,classes was not specified,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,963,check that coef_ haven't been re-allocated,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,984,check that coef_ haven't been re-allocated,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1005,Partial_fit should work after initial fit in the multiclass case.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1006,Non-regression test for #2496; fit would previously produce a,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1007,Fortran-ordered coef_ that subsequent partial_fit couldn't handle.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1010,no exception here,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1067,Test multiple calls of fit w/ different shaped inputs.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1072,Non-regression test: try fitting with a different label set.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1077,,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1078,Regression Test Case,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1082,Check that SGD gives any results.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1090,Tests the average regressor matches the naive implementation,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1100,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1120,Tests whether the partial fit yields the same average as the fit,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1129,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1150,Checks the average weights on data with 0s,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1179,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1188,simple linear function with noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1205,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1215,simple linear function with noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1233,simple linear function without noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1242,simple linear function with noise,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1254,Check that the SGD output is consistent with coordinate descent,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1259,ground_truth linear model that generate y from X and to which the,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1260,models should converge if the regularizer would be set to 0.0,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1264,XXX: alpha = 0.1 seems to cause convergence problems,SATD
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1295,check that coef_ haven't been re-allocated,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1327,Test if l1 ratio extremes match L1 and L2 penalty settings.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1332,test if elasticnet with l1_ratio near 1 gives same result as pure l1,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1340,test if elasticnet with l1_ratio near 0 gives same result as pure l2,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1351,Generate some weird data with hugely unscaled features,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1360,Use MinMaxScaler to scale the data without introducing a numerical,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1361,instability (computing the standard deviation naively is not possible,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1362,on this data),not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1366,Define a ground truth on the scaled data,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1373,smoke test: model is stable on scaled data,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1377,model is numerically unstable on unscaled data,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1385,Non regression test case for numerical stability on scaled problems,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1386,where the gradient can still explode with some losses,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1397,Non regression tests for numerical stability issues caused by large,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1398,regularization parameters,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1408,Test that the tol parameter behaves as expected,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1412,"With tol is None, the number of iteration should be equal to max_iter",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1418,"If tol is not None, the number of iteration should be less than max_iter",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1425,A larger tol should yield a smaller number of iteration,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1431,Strict tolerance and small max_iter should trigger a warning,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1438,Test gradient of different loss functions,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1439,"cases is a list of (p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1445,Test Hinge (hinge / perceptron),not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1446,hinge,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1449,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1456,perceptron,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1459,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1468,Test SquaredHinge,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1471,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1479,Test Log (logistic loss),not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1482,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1496,Test SquaredLoss,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1499,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1507,Test Huber,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1510,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1518,Test ModifiedHuber,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1521,"(p, y, expected)",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1530,Test EpsilonInsensitive,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1541,Test SquaredEpsilonInsensitive,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1552,This is a non-regression test for a bad interaction between,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1553,early stopping internal attribute and thread-based parallelism.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1564,This is a non-regression test for a bad interaction between,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1565,early stopping internal attribute and process-based multi-core,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1566,parallelism.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1583,"This is a non-regression smoke test. In the multi-class case,",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1584,SGDClassifier.fit fits each class in a one-versus-all fashion using,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1585,"joblib.Parallel.  However, each OvA step updates the coef_ attribute of",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1586,"the estimator in-place. Internally, SGDClassifier calls Parallel using",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1587,require='sharedmem'. This test makes sure SGDClassifier.fit works,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1588,consistently even when the user asks for a backend that does not provide,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1589,sharedmem semantics.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1591,We further test a case where memmapping would have been used if,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1592,SGDClassifier.fit was called from a loky or multiprocessing backend. In,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1593,"this specific case, in-place modification of clf.coef_ would have caused",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1594,a segmentation fault when trying to write in a readonly memory mapped,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1595,buffer.,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1602,Create a classification problem with 50000 features and 20 classes. Using,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1603,loky or multiprocessing this make the clf.coef_ exceed the threshold,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1604,above which memmaping is used in joblib and loky (1MB as of 2018/11/1).,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1609,Begin by fitting a SGD classifier sequentially,not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1614,"Fit a SGDClassifier using the specified backend, and make sure the",not
scikit-learn/sklearn/linear_model/tests/test_sgd.py,1615,coefficients are equal to those obtained using a sequential fit,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,74,Ridge regression convergence test using score,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,75,"TODO: for this test to be robust, we should use a dataset instead",SATD
scikit-learn/sklearn/linear_model/tests/test_ridge.py,76,of np.random.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,80,With more samples than features,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,91,Currently the only solvers to support sample_weight.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,95,With more features than samples,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,104,Currently the only solvers to support sample_weight.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,119,test on a singular matrix,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,147,Sample weight can be implemented via a simple rescaling,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,148,for the square loss.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,167,TODO: loop over sparse data as well,SATD
scikit-learn/sklearn/linear_model/tests/test_ridge.py,168,Note: parametrizing this test with pytest results in failed,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,169,"assertions, meaning that is is not extremely robust",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,183,Ridge with explicit sample_weight,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,190,Closed form of the weighted regularized least square,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,191,theta = (X^T W X + alpha I)^(-1) * X^T W y,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,213,Test shape of coef_ and intercept_,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,237,Test intercept with multiple targets GH issue #708,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,255,Test BayesianRegression ridge classifier,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,256,TODO: test also n_samples > n_features,SATD
scikit-learn/sklearn/linear_model/tests/test_ridge.py,277,"On alpha=0., Ridge and OLS yield the same solution.",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,280,we need more samples than features,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,298,Tests the ridge object using individual penalties,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,318,Test error is raised when number of targets and penalties do not match.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,479,checking on asymmetric scoring,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,534,ignore warning from GridSearchCV: FutureWarning: The default,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,535,of the `iid` parameter will change from True to False in version 0.22,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,536,and will be removed in 0.24,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,593,test that can work with both dense or sparse matrices,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,601,check best alpha,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,606,check that we get same best alpha with custom loss_func,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,613,check that we get same best alpha with custom score_func,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,620,check that we get same best alpha with a scorer,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,626,check that we get same best alpha with sample weights,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,632,simulate several responses,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,679,Check that `cv_values_` is not stored when store_cv_values is False,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,692,check that the best_score_ is store,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,707,simulate several responses,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,741,non-regression test for #14672,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,742,check that RidgeClassifierCV works with all sort of scoring and,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,743,cross-validation,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,746,Smoke test to check that fit/predict does not raise error,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,753,check that custom scoring is working as expected,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,754,check the tie breaking strategy (keep the first alpha tried),not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,765,"In case of tie score, the first alphas will be kept",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,782,test dense matrix,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,784,test sparse matrix,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,786,test that the outputs are the same,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,808,Test class weights.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,817,we give a small weights to class 1,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,821,now the hyperplane should rotate clock-wise and,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,822,the prediction on this point should shift,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,825,check if class_weight = 'balanced' can handle negative labels.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,830,"class_weight = 'balanced', and class_weight = None should return",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,831,same values when y has equal number of all labels,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,847,"Iris is balanced, so no effect expected for using 'balanced' weights",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,854,"Inflate importance of class 1, check against user-defined weights",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,864,Check that sample_weight and class_weight are multiplicative,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,873,Test class weights for cross validated ridge classifier.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,881,we give a small weights to class 1,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,904,with len(y.shape) == 1,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,909,with len(y.shape) == 2,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,936,with len(y.shape) == 1,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,941,with len(y.shape) == 2,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,954,There are different algorithms for n_samples > n_features,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,955,"and the opposite, so test them both.",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,965,Check using GridSearchCV directly,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,975,Sample weights must be either scalar or 1D,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,993,"make sure the ""OK"" sample weights actually work",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1014,Sample weights must work with sparse matrices,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1049,Integers,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1059,Negative integers,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1065,Negative floats,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1073,Tests whether a ValueError is raised if a non-identified solver,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1074,is passed to ridge_regression,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1098,Test that self.n_iter_ is correct.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1120,for now only sparse_cg can correctly fit an intercept with sparse X with,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1121,default tol and max_iter.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1122,sag is tested separately in test_ridge_fit_intercept_sparse_sag,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1123,because it requires more iterations and should raise a warning if default,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1124,max_iter is used.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1125,"other solvers raise an exception, as checked in",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1126,test_ridge_fit_intercept_sparse_error,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1127,,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1128,"""auto"" should switch to ""sparse_cg"" when X is sparse",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1129,"so the reference we use for both (""auto"" and ""sparse_cg"") is",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1130,"Ridge(solver=""sparse_cg""), fitted using the dense representation (note",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1131,"that ""sparse_cg"" can fit sparse or dense data)",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1182,test excludes 'svd' solver because it raises exception for sparse inputs,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1240,Check type consistency 32bits,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1245,Check type consistency 64 bits,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1250,Do the actual checks at once for easier debug,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1259,Test different alphas in cholesky solver to ensure full coverage.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1260,This test is separated from test_dtype_match for clarity.,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1270,Check type consistency 32bits,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1275,Check type consistency 64 bits,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1280,"Do all the checks at once, like this is easier to debug",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1299,XXX: Sparse CG seems to be far less numerically stable than the,SATD
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1300,"others, maybe we should not enable float32 for this one.",not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1320,check that Fortran array are converted when using SAG solver,not
scikit-learn/sklearn/linear_model/tests/test_ridge.py,1322,for the order of X and y to not be C-ordered arrays,not
scikit-learn/sklearn/linear_model/tests/test_omp.py,1,Author: Vlad Niculae,not
scikit-learn/sklearn/linear_model/tests/test_omp.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_omp.py,26,Make X not of norm 1 for testing,not
scikit-learn/sklearn/linear_model/tests/test_omp.py,30,"this makes X (n_samples, n_features)",not
scikit-learn/sklearn/linear_model/tests/test_omp.py,31,"and y (n_samples, 3)",not
scikit-learn/sklearn/linear_model/tests/test_omp.py,109,Non-regression test for:,not
scikit-learn/sklearn/linear_model/tests/test_omp.py,110,https://github.com/scikit-learn/scikit-learn/issues/5956,not
scikit-learn/sklearn/linear_model/tests/test_omp.py,163,"X[:, 21] should be selected first, then X[:, 0] selected second,",not
scikit-learn/sklearn/linear_model/tests/test_omp.py,164,"which will take X[:, 21]'s place in case the algorithm does",not
scikit-learn/sklearn/linear_model/tests/test_omp.py,165,column swapping for optimization (which is the case at the moment),not
scikit-learn/sklearn/linear_model/tests/test_omp.py,222,Use small simple data; it's a sanity check but OMP can stop early,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,19,Generate coordinates of line,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,24,Add some faulty data,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,39,Estimate parameters of corrupted data,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,42,Ground truth / reference inlier mask,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,92,there is a 1e-9 chance it will take these many trials. No good reason,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,93,"1e-2 isn't enough, can still happen",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,94,2 is the what ransac defines  as min_samples = X.shape[1] + 1,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,153,When residual_threshold=0.0 there are no inliers and a,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,154,ValueError with a message should be raised,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,344,3-D target values,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,347,Estimate parameters of corrupted data,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,350,Ground truth / reference inlier mask,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,375,multi-dimensional,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,384,one-dimensional,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,403,Estimate parameters of corrupted data,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,406,Ground truth / reference inlier mask,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,415,Numbers hand-calculated and confirmed on page 119 (Table 4.3) in,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,416,"Hartley, R.~I. and Zisserman, A., 2004,",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,417,"Multiple View Geometry in Computer Vision, Second Edition,",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,418,"Cambridge University Press, ISBN: 0521540518",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,420,"e = 0%, min_samples = X",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,423,"e = 5%, min_samples = 2",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,425,"e = 10%, min_samples = 2",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,427,"e = 30%, min_samples = 2",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,429,"e = 50%, min_samples = 2",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,432,"e = 5%, min_samples = 8",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,434,"e = 10%, min_samples = 8",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,436,"e = 30%, min_samples = 8",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,438,"e = 50%, min_samples = 8",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,441,"e = 0%, min_samples = 10",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,459,sanity check,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,465,check that mask is correct,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,468,"check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,469,"X = X1 repeated n1 times, X2 repeated n2 times and so forth",not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,493,check that if base_estimator.fit doesn't support,not
scikit-learn/sklearn/linear_model/tests/test_ransac.py,494,"sample_weight, raises error",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,2,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,3,,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,61,check with initial values of alpha and lambda (see code for the values),not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,66,value of the parameters of the Gamma hyperpriors,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,72,compute score using formula of docstring,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,80,compute score with BayesianRidge,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,90,Test correctness of lambda_ and alpha_ parameters (GitHub issue #8224),not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,94,A Ridge regression model using an alpha value equal to the ratio of,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,95,lambda_ and alpha_ from the Bayesian Ridge model must be identical,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,103,Test correctness of the sample_weights method,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,108,A Ridge regression model using an alpha value equal to the ratio of,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,109,lambda_ and alpha_ from the Bayesian Ridge model must be identical,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,118,Test BayesianRidge on toy,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,124,Check that the model could approximately learn the identity function,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,130,"Test BayesianRidge with initial values (alpha_init, lambda_init)",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,132,y = (x^3 - 6x^2 + 8x) / 3,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,134,"In this case, starting from the default initial values will increase",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,135,"the bias of the fitted curve. So, lambda_init should be small.",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,137,Check the R2 score nearly equals to one.,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,143,Test BayesianRidge and ARDRegression predictions for edge case of,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,144,constant target vectors,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,161,Test BayesianRidge and ARDRegression standard dev. for edge case of,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,162,constant target vector,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,163,The standard dev. should be relatively small (< 0.01 is tested here),not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,179,Checks that `sigma_` is updated correctly after the last iteration,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,180,of the ARDRegression algorithm. See issue #10128.,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,186,"With the inputs above, ARDRegression prunes both of the two coefficients",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,187,"in the first iteration. Hence, the expected shape of `sigma_` is (0, 0).",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,189,Ensure that no error is thrown at prediction stage,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,194,Test BayesianRegression ARD classifier,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,200,Check that the model could approximately learn the identity function,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,208,Check that ARD converges with reasonable accuracy on an easy problem,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,209,(Github issue #14055),not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,221,Test return_std option for both Bayesian regressors,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,254,make sure the two update_sigma() helpers are equivalent. The woodbury,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,255,"formula is used when n_samples < n_features, and the other one is used",not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,256,otherwise.,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,260,set n_samples == n_features to avoid instability issues when inverting,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,261,the matrices. Using the woodbury formula would be unstable when,not
scikit-learn/sklearn/linear_model/tests/test_bayes.py,262,n_samples > n_features,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,5,Author: Florian Wilhelm <florian.wilhelm@gmail.com>,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,37,"Linear model y = 3*x + N(2, 0.1**2)",not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,48,Add some outliers,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,66,"Linear model y = 5*x_1 + 10*x_2 + N(1, 0.1**2)",not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,72,Add some outliers,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,82,"Linear model y = 5*x_1 + 10*x_2  + 42*x_3 + 7*x_4 + N(1, 0.1**2)",not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,88,Add some outliers,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,97,Check startvalue is element of X and solution,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,101,Check startvalue is not the solution,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,106,Check startvalue is not the solution but element of X,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,111,Check that a single vector is identity,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,121,Check first two iterations,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,126,Check fix point,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,137,Test larger problem and for exact solution in 1d case,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,153,Check if median is solution of the Fermat-Weber location problem,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,156,Check when maximum iteration is exceeded a warning is emitted,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,162,Check that Least Squares fails,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,165,Check that Theil-Sen works,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,173,Check that Least Squares fails,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,176,Check that Theil-Sen works,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,185,Check that Least Squares fails,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,188,Check that Theil-Sen works,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,240,Check for exact the same results as Least Squares,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,246,Check that Theil-Sen can be verbose,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,256,Check that Least Squares fails,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,259,Check that Theil-Sen works,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,272,Check that Theil-Sen falls back to Least Squares if fit_intercept=False,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,277,Check fit_intercept=True case. This will not be equal to the Least,not
scikit-learn/sklearn/linear_model/tests/test_theil_sen.py,278,Squares solution since the intercept is calculated differently.,not
scikit-learn/sklearn/linear_model/_glm/glm.py,5,Author: Christian Lorentzen <lorentzen.ch@googlemail.com>,not
scikit-learn/sklearn/linear_model/_glm/glm.py,6,some parts and tricks stolen from other sklearn files.,not
scikit-learn/sklearn/linear_model/_glm/glm.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_glm/glm.py,47,same as X.T @ temp,not
scikit-learn/sklearn/linear_model/_glm/glm.py,170,Guarantee that self._link_instance is set to an instance of,not
scikit-learn/sklearn/linear_model/_glm/glm.py,171,class BaseLink,not
scikit-learn/sklearn/linear_model/_glm/glm.py,234,TODO: if alpha=0 check that X is not rank deficient,SATD
scikit-learn/sklearn/linear_model/_glm/glm.py,236,rescaling of sample_weight,not
scikit-learn/sklearn/linear_model/_glm/glm.py,237,,not
scikit-learn/sklearn/linear_model/_glm/glm.py,238,IMPORTANT NOTE: Since we want to minimize,not
scikit-learn/sklearn/linear_model/_glm/glm.py,239,"1/(2*sum(sample_weight)) * deviance + L2,",not
scikit-learn/sklearn/linear_model/_glm/glm.py,240,"deviance = sum(sample_weight * unit_deviance),",not
scikit-learn/sklearn/linear_model/_glm/glm.py,241,we rescale weights such that sum(weights) = 1 and this becomes,not
scikit-learn/sklearn/linear_model/_glm/glm.py,242,1/2*deviance + L2 with deviance=sum(weights * unit_deviance),not
scikit-learn/sklearn/linear_model/_glm/glm.py,258,algorithms for optimization,not
scikit-learn/sklearn/linear_model/_glm/glm.py,266,offset if coef[0] is intercept,not
scikit-learn/sklearn/linear_model/_glm/glm.py,292,set intercept to zero as the other linear models do,not
scikit-learn/sklearn/linear_model/_glm/glm.py,330,check_array is done in _linear_predictor,not
scikit-learn/sklearn/linear_model/_glm/glm.py,366,"Note, default score defined in RegressorMixin is R^2 score.",not
scikit-learn/sklearn/linear_model/_glm/glm.py,367,TODO: make D^2 a score function in module metrics (and thereby get,SATD
scikit-learn/sklearn/linear_model/_glm/glm.py,368,input validation and so on),not
scikit-learn/sklearn/linear_model/_glm/glm.py,377,create the _family_instance if fit wasn't called yet.,not
scikit-learn/sklearn/linear_model/_glm/glm.py,443,Make this attribute read-only to avoid mis-uses e.g. in GridSearch.,not
scikit-learn/sklearn/linear_model/_glm/glm.py,506,Make this attribute read-only to avoid mis-uses e.g. in GridSearch.,not
scikit-learn/sklearn/linear_model/_glm/glm.py,602,We use a property with a setter to make sure that the family is,not
scikit-learn/sklearn/linear_model/_glm/glm.py,603,"always a Tweedie distribution, and that self.power and",not
scikit-learn/sklearn/linear_model/_glm/glm.py,604,self.family.power are identical by construction.,not
scikit-learn/sklearn/linear_model/_glm/glm.py,606,TODO: make the returned object immutable,SATD
scikit-learn/sklearn/linear_model/_glm/link.py,5,Author: Christian Lorentzen <lorentzen.ch@googlemail.com>,not
scikit-learn/sklearn/linear_model/_glm/link.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_glm/__init__.py,1,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,1,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,2,,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,42,scalar value but not positive,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,48,Positive weights are accepted,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,51,2d array,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,56,1d but wrong length,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,70,in range of all distributions,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,85,in range of all distributions,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,102,Make sure link='auto' delivers the expected link function,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,103,in range of all distributions,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,204,sample_weight=np.ones(..) should be equivalent to sample_weight=None,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,209,"sample_weight are normalized to 1 so, scaling them has no effect",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,214,setting one element of sample_weight to 0 is equivalent to removing,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,215,the correspoding sample,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,223,check that multiplying sample_weight by 2 is equivalent,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,224,to repeating correspoding samples twice,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,282,"As we intentionally set max_iter=1, L-BFGS-B will issue a",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,283,ConvergenceWarning which we here simply ignore.,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,290,The two model are not exactly identical since the lbfgs solver,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,291,"computes the approximate hessian from previous iterations, which",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,292,will not be strictly identical in the case of a warm start.,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,326,"GLM has 1/(2*n) * Loss + 1/2*L2, Ridge has Loss + L2",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,348,"library(""glmnet"")",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,349,options(digits=10),not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,350,"df <- data.frame(a=c(-2,-1,1,2), b=c(0,0,1,1), y=c(0,1,1,2))",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,351,"x <- data.matrix(df[,c(""a"", ""b"")])",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,352,y <- df$y,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,353,"fit <- glmnet(x=x, y=y, alpha=0, intercept=T, family=""poisson"",",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,354,"standardize=F, thresh=1e-10, nlambda=10000)",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,355,"coef(fit, s=1)",not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,356,(Intercept) -0.12889386979,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,357,a            0.29019207995,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,358,b            0.03741173122,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,379,Make sure the family attribute is read-only to prevent searching over it,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,380,e.g. in a grid search,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,390,Make sure the family attribute is read-only to prevent searching over it,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,391,e.g. in a grid search,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,401,Make sure the family attribute is always a TweedieDistribution and that,not
scikit-learn/sklearn/linear_model/_glm/tests/test_glm.py,402,the power attribute is properly updated,not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,1,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,2,,not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,26,"careful for large x, note expit(36) = 1",not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,27,limit max eta to 15,not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,30,"if g(h(x)) = x, then g'(h(x)) = 1/h'(x)",not
scikit-learn/sklearn/linear_model/_glm/tests/test_link.py,31,"g = link, h = link.inverse",not
scikit-learn/sklearn/linear_model/_glm/tests/__init__.py,1,License: BSD 3 clause,not
scikit-learn/sklearn/tests/test_calibration.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/tests/test_calibration.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/tests/test_calibration.py,34,MultinomialNB only allows positive X,not
scikit-learn/sklearn/tests/test_calibration.py,36,split train and test,not
scikit-learn/sklearn/tests/test_calibration.py,41,Naive-Bayes,not
scikit-learn/sklearn/tests/test_calibration.py,48,Naive Bayes with calibration,not
scikit-learn/sklearn/tests/test_calibration.py,54,Note that this fit overwrites the fit on the entire training,not
scikit-learn/sklearn/tests/test_calibration.py,55,set,not
scikit-learn/sklearn/tests/test_calibration.py,59,Check that brier score has improved after calibration,not
scikit-learn/sklearn/tests/test_calibration.py,63,"Check invariance against relabeling [0, 1] -> [1, 2]",not
scikit-learn/sklearn/tests/test_calibration.py,69,"Check invariance against relabeling [0, 1] -> [-1, 1]",not
scikit-learn/sklearn/tests/test_calibration.py,75,"Check invariance against relabeling [0, 1] -> [1, 0]",not
scikit-learn/sklearn/tests/test_calibration.py,84,Isotonic calibration is not invariant against relabeling,not
scikit-learn/sklearn/tests/test_calibration.py,85,but should improve in both cases,not
scikit-learn/sklearn/tests/test_calibration.py,90,Check failure cases:,not
scikit-learn/sklearn/tests/test_calibration.py,91,"only ""isotonic"" and ""sigmoid"" should be accepted as methods",not
scikit-learn/sklearn/tests/test_calibration.py,95,base-estimators should provide either decision_function or,not
scikit-learn/sklearn/tests/test_calibration.py,96,"predict_proba (most regressors, for instance, should fail)",not
scikit-learn/sklearn/tests/test_calibration.py,118,"As the weights are used for the calibration, they should still yield",not
scikit-learn/sklearn/tests/test_calibration.py,119,a different predictions,not
scikit-learn/sklearn/tests/test_calibration.py,129,test multi-class setting with classifier that implements,not
scikit-learn/sklearn/tests/test_calibration.py,130,only decision function,not
scikit-learn/sklearn/tests/test_calibration.py,135,Use categorical labels to check that CalibratedClassifierCV supports,not
scikit-learn/sklearn/tests/test_calibration.py,136,them correctly,not
scikit-learn/sklearn/tests/test_calibration.py,150,Check that log-loss of calibrated classifier is smaller than,not
scikit-learn/sklearn/tests/test_calibration.py,151,log-loss of naively turned OvR decision function to probabilities,not
scikit-learn/sklearn/tests/test_calibration.py,152,via softmax,not
scikit-learn/sklearn/tests/test_calibration.py,162,Test that calibration of a multiclass classifier decreases log-loss,not
scikit-learn/sklearn/tests/test_calibration.py,163,for RandomForestClassifier,not
scikit-learn/sklearn/tests/test_calibration.py,189,MultinomialNB only allows positive X,not
scikit-learn/sklearn/tests/test_calibration.py,191,split train and test,not
scikit-learn/sklearn/tests/test_calibration.py,199,Naive-Bayes,not
scikit-learn/sklearn/tests/test_calibration.py,204,Naive Bayes with calibration,not
scikit-learn/sklearn/tests/test_calibration.py,227,computed from my python port of the C++ code in LibSVM,not
scikit-learn/sklearn/tests/test_calibration.py,235,check that _SigmoidCalibration().fit only accepts 1d array or 2d column,not
scikit-learn/sklearn/tests/test_calibration.py,236,arrays,not
scikit-learn/sklearn/tests/test_calibration.py,255,"probabilities outside [0, 1] should not be accepted when normalize",not
scikit-learn/sklearn/tests/test_calibration.py,256,is set to False,not
scikit-learn/sklearn/tests/test_calibration.py,260,test that quantiles work as expected,not
scikit-learn/sklearn/tests/test_calibration.py,271,Check that error is raised when invalid strategy is selected,not
scikit-learn/sklearn/tests/test_calibration.py,291,Test that sum of probabilities is 1. A non-regression test for,not
scikit-learn/sklearn/tests/test_calibration.py,292,issue #7796,not
scikit-learn/sklearn/tests/test_calibration.py,305,Test to check calibration works fine when train set in a test-train,not
scikit-learn/sklearn/tests/test_calibration.py,306,split does not contain all classes,not
scikit-learn/sklearn/tests/test_calibration.py,307,"Since this test uses LOO, at each iteration train set will not contain a",not
scikit-learn/sklearn/tests/test_calibration.py,308,class label,not
scikit-learn/sklearn/tests/test_calibration.py,338,toy decision function that just needs to have the right shape:,not
scikit-learn/sklearn/tests/test_calibration.py,342,we should be able to fit this classifier with no error,not
scikit-learn/sklearn/tests/test_dummy.py,23,We know that we can have division by zero,not
scikit-learn/sklearn/tests/test_dummy.py,41,We know that we can have division by zero,not
scikit-learn/sklearn/tests/test_dummy.py,46,1d case,not
scikit-learn/sklearn/tests/test_dummy.py,47,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,54,2d case,not
scikit-learn/sklearn/tests/test_dummy.py,66,2d case only,not
scikit-learn/sklearn/tests/test_dummy.py,67,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,87,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,105,non-regression test added in,not
scikit-learn/sklearn/tests/test_dummy.py,106,https://github.com/scikit-learn/scikit-learn/pull/13545,not
scikit-learn/sklearn/tests/test_dummy.py,121,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,140,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,154,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,177,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,191,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,270,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,290,Correctness oracle,not
scikit-learn/sklearn/tests/test_dummy.py,309,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,329,Correctness oracle,not
scikit-learn/sklearn/tests/test_dummy.py,344,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,377,Correctness oracle,not
scikit-learn/sklearn/tests/test_dummy.py,387,Correctness oracle,not
scikit-learn/sklearn/tests/test_dummy.py,400,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,401,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,431,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,450,test with 2d array,not
scikit-learn/sklearn/tests/test_dummy.py,456,Correctness oracle,not
scikit-learn/sklearn/tests/test_dummy.py,470,when strategy = 'mean',not
scikit-learn/sklearn/tests/test_dummy.py,503,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,511,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,520,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,575,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,593,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,616,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,639,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,699,ignored,not
scikit-learn/sklearn/tests/test_dummy.py,705,there should be two elements when return_std is True,not
scikit-learn/sklearn/tests/test_dummy.py,707,the second element should be all zeros,not
scikit-learn/sklearn/tests/test_dummy.py,759,0.24,not
scikit-learn/sklearn/tests/test_dummy.py,779,TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates,SATD
scikit-learn/sklearn/tests/test_naive_bayes.py,27,Data is just 6 separable points in the plane,not
scikit-learn/sklearn/tests/test_naive_bayes.py,31,A bit more random tests,not
scikit-learn/sklearn/tests/test_naive_bayes.py,36,Data is 6 random integer points in a 100 dimensional space classified to,not
scikit-learn/sklearn/tests/test_naive_bayes.py,37,three classes.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,43,Gaussian Naive Bayes classification.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,44,This checks that GaussianNB implements fit and predict and returns,not
scikit-learn/sklearn/tests/test_naive_bayes.py,45,correct values for a simple toy dataset.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,55,Test whether label mismatch between target y and classes raises,not
scikit-learn/sklearn/tests/test_naive_bayes.py,56,an Error,not
scikit-learn/sklearn/tests/test_naive_bayes.py,57,FIXME Remove this test once the more general partial_fit tests are merged,SATD
scikit-learn/sklearn/tests/test_naive_bayes.py,62,Test whether class priors are properly set.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,67,Check that the class priors sum to 1,not
scikit-learn/sklearn/tests/test_naive_bayes.py,73,Sample weights all being 1 should not change results,not
scikit-learn/sklearn/tests/test_naive_bayes.py,81,Fitting twice with half sample-weights should result,not
scikit-learn/sklearn/tests/test_naive_bayes.py,82,in same result as fitting once with full weights,not
scikit-learn/sklearn/tests/test_naive_bayes.py,91,Check that duplicate entries and correspondingly increased sample,not
scikit-learn/sklearn/tests/test_naive_bayes.py,92,weights yield the same result,not
scikit-learn/sklearn/tests/test_naive_bayes.py,119,"test whether the class prior sum is properly tested""""""",not
scikit-learn/sklearn/tests/test_naive_bayes.py,126,smoke test for issue #9633,not
scikit-learn/sklearn/tests/test_naive_bayes.py,152,Create an empty array,not
scikit-learn/sklearn/tests/test_naive_bayes.py,167,Fit for the first time the GNB,not
scikit-learn/sklearn/tests/test_naive_bayes.py,169,Partial fit a second time with an incoherent X,not
scikit-learn/sklearn/tests/test_naive_bayes.py,188,Scaling the data should not change the prediction results,not
scikit-learn/sklearn/tests/test_naive_bayes.py,199,Test whether class priors are properly set.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,221,all categories have to appear in the first partial fit,not
scikit-learn/sklearn/tests/test_naive_bayes.py,227,the categories for each feature of CategoricalNB are mapped to an,not
scikit-learn/sklearn/tests/test_naive_bayes.py,228,index chronologically with each call of partial fit and therefore,not
scikit-learn/sklearn/tests/test_naive_bayes.py,229,the category_count matrices cannot be compared for equality,not
scikit-learn/sklearn/tests/test_naive_bayes.py,236,assert category 0 occurs 1x in the first class and 0x in the 2nd,not
scikit-learn/sklearn/tests/test_naive_bayes.py,237,class,not
scikit-learn/sklearn/tests/test_naive_bayes.py,239,assert category 1 occurs 0x in the first class and 2x in the 2nd,not
scikit-learn/sklearn/tests/test_naive_bayes.py,240,class,not
scikit-learn/sklearn/tests/test_naive_bayes.py,243,assert category 0 occurs 0x in the first class and 1x in the 2nd,not
scikit-learn/sklearn/tests/test_naive_bayes.py,244,class,not
scikit-learn/sklearn/tests/test_naive_bayes.py,246,assert category 1 occurs 1x in the first class and 1x in the 2nd,not
scikit-learn/sklearn/tests/test_naive_bayes.py,247,class,not
scikit-learn/sklearn/tests/test_naive_bayes.py,256,Test picklability of discrete naive Bayes classifiers,not
scikit-learn/sklearn/tests/test_naive_bayes.py,267,Test pickling of estimator trained with partial_fit,not
scikit-learn/sklearn/tests/test_naive_bayes.py,279,Test input checks for the fit method,not
scikit-learn/sklearn/tests/test_naive_bayes.py,281,check shape consistency for number of samples at fit time,not
scikit-learn/sklearn/tests/test_naive_bayes.py,284,check shape consistency for number of input features at predict time,not
scikit-learn/sklearn/tests/test_naive_bayes.py,291,check shape consistency,not
scikit-learn/sklearn/tests/test_naive_bayes.py,295,classes is required for first call to partial fit,not
scikit-learn/sklearn/tests/test_naive_bayes.py,298,check consistency of consecutive classes values,not
scikit-learn/sklearn/tests/test_naive_bayes.py,304,check consistency of input shape for partial_fit,not
scikit-learn/sklearn/tests/test_naive_bayes.py,307,check consistency of input shape for predict,not
scikit-learn/sklearn/tests/test_naive_bayes.py,312,Test discrete NB classes' probability scores,not
scikit-learn/sklearn/tests/test_naive_bayes.py,314,The 100s below distinguish Bernoulli from multinomial.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,315,FIXME: write a test to show this.,SATD
scikit-learn/sklearn/tests/test_naive_bayes.py,319,test binary case (1-d output),not
scikit-learn/sklearn/tests/test_naive_bayes.py,320,"2 is regression test for binary case, 02e673",not
scikit-learn/sklearn/tests/test_naive_bayes.py,329,"test multiclass case (2-d output, must sum to one)",not
scikit-learn/sklearn/tests/test_naive_bayes.py,344,Test whether discrete NB classes fit a uniform prior,not
scikit-learn/sklearn/tests/test_naive_bayes.py,345,when fit_prior=False and class_prior=None,not
scikit-learn/sklearn/tests/test_naive_bayes.py,356,Test whether discrete NB classes use provided prior,not
scikit-learn/sklearn/tests/test_naive_bayes.py,363,Inconsistent number of classes with prior,not
scikit-learn/sklearn/tests/test_naive_bayes.py,371,Test whether discrete NB classes use provided prior,not
scikit-learn/sklearn/tests/test_naive_bayes.py,372,when using partial_fit,not
scikit-learn/sklearn/tests/test_naive_bayes.py,391,check shape consistency for number of samples at fit time,not
scikit-learn/sklearn/tests/test_naive_bayes.py,404,Check sample weight using the partial_fit method,not
scikit-learn/sklearn/tests/test_naive_bayes.py,415,coef_ and intercept_ should have shapes as in other linear models.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,416,Non-regression test for issue #2127.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,418,binary classification,not
scikit-learn/sklearn/tests/test_naive_bayes.py,428,Test Multinomial Naive Bayes classification.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,429,This checks that MultinomialNB implements fit and predict and returns,not
scikit-learn/sklearn/tests/test_naive_bayes.py,430,correct values for a simple toy dataset.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,437,Check the ability to predict the learning set.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,444,Verify that np.log(clf.predict_proba(X)) gives the same results as,not
scikit-learn/sklearn/tests/test_naive_bayes.py,445,clf.predict_log_proba(X),not
scikit-learn/sklearn/tests/test_naive_bayes.py,450,Check that incremental fitting yields the same results,not
scikit-learn/sklearn/tests/test_naive_bayes.py,465,Partial fit on the whole data at once should be the same as fit too,not
scikit-learn/sklearn/tests/test_naive_bayes.py,479,test smoothing of prior for yet unobserved targets,not
scikit-learn/sklearn/tests/test_naive_bayes.py,481,Create toy training data,not
scikit-learn/sklearn/tests/test_naive_bayes.py,495,add a training example with previously unobserved class,not
scikit-learn/sklearn/tests/test_naive_bayes.py,517,Tests that BernoulliNB when alpha=1.0 gives the same values as,not
scikit-learn/sklearn/tests/test_naive_bayes.py,518,"those given for the toy example in Manning, Raghavan, and",not
scikit-learn/sklearn/tests/test_naive_bayes.py,519,"Schuetze's ""Introduction to Information Retrieval"" book:",not
scikit-learn/sklearn/tests/test_naive_bayes.py,520,https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html,not
scikit-learn/sklearn/tests/test_naive_bayes.py,522,Training data points are:,not
scikit-learn/sklearn/tests/test_naive_bayes.py,523,Chinese Beijing Chinese (class: China),not
scikit-learn/sklearn/tests/test_naive_bayes.py,524,Chinese Chinese Shanghai (class: China),not
scikit-learn/sklearn/tests/test_naive_bayes.py,525,Chinese Macao (class: China),not
scikit-learn/sklearn/tests/test_naive_bayes.py,526,Tokyo Japan Chinese (class: Japan),not
scikit-learn/sklearn/tests/test_naive_bayes.py,528,"Features are Beijing, Chinese, Japan, Macao, Shanghai, and Tokyo",not
scikit-learn/sklearn/tests/test_naive_bayes.py,534,"Classes are China (0), Japan (1)",not
scikit-learn/sklearn/tests/test_naive_bayes.py,537,Fit BernoulliBN w/ alpha = 1.0,not
scikit-learn/sklearn/tests/test_naive_bayes.py,541,Check the class prior is correct,not
scikit-learn/sklearn/tests/test_naive_bayes.py,545,Check the feature probabilities are correct,not
scikit-learn/sklearn/tests/test_naive_bayes.py,551,Testing data point is:,not
scikit-learn/sklearn/tests/test_naive_bayes.py,552,Chinese Chinese Chinese Tokyo Japan,not
scikit-learn/sklearn/tests/test_naive_bayes.py,555,Check the predictive probabilities are correct,not
scikit-learn/sklearn/tests/test_naive_bayes.py,563,Test for issue #4268.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,564,Tests that the feature log prob value computed by BernoulliNB when,not
scikit-learn/sklearn/tests/test_naive_bayes.py,565,"alpha=1.0 is equal to the expression given in Manning, Raghavan,",not
scikit-learn/sklearn/tests/test_naive_bayes.py,566,"and Schuetze's ""Introduction to Information Retrieval"" book:",not
scikit-learn/sklearn/tests/test_naive_bayes.py,567,http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html,not
scikit-learn/sklearn/tests/test_naive_bayes.py,572,Fit Bernoulli NB w/ alpha = 1.0,not
scikit-learn/sklearn/tests/test_naive_bayes.py,576,Manually form the (log) numerator and denominator that,not
scikit-learn/sklearn/tests/test_naive_bayes.py,577,constitute P(feature presence | class),not
scikit-learn/sklearn/tests/test_naive_bayes.py,581,Check manual estimate matches,not
scikit-learn/sklearn/tests/test_naive_bayes.py,586,"Tests ComplementNB when alpha=1.0 for the toy example in Manning,",not
scikit-learn/sklearn/tests/test_naive_bayes.py,587,"Raghavan, and Schuetze's ""Introduction to Information Retrieval"" book:",not
scikit-learn/sklearn/tests/test_naive_bayes.py,588,https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html,not
scikit-learn/sklearn/tests/test_naive_bayes.py,590,Training data points are:,not
scikit-learn/sklearn/tests/test_naive_bayes.py,591,Chinese Beijing Chinese (class: China),not
scikit-learn/sklearn/tests/test_naive_bayes.py,592,Chinese Chinese Shanghai (class: China),not
scikit-learn/sklearn/tests/test_naive_bayes.py,593,Chinese Macao (class: China),not
scikit-learn/sklearn/tests/test_naive_bayes.py,594,Tokyo Japan Chinese (class: Japan),not
scikit-learn/sklearn/tests/test_naive_bayes.py,596,"Features are Beijing, Chinese, Japan, Macao, Shanghai, and Tokyo.",not
scikit-learn/sklearn/tests/test_naive_bayes.py,602,"Classes are China (0), Japan (1).",not
scikit-learn/sklearn/tests/test_naive_bayes.py,605,Check that weights are correct. See steps 4-6 in Table 4 of,not
scikit-learn/sklearn/tests/test_naive_bayes.py,606,Rennie et al. (2003).,not
scikit-learn/sklearn/tests/test_naive_bayes.py,631,Verify inputs are nonnegative.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,637,Check that counts/weights are correct.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,652,Check the ability to predict the training set.,not
scikit-learn/sklearn/tests/test_naive_bayes.py,663,Check error is raised for X with negative entries,not
scikit-learn/sklearn/tests/test_naive_bayes.py,670,Check error is raised for incorrect X,not
scikit-learn/sklearn/tests/test_naive_bayes.py,675,Test alpha,not
scikit-learn/sklearn/tests/test_naive_bayes.py,677,alpha=1 increases the count of all categories by one so the final,not
scikit-learn/sklearn/tests/test_naive_bayes.py,678,probability for each category is not 50/50 but 1/3 to 2/3,not
scikit-learn/sklearn/tests/test_naive_bayes.py,684,Assert category_count has counted all features,not
scikit-learn/sklearn/tests/test_naive_bayes.py,687,Check sample_weight,not
scikit-learn/sklearn/tests/test_naive_bayes.py,704,Setting alpha=0 should not output nan results when p(x_i|y_j)=0 is a case,not
scikit-learn/sklearn/tests/test_naive_bayes.py,724,Test sparse X,not
scikit-learn/sklearn/tests/test_naive_bayes.py,736,Test for alpha < 0,not
scikit-learn/sklearn/tests/test_naive_bayes.py,760,Setting alpha=np.array with same length,not
scikit-learn/sklearn/tests/test_naive_bayes.py,761,as number of features should be fine,not
scikit-learn/sklearn/tests/test_naive_bayes.py,766,Test feature probabilities uses pseudo-counts (alpha),not
scikit-learn/sklearn/tests/test_naive_bayes.py,770,Test predictions,not
scikit-learn/sklearn/tests/test_naive_bayes.py,774,Test alpha non-negative,not
scikit-learn/sklearn/tests/test_naive_bayes.py,781,Test that too small pseudo-counts are replaced,not
scikit-learn/sklearn/tests/test_naive_bayes.py,790,Test correct dimensions,not
scikit-learn/sklearn/tests/test_naive_bayes.py,799,Non regression test to make sure that any further refactoring / optim,not
scikit-learn/sklearn/tests/test_naive_bayes.py,800,of the NB models do not harm the performance on a slightly non-linearly,not
scikit-learn/sklearn/tests/test_naive_bayes.py,801,separable dataset,not
scikit-learn/sklearn/tests/test_naive_bayes.py,806,Multinomial NB,not
scikit-learn/sklearn/tests/test_naive_bayes.py,813,Bernoulli NB,not
scikit-learn/sklearn/tests/test_naive_bayes.py,820,Gaussian NB,not
scikit-learn/sklearn/tests/test_naive_bayes.py,831,TODO: remove in 0.24,SATD
scikit-learn/sklearn/tests/test_isotonic.py,21,check that fit is permutation invariant.,not
scikit-learn/sklearn/tests/test_isotonic.py,22,regression test of missing sorting of sample-weights,not
scikit-learn/sklearn/tests/test_isotonic.py,47,Check that we got increasing=True and no warnings,not
scikit-learn/sklearn/tests/test_isotonic.py,56,Check that we got increasing=True and no warnings,not
scikit-learn/sklearn/tests/test_isotonic.py,65,Check that we got increasing=False and no warnings,not
scikit-learn/sklearn/tests/test_isotonic.py,74,Check that we got increasing=False and no warnings,not
scikit-learn/sklearn/tests/test_isotonic.py,83,Check that we got increasing=False and CI interval warning,not
scikit-learn/sklearn/tests/test_isotonic.py,106,check that it is immune to permutation,not
scikit-learn/sklearn/tests/test_isotonic.py,113,check we don't crash when all x are equal:,not
scikit-learn/sklearn/tests/test_isotonic.py,119,Setup examples with ties on minimum,not
scikit-learn/sklearn/tests/test_isotonic.py,124,Check that we get identical results for fit/transform and fit_transform,not
scikit-learn/sklearn/tests/test_isotonic.py,132,Setup examples with ties on maximum,not
scikit-learn/sklearn/tests/test_isotonic.py,137,Check that we get identical results for fit/transform and fit_transform,not
scikit-learn/sklearn/tests/test_isotonic.py,167,"Check fit, transform and fit_transform",not
scikit-learn/sklearn/tests/test_isotonic.py,206,Set y and x for decreasing,not
scikit-learn/sklearn/tests/test_isotonic.py,210,Create model and fit_transform,not
scikit-learn/sklearn/tests/test_isotonic.py,215,work-around for pearson divide warnings in scipy <= 0.17.0,SATD
scikit-learn/sklearn/tests/test_isotonic.py,219,Check that relationship decreases,not
scikit-learn/sklearn/tests/test_isotonic.py,225,Set y and x for decreasing,not
scikit-learn/sklearn/tests/test_isotonic.py,229,Create model and fit_transform,not
scikit-learn/sklearn/tests/test_isotonic.py,234,work-around for pearson divide warnings in scipy <= 0.17.0,SATD
scikit-learn/sklearn/tests/test_isotonic.py,238,Check that relationship increases,not
scikit-learn/sklearn/tests/test_isotonic.py,253,check if default value of sample_weight parameter is one,not
scikit-learn/sklearn/tests/test_isotonic.py,255,random test data,not
scikit-learn/sklearn/tests/test_isotonic.py,260,check if value is correctly used,not
scikit-learn/sklearn/tests/test_isotonic.py,269,check if min value is used correctly,not
scikit-learn/sklearn/tests/test_isotonic.py,291,Set y and x,not
scikit-learn/sklearn/tests/test_isotonic.py,295,Create model and fit,not
scikit-learn/sklearn/tests/test_isotonic.py,299,Check that an exception is thrown,not
scikit-learn/sklearn/tests/test_isotonic.py,304,Set y and x,not
scikit-learn/sklearn/tests/test_isotonic.py,308,Create model and fit,not
scikit-learn/sklearn/tests/test_isotonic.py,312,Predict from  training and test x and check that min/max match.,not
scikit-learn/sklearn/tests/test_isotonic.py,320,Set y and x,not
scikit-learn/sklearn/tests/test_isotonic.py,324,Create model and fit,not
scikit-learn/sklearn/tests/test_isotonic.py,328,Predict from  training and test x and check that we have two NaNs.,not
scikit-learn/sklearn/tests/test_isotonic.py,334,Set y and x,not
scikit-learn/sklearn/tests/test_isotonic.py,338,Create model and fit,not
scikit-learn/sklearn/tests/test_isotonic.py,341,Make sure that we throw an error for bad out_of_bounds value,not
scikit-learn/sklearn/tests/test_isotonic.py,346,Set y and x,not
scikit-learn/sklearn/tests/test_isotonic.py,350,Create model and fit,not
scikit-learn/sklearn/tests/test_isotonic.py,353,Make sure that we throw an error for bad out_of_bounds value in transform,not
scikit-learn/sklearn/tests/test_isotonic.py,363,Create model and fit,not
scikit-learn/sklearn/tests/test_isotonic.py,383,Test from @NelleV's issue:,not
scikit-learn/sklearn/tests/test_isotonic.py,384,https://github.com/scikit-learn/scikit-learn/issues/6921,not
scikit-learn/sklearn/tests/test_isotonic.py,393,Also test decreasing case since the logic there is different,not
scikit-learn/sklearn/tests/test_isotonic.py,399,"Finally, test with only one bound",not
scikit-learn/sklearn/tests/test_isotonic.py,406,Test from @ogrisel's issue:,not
scikit-learn/sklearn/tests/test_isotonic.py,407,https://github.com/scikit-learn/scikit-learn/issues/4297,not
scikit-learn/sklearn/tests/test_isotonic.py,409,Get deterministic RNG with seed,not
scikit-learn/sklearn/tests/test_isotonic.py,412,Create regression and samples,not
scikit-learn/sklearn/tests/test_isotonic.py,418,Get some random weights and zero out,not
scikit-learn/sklearn/tests/test_isotonic.py,423,This will hang in failure case.,not
scikit-learn/sklearn/tests/test_isotonic.py,428,test that the faster prediction change doesn't,not
scikit-learn/sklearn/tests/test_isotonic.py,429,affect out-of-sample predictions:,not
scikit-learn/sklearn/tests/test_isotonic.py,430,https://github.com/scikit-learn/scikit-learn/pull/6206,not
scikit-learn/sklearn/tests/test_isotonic.py,433,"X values over the -10,10 range",not
scikit-learn/sklearn/tests/test_isotonic.py,439,we also want to test that everything still works when some weights are 0,not
scikit-learn/sklearn/tests/test_isotonic.py,445,"Build interpolation function with ALL input data, not just the",not
scikit-learn/sklearn/tests/test_isotonic.py,446,non-redundant subset. The following 2 lines are taken from the,not
scikit-learn/sklearn/tests/test_isotonic.py,447,".fit() method, without removing unnecessary points",not
scikit-learn/sklearn/tests/test_isotonic.py,453,fit with just the necessary data,not
scikit-learn/sklearn/tests/test_isotonic.py,464,https://github.com/scikit-learn/scikit-learn/issues/6628,not
scikit-learn/sklearn/tests/test_isotonic.py,494,regression test for #15004,not
scikit-learn/sklearn/tests/test_isotonic.py,495,check that data are converted when X and y dtype differ,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,23,Data is just 6 separable points in the plane,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,28,Degenerate data with only one feature (still should be separable),not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,31,Data is just 9 separable points in the plane,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,37,Degenerate data with 1 feature (still should be separable),not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,41,Data that has zero variance in one dimension and needs regularization,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,45,One element class,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,48,Data with less samples in a class than n_features,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,58,Test LDA classification.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,59,This checks that LDA implements fit and predict and returns correct,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,60,values for simple toy data.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,67,Assert that it works with 1D data,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,71,Test probability estimates,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,79,"Primarily test for commit 2f34950 -- ""reuse"" of priors",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,81,LDA shouldn't be able to separate those,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,84,Test invalid shrinkages,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,91,Test unknown solver,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,118,check that the empirical means and covariances are close enough to the,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,119,one used to generate the data,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,123,implement the method to compute the probability given in The Elements,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,124,"of Statistical Learning (cf. p.127, Sect. 4.4.5 ""Logistic Regression",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,125,"or LDA?"")",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,150,check the consistency of the computed probability,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,151,all probabilities should sum to one,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,158,check that the probability of LDA are close to the theoretical,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,159,probabilties,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,166,Test priors (negative priors),not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,172,Test that priors passed as a list are correctly handled (run to see if,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,173,failure),not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,177,Test that priors always sum to 1,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,186,Test if the coefficients of the solvers are approximately the same.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,207,Test LDA transform.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,222,"Test if the sum of the normalized eigen vectors values equals 1,",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,223,Also tests whether the explained_variance_ratio_ formed by the,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,224,eigen solver is the same as the explained_variance_ratio_ formed,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,225,by the svd solver,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,248,arrange four classes with their means in a kite-shaped pattern,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,249,"the longer distance should be transformed to the first component, and",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,250,the shorter distance to the second component.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,253,"We construct perfectly symmetric distributions, so the LDA can estimate",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,254,precise means.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,261,Fit LDA and transform the means,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,270,the transformed within-class covariance should be the identity matrix,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,273,the means of classes 0 and 3 should lie on the first component,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,276,the means of classes 1 and 2 should lie on the second component,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,281,Test if classification works correctly with differently scaled features.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,284,use uniform distribution of features to make sure there is absolutely no,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,285,overlap between classes.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,293,should be able to separate the data perfectly,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,299,Test for solver 'lsqr' and 'eigen',not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,300,'store_covariance' has no effect on 'lsqr' and 'eigen' solvers,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,305,Test the actual attribute:,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,315,"Test for SVD solver, the default is to not set the covariances_ attribute",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,319,Test the actual attribute:,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,336,we create n_classes labels by repeating and truncating a,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,337,range(n_classes) until n_samples,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,342,"if n_components <= min(n_classes - 1, n_features), no warning",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,348,"if n_components > min(n_classes - 1, n_features), raise error.",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,349,"We test one unit higher than max_components, and then something",not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,350,larger than both n_features and n_classes - 1 to ensure the test,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,351,works for any value of n_component,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,378,Check value consistency between types,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,384,QDA classification.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,385,This checks that QDA implements fit and predict and returns,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,386,correct values for a simple toy dataset.,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,391,Assure that it works with 1D data,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,395,Test probas estimates,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,402,QDA shouldn't be able to separate those,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,405,Classes should have at least 2 elements,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,423,The default is to not set the covariances_ attribute,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,427,Test the actual attribute:,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,443,the default is reg_param=0. and will cause issues,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,444,when there is a constant variable,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,450,adding a little regularization fixes the problem,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,457,Case n_samples_in_a_class < n_features,not
scikit-learn/sklearn/tests/test_discriminant_analysis.py,469,make features correlated,not
scikit-learn/sklearn/tests/test_random_projection.py,37,Make some random data with uniformly located non zero entries with,not
scikit-learn/sklearn/tests/test_random_projection.py,38,Gaussian distributed values,not
scikit-learn/sklearn/tests/test_random_projection.py,61,,not
scikit-learn/sklearn/tests/test_random_projection.py,62,test on JL lemma,not
scikit-learn/sklearn/tests/test_random_projection.py,63,,not
scikit-learn/sklearn/tests/test_random_projection.py,82,,not
scikit-learn/sklearn/tests/test_random_projection.py,83,tests random matrix generation,not
scikit-learn/sklearn/tests/test_random_projection.py,84,,not
scikit-learn/sklearn/tests/test_random_projection.py,101,All random matrix should produce a transformation matrix,not
scikit-learn/sklearn/tests/test_random_projection.py,102,with zero mean and unit norm for each columns,not
scikit-learn/sklearn/tests/test_random_projection.py,120,Check basic properties of random matrix generation,not
scikit-learn/sklearn/tests/test_random_projection.py,136,Check some statical properties of Gaussian random matrix,not
scikit-learn/sklearn/tests/test_random_projection.py,137,Check that the random matrix follow the proper distribution.,not
scikit-learn/sklearn/tests/test_random_projection.py,138,Let's say that each element of a_{ij} of A is taken from,not
scikit-learn/sklearn/tests/test_random_projection.py,139,"a_ij ~ N(0.0, 1 / n_components).",not
scikit-learn/sklearn/tests/test_random_projection.py,140,,not
scikit-learn/sklearn/tests/test_random_projection.py,150,Check some statical properties of sparse random matrix,not
scikit-learn/sklearn/tests/test_random_projection.py,163,Check possible values,not
scikit-learn/sklearn/tests/test_random_projection.py,174,Check that the random matrix follow the proper distribution.,not
scikit-learn/sklearn/tests/test_random_projection.py,175,Let's say that each element of a_{ij} of A is taken from,not
scikit-learn/sklearn/tests/test_random_projection.py,176,,not
scikit-learn/sklearn/tests/test_random_projection.py,177,- -sqrt(s) / sqrt(n_components)   with probability 1 / 2s,not
scikit-learn/sklearn/tests/test_random_projection.py,178,-  0                              with probability 1 - 1 / s,not
scikit-learn/sklearn/tests/test_random_projection.py,179,- +sqrt(s) / sqrt(n_components)   with probability 1 / 2s,not
scikit-learn/sklearn/tests/test_random_projection.py,180,,not
scikit-learn/sklearn/tests/test_random_projection.py,198,,not
scikit-learn/sklearn/tests/test_random_projection.py,199,tests on random projection transformer,not
scikit-learn/sklearn/tests/test_random_projection.py,200,,not
scikit-learn/sklearn/tests/test_random_projection.py,248,remove 0 distances to avoid division by 0,not
scikit-learn/sklearn/tests/test_random_projection.py,258,remove 0 distances to avoid division by 0,not
scikit-learn/sklearn/tests/test_random_projection.py,263,check that the automatically tuned values for the density respect the,not
scikit-learn/sklearn/tests/test_random_projection.py,264,contract for eps: pairwise distances are preserved according to the,not
scikit-learn/sklearn/tests/test_random_projection.py,265,Johnson-Lindenstrauss lemma,not
scikit-learn/sklearn/tests/test_random_projection.py,272,"when using sparse input, the projected data can be forced to be a",not
scikit-learn/sklearn/tests/test_random_projection.py,273,dense numpy array,not
scikit-learn/sklearn/tests/test_random_projection.py,282,the output can be left to a sparse matrix instead,not
scikit-learn/sklearn/tests/test_random_projection.py,286,output for dense input will stay dense:,not
scikit-learn/sklearn/tests/test_random_projection.py,289,output for sparse output will be sparse:,not
scikit-learn/sklearn/tests/test_random_projection.py,299,the number of components is adjusted from the shape of the training,not
scikit-learn/sklearn/tests/test_random_projection.py,300,set,not
scikit-learn/sklearn/tests/test_random_projection.py,313,once the RP is 'fitted' the projection is always the same,not
scikit-learn/sklearn/tests/test_random_projection.py,317,fit transform with same random seed will lead to the same results,not
scikit-learn/sklearn/tests/test_random_projection.py,322,Try to transform with an input X of size different from fitted.,not
scikit-learn/sklearn/tests/test_random_projection.py,325,it is also possible to fix the number of components and the density,not
scikit-learn/sklearn/tests/test_random_projection.py,326,level,not
scikit-learn/sklearn/tests/test_random_projection.py,333,close to 1% density,not
scikit-learn/sklearn/tests/test_random_projection.py,334,close to 1% density,not
scikit-learn/sklearn/tests/test_random_projection.py,359,TODO remove in 0.24,SATD
scikit-learn/sklearn/tests/test_multioutput.py,79,Test multi target regression raises,not
scikit-learn/sklearn/tests/test_multioutput.py,111,no exception should be raised if the base estimator supports weights,not
scikit-learn/sklearn/tests/test_multioutput.py,117,weighted regressor,not
scikit-learn/sklearn/tests/test_multioutput.py,124,weighted with different weights,not
scikit-learn/sklearn/tests/test_multioutput.py,133,weighted regressor,not
scikit-learn/sklearn/tests/test_multioutput.py,140,"unweighted, but with repeated samples",not
scikit-learn/sklearn/tests/test_multioutput.py,150,Import the data,not
scikit-learn/sklearn/tests/test_multioutput.py,152,create a multiple targets by randomized shuffling and concatenating y.,not
scikit-learn/sklearn/tests/test_multioutput.py,172,parallelism requires this to be the case for a sane implementation,not
scikit-learn/sklearn/tests/test_multioutput.py,176,check multioutput has predict_proba,not
scikit-learn/sklearn/tests/test_multioutput.py,178,default SGDClassifier has loss='hinge',not
scikit-learn/sklearn/tests/test_multioutput.py,179,which does not expose a predict_proba method,not
scikit-learn/sklearn/tests/test_multioutput.py,185,case where predict_proba attribute exists,not
scikit-learn/sklearn/tests/test_multioutput.py,192,check predict_proba passes,not
scikit-learn/sklearn/tests/test_multioutput.py,197,inner function for custom scoring,not
scikit-learn/sklearn/tests/test_multioutput.py,210,SGDClassifier defaults to loss='hinge' which is not a probabilistic,not
scikit-learn/sklearn/tests/test_multioutput.py,211,loss function; therefore it does not expose a predict_proba method,not
scikit-learn/sklearn/tests/test_multioutput.py,221,test if multi_target initializes correctly with base estimator and fit,not
scikit-learn/sklearn/tests/test_multioutput.py,222,assert predictions work as expected for predict,not
scikit-learn/sklearn/tests/test_multioutput.py,227,train the multi_target_linear and also get the predictions.,not
scikit-learn/sklearn/tests/test_multioutput.py,239,train the linear classification with each column and assert that,not
scikit-learn/sklearn/tests/test_multioutput.py,240,predictions are equal after first partial_fit and second partial_fit,not
scikit-learn/sklearn/tests/test_multioutput.py,242,create a clone with the same state,not
scikit-learn/sklearn/tests/test_multioutput.py,260,test if multi_target initializes correctly with base estimator and fit,not
scikit-learn/sklearn/tests/test_multioutput.py,261,"assert predictions work as expected for predict, prodict_proba and score",not
scikit-learn/sklearn/tests/test_multioutput.py,266,train the multi_target_forest and also get the predictions.,not
scikit-learn/sklearn/tests/test_multioutput.py,281,train the forest with each column and assert that predictions are equal,not
scikit-learn/sklearn/tests/test_multioutput.py,283,create a clone with the same state,not
scikit-learn/sklearn/tests/test_multioutput.py,291,test to check meta of meta estimators,not
scikit-learn/sklearn/tests/test_multioutput.py,301,train the forest with each column and assert that predictions are equal,not
scikit-learn/sklearn/tests/test_multioutput.py,303,create a clone,not
scikit-learn/sklearn/tests/test_multioutput.py,312,make test deterministic,not
scikit-learn/sklearn/tests/test_multioutput.py,315,random features,not
scikit-learn/sklearn/tests/test_multioutput.py,318,random labels,not
scikit-learn/sklearn/tests/test_multioutput.py,319,2 classes,not
scikit-learn/sklearn/tests/test_multioutput.py,320,3 classes,not
scikit-learn/sklearn/tests/test_multioutput.py,346,weighted classifier,not
scikit-learn/sklearn/tests/test_multioutput.py,354,"unweighted, but with repeated samples",not
scikit-learn/sklearn/tests/test_multioutput.py,366,weighted classifier,not
scikit-learn/sklearn/tests/test_multioutput.py,374,"unweighted, but with repeated samples",not
scikit-learn/sklearn/tests/test_multioutput.py,385,"NotFittedError when fit is not done but score, predict and",not
scikit-learn/sklearn/tests/test_multioutput.py,386,and predict_proba are called,not
scikit-learn/sklearn/tests/test_multioutput.py,392,ValueError when number of outputs is different,not
scikit-learn/sklearn/tests/test_multioutput.py,393,for fit and score,not
scikit-learn/sklearn/tests/test_multioutput.py,397,ValueError when y is continuous,not
scikit-learn/sklearn/tests/test_multioutput.py,402,Generate a multilabel data set from a multiclass dataset as a way of,not
scikit-learn/sklearn/tests/test_multioutput.py,403,by representing the integer number of the original class using a binary,not
scikit-learn/sklearn/tests/test_multioutput.py,404,encoding.,not
scikit-learn/sklearn/tests/test_multioutput.py,417,Fit classifier chain and verify predict performance using LinearSVC,not
scikit-learn/sklearn/tests/test_multioutput.py,433,Fit classifier chain with sparse data,not
scikit-learn/sklearn/tests/test_multioutput.py,449,Verify that an ensemble of classifier chains (each of length,not
scikit-learn/sklearn/tests/test_multioutput.py,450,N) can achieve a higher Jaccard similarity score than N independent,not
scikit-learn/sklearn/tests/test_multioutput.py,451,models,not
scikit-learn/sklearn/tests/test_multioutput.py,471,Fit base chain and verify predict performance,not
scikit-learn/sklearn/tests/test_multioutput.py,490,Fit base chain with sparse data cross_val_predict,not
scikit-learn/sklearn/tests/test_multioutput.py,502,Fit base chain with random order,not
scikit-learn/sklearn/tests/test_multioutput.py,514,Randomly ordered chain should behave identically to a fixed order,not
scikit-learn/sklearn/tests/test_multioutput.py,515,chain with the same order.,not
scikit-learn/sklearn/tests/test_multioutput.py,522,Fit chain with cross_val_predict and verify predict,not
scikit-learn/sklearn/tests/test_multioutput.py,523,performance,not
scikit-learn/sklearn/tests/test_multioutput.py,549,Tests classes_ attribute of multioutput classifiers,not
scikit-learn/sklearn/tests/test_multioutput.py,550,RandomForestClassifier supports multioutput out-of-the-box,not
scikit-learn/sklearn/tests/test_multioutput.py,559,TODO: remove in 0.24,SATD
scikit-learn/sklearn/tests/test_multioutput.py,595,Make sure fit_params are properly propagated to the sub-estimators,not
scikit-learn/sklearn/tests/test_multioutput.py,608,Fitting with params,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,15,generate data,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,28,test that AdditiveChi2Sampler approximates kernel on random data,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,30,compute exact kernel,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,31,abbreviations for easier formula,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,37,reduce to n_samples_x x n_samples_y by summing over features,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,40,approximate kernel mapping,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,55,test error is raised on negative input,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,60,test error on invalid sample_steps,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,64,test that the sample interval is set correctly,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,68,test that the sample_interval is initialized correctly,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,72,test that the sample_interval is changed in the fit method,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,76,test that the sample_interval is set correctly,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,86,test that RBFSampler approximates kernel on random data,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,88,compute exact kernel,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,90,set on negative component but greater than c to ensure that the kernel,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,91,approximation is valid on the group (-c; +\infty) endowed with the skewed,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,92,multiplication.,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,95,abbreviations for easier formula,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,99,we do it in log-space in the hope that it's more stable,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,100,this array is n_samples_x x n_samples_y big x n_features,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,103,reduce to n_samples_x x n_samples_y by summing over features in log-space,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,106,approximate kernel mapping,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,119,test error is raised on when inputs contains values smaller than -c,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,138,test that RBFSampler approximates kernel on random data,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,139,compute exact kernel,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,143,approximate kernel mapping,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,150,close to unbiased,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,152,nothing too far off,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,153,mean is fairly close,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,157,Regression test: kernel approx. transformers should work on lists,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,158,No assertions; the old versions would simply crash,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,169,some basic tests,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,173,With n_components = n_samples this is exact,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,182,test callable kernel,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,187,test that available kernels fit and transform,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,199,rbf kernel should behave as gamma=None by default,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,200,aka gamma = 1 / n_features,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,207,chi2 kernel should behave as gamma=1 by default,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,216,test that nystroem works with singular kernel matrix,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,219,duplicate samples,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,232,Non-regression: Nystroem should pass other parameters beside gamma.,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,244,Test Nystroem on a callable.,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,255,test input validation,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,261,"if degree, gamma or coef0 is passed, we raise a warning",not
scikit-learn/sklearn/tests/test_kernel_approximation.py,271,Non-regression: test Nystroem on precomputed kernel.,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,272,PR - 14706,not
scikit-learn/sklearn/tests/test_kernel_approximation.py,281,"if degree, gamma or coef0 is passed, we raise a ValueError",not
scikit-learn/sklearn/tests/test_kernel_ridge.py,40,"alpha=0 causes a LinAlgError in computing the dual coefficients,",not
scikit-learn/sklearn/tests/test_kernel_ridge.py,41,which causes a fallback to a lstsq solver. This is tested here.,not
scikit-learn/sklearn/tests/test_kernel_ridge.py,65,precomputed kernel,not
scikit-learn/sklearn/tests/test_init.py,1,Basic unittests to test functioning of module's top-level,not
scikit-learn/sklearn/tests/test_init.py,9,noqa,not
scikit-learn/sklearn/tests/test_init.py,16,Test either above import has failed for some reason,not
scikit-learn/sklearn/tests/test_init.py,17,"""import *"" is discouraged outside of the module level, hence we",not
scikit-learn/sklearn/tests/test_init.py,18,rely on setting up the variable above,not
scikit-learn/sklearn/tests/test_common.py,5,Authors: Andreas Mueller <amueller@ais.uni-bonn.de>,not
scikit-learn/sklearn/tests/test_common.py,6,Gael Varoquaux gael.varoquaux@normalesup.org,not
scikit-learn/sklearn/tests/test_common.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/tests/test_common.py,41,test that all_estimators doesn't find abstract classes.,not
scikit-learn/sklearn/tests/test_common.py,80,Common tests for estimator instances,not
scikit-learn/sklearn/tests/test_common.py,95,"ignore deprecated open(.., 'U') in numpy distutils",not
scikit-learn/sklearn/tests/test_common.py,97,"Smoke test the 'configure' step of setup, this tests all the",not
scikit-learn/sklearn/tests/test_common.py,98,'configure' functions in the setup.pys in scikit-learn,not
scikit-learn/sklearn/tests/test_common.py,99,This test requires Cython which is not necessarily there when running,not
scikit-learn/sklearn/tests/test_common.py,100,the tests of an installed version of scikit-learn or when scikit-learn,not
scikit-learn/sklearn/tests/test_common.py,101,is installed in editable mode by pip build isolation enabled.,not
scikit-learn/sklearn/tests/test_common.py,108,XXX unreached code as of v0.22,SATD
scikit-learn/sklearn/tests/test_common.py,115,The configuration spits out warnings when not finding,not
scikit-learn/sklearn/tests/test_common.py,116,Blas/Atlas development headers,not
scikit-learn/sklearn/tests/test_common.py,132,FIXME,SATD
scikit-learn/sklearn/tests/test_common.py,148,Smoke test to check that any name in a __all__ list is actually defined,not
scikit-learn/sklearn/tests/test_common.py,149,in the namespace of the module or package.,not
scikit-learn/sklearn/tests/test_common.py,175,"Ensure that for each contentful subpackage, there is a test directory",not
scikit-learn/sklearn/tests/test_common.py,176,within it that is also a subpackage (i.e. a directory with __init__.py),not
scikit-learn/sklearn/tests/test_common.py,198,Make sure passing classes to check_estimator or parametrize_with_checks,not
scikit-learn/sklearn/tests/test_common.py,199,raises an error,not
scikit-learn/sklearn/tests/test_metaestimators.py,49,Ensures specified metaestimators have methods iff subestimator does,not
scikit-learn/sklearn/tests/test_metaestimators.py,119,delegation before fit raises a NotFittedError,not
scikit-learn/sklearn/tests/test_metaestimators.py,132,smoke test delegation,not
scikit-learn/sklearn/tests/test_base.py,1,Author: Gael Varoquaux,not
scikit-learn/sklearn/tests/test_base.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/tests/test_base.py,30,,not
scikit-learn/sklearn/tests/test_base.py,31,A few test classes,not
scikit-learn/sklearn/tests/test_base.py,108,,not
scikit-learn/sklearn/tests/test_base.py,109,The tests,not
scikit-learn/sklearn/tests/test_base.py,112,Tests that clone creates a correct deep copy.,not
scikit-learn/sklearn/tests/test_base.py,113,"We create an estimator, make a copy of its original state",not
scikit-learn/sklearn/tests/test_base.py,114,"(which, in this case, is the current state of the estimator),",not
scikit-learn/sklearn/tests/test_base.py,115,and check that the obtained copy is a correct deep copy.,not
scikit-learn/sklearn/tests/test_base.py,130,Tests that clone doesn't copy everything.,not
scikit-learn/sklearn/tests/test_base.py,131,"We first create an estimator, give it an own attribute, and",not
scikit-learn/sklearn/tests/test_base.py,132,make a copy of its original state. Then we check that the copy doesn't,not
scikit-learn/sklearn/tests/test_base.py,133,have the specific attribute we manually added to the initial estimator.,not
scikit-learn/sklearn/tests/test_base.py,144,Check that clone raises an error on buggy estimators.,not
scikit-learn/sklearn/tests/test_base.py,160,Regression test for cloning estimators with empty arrays,not
scikit-learn/sklearn/tests/test_base.py,171,Regression test for cloning estimators with default parameter as np.nan,not
scikit-learn/sklearn/tests/test_base.py,192,Check that clone works for parameters that are types rather than,not
scikit-learn/sklearn/tests/test_base.py,193,instances,not
scikit-learn/sklearn/tests/test_base.py,201,Check that clone raises expected error message when,not
scikit-learn/sklearn/tests/test_base.py,202,cloning class rather than instance,not
scikit-learn/sklearn/tests/test_base.py,209,Smoke test the repr of the base estimator.,not
scikit-learn/sklearn/tests/test_base.py,222,Smoke test the str of the base estimator,not
scikit-learn/sklearn/tests/test_base.py,248,test nested estimator parameter setting,not
scikit-learn/sklearn/tests/test_base.py,250,non-existing parameter in svc,not
scikit-learn/sklearn/tests/test_base.py,252,non-existing parameter of pipeline,not
scikit-learn/sklearn/tests/test_base.py,254,we don't currently catch if the things in pipeline are estimators,not
scikit-learn/sklearn/tests/test_base.py,255,"bad_pipeline = Pipeline([(""bad"", NoEstimator())])",not
scikit-learn/sklearn/tests/test_base.py,256,"assert_raises(AttributeError, bad_pipeline.set_params,",not
scikit-learn/sklearn/tests/test_base.py,257,bad__stupid_param=True),SATD
scikit-learn/sklearn/tests/test_base.py,261,Make sure all parameters are passed together to set_params,not
scikit-learn/sklearn/tests/test_base.py,262,of nested estimator. Regression test for #9944,not
scikit-learn/sklearn/tests/test_base.py,267,expected_kwargs is in test scope,not
scikit-learn/sklearn/tests/test_base.py,279,"Check that set_params tries to set SVC().C, not",not
scikit-learn/sklearn/tests/test_base.py,280,DecisionTreeClassifier().C,not
scikit-learn/sklearn/tests/test_base.py,290,test both ClassifierMixin and RegressorMixin,not
scikit-learn/sklearn/tests/test_base.py,298,generate random sample weights,not
scikit-learn/sklearn/tests/test_base.py,300,check that the score with and without sample weights are different,not
scikit-learn/sklearn/tests/test_base.py,335,build and clone estimator,not
scikit-learn/sklearn/tests/test_base.py,341,the test,not
scikit-learn/sklearn/tests/test_base.py,353,test that we can predict with the restored decision tree classifier,not
scikit-learn/sklearn/tests/test_base.py,389,"TreeNoVersion has no getstate, like pre-0.18",not
scikit-learn/sklearn/tests/test_base.py,397,check we got the warning about using pre-0.18 pickle,not
scikit-learn/sklearn/tests/test_base.py,485,test that changing tags by inheritance is not allowed,not
scikit-learn/sklearn/tests/test_base.py,518,Checks the display configuration flag controls the json output,not
scikit-learn/sklearn/tests/test_base.py,531,Checks the display configuration flag controls the html output,not
scikit-learn/sklearn/tests/test_import_deprecations.py,9,We are deprecating importing anything that isn't in an __init__ file and,not
scikit-learn/sklearn/tests/test_import_deprecations.py,10,remaming most file.py into _file.py.,not
scikit-learn/sklearn/tests/test_import_deprecations.py,11,"This test makes sure imports are still possible but deprecated, with the",not
scikit-learn/sklearn/tests/test_import_deprecations.py,12,appropriate error message.,not
scikit-learn/sklearn/tests/test_import_deprecations.py,20,"Make sure that ""from deprecated_path import importee"" is still possible",not
scikit-learn/sklearn/tests/test_import_deprecations.py,21,but raises a warning,not
scikit-learn/sklearn/tests/test_import_deprecations.py,22,"We only need one entry per file, no need to check multiple imports from",not
scikit-learn/sklearn/tests/test_import_deprecations.py,23,the same file.,not
scikit-learn/sklearn/tests/test_import_deprecations.py,25,TODO: remove in 0.24,SATD
scikit-learn/sklearn/tests/test_import_deprecations.py,27,Special case for:,not
scikit-learn/sklearn/tests/test_import_deprecations.py,28,https://github.com/scikit-learn/scikit-learn/issues/15842,not
scikit-learn/sklearn/tests/test_build.py,10,Check that sklearn is built with OpenMP-based parallelism enabled.,not
scikit-learn/sklearn/tests/test_build.py,11,This test can be skipped by setting the environment variable,not
scikit-learn/sklearn/tests/test_build.py,12,``SKLEARN_SKIP_OPENMP_TEST``.,not
scikit-learn/sklearn/tests/test_check_build.py,5,Author: G Varoquaux,not
scikit-learn/sklearn/tests/test_check_build.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/tests/test_multiclass.py,45,Fail on multioutput data,not
scikit-learn/sklearn/tests/test_multiclass.py,55,Test that check_classification_target return correct type. #5782,not
scikit-learn/sklearn/tests/test_multiclass.py,62,A classifier which implements decision_function.,not
scikit-learn/sklearn/tests/test_multiclass.py,71,A classifier which implements predict_proba.,not
scikit-learn/sklearn/tests/test_multiclass.py,78,Test if partial_fit is working as intended,not
scikit-learn/sklearn/tests/test_multiclass.py,91,Test when mini batches doesn't have all classes,not
scikit-learn/sklearn/tests/test_multiclass.py,92,with SGDClassifier,not
scikit-learn/sklearn/tests/test_multiclass.py,106,test partial_fit only exists if estimator has it:,not
scikit-learn/sklearn/tests/test_multiclass.py,116,A new class value which was not in the first call of partial_fit,not
scikit-learn/sklearn/tests/test_multiclass.py,117,It should raise ValueError,not
scikit-learn/sklearn/tests/test_multiclass.py,125,test that ovr and ovo work on regressors which don't have a decision_,not
scikit-learn/sklearn/tests/test_multiclass.py,126,function,not
scikit-learn/sklearn/tests/test_multiclass.py,131,we are doing something sensible,not
scikit-learn/sklearn/tests/test_multiclass.py,138,we are doing something sensible,not
scikit-learn/sklearn/tests/test_multiclass.py,168,Test predict_proba,not
scikit-learn/sklearn/tests/test_multiclass.py,171,predict assigns a label if the probability that the,not
scikit-learn/sklearn/tests/test_multiclass.py,172,sample has the label is greater than 0.5.,not
scikit-learn/sklearn/tests/test_multiclass.py,176,Test decision_function,not
scikit-learn/sklearn/tests/test_multiclass.py,184,Test that ovr works with classes that are always present or absent.,not
scikit-learn/sklearn/tests/test_multiclass.py,185,Note: tests is the case where _ConstantPredictor is utilised,not
scikit-learn/sklearn/tests/test_multiclass.py,189,Build an indicator matrix where two features are always on.,not
scikit-learn/sklearn/tests/test_multiclass.py,190,"As list of lists, it would be: [[int(i >= 5), 2, 3] for i in range(10)]",not
scikit-learn/sklearn/tests/test_multiclass.py,205,y has a constantly absent label,not
scikit-learn/sklearn/tests/test_multiclass.py,207,variable label,not
scikit-learn/sklearn/tests/test_multiclass.py,215,Toy dataset where features correspond directly to labels.,not
scikit-learn/sklearn/tests/test_multiclass.py,234,test input as label indicator matrix,not
scikit-learn/sklearn/tests/test_multiclass.py,241,Toy dataset where features correspond directly to labels.,not
scikit-learn/sklearn/tests/test_multiclass.py,264,test input as label indicator matrix,not
scikit-learn/sklearn/tests/test_multiclass.py,279,Toy dataset where features correspond directly to labels.,not
scikit-learn/sklearn/tests/test_multiclass.py,341,Decision function only estimator.,not
scikit-learn/sklearn/tests/test_multiclass.py,345,"Estimator with predict_proba disabled, depending on parameters.",not
scikit-learn/sklearn/tests/test_multiclass.py,352,Estimator which can get predict_proba enabled after fitting,not
scikit-learn/sklearn/tests/test_multiclass.py,363,predict assigns a label if the probability that the,not
scikit-learn/sklearn/tests/test_multiclass.py,364,sample has the label is greater than 0.5.,not
scikit-learn/sklearn/tests/test_multiclass.py,376,Decision function only estimator.,not
scikit-learn/sklearn/tests/test_multiclass.py,384,predict assigns a label if the probability that the,not
scikit-learn/sklearn/tests/test_multiclass.py,385,sample has the label is greater than 0.5.,not
scikit-learn/sklearn/tests/test_multiclass.py,426,Test with pipeline of length one,not
scikit-learn/sklearn/tests/test_multiclass.py,427,This test is needed because the multiclass estimators may fail to detect,not
scikit-learn/sklearn/tests/test_multiclass.py,428,the presence of predict_proba or decision_function.,not
scikit-learn/sklearn/tests/test_multiclass.py,440,SVC has sparse coef with sparse input data,not
scikit-learn/sklearn/tests/test_multiclass.py,444,test with dense and sparse coef,not
scikit-learn/sklearn/tests/test_multiclass.py,449,don't densify sparse coefficients,not
scikit-learn/sklearn/tests/test_multiclass.py,455,Not fitted exception!,not
scikit-learn/sklearn/tests/test_multiclass.py,457,lambda is needed because we don't want coef_ to be evaluated right away,not
scikit-learn/sklearn/tests/test_multiclass.py,460,Doesn't have coef_ exception!,not
scikit-learn/sklearn/tests/test_multiclass.py,472,Test that OneVsOne fitting works with a list of targets and yields the,not
scikit-learn/sklearn/tests/test_multiclass.py,473,same output as predict from an array,not
scikit-learn/sklearn/tests/test_multiclass.py,483,A classifier which implements decision_function.,not
scikit-learn/sklearn/tests/test_multiclass.py,488,A classifier which implements predict_proba.,not
scikit-learn/sklearn/tests/test_multiclass.py,509,Test when mini-batches have binary target classes,not
scikit-learn/sklearn/tests/test_multiclass.py,531,raises error when mini-batch does not have classes from all_classes,not
scikit-learn/sklearn/tests/test_multiclass.py,540,test partial_fit only exists if estimator has it:,not
scikit-learn/sklearn/tests/test_multiclass.py,549,first binary,not
scikit-learn/sklearn/tests/test_multiclass.py,554,then multi-class,not
scikit-learn/sklearn/tests/test_multiclass.py,561,Compute the votes,not
scikit-learn/sklearn/tests/test_multiclass.py,572,Extract votes and verify,not
scikit-learn/sklearn/tests/test_multiclass.py,576,"For each sample and each class, there only 3 possible vote levels",not
scikit-learn/sklearn/tests/test_multiclass.py,577,because they are only 3 distinct class pairs thus 3 distinct,not
scikit-learn/sklearn/tests/test_multiclass.py,578,binary classifiers.,not
scikit-learn/sklearn/tests/test_multiclass.py,579,"Therefore, sorting predictions based on votes would yield",not
scikit-learn/sklearn/tests/test_multiclass.py,580,mostly tied predictions:,not
scikit-learn/sklearn/tests/test_multiclass.py,583,The OVO decision function on the other hand is able to resolve,not
scikit-learn/sklearn/tests/test_multiclass.py,584,most of the ties on this data as it combines both the vote counts,not
scikit-learn/sklearn/tests/test_multiclass.py,585,and the aggregated confidence levels of the binary classifiers,not
scikit-learn/sklearn/tests/test_multiclass.py,586,to compute the aggregate decision function. The iris dataset,not
scikit-learn/sklearn/tests/test_multiclass.py,587,has 150 samples with a couple of duplicates. The OvO decisions,not
scikit-learn/sklearn/tests/test_multiclass.py,588,can resolve most of the ties:,not
scikit-learn/sklearn/tests/test_multiclass.py,602,"Test that ties are broken using the decision function,",not
scikit-learn/sklearn/tests/test_multiclass.py,603,not defaulting to the smallest label,not
scikit-learn/sklearn/tests/test_multiclass.py,611,"Classifiers are in order 0-1, 0-2, 1-2",not
scikit-learn/sklearn/tests/test_multiclass.py,612,Use decision_function to compute the votes and the normalized,not
scikit-learn/sklearn/tests/test_multiclass.py,613,"sum_of_confidences, which is used to disambiguate when there is a tie in",not
scikit-learn/sklearn/tests/test_multiclass.py,614,votes.,not
scikit-learn/sklearn/tests/test_multiclass.py,618,"For the first point, there is one vote per class",not
scikit-learn/sklearn/tests/test_multiclass.py,620,"For the rest, there is no tie and the prediction is the argmax",not
scikit-learn/sklearn/tests/test_multiclass.py,622,"For the tie, the prediction is the class with the highest score",not
scikit-learn/sklearn/tests/test_multiclass.py,627,test that ties can not only be won by the first two labels,not
scikit-learn/sklearn/tests/test_multiclass.py,631,cycle through labels so that each label wins once,not
scikit-learn/sklearn/tests/test_multiclass.py,641,Test that the OvO doesn't mess up the encoding of string labels,not
scikit-learn/sklearn/tests/test_multiclass.py,651,Test error for OvO with one class,not
scikit-learn/sklearn/tests/test_multiclass.py,660,Test that the OvO errors on float targets,not
scikit-learn/sklearn/tests/test_multiclass.py,674,A classifier which implements decision_function.,not
scikit-learn/sklearn/tests/test_multiclass.py,680,A classifier which implements predict_proba.,not
scikit-learn/sklearn/tests/test_multiclass.py,697,Test that the OCC errors on float targets,not
scikit-learn/sklearn/tests/test_config.py,10,Not using as a context manager affects nothing,not
scikit-learn/sklearn/tests/test_config.py,32,global setting will not be retained outside of context that,not
scikit-learn/sklearn/tests/test_config.py,33,did not modify this setting,not
scikit-learn/sklearn/tests/test_config.py,45,No positional arguments,not
scikit-learn/sklearn/tests/test_config.py,47,No unknown arguments,not
scikit-learn/sklearn/tests/test_config.py,73,No unknown arguments,not
scikit-learn/sklearn/tests/test_pipeline.py,37,noqa,not
scikit-learn/sklearn/tests/test_pipeline.py,144,store timestamp to figure out whether the result of 'fit' has been,not
scikit-learn/sklearn/tests/test_pipeline.py,145,cached or not,not
scikit-learn/sklearn/tests/test_pipeline.py,162,Test the various init parameters of the pipeline.,not
scikit-learn/sklearn/tests/test_pipeline.py,164,Check that we can't instantiate pipelines with objects without fit,not
scikit-learn/sklearn/tests/test_pipeline.py,165,method,not
scikit-learn/sklearn/tests/test_pipeline.py,171,Smoke test with only an estimator,not
scikit-learn/sklearn/tests/test_pipeline.py,178,Check that params are set,not
scikit-learn/sklearn/tests/test_pipeline.py,182,Smoke test the repr:,not
scikit-learn/sklearn/tests/test_pipeline.py,185,Test with two objects,not
scikit-learn/sklearn/tests/test_pipeline.py,190,Check that estimators are not cloned on pipeline construction,not
scikit-learn/sklearn/tests/test_pipeline.py,194,Check that we can't instantiate with non-transformers on the way,not
scikit-learn/sklearn/tests/test_pipeline.py,195,"Note that NoTrans implements fit, but not transform",not
scikit-learn/sklearn/tests/test_pipeline.py,201,Check that params are set,not
scikit-learn/sklearn/tests/test_pipeline.py,204,Smoke test the repr:,not
scikit-learn/sklearn/tests/test_pipeline.py,207,Check that params are not set when naming them wrong,not
scikit-learn/sklearn/tests/test_pipeline.py,210,Test clone,not
scikit-learn/sklearn/tests/test_pipeline.py,214,"Check that apart from estimators, the parameters are the same",not
scikit-learn/sklearn/tests/test_pipeline.py,224,Remove estimators that where copied,not
scikit-learn/sklearn/tests/test_pipeline.py,233,Pipeline accepts steps as tuple,not
scikit-learn/sklearn/tests/test_pipeline.py,245,Test the various methods of the pipeline (anova).,not
scikit-learn/sklearn/tests/test_pipeline.py,248,Test with Anova + LogisticRegression,not
scikit-learn/sklearn/tests/test_pipeline.py,260,Test that the pipeline can take fit parameters,not
scikit-learn/sklearn/tests/test_pipeline.py,263,classifier should return True,not
scikit-learn/sklearn/tests/test_pipeline.py,265,and transformer params should not be changed,not
scikit-learn/sklearn/tests/test_pipeline.py,268,invalid parameters should raise an error message,not
scikit-learn/sklearn/tests/test_pipeline.py,277,Pipeline should pass sample_weight,not
scikit-learn/sklearn/tests/test_pipeline.py,288,When sample_weight is None it shouldn't be passed,not
scikit-learn/sklearn/tests/test_pipeline.py,302,Test pipeline raises set params error message for nested models.,not
scikit-learn/sklearn/tests/test_pipeline.py,305,expected error message,not
scikit-learn/sklearn/tests/test_pipeline.py,315,nested model check,not
scikit-learn/sklearn/tests/test_pipeline.py,323,Test the various methods of the pipeline (pca + svm).,not
scikit-learn/sklearn/tests/test_pipeline.py,326,Test with PCA + SVC,not
scikit-learn/sklearn/tests/test_pipeline.py,339,Test that the score_samples method is implemented on a pipeline.,not
scikit-learn/sklearn/tests/test_pipeline.py,340,Test that the score_samples method on pipeline yields same results as,not
scikit-learn/sklearn/tests/test_pipeline.py,341,applying transform and score_samples steps separately.,not
scikit-learn/sklearn/tests/test_pipeline.py,346,Check the shapes,not
scikit-learn/sklearn/tests/test_pipeline.py,348,Check the values,not
scikit-learn/sklearn/tests/test_pipeline.py,356,Test that a pipeline does not have score_samples method when the final,not
scikit-learn/sklearn/tests/test_pipeline.py,357,step of the pipeline does not have score_samples defined.,not
scikit-learn/sklearn/tests/test_pipeline.py,367,Test the various methods of the pipeline (preprocessing + svm).,not
scikit-learn/sklearn/tests/test_pipeline.py,380,check shapes of various prediction functions,not
scikit-learn/sklearn/tests/test_pipeline.py,397,test that the fit_predict method is implemented on a pipeline,not
scikit-learn/sklearn/tests/test_pipeline.py,398,test that the fit_predict on pipeline yields same results as applying,not
scikit-learn/sklearn/tests/test_pipeline.py,399,transform and clustering steps separately,not
scikit-learn/sklearn/tests/test_pipeline.py,402,"As pipeline doesn't clone estimators on construction,",not
scikit-learn/sklearn/tests/test_pipeline.py,403,it must have its own estimators,not
scikit-learn/sklearn/tests/test_pipeline.py,407,first compute the transform and clustering step separately,not
scikit-learn/sklearn/tests/test_pipeline.py,411,use a pipeline to do the transform and clustering in one step,not
scikit-learn/sklearn/tests/test_pipeline.py,422,tests that a pipeline does not have fit_predict method when final,not
scikit-learn/sklearn/tests/test_pipeline.py,423,step of pipeline does not have fit_predict defined,not
scikit-learn/sklearn/tests/test_pipeline.py,433,tests that Pipeline passes fit_params to intermediate steps,not
scikit-learn/sklearn/tests/test_pipeline.py,434,when fit_predict is invoked,not
scikit-learn/sklearn/tests/test_pipeline.py,446,tests that Pipeline passes predict_params to the final estimator,not
scikit-learn/sklearn/tests/test_pipeline.py,447,when predict is invoked,not
scikit-learn/sklearn/tests/test_pipeline.py,456,basic sanity check for feature union,not
scikit-learn/sklearn/tests/test_pipeline.py,467,check if it does the expected thing,not
scikit-learn/sklearn/tests/test_pipeline.py,472,test if it also works for sparse input,not
scikit-learn/sklearn/tests/test_pipeline.py,473,We use a different svd object to control the random_state stream,not
scikit-learn/sklearn/tests/test_pipeline.py,479,Test clone,not
scikit-learn/sklearn/tests/test_pipeline.py,483,test setting parameters,not
scikit-learn/sklearn/tests/test_pipeline.py,487,test it works with transformers missing fit_transform,not
scikit-learn/sklearn/tests/test_pipeline.py,492,test error if some elements do not support transform,not
scikit-learn/sklearn/tests/test_pipeline.py,499,test that init accepts tuples,not
scikit-learn/sklearn/tests/test_pipeline.py,519,invalid keyword parameters should raise an error message,not
scikit-learn/sklearn/tests/test_pipeline.py,528,Test whether pipeline works with a transformer at the end.,not
scikit-learn/sklearn/tests/test_pipeline.py,529,Also test pipeline.transform and pipeline.inverse_transform,not
scikit-learn/sklearn/tests/test_pipeline.py,534,test transform and fit_transform:,not
scikit-learn/sklearn/tests/test_pipeline.py,547,Test whether pipeline works with a transformer missing fit_transform,not
scikit-learn/sklearn/tests/test_pipeline.py,553,test fit_transform:,not
scikit-learn/sklearn/tests/test_pipeline.py,588,Directly setting attr,not
scikit-learn/sklearn/tests/test_pipeline.py,594,Using set_params,not
scikit-learn/sklearn/tests/test_pipeline.py,598,Using set_params to replace single step,not
scikit-learn/sklearn/tests/test_pipeline.py,602,With invalid data,not
scikit-learn/sklearn/tests/test_pipeline.py,613,Test access via named_steps bunch object,not
scikit-learn/sklearn/tests/test_pipeline.py,619,Test bunch with conflict attribute of dict,not
scikit-learn/sklearn/tests/test_pipeline.py,686,"for other methods, ensure no AttributeErrors on None:",not
scikit-learn/sklearn/tests/test_pipeline.py,700,mult2 and mult3 are active,not
scikit-learn/sklearn/tests/test_pipeline.py,709,Check 'passthrough' step at construction time,not
scikit-learn/sklearn/tests/test_pipeline.py,768,test feature union with transformer weights,not
scikit-learn/sklearn/tests/test_pipeline.py,773,test using fit followed by transform,not
scikit-learn/sklearn/tests/test_pipeline.py,778,test using fit_transform,not
scikit-learn/sklearn/tests/test_pipeline.py,782,test it works with transformers missing fit_transform,not
scikit-learn/sklearn/tests/test_pipeline.py,786,check against expected result,not
scikit-learn/sklearn/tests/test_pipeline.py,788,We use a different pca object to control the random_state stream,not
scikit-learn/sklearn/tests/test_pipeline.py,800,test that n_jobs work for FeatureUnion,not
scikit-learn/sklearn/tests/test_pipeline.py,830,fit_transform should behave the same,not
scikit-learn/sklearn/tests/test_pipeline.py,837,transformers should stay fit after fit_transform,not
scikit-learn/sklearn/tests/test_pipeline.py,887,Directly setting attr,not
scikit-learn/sklearn/tests/test_pipeline.py,892,Using set_params,not
scikit-learn/sklearn/tests/test_pipeline.py,897,Using set_params to replace single step,not
scikit-learn/sklearn/tests/test_pipeline.py,903,TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion,SATD
scikit-learn/sklearn/tests/test_pipeline.py,932,check we can change back,not
scikit-learn/sklearn/tests/test_pipeline.py,938,Check 'drop' step at construction time,not
scikit-learn/sklearn/tests/test_pipeline.py,951,we validate in construction (despite scikit-learn convention),not
scikit-learn/sklearn/tests/test_pipeline.py,959,three ways to make invalid:,not
scikit-learn/sklearn/tests/test_pipeline.py,960,- construction,not
scikit-learn/sklearn/tests/test_pipeline.py,964,- setattr,not
scikit-learn/sklearn/tests/test_pipeline.py,971,- set_params,not
scikit-learn/sklearn/tests/test_pipeline.py,990,Test that an error is raised when memory is not a string or a Memory,not
scikit-learn/sklearn/tests/test_pipeline.py,991,instance,not
scikit-learn/sklearn/tests/test_pipeline.py,994,Define memory as an integer,not
scikit-learn/sklearn/tests/test_pipeline.py,1031,Deal with change of API in joblib,not
scikit-learn/sklearn/tests/test_pipeline.py,1035,Test with Transformer + SVC,not
scikit-learn/sklearn/tests/test_pipeline.py,1042,Memoize the transformer at the first fit,not
scikit-learn/sklearn/tests/test_pipeline.py,1045,Get the time stamp of the transformer in the cached pipeline,not
scikit-learn/sklearn/tests/test_pipeline.py,1047,Check that cached_pipe and pipe yield identical results,not
scikit-learn/sklearn/tests/test_pipeline.py,1056,Check that we are reading the cache while fitting,not
scikit-learn/sklearn/tests/test_pipeline.py,1057,a second time,not
scikit-learn/sklearn/tests/test_pipeline.py,1059,Check that cached_pipe and pipe yield identical results,not
scikit-learn/sklearn/tests/test_pipeline.py,1068,Create a new pipeline with cloned estimators,not
scikit-learn/sklearn/tests/test_pipeline.py,1069,Check that even changing the name step does not affect the cache hit,not
scikit-learn/sklearn/tests/test_pipeline.py,1076,Check that cached_pipe and pipe yield identical results,not
scikit-learn/sklearn/tests/test_pipeline.py,1093,Deal with change of API in joblib,not
scikit-learn/sklearn/tests/test_pipeline.py,1167,make sure pipelines delegate n_features_in to the first step,not
scikit-learn/sklearn/tests/test_pipeline.py,1179,if the first step has the n_features_in attribute then the pipeline also,not
scikit-learn/sklearn/tests/test_pipeline.py,1180,"has it, even though it isn't fitted.",not
scikit-learn/sklearn/tests/test_pipeline.py,1190,make sure FeatureUnion delegates n_features_in to the first transformer,not
scikit-learn/sklearn/tests/test_pipeline.py,1201,if the first step has the n_features_in attribute then the feature_union,not
scikit-learn/sklearn/tests/test_pipeline.py,1202,"also has it, even though it isn't fitted.",not
scikit-learn/sklearn/tests/test_pipeline.py,1210,Regression test for issue: #15117,not
scikit-learn/sklearn/tests/test_pipeline.py,1232,TODO: Remove in 0.24 when None is removed,SATD
scikit-learn/sklearn/tests/test_docstring_parameters.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,2,Raghav RV <rvraghav93@gmail.com>,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,29,"walk_packages() ignores DeprecationWarnings, now we need to ignore",not
scikit-learn/sklearn/tests/test_docstring_parameters.py,30,FutureWarnings,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,36,"mypy error: Module has no attribute ""__path__""",not
scikit-learn/sklearn/tests/test_docstring_parameters.py,37,type: ignore  # mypy issue #1422,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,41,functions to ignore args / docstring of,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,50,Methods where y param should be ignored if y=None by default,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,61,numpydoc 0.8.0's docscrape tool raises because of collections.abc under,SATD
scikit-learn/sklearn/tests/test_docstring_parameters.py,62,Python 3.7,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,67,Test module docstring formatting,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,69,Skip test if numpydoc is not found,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,73,XXX unreached code as of v0.22,SATD
scikit-learn/sklearn/tests/test_docstring_parameters.py,79,We cannot always control these docstrings,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,84,Exclude imported classes,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,111,Now skip docstring test for y when y is None,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,112,by default for API reason,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,117,ignore y for fit and score,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,125,Exclude imported functions,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,128,Don't test private methods / functions,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,145,Test that there are no tabs in our source files,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,153,because we don't import,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,156,TODO: Remove when minimum python version is 3.7,SATD
scikit-learn/sklearn/tests/test_docstring_parameters.py,157,unwrap to get module because Pep562 backport wraps the original,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,158,module,not
scikit-learn/sklearn/tests/test_docstring_parameters.py,164,"user probably should have run ""make clean""",not
scikit-learn/sklearn/tests/test_docstring_parameters.py,201,TO BE REMOVED for v0.25 (avoid FutureWarning),not
scikit-learn/sklearn/tests/test_docstring_parameters.py,225,"As certain attributes are present ""only"" if a certain parameter is",not
scikit-learn/sklearn/tests/test_docstring_parameters.py,226,"provided, this checks if the word ""only"" is present in the attribute",not
scikit-learn/sklearn/tests/test_docstring_parameters.py,227,"description, and if not the attribute is required to be present.",not
scikit-learn/sklearn/neighbors/_regression.py,3,Authors: Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/neighbors/_regression.py,4,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/neighbors/_regression.py,5,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/neighbors/_regression.py,6,Sparseness support by Lars Buitinck,not
scikit-learn/sklearn/neighbors/_regression.py,7,Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/neighbors/_regression.py,8,Empty radius support by Andreas Bjerre-Nielsen,not
scikit-learn/sklearn/neighbors/_regression.py,9,,not
scikit-learn/sklearn/neighbors/_regression.py,10,"License: BSD 3 clause (C) INRIA, University of Amsterdam,",not
scikit-learn/sklearn/neighbors/_regression.py,11,University of Copenhagen,not
scikit-learn/sklearn/neighbors/_regression.py,157,For cross-validation routines to split data correctly,not
scikit-learn/sklearn/neighbors/_classification.py,3,Authors: Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/neighbors/_classification.py,4,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/neighbors/_classification.py,5,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/neighbors/_classification.py,6,Sparseness support by Lars Buitinck,not
scikit-learn/sklearn/neighbors/_classification.py,7,Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/neighbors/_classification.py,8,,not
scikit-learn/sklearn/neighbors/_classification.py,9,"License: BSD 3 clause (C) INRIA, University of Amsterdam",not
scikit-learn/sklearn/neighbors/_classification.py,239,a simple ':' index doesn't work right,not
scikit-learn/sklearn/neighbors/_classification.py,240,loop is O(n_neighbors),not
scikit-learn/sklearn/neighbors/_classification.py,243,"normalize 'votes' into real [0,1] probabilities",not
scikit-learn/sklearn/neighbors/_classification.py,422,"iterate over multi-output, get the most frequest label for each",not
scikit-learn/sklearn/neighbors/_classification.py,423,output.,not
scikit-learn/sklearn/neighbors/_classification.py,443,ensure the outlier lable for each output is a scalar.,not
scikit-learn/sklearn/neighbors/_classification.py,448,ensure the dtype of outlier label is consistent with y.,not
scikit-learn/sklearn/neighbors/_classification.py,483,"iterate over multi-output, assign labels based on probabilities",not
scikit-learn/sklearn/neighbors/_classification.py,484,of each output.,not
scikit-learn/sklearn/neighbors/_classification.py,542,"iterate over multi-output, measure probabilities of the k-th output.",not
scikit-learn/sklearn/neighbors/_classification.py,550,samples have different size of neighbors within the same radius,not
scikit-learn/sklearn/neighbors/_classification.py,573,"normalize 'votes' into real [0,1] probabilities",not
scikit-learn/sklearn/neighbors/_kde.py,5,Author: Jake Vanderplas <jakevdp@cs.washington.edu>,not
scikit-learn/sklearn/neighbors/_kde.py,24,TODO: implement a brute force version for testing purposes,SATD
scikit-learn/sklearn/neighbors/_kde.py,25,TODO: bandwidth estimation,SATD
scikit-learn/sklearn/neighbors/_kde.py,26,TODO: create a density estimation base class?,SATD
scikit-learn/sklearn/neighbors/_kde.py,107,run the choose algorithm code so that exceptions will happen here,not
scikit-learn/sklearn/neighbors/_kde.py,108,"we're using clone() in the GenerativeBayes classifier,",not
scikit-learn/sklearn/neighbors/_kde.py,109,so we can't do this kind of logic in __init__,not
scikit-learn/sklearn/neighbors/_kde.py,118,"given the algorithm string + metric string, choose the optimal",not
scikit-learn/sklearn/neighbors/_kde.py,119,algorithm to compute the result.,not
scikit-learn/sklearn/neighbors/_kde.py,121,use KD Tree if possible,not
scikit-learn/sklearn/neighbors/_kde.py,192,The returned density is normalized to the number of points.,not
scikit-learn/sklearn/neighbors/_kde.py,193,"For it to be a probability, we must scale it.  For this reason",not
scikit-learn/sklearn/neighbors/_kde.py,194,we'll also scale atol.,not
scikit-learn/sklearn/neighbors/_kde.py,250,TODO: implement sampling for other valid kernel shapes,SATD
scikit-learn/sklearn/neighbors/_kde.py,268,"we first draw points from a d-dimensional normal distribution,",not
scikit-learn/sklearn/neighbors/_kde.py,269,then use an incomplete gamma function to map them to a uniform,not
scikit-learn/sklearn/neighbors/_kde.py,270,d-dimensional tophat distribution.,not
scikit-learn/sklearn/neighbors/_graph.py,3,Author: Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/neighbors/_graph.py,4,Tom Dupre la Tour,not
scikit-learn/sklearn/neighbors/_graph.py,5,,not
scikit-learn/sklearn/neighbors/_graph.py,6,"License: BSD 3 clause (C) INRIA, University of Amsterdam",not
scikit-learn/sklearn/neighbors/_graph.py,33,it does not include each sample as its own neighbors,not
scikit-learn/sklearn/neighbors/_nca.py,1,coding: utf-8,not
scikit-learn/sklearn/neighbors/_nca.py,6,Authors: William de Vazelhes <wdevazelhes@gmail.com>,not
scikit-learn/sklearn/neighbors/_nca.py,7,John Chiotellis <ioannis.chiotellis@in.tum.de>,not
scikit-learn/sklearn/neighbors/_nca.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/neighbors/_nca.py,195,"Verify inputs X and y and NCA parameters, and transform a copy if",not
scikit-learn/sklearn/neighbors/_nca.py,196,needed,not
scikit-learn/sklearn/neighbors/_nca.py,199,Initialize the random generator,not
scikit-learn/sklearn/neighbors/_nca.py,202,Measure the total training time,not
scikit-learn/sklearn/neighbors/_nca.py,205,Compute a mask that stays fixed during optimization:,not
scikit-learn/sklearn/neighbors/_nca.py,207,"(n_samples, n_samples)",not
scikit-learn/sklearn/neighbors/_nca.py,209,Initialize the transformation,not
scikit-learn/sklearn/neighbors/_nca.py,212,Create a dictionary of parameters to be passed to the optimizer,not
scikit-learn/sklearn/neighbors/_nca.py,224,Call the optimizer,not
scikit-learn/sklearn/neighbors/_nca.py,228,Reshape the solution found by the optimizer,not
scikit-learn/sklearn/neighbors/_nca.py,231,Stop timer,not
scikit-learn/sklearn/neighbors/_nca.py,236,Warn the user if the algorithm did not converge,not
scikit-learn/sklearn/neighbors/_nca.py,303,"Validate the inputs X and y, and converts y to numerical classes.",not
scikit-learn/sklearn/neighbors/_nca.py,308,Check the preferred dimensionality of the projected space,not
scikit-learn/sklearn/neighbors/_nca.py,320,"If warm_start is enabled, check that the inputs are consistent",not
scikit-learn/sklearn/neighbors/_nca.py,338,Check how the linear transformation should be initialized,not
scikit-learn/sklearn/neighbors/_nca.py,344,Assert that init.shape[1] = X.shape[1],not
scikit-learn/sklearn/neighbors/_nca.py,352,Assert that init.shape[0] <= init.shape[1],not
scikit-learn/sklearn/neighbors/_nca.py,361,Assert that self.n_components = init.shape[0],not
scikit-learn/sklearn/neighbors/_nca.py,497,"(n_samples, n_components)",not
scikit-learn/sklearn/neighbors/_nca.py,499,Compute softmax distances,not
scikit-learn/sklearn/neighbors/_nca.py,502,"(n_samples, n_samples)",not
scikit-learn/sklearn/neighbors/_nca.py,504,Compute loss,not
scikit-learn/sklearn/neighbors/_nca.py,506,"(n_samples, 1)",not
scikit-learn/sklearn/neighbors/_nca.py,509,Compute gradient of loss w.r.t. `transform`,not
scikit-learn/sklearn/neighbors/_nca.py,514,time complexity of the gradient: O(n_components x n_samples x (,not
scikit-learn/sklearn/neighbors/_nca.py,515,n_samples + n_features)),not
scikit-learn/sklearn/neighbors/_lof.py,1,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,not
scikit-learn/sklearn/neighbors/_lof.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/neighbors/_lof.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/neighbors/_lof.py,206,"As fit_predict would be different from fit.predict, fit_predict is",not
scikit-learn/sklearn/neighbors/_lof.py,207,only available for outlier detection (novelty=False),not
scikit-learn/sklearn/neighbors/_lof.py,234,"As fit_predict would be different from fit.predict, fit_predict is",not
scikit-learn/sklearn/neighbors/_lof.py,235,only available for outlier detection (novelty=False),not
scikit-learn/sklearn/neighbors/_lof.py,277,Compute lof score over training samples to define offset_:,not
scikit-learn/sklearn/neighbors/_lof.py,284,"inliers score around -1 (the higher, the less abnormal).",not
scikit-learn/sklearn/neighbors/_lof.py,488,as bigger is better:,SATD
scikit-learn/sklearn/neighbors/_lof.py,516,1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,6,Author: Robert Layton <robertlayton@gmail.com>,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,7,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,8,,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,109,"If X is sparse and the metric is ""manhattan"", store it in a csc",not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,110,format is easier to calculate the median.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,130,Mask mapping each class to its members.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,132,Number of clusters in each class.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,141,XXX: Update other averaging methods according to the metrics.,SATD
scikit-learn/sklearn/neighbors/_nearest_centroid.py,143,NumPy does not calculate median of sparse matrices.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,159,m parameter for determining deviation,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,161,Calculate deviation using the standard deviation of centroids.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,165,To deter outliers from affecting the results.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,166,Reshape to allow broadcasting.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,169,"Soft thresholding: if the deviation crosses 0 during shrinking,",not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,170,it becomes zero.,not
scikit-learn/sklearn/neighbors/_nearest_centroid.py,175,Now adjust the centroids using the deviation,not
scikit-learn/sklearn/neighbors/_base.py,2,Authors: Jake Vanderplas <vanderplas@astro.washington.edu>,not
scikit-learn/sklearn/neighbors/_base.py,3,Fabian Pedregosa <fabian.pedregosa@inria.fr>,not
scikit-learn/sklearn/neighbors/_base.py,4,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/neighbors/_base.py,5,Sparseness support by Lars Buitinck,not
scikit-learn/sklearn/neighbors/_base.py,6,Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>,not
scikit-learn/sklearn/neighbors/_base.py,7,,not
scikit-learn/sklearn/neighbors/_base.py,8,"License: BSD 3 clause (C) INRIA, University of Amsterdam",not
scikit-learn/sklearn/neighbors/_base.py,35,The following list comes from the,not
scikit-learn/sklearn/neighbors/_base.py,36,sklearn.metrics.pairwise doc string,not
scikit-learn/sklearn/neighbors/_base.py,82,if user attempts to classify a point that was zero distance from one,not
scikit-learn/sklearn/neighbors/_base.py,83,"or more training points, those training points are weighted as 1.0",not
scikit-learn/sklearn/neighbors/_base.py,84,and the other points as 0.0,not
scikit-learn/sklearn/neighbors/_base.py,87,check if point_dist is iterable,not
scikit-learn/sklearn/neighbors/_base.py,88,(ex: RadiusNeighborClassifier.predict may set an element of,not
scikit-learn/sklearn/neighbors/_base.py,89,dist to 1e-6 to represent an 'outlier'),not
scikit-learn/sklearn/neighbors/_base.py,172,if each sample has the same number of provided neighbors,not
scikit-learn/sklearn/neighbors/_base.py,217,number of neighbors by samples,not
scikit-learn/sklearn/neighbors/_base.py,228,if each sample has the same number of provided neighbors,not
scikit-learn/sklearn/neighbors/_base.py,325,callable metric is only valid for brute force and ball_tree,not
scikit-learn/sklearn/neighbors/_base.py,361,"For minkowski distance, use more efficient methods where available",not
scikit-learn/sklearn/neighbors/_base.py,407,Precomputed matrix X must be squared,not
scikit-learn/sklearn/neighbors/_base.py,436,"A tree approach is better for small number of neighbors,",SATD
scikit-learn/sklearn/neighbors/_base.py,437,and KDTree is generally faster when available,not
scikit-learn/sklearn/neighbors/_base.py,482,For cross-validation routines to split data correctly,not
scikit-learn/sklearn/neighbors/_base.py,521,"argpartition doesn't guarantee sorted order, so we sort again",not
scikit-learn/sklearn/neighbors/_base.py,610,Include an extra neighbor to account for the sample itself being,not
scikit-learn/sklearn/neighbors/_base.py,611,"returned, which is removed later",not
scikit-learn/sklearn/neighbors/_base.py,635,"for efficiency, use squared euclidean distances",not
scikit-learn/sklearn/neighbors/_base.py,654,Deal with change of API in joblib,not
scikit-learn/sklearn/neighbors/_base.py,680,"If the query data is the same as the indexed data, we would like",not
scikit-learn/sklearn/neighbors/_base.py,681,"to ignore the first nearest neighbor of every sample, i.e",not
scikit-learn/sklearn/neighbors/_base.py,682,the sample itself.,not
scikit-learn/sklearn/neighbors/_base.py,692,Corner case: When the number of duplicates are more,not
scikit-learn/sklearn/neighbors/_base.py,693,"than the number of neighbors, the first NN will not",not
scikit-learn/sklearn/neighbors/_base.py,694,"be the sample, but a duplicate.",not
scikit-learn/sklearn/neighbors/_base.py,695,In that case mask the first duplicate.,not
scikit-learn/sklearn/neighbors/_base.py,755,check the input only in self.kneighbors,not
scikit-learn/sklearn/neighbors/_base.py,757,construct CSR matrix representation of the k-NN graph,not
scikit-learn/sklearn/neighbors/_base.py,926,"for efficiency, use squared euclidean distances",not
scikit-learn/sklearn/neighbors/_base.py,960,Deal with change of API in joblib,not
scikit-learn/sklearn/neighbors/_base.py,985,"If the query data is the same as the indexed data, we would like",not
scikit-learn/sklearn/neighbors/_base.py,986,"to ignore the first nearest neighbor of every sample, i.e",not
scikit-learn/sklearn/neighbors/_base.py,987,the sample itself.,not
scikit-learn/sklearn/neighbors/_base.py,1059,check the input only in self.radius_neighbors,not
scikit-learn/sklearn/neighbors/_base.py,1064,construct CSR matrix representation of the NN graph,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,16,"XXX Duplicated in test_neighbors_tree, test_kde",SATD
scikit-learn/sklearn/neighbors/tests/test_kde.py,76,draw a tophat sample,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,81,check that samples are in the right range,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,88,"5 standard deviations is safe for 100 samples, but there's a",not
scikit-learn/sklearn/neighbors/tests/test_kde.py,89,very small chance this test could fail.,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,92,check unsupported kernels,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,97,non-regression test: used to return a scalar,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,108,Smoke test for various metrics and algorithms,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,110,2 features required for haversine dist.,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,125,FIXME,SATD
scikit-learn/sklearn/neighbors/tests/test_kde.py,126,rng = np.random.RandomState(0),not
scikit-learn/sklearn/neighbors/tests/test_kde.py,127,"X = rng.random_sample((n_samples, n_features))",not
scikit-learn/sklearn/neighbors/tests/test_kde.py,128,"Y = rng.random_sample((n_samples, n_features))",not
scikit-learn/sklearn/neighbors/tests/test_kde.py,150,test that kde plays nice in pipelines and grid-searches,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,178,Test that adding a constant sample weight has no effect,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,188,Test equivalence between sampling and (integer) weights,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,198,Test that sample weights has a non-trivial effect,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,202,Test invariance with respect to arbitrary scaling,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,210,Check sample weighting raises errors.,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,222,Make sure that predictions are the same before and after pickling. Used,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,223,to be a bug because sample_weights wasn't pickled and the resulting tree,SATD
scikit-learn/sklearn/neighbors/tests/test_kde.py,224,would miss some info.,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,243,Check that predict raises an exception in an unfitted estimator.,not
scikit-learn/sklearn/neighbors/tests/test_kde.py,244,Unfitted estimators should raise a NotFittedError.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,1,License: BSD 3 clause,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,100,roundoff error can cause test to fail,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,121,roundoff error can cause test to fail,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,197,simultaneous sort rows using function,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,200,simultaneous sort rows using numpy,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,212,Compare gaussian KDE results to scipy.stats.gaussian_kde,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,248,"don't check indices here: if there are any duplicate distances,",not
scikit-learn/sklearn/neighbors/tests/test_neighbors_tree.py,249,the indices may not match.  Distances should not have this problem.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,33,load and shuffle iris dataset,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,39,load and shuffle digits,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,53,Filter deprecation warnings.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,64,"Dist could be multidimensional, flatten it so all values",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,65,can be looped,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,73,Test unsupervised neighbors methods,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,99,test the types of valid input into NearestNeighbors,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,118,Test to check whether n_neighbors is integer,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,142,Note: smaller samples may result in spurious test success,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,148,"TODO: also test radius_neighbors, but requires different assertion",SATD
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,150,As a feature matrix (n_samples by n_features),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,155,As a dense distance matrix (n_samples by n_samples),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,163,Check auto works too,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,171,Check X=None in prediction,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,177,Must raise a ValueError if the matrix is not of correct shape,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,209,We do not test RadiusNeighborsClassifier and RadiusNeighborsRegressor,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,210,since the precomputed neighbors graph is built with k neighbors only.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,227,We do not test KNeighborsClassifier and KNeighborsRegressor,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,228,since the precomputed neighbors graph is built with a radius.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,237,"Test that _is_sorted_by_data works as expected. In CSR sparse matrix,",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,238,"entries in each row can be sorted by indices, by data, or unsorted.",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,239,"_is_sorted_by_data should return True when entries are sorted by data,",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,240,and False in all other cases.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,242,Test with sorted 1D array,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,245,Test with unsorted 1D array,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,249,"Test when the data is sorted in each sample, but not necessarily",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,250,between samples,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,254,Test with duplicates entries in X.indptr,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,262,Test that _check_precomputed returns a graph sorted by data,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,268,est with a different number of nonzero entries for each sample,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,287,Ensures enough number of nearest neighbors,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,294,Checks error with inconsistent distance matrix,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,303,Ensure array is split correctly,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,320,Test unsupervised radius-based query,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,338,sort the results: this is not done automatically for,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,339,radius searches,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,363,Test k-neighbors classification,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,380,Test prediction with y_str,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,389,Test k-neighbors classification,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,402,Test KNeighborsClassifier.predict_proba() method,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,410,cityblock dist,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,420,Check that it also works with non integer labels,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,424,Check that it works with weights='distance',not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,438,Test radius-based classification,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,461,Test radius-based classifier when no neighbors found.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,462,In this case it should rise an informative exception,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,468,no outliers,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,469,one outlier,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,487,Test radius-based classifier when no neighbors found and outliers,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,488,are labeled.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,495,no outliers,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,496,one outlier,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,514,test outlier_labeling of using predict_proba(),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,519,test outlier_label scalar verification,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,525,test invalid outlier_label dtype,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,531,test most frequent,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,537,test manual label in y,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,545,test manual label out of y warning,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,552,test multi output same outlier label,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,562,test multi output different outlier label,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,573,test inconsistent outlier label list length,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,581,"Test radius-based classifier, when distance to a sample is zero.",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,602,"Test radius-based regressor, when distance to a sample is zero.",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,615,we don't test for weights=_weight_func since user will be expected,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,616,to handle zero distances themselves in the function.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,653,check that we can pass precomputed distances to,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,654,NearestNeighbors.radius_neighbors(),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,655,non-regression test for,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,656,https://github.com/scikit-learn/scikit-learn/issues/16036,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,677,Test k-NN classifier on multioutput data,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,691,Stack single output prediction,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,702,Multioutput prediction,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,717,Test k-NN classifier on sparse matrices,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,718,"Like the above, but with various types of sparse matrices",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,736,Test k-NN classifier on multioutput data,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,750,Stack single output prediction,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,764,Multioutput prediction,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,773,Check proba,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,786,Test k-neighbors regression,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,808,Test k-neighbors in multi-output regression with uniform weight,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,839,Test k-neighbors in multi-output regression,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,866,Test radius-based neighbors regression,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,886,test that nan is returned when no nearby observations,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,903,Test radius neighbors in multi-output regression (uniform weight),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,937,Test k-neighbors in multi-output regression with various weight,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,965,Test radius-based regression on sparse matrices,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,966,"Like the above, but with various types of sparse matrices",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,989,Sanity checks on the iris dataset,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,990,Puts three points of each label in the plane and performs a,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,991,nearest neighbor query on points near the decision boundary.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1010,Sanity check on the digits dataset,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1011,the 'brute' algorithm has been observed to fail if the input,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1012,dtype is uint8 due to overflow in distance calculations.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1030,Test kneighbors_graph to build the k-Nearest Neighbor graph.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1033,n_neighbors = 1,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1045,n_neighbors = 2,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1061,n_neighbors = 3,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1070,Test kneighbors_graph to build the k-Nearest Neighbor graph,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1071,for sparse input.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1088,Test radius_neighbors_graph to build the Nearest Neighbor graph.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1108,Test radius_neighbors_graph to build the Nearest Neighbor graph,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1109,for sparse input.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1126,Test bad argument values: these should all raise ValueErrors,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1187,Test computing the neighbors for various metrics,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1188,create a symmetric matrix,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1212,KD tree doesn't support all metrics,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1225,Haversine distance only accepts 2D data,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1266,check that there is a metric that is valid for brute,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1267,but not ball_tree (so we actually test something),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1271,Metric which don't required any additional parameter,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1300,Metric with parameter,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1332,Find a reasonable radius.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1337,Test kneighbors_graph,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1345,Test radiusneighbors_graph,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1353,"Raise error when wrong parameters are supplied,",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1370,Test kneighbors et.al when query is not training data,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1380,Test neighbors.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1388,Test the graph variants.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1399,Test kneighbors et.al when query is None,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1414,Test the graph variants.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1431,Test behavior of kneighbors when duplicates are present in query,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1437,Do not do anything special to duplicates.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1459,Mask the first duplicates when n_duplicates > n_neighbors.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1467,Test that zeros are explicitly marked in kneighbors_graph.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1479,Test include_self parameter in neighbors_graph,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1549,Non-regression test which ensure the knn methods are properly working,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1550,even when forcing the global joblib backend.,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1576,Metric accepting sparse matrix input (only),not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1580,Population matrix,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1586,Query matrix,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1595,GS indices of nearest neighbours in `X` for `sparse_metric`,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1604,ignore conversion to boolean in pairwise_distances,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1607,Non-regression test for #4523,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1608,'brute': uses scipy.spatial.distance through pairwise_distances,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1609,'ball_tree': uses sklearn.neighbors._dist_metrics,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1638,Test chaining KNeighborsTransformer and classifiers/regressors,not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1646,"We precompute more neighbors than necessary, to have equivalence between",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1647,"k-neighbors estimator after radius-neighbors transformer, and vice-versa.",not
scikit-learn/sklearn/neighbors/tests/test_neighbors.py,1667,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,1,coding: utf-8,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,6,Authors: William de Vazelhes <wdevazelhes@gmail.com>,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,7,John Chiotellis <ioannis.chiotellis@in.tum.de>,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,8,License: BSD 3 clause,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,26,load and shuffle iris dataset,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,72,initialize the loss to very high,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,73,Initialize a fake NCA and variables needed to compute the loss:,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,91,test that points are collapsed into one point,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,102,"Initialize the transformation `M`, as well as `X` and `y` and `NCA`",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,117,compute relative error,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,123,Test that invalid parameters raise value error,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,129,TypeError,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,136,ValueError,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,168,Fail if transformation input dimension does not match inputs dimensions,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,174,Fail if transformation output dimension is larger than,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,175,transformation input dimension,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,177,len(transformation) > len(transformation[0]),not
scikit-learn/sklearn/neighbors/tests/test_nca.py,182,Pass otherwise,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,194,n_components = X.shape[1] != transformation.shape[0],not
scikit-learn/sklearn/neighbors/tests/test_nca.py,205,n_components > X.shape[1],not
scikit-learn/sklearn/neighbors/tests/test_nca.py,216,n_components < X.shape[1],not
scikit-learn/sklearn/neighbors/tests/test_nca.py,225,Start learning from scratch,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,229,Initialize with random,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,233,Initialize with auto,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,237,Initialize with PCA,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,241,Initialize with LDA,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,249,init.shape[1] must match X.shape[1],not
scikit-learn/sklearn/neighbors/tests/test_nca.py,259,init.shape[0] must be <= init.shape[1],not
scikit-learn/sklearn/neighbors/tests/test_nca.py,269,init.shape[0] must match n_components,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,287,Test that auto choose the init as expected with every configuration,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,288,"of order of n_samples, n_features, n_classes and n_components.",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,296,"n_classes > n_samples is impossible, and n_classes == n_samples",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,297,throws an error from lda but is an absurd case,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,302,"this would return a ValueError, which is already tested in",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,303,test_params_validation,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,338,A 1-iteration second fit on same data should give almost same result,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,339,"with warm starting, and quite different result without warm starting.",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,370,"assert there is proper output when verbose = 1, for every initialization",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,371,except auto because auto will call one of the others,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,385,check output,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,387,"if pca or lda init, an additional line is printed, so we test",not
scikit-learn/sklearn/neighbors/tests/test_nca.py,388,it and remove it to test the rest equally among initializations,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,399,The following regex will match for instance:,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,400,'[NeighborhoodComponentsAnalysis]  0    6.988936e+01   0.01',not
scikit-learn/sklearn/neighbors/tests/test_nca.py,409,assert by default there is no output (verbose=0),not
scikit-learn/sklearn/neighbors/tests/test_nca.py,413,check output,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,421,one singleton class,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,430,One non-singleton class,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,441,Only singleton classes,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,478,assert that my_cb is called,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,484,check output,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,496,Initialize a fake NCA and variables needed to call the loss,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,497,function:,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,527,check that no error is raised when parameters have numpy integer or,not
scikit-learn/sklearn/neighbors/tests/test_nca.py,528,floating types.,not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,29,make boolean arrays: ones and zeros,not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,106,Based on https://github.com/scipy/scipy/pull/7373,not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,107,"When comparing two all-zero vectors, scipy>=1.2.0 jaccard metric",not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,108,"was changed to return 0, instead of nan.",not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,167,Check if both callable metric and predefined metric initialized,not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,168,DistanceMetric object is picklable,not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,193,Regression test for #6288,not
scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py,194,"Previously, a metric requiring a particular input dimension would fail",not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,11,Introduce a point into a quad tree with boundaries not easy to compute.,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,14,check a random case,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,16,check the case where only 0 are inserted,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,18,check the case where only negative are inserted,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,20,check the case where only small numbers are inserted,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,30,Introduce a point into a quad tree where a similar point already exists.,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,31,Test will hang if it doesn't complete.,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,34,check the case where points are actually different,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,36,check the case where points are the same on X axis,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,38,check the case where points are arbitrarily close on X axis,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,40,check the case where points are the same on Y axis,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,42,check the case where points are arbitrarily close on Y axis,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,44,check the case where points are arbitrarily close on both axes,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,48,check the case where points are arbitrarily close on both axes,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,49,close to machine epsilon - x axis,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,53,check the case where points are arbitrarily close on both axes,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,54,close to machine epsilon - y axis,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,95,Assert that the first 5 are indeed duplicated and that the next,not
scikit-learn/sklearn/neighbors/tests/test_quad_tree.py,96,ones are single point leaf,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,13,toy sample,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,15,Sparse matrix,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,21,also load the iris dataset,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,22,and randomly permute it,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,31,"Check classification on a toy dataset, including sparse versions.",not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,36,"Same test, but with a sparse matrix to fit and test.",not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,41,"Fit with sparse, test with non-sparse",not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,46,"Fit with non-sparse, test with sparse",not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,51,Fit and predict with non-CSR sparse matrices,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,64,Check consistency on dataset iris.,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,72,"Check consistency on dataset iris, when using shrinkage.",not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,85,classification,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,100,Ensure that the shrinking is correct.,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,101,"The expected result is calculated by R (pamr),",not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,102,which is implemented by the author of the original paper.,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,103,(One need to modify the code to output the new centroid in pamr.predict),not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,124,Test that NearestCentroid gives same results on translated data,not
scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py,141,Test the manhattan metric.,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,1,Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,2,Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,27,load the iris dataset,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,28,and randomly permute it,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,37,Toy sample (the last two samples are outliers):,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,40,Test LocalOutlierFactor:,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,45,Assert largest outlier score is smaller than smallest inlier score:,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,48,Assert predict() works:,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,56,Generate train/test data,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,61,Generate some abnormal novel observations,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,66,fit the model for novelty detection,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,69,"predict scores (the lower, the more normal)",not
scikit-learn/sklearn/neighbors/tests/test_lof.py,72,check that roc_auc is good,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,77,toy samples:,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,86,check predict(),not
scikit-learn/sklearn/neighbors/tests/test_lof.py,89,check predict(one sample not in train),not
scikit-learn/sklearn/neighbors/tests/test_lof.py,92,check predict(one sample already in train),not
scikit-learn/sklearn/neighbors/tests/test_lof.py,99,Note: smaller samples may result in spurious test success,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,105,As a feature matrix (n_samples by n_features),not
scikit-learn/sklearn/neighbors/tests/test_lof.py,111,As a dense distance matrix (n_samples by n_samples),not
scikit-learn/sklearn/neighbors/tests/test_lof.py,158,check errors for novelty=False,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,161,"predict, decision_function and score_samples raise ValueError",not
scikit-learn/sklearn/neighbors/tests/test_lof.py,166,check errors for novelty=True,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,173,check that the scores of the training samples are still accessible,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,174,when novelty=True through the negative_outlier_factor_ attribute,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,177,fit with novelty=False,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,182,fit with novelty=True,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,191,check availability of prediction methods depending on novelty value.,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,194,when novelty=True,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,202,when novelty=False,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,213,the common tests are run for the default LOF (novelty=False).,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,214,here we run these common tests for LOF when novelty=True,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,220,the number of predicted outliers should be equal to the number of,not
scikit-learn/sklearn/neighbors/tests/test_lof.py,221,expected outliers unless there are ties in the abnormality scores.,not
scikit-learn/sklearn/neighbors/tests/test_graph.py,9,Test the number of neighbors returned,not
scikit-learn/sklearn/neighbors/tests/test_graph.py,20,with n_neighbors,not
scikit-learn/sklearn/neighbors/tests/test_graph.py,36,with radius,not
scikit-learn/sklearn/neighbors/tests/test_graph.py,61,Test that the diagonal is explicitly stored in the sparse graph,not
scikit-learn/sklearn/neighbors/tests/test_graph.py,77,Using transform on new data should not always have zero diagonal,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,30,Test chaining KNeighborsTransformer and SpectralClustering,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,34,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,47,Test chaining KNeighborsTransformer and SpectralEmbedding,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,59,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,72,Test chaining RadiusNeighborsTransformer and DBSCAN,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,77,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,89,Test chaining KNeighborsTransformer and Isomap with,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,90,neighbors_algorithm='precomputed',not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,97,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,115,Test chaining KNeighborsTransformer and TSNE,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,125,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,140,Test chaining KNeighborsTransformer and LocalOutlierFactor,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,146,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,160,Test chaining KNeighborsTransformer and LocalOutlierFactor,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,167,compare the chained version and the compact version,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,181,Test chaining KNeighborsTransformer and classifiers/regressors,not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,189,"We precompute more neighbors than necessary, to have equivalence between",not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,190,"k-neighbors estimator after radius-neighbors transformer, and vice-versa.",not
scikit-learn/sklearn/neighbors/tests/test_neighbors_pipeline.py,212,compare the chained version and the compact version,not
scikit-learn/sklearn/decomposition/_nmf.py,3,Author: Vlad Niculae,not
scikit-learn/sklearn/decomposition/_nmf.py,4,Lars Buitinck,not
scikit-learn/sklearn/decomposition/_nmf.py,5,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/decomposition/_nmf.py,6,Tom Dupre la Tour,not
scikit-learn/sklearn/decomposition/_nmf.py,7,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_nmf.py,92,The method can be called with scalars,not
scikit-learn/sklearn/decomposition/_nmf.py,98,Frobenius norm,not
scikit-learn/sklearn/decomposition/_nmf.py,100,"Avoid the creation of the dense np.dot(W, H) if X is sparse.",not
scikit-learn/sklearn/decomposition/_nmf.py,115,"compute np.dot(W, H) only where X is nonzero",not
scikit-learn/sklearn/decomposition/_nmf.py,123,do not affect the zeros: here 0 ** (-1) = 0 and not infinity,not
scikit-learn/sklearn/decomposition/_nmf.py,128,used to avoid division by zero,not
scikit-learn/sklearn/decomposition/_nmf.py,131,generalized Kullback-Leibler divergence,not
scikit-learn/sklearn/decomposition/_nmf.py,133,"fast and memory efficient computation of np.sum(np.dot(W, H))",not
scikit-learn/sklearn/decomposition/_nmf.py,135,computes np.sum(X * log(X / WH)) only where X is nonzero,not
scikit-learn/sklearn/decomposition/_nmf.py,138,"add full np.sum(np.dot(W, H)) - np.sum(X)",not
scikit-learn/sklearn/decomposition/_nmf.py,141,Itakura-Saito divergence,not
scikit-learn/sklearn/decomposition/_nmf.py,146,"beta-divergence, beta not in (0, 1, 2)",not
scikit-learn/sklearn/decomposition/_nmf.py,149,"slow loop, but memory efficient computation of :",not
scikit-learn/sklearn/decomposition/_nmf.py,150,"np.sum(np.dot(W, H) ** beta)",not
scikit-learn/sklearn/decomposition/_nmf.py,218,'mu' is the only solver that handles other beta losses than 'frobenius',not
scikit-learn/sklearn/decomposition/_nmf.py,325,Random initialization,not
scikit-learn/sklearn/decomposition/_nmf.py,337,NNDSVD initialization,not
scikit-learn/sklearn/decomposition/_nmf.py,342,The leading singular triplet is non-negative,not
scikit-learn/sklearn/decomposition/_nmf.py,343,so it can be used as is for initialization.,not
scikit-learn/sklearn/decomposition/_nmf.py,350,extract positive and negative parts of column vectors,not
scikit-learn/sklearn/decomposition/_nmf.py,354,and their norms,not
scikit-learn/sklearn/decomposition/_nmf.py,360,choose update,not
scikit-learn/sklearn/decomposition/_nmf.py,410,L2 regularization corresponds to increase of the diagonal of HHt,not
scikit-learn/sklearn/decomposition/_nmf.py,412,adds l2_reg only on the diagonal,not
scikit-learn/sklearn/decomposition/_nmf.py,414,L1 regularization corresponds to decrease of each element of XHt,not
scikit-learn/sklearn/decomposition/_nmf.py,422,The following seems to be required on 64-bit Windows w/ Python 3.5.,not
scikit-learn/sklearn/decomposition/_nmf.py,499,so W and Ht are both in C order in memory,not
scikit-learn/sklearn/decomposition/_nmf.py,508,Update W,not
scikit-learn/sklearn/decomposition/_nmf.py,511,Update H,not
scikit-learn/sklearn/decomposition/_nmf.py,537,Numerator,not
scikit-learn/sklearn/decomposition/_nmf.py,541,"avoid a copy of XHt, which will be re-computed (update_H=True)",not
scikit-learn/sklearn/decomposition/_nmf.py,544,"preserve the XHt, which is not re-computed (update_H=False)",not
scikit-learn/sklearn/decomposition/_nmf.py,547,Denominator,not
scikit-learn/sklearn/decomposition/_nmf.py,553,Numerator,not
scikit-learn/sklearn/decomposition/_nmf.py,554,"if X is sparse, compute WH only where X is non zero",not
scikit-learn/sklearn/decomposition/_nmf.py,562,copy used in the Denominator,not
scikit-learn/sklearn/decomposition/_nmf.py,567,to avoid taking a negative power of zero,not
scikit-learn/sklearn/decomposition/_nmf.py,574,speeds up computation time,not
scikit-learn/sklearn/decomposition/_nmf.py,575,refer to /numpy/numpy/issues/9363,not
scikit-learn/sklearn/decomposition/_nmf.py,578,element-wise multiplication,not
scikit-learn/sklearn/decomposition/_nmf.py,582,element-wise multiplication,not
scikit-learn/sklearn/decomposition/_nmf.py,585,"here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)",not
scikit-learn/sklearn/decomposition/_nmf.py,588,Denominator,not
scikit-learn/sklearn/decomposition/_nmf.py,591,"shape(n_components, )",not
scikit-learn/sklearn/decomposition/_nmf.py,595,"computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)",not
scikit-learn/sklearn/decomposition/_nmf.py,597,memory efficient computation,not
scikit-learn/sklearn/decomposition/_nmf.py,598,"(compute row by row, avoiding the dense matrix WH)",not
scikit-learn/sklearn/decomposition/_nmf.py,611,Add L1 and L2 regularization,not
scikit-learn/sklearn/decomposition/_nmf.py,621,"gamma is in ]0, 1]",not
scikit-learn/sklearn/decomposition/_nmf.py,635,Numerator,not
scikit-learn/sklearn/decomposition/_nmf.py,643,copy used in the Denominator,not
scikit-learn/sklearn/decomposition/_nmf.py,648,to avoid division by zero,not
scikit-learn/sklearn/decomposition/_nmf.py,655,speeds up computation time,not
scikit-learn/sklearn/decomposition/_nmf.py,656,refer to /numpy/numpy/issues/9363,not
scikit-learn/sklearn/decomposition/_nmf.py,659,element-wise multiplication,not
scikit-learn/sklearn/decomposition/_nmf.py,663,element-wise multiplication,not
scikit-learn/sklearn/decomposition/_nmf.py,666,"here numerator = dot(W.T, (dot(W, H) ** (beta_loss - 2)) * X)",not
scikit-learn/sklearn/decomposition/_nmf.py,669,Denominator,not
scikit-learn/sklearn/decomposition/_nmf.py,671,"shape(n_components, )",not
scikit-learn/sklearn/decomposition/_nmf.py,675,"beta_loss not in (1, 2)",not
scikit-learn/sklearn/decomposition/_nmf.py,677,"computation of WtWH = dot(W.T, dot(W, H) ** beta_loss - 1)",not
scikit-learn/sklearn/decomposition/_nmf.py,679,memory efficient computation,not
scikit-learn/sklearn/decomposition/_nmf.py,680,"(compute column by column, avoiding the dense matrix WH)",not
scikit-learn/sklearn/decomposition/_nmf.py,693,Add L1 and L2 regularization,not
scikit-learn/sklearn/decomposition/_nmf.py,703,"gamma is in ]0, 1]",not
scikit-learn/sklearn/decomposition/_nmf.py,784,gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011],not
scikit-learn/sklearn/decomposition/_nmf.py,792,used for the convergence criterion,not
scikit-learn/sklearn/decomposition/_nmf.py,798,update W,not
scikit-learn/sklearn/decomposition/_nmf.py,799,"H_sum, HHt and XHt are saved and reused if not update_H",not
scikit-learn/sklearn/decomposition/_nmf.py,805,necessary for stability with beta_loss < 1,not
scikit-learn/sklearn/decomposition/_nmf.py,809,update H,not
scikit-learn/sklearn/decomposition/_nmf.py,815,These values will be recomputed since H changed,not
scikit-learn/sklearn/decomposition/_nmf.py,818,necessary for stability with beta_loss < 1,not
scikit-learn/sklearn/decomposition/_nmf.py,822,test convergence criterion every 10 iterations,not
scikit-learn/sklearn/decomposition/_nmf.py,835,do not print if we have already printed in the convergence test,not
scikit-learn/sklearn/decomposition/_nmf.py,1031,"check W and H, or initialize them",not
scikit-learn/sklearn/decomposition/_nmf.py,1044,'mu' solver should not be initialized by zeros,not
scikit-learn/sklearn/decomposition/_sparse_pca.py,2,"Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort",not
scikit-learn/sklearn/decomposition/_sparse_pca.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_sparse_pca.py,17,FIXME: remove in 0.24,SATD
scikit-learn/sklearn/decomposition/_pca.py,4,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/decomposition/_pca.py,5,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/decomposition/_pca.py,6,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/decomposition/_pca.py,7,Denis A. Engemann <denis-alexander.engemann@inria.fr>,not
scikit-learn/sklearn/decomposition/_pca.py,8,Michael Eickenberg <michael.eickenberg@inria.fr>,not
scikit-learn/sklearn/decomposition/_pca.py,9,Giorgio Patrini <giorgio.patrini@anu.edu.au>,not
scikit-learn/sklearn/decomposition/_pca.py,10,,not
scikit-learn/sklearn/decomposition/_pca.py,11,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_pca.py,66,"When the tested rank is associated with a small eigenvalue, there's",not
scikit-learn/sklearn/decomposition/_pca.py,67,no point in computing the log-likelihood: it's going to be very,not
scikit-learn/sklearn/decomposition/_pca.py,68,"small and won't be the max anyway. Also, it can lead to numerical",not
scikit-learn/sklearn/decomposition/_pca.py,69,"issues below when computing pa, in particular in log((spectrum[i] -",not
scikit-learn/sklearn/decomposition/_pca.py,70,spectrum[j]) because this will take the log of something very small.,not
scikit-learn/sklearn/decomposition/_pca.py,106,we don't want to return n_components = 0,not
scikit-learn/sklearn/decomposition/_pca.py,380,X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples),not
scikit-learn/sklearn/decomposition/_pca.py,383,X_new = X * V = U * S * Vt * V = U * S,not
scikit-learn/sklearn/decomposition/_pca.py,391,Raise an error for sparse input.,not
scikit-learn/sklearn/decomposition/_pca.py,392,This is more informative than the generic one raised by check_array.,not
scikit-learn/sklearn/decomposition/_pca.py,400,Handle n_components==None,not
scikit-learn/sklearn/decomposition/_pca.py,409,Handle svd_solver,not
scikit-learn/sklearn/decomposition/_pca.py,412,"Small problem or n_components == 'mle', just call full PCA",not
scikit-learn/sklearn/decomposition/_pca.py,417,"This is also the case of n_components in (0,1)",not
scikit-learn/sklearn/decomposition/_pca.py,421,Call different fits for either full or truncated SVD,not
scikit-learn/sklearn/decomposition/_pca.py,450,Center data,not
scikit-learn/sklearn/decomposition/_pca.py,455,flip eigenvectors' sign to enforce deterministic output,not
scikit-learn/sklearn/decomposition/_pca.py,460,Get variance explained by singular values,not
scikit-learn/sklearn/decomposition/_pca.py,464,Store the singular values.,not
scikit-learn/sklearn/decomposition/_pca.py,466,Postprocess the number of components required,not
scikit-learn/sklearn/decomposition/_pca.py,471,number of components for which the cumulated explained,not
scikit-learn/sklearn/decomposition/_pca.py,472,variance percentage is superior to the desired threshold,not
scikit-learn/sklearn/decomposition/_pca.py,473,side='right' ensures that number of features selected,not
scikit-learn/sklearn/decomposition/_pca.py,474,their variance is always greater than n_components float,not
scikit-learn/sklearn/decomposition/_pca.py,475,passed. More discussion in issue: #15669,not
scikit-learn/sklearn/decomposition/_pca.py,479,Compute noise covariance using Probabilistic PCA model,not
scikit-learn/sklearn/decomposition/_pca.py,480,The sigma2 maximum likelihood (cf. eq. 12.46),not
scikit-learn/sklearn/decomposition/_pca.py,526,Center data,not
scikit-learn/sklearn/decomposition/_pca.py,531,"random init solution, as ARPACK does it internally",not
scikit-learn/sklearn/decomposition/_pca.py,534,svds doesn't abide by scipy.linalg.svd/randomized_svd,not
scikit-learn/sklearn/decomposition/_pca.py,535,"conventions, so reverse its outputs.",not
scikit-learn/sklearn/decomposition/_pca.py,537,flip eigenvectors' sign to enforce deterministic output,not
scikit-learn/sklearn/decomposition/_pca.py,541,sign flipping is done inside,not
scikit-learn/sklearn/decomposition/_pca.py,551,Get variance explained by singular values,not
scikit-learn/sklearn/decomposition/_pca.py,556,Store the singular values.,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,16,Author: Christian Osendorfer <osendorf@gmail.com>,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,17,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,18,Denis A. Engemann <denis-alexander.engemann@inria.fr>,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,20,License: BSD3,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,182,some constant terms,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,200,we'll modify svd outputs to return unexplained variance,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,201,to allow for unified computation of loglikelihood,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,220,SMALL helps numerics,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,224,Use 'maximum' here to avoid sqrt problems.,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,229,loglikelihood,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,294,modify diag inplace,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,309,handle corner cases first,not
scikit-learn/sklearn/decomposition/_factor_analysis.py,315,Get precision using matrix inversion lemma,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,3,Author: Kyle Kastner <kastnerkyle@gmail.com>,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,4,Giorgio Patrini,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,271,This is the first partial_fit,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,277,Update stats - they are 0 if this is the first step,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,284,Whitening,not
scikit-learn/sklearn/decomposition/_incremental_pca.py,286,"If it is the first step, simply whiten X",not
scikit-learn/sklearn/decomposition/_incremental_pca.py,291,Build matrix of combined previous basis and new data,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,3,Author: Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,184,center kernel,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,192,compute eigenvectors,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,206,"initialize with [-1,1] as in ARPACK",not
scikit-learn/sklearn/decomposition/_kernel_pca.py,214,make sure that the eigenvalues are ok and fix numerical issues,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,218,flip eigenvectors' sign to enforce deterministic output,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,222,sort eigenvectors in descending order,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,227,remove eigenvectors with a zero eigenvalue (null space) if required,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,232,Maintenance note on Eigenvectors normalization,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,233,----------------------------------------------,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,234,there is a link between,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,235,the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)',not
scikit-learn/sklearn/decomposition/_kernel_pca.py,236,if v is an eigenvector of K,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,237,then Phi(X)v  is an eigenvector of Phi(X)Phi(X)',not
scikit-learn/sklearn/decomposition/_kernel_pca.py,238,if u is an eigenvector of Phi(X)Phi(X)',not
scikit-learn/sklearn/decomposition/_kernel_pca.py,239,then Phi(X)'u is an eigenvector of Phi(X)'Phi(X),not
scikit-learn/sklearn/decomposition/_kernel_pca.py,240,,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,241,"At this stage our self.alphas_ (the v) have norm 1, we need to scale",not
scikit-learn/sklearn/decomposition/_kernel_pca.py,242,them so that eigenvectors in kernel feature space (the u) have norm=1,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,243,instead,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,244,,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,245,We COULD scale them here:,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,246,self.alphas_ = self.alphas_ / np.sqrt(self.lambdas_),not
scikit-learn/sklearn/decomposition/_kernel_pca.py,247,,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,248,"But choose to perform that LATER when needed, in `fit()` and in",not
scikit-learn/sklearn/decomposition/_kernel_pca.py,249,`transform()`.,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,284,"no need to use the kernel to transform X, use shortcut expression",not
scikit-learn/sklearn/decomposition/_kernel_pca.py,307,"no need to use the kernel to transform X, use shortcut expression",not
scikit-learn/sklearn/decomposition/_kernel_pca.py,328,Compute centered gram matrix between X and training data X_fit_,not
scikit-learn/sklearn/decomposition/_kernel_pca.py,331,scale eigenvectors (properly account for null-space for dot product),not
scikit-learn/sklearn/decomposition/_kernel_pca.py,337,Project with a scalar product between K and the scaled eigenvectors,not
scikit-learn/sklearn/decomposition/_lda.py,11,Author: Chyi-Kwei Yau,not
scikit-learn/sklearn/decomposition/_lda.py,12,Author: Matthew D. Hoffman (original onlineldavb implementation),not
scikit-learn/sklearn/decomposition/_lda.py,83,"In the literature, this is `exp(E[log(theta)])`",not
scikit-learn/sklearn/decomposition/_lda.py,86,diff on `component_` (only calculate it when `cal_diff` is True),not
scikit-learn/sklearn/decomposition/_lda.py,103,"The next one is a copy, since the inner loop overwrites it.",not
scikit-learn/sklearn/decomposition/_lda.py,107,Iterate between `doc_topic_d` and `norm_phi` until convergence,not
scikit-learn/sklearn/decomposition/_lda.py,111,The optimal phi_{dwk} is proportional to,not
scikit-learn/sklearn/decomposition/_lda.py,112,exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).,not
scikit-learn/sklearn/decomposition/_lda.py,117,"Note: adds doc_topic_prior to doc_topic_d, in-place.",not
scikit-learn/sklearn/decomposition/_lda.py,125,Contribution of document d to the expected sufficient,not
scikit-learn/sklearn/decomposition/_lda.py,126,statistics for the M step.,not
scikit-learn/sklearn/decomposition/_lda.py,348,"In the literature, this is called `lambda`",not
scikit-learn/sklearn/decomposition/_lda.py,352,"In the literature, this is `exp(E[log(beta)])`",not
scikit-learn/sklearn/decomposition/_lda.py,386,Run e-step in parallel,not
scikit-learn/sklearn/decomposition/_lda.py,389,TODO: make Parallel._effective_n_jobs public instead?,SATD
scikit-learn/sklearn/decomposition/_lda.py,403,merge result,not
scikit-learn/sklearn/decomposition/_lda.py,408,This step finishes computing the sufficient statistics for the,not
scikit-learn/sklearn/decomposition/_lda.py,409,M-step.,not
scikit-learn/sklearn/decomposition/_lda.py,446,E-step,not
scikit-learn/sklearn/decomposition/_lda.py,450,M-step,not
scikit-learn/sklearn/decomposition/_lda.py,454,online update,not
scikit-learn/sklearn/decomposition/_lda.py,455,"In the literature, the weight is `rho`",not
scikit-learn/sklearn/decomposition/_lda.py,463,update `component_` related variables,not
scikit-learn/sklearn/decomposition/_lda.py,504,"In theory reset should be equal to `first_time`, but there are tests",not
scikit-learn/sklearn/decomposition/_lda.py,505,checking the input number of feature and they expect a specific,not
scikit-learn/sklearn/decomposition/_lda.py,506,"string, which is not the same one raised by check_n_features. So we",not
scikit-learn/sklearn/decomposition/_lda.py,507,don't check n_features_in_ here for now (it's done with adhoc code in,not
scikit-learn/sklearn/decomposition/_lda.py,508,the estimator anyway).,not
scikit-learn/sklearn/decomposition/_lda.py,509,TODO: set reset=first_time when addressing reset in,SATD
scikit-learn/sklearn/decomposition/_lda.py,510,predict/transform/etc.,not
scikit-learn/sklearn/decomposition/_lda.py,517,initialize parameters or check,not
scikit-learn/sklearn/decomposition/_lda.py,565,initialize parameters,not
scikit-learn/sklearn/decomposition/_lda.py,567,change to perplexity later,not
scikit-learn/sklearn/decomposition/_lda.py,578,batch update,not
scikit-learn/sklearn/decomposition/_lda.py,582,check perplexity,not
scikit-learn/sklearn/decomposition/_lda.py,601,calculate final perplexity value on train set,not
scikit-learn/sklearn/decomposition/_lda.py,625,make sure feature size is the same in fitted model and in X,not
scikit-learn/sklearn/decomposition/_lda.py,688,calculate log-likelihood,not
scikit-learn/sklearn/decomposition/_lda.py,709,"E[log p(docs | theta, beta)]",not
scikit-learn/sklearn/decomposition/_lda.py,722,compute E[log p(theta | alpha) - log q(theta | gamma)],not
scikit-learn/sklearn/decomposition/_lda.py,726,Compensate for the subsampling of the population of documents,not
scikit-learn/sklearn/decomposition/_lda.py,731,E[log p(beta | eta) - log q (beta | lambda)],not
scikit-learn/sklearn/decomposition/__init__.py,7,TODO: remove me in 0.24 (as well as the noqa markers) and,SATD
scikit-learn/sklearn/decomposition/__init__.py,8,import the dict_learning func directly from the ._dict_learning,not
scikit-learn/sklearn/decomposition/__init__.py,9,module instead.,not
scikit-learn/sklearn/decomposition/__init__.py,10,Pre-cache the import of the deprecated module so that import,not
scikit-learn/sklearn/decomposition/__init__.py,11,sklearn.decomposition.dict_learning returns the function as in,not
scikit-learn/sklearn/decomposition/__init__.py,12,"0.21, instead of the module.",not
scikit-learn/sklearn/decomposition/__init__.py,13,https://github.com/scikit-learn/scikit-learn/issues/15842,not
scikit-learn/sklearn/decomposition/__init__.py,20,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,21,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,22,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,23,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,24,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,25,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,26,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,29,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,30,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,31,noqa,not
scikit-learn/sklearn/decomposition/__init__.py,32,noqa,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,4,Author: Lars Buitinck,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,5,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,6,Michael Becker <mike@beckerfuffle.com>,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,7,License: 3-clause BSD.,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,170,svds doesn't abide by scipy.linalg.svd/randomized_svd,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,171,"conventions, so reverse its outputs.",not
scikit-learn/sklearn/decomposition/_truncated_svd.py,189,Calculate explained variance & explained variance ratio,not
scikit-learn/sklearn/decomposition/_truncated_svd.py,198,Store the singular values.,not
scikit-learn/sklearn/decomposition/_base.py,3,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/decomposition/_base.py,4,Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/decomposition/_base.py,5,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/decomposition/_base.py,6,Denis A. Engemann <denis-alexander.engemann@inria.fr>,not
scikit-learn/sklearn/decomposition/_base.py,7,Kyle Kastner <kastnerkyle@gmail.com>,not
scikit-learn/sklearn/decomposition/_base.py,8,,not
scikit-learn/sklearn/decomposition/_base.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_base.py,44,modify diag inplace,not
scikit-learn/sklearn/decomposition/_base.py,60,handle corner cases first,not
scikit-learn/sklearn/decomposition/_base.py,66,Get precision using matrix inversion lemma,not
scikit-learn/sklearn/decomposition/_dict_learning.py,3,"Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort",not
scikit-learn/sklearn/decomposition/_dict_learning.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_dict_learning.py,115,overwriting cov is safe,not
scikit-learn/sklearn/decomposition/_dict_learning.py,122,account for scaling,not
scikit-learn/sklearn/decomposition/_dict_learning.py,126,"Not passing in verbose=max(0, verbose-1) because Lars.fit already",not
scikit-learn/sklearn/decomposition/_dict_learning.py,127,corrects the verbosity level.,not
scikit-learn/sklearn/decomposition/_dict_learning.py,138,account for scaling,not
scikit-learn/sklearn/decomposition/_dict_learning.py,140,TODO: Make verbosity argument for Lasso?,SATD
scikit-learn/sklearn/decomposition/_dict_learning.py,141,sklearn.linear_model.coordinate_descent.enet_path has a verbosity,not
scikit-learn/sklearn/decomposition/_dict_learning.py,142,argument that we could pass in from Lasso.,not
scikit-learn/sklearn/decomposition/_dict_learning.py,157,"Not passing in verbose=max(0, verbose-1) because Lars.fit already",not
scikit-learn/sklearn/decomposition/_dict_learning.py,158,corrects the verbosity level.,not
scikit-learn/sklearn/decomposition/_dict_learning.py,187,XXX : could be moved to the linear_model module,SATD
scikit-learn/sklearn/decomposition/_dict_learning.py,321,Enter parallel code block,not
scikit-learn/sklearn/decomposition/_dict_learning.py,383,Get BLAS functions,not
scikit-learn/sklearn/decomposition/_dict_learning.py,387,"Residuals, computed with BLAS for speed and efficiency",not
scikit-learn/sklearn/decomposition/_dict_learning.py,388,R <- -1.0 * U * V^T + 1.0 * Y,not
scikit-learn/sklearn/decomposition/_dict_learning.py,389,Outputs R as Fortran array for efficiency,not
scikit-learn/sklearn/decomposition/_dict_learning.py,392,R <- 1.0 * U_k * V_k^T + R,not
scikit-learn/sklearn/decomposition/_dict_learning.py,397,Scale k'th atom,not
scikit-learn/sklearn/decomposition/_dict_learning.py,398,(U_k * U_k) ** 0.5,not
scikit-learn/sklearn/decomposition/_dict_learning.py,409,Setting corresponding coefs to 0,not
scikit-learn/sklearn/decomposition/_dict_learning.py,411,(U_k * U_k) ** 0.5,not
scikit-learn/sklearn/decomposition/_dict_learning.py,416,R <- -1.0 * U_k * V_k^T + R,not
scikit-learn/sklearn/decomposition/_dict_learning.py,539,Avoid integer division problems,not
scikit-learn/sklearn/decomposition/_dict_learning.py,543,Init the code and the dictionary with SVD of Y,not
scikit-learn/sklearn/decomposition/_dict_learning.py,546,"Don't copy V, it will happen below",not
scikit-learn/sklearn/decomposition/_dict_learning.py,552,True even if n_components=None,not
scikit-learn/sklearn/decomposition/_dict_learning.py,560,"Fortran-order dict, as we are going to access its row vectors",not
scikit-learn/sklearn/decomposition/_dict_learning.py,571,"If max_iter is 0, number of iterations returned should be zero",not
scikit-learn/sklearn/decomposition/_dict_learning.py,584,Update code,not
scikit-learn/sklearn/decomposition/_dict_learning.py,588,Update dictionary,not
scikit-learn/sklearn/decomposition/_dict_learning.py,595,Cost function,not
scikit-learn/sklearn/decomposition/_dict_learning.py,601,assert(dE >= -tol * errors[-1]),not
scikit-learn/sklearn/decomposition/_dict_learning.py,604,A line return,not
scikit-learn/sklearn/decomposition/_dict_learning.py,761,Avoid integer division problems,not
scikit-learn/sklearn/decomposition/_dict_learning.py,765,Init V with SVD of X,not
scikit-learn/sklearn/decomposition/_dict_learning.py,797,The covariance of the dictionary,not
scikit-learn/sklearn/decomposition/_dict_learning.py,800,The data approximation,not
scikit-learn/sklearn/decomposition/_dict_learning.py,806,"If n_iter is zero, we need to return zero.",not
scikit-learn/sklearn/decomposition/_dict_learning.py,826,Update the auxiliary variables,not
scikit-learn/sklearn/decomposition/_dict_learning.py,838,Update dictionary,not
scikit-learn/sklearn/decomposition/_dict_learning.py,842,XXX: Can the residuals be of any use?,SATD
scikit-learn/sklearn/decomposition/_dict_learning.py,844,Maybe we need a stopping criteria based on the amount of,not
scikit-learn/sklearn/decomposition/_dict_learning.py,845,modification in the dictionary,not
scikit-learn/sklearn/decomposition/_dict_learning.py,924,feature vector is split into a positive and negative side,not
scikit-learn/sklearn/decomposition/_dict_learning.py,1449,Keep track of the state of the algorithm to be able to do,SATD
scikit-learn/sklearn/decomposition/_dict_learning.py,1450,some online fitting (partial_fit),not
scikit-learn/sklearn/decomposition/_dict_learning.py,1501,Keep track of the state of the algorithm to be able to do,SATD
scikit-learn/sklearn/decomposition/_dict_learning.py,1502,some online fitting (partial_fit),not
scikit-learn/sklearn/decomposition/_fastica.py,8,"Authors: Pierre Lafaye de Micheaux, Stefan van der Walt, Gael Varoquaux,",not
scikit-learn/sklearn/decomposition/_fastica.py,9,"Bertrand Thirion, Alexandre Gramfort, Denis A. Engemann",not
scikit-learn/sklearn/decomposition/_fastica.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/_fastica.py,58,u (resp. s) contains the eigenvectors (resp. square roots of,not
scikit-learn/sklearn/decomposition/_fastica.py,59,the eigenvalues) of W * W.T,not
scikit-learn/sklearn/decomposition/_fastica.py,73,j is the index of the extracted component,not
scikit-learn/sklearn/decomposition/_fastica.py,112,"builtin max, abs are faster than numpy counter parts.",not
scikit-learn/sklearn/decomposition/_fastica.py,125,Some standard non-linear functions.,not
scikit-learn/sklearn/decomposition/_fastica.py,126,"XXX: these should be optimized, as they can be a bottleneck.",SATD
scikit-learn/sklearn/decomposition/_fastica.py,128,comment it out?,not
scikit-learn/sklearn/decomposition/_fastica.py,131,apply the tanh inplace,not
scikit-learn/sklearn/decomposition/_fastica.py,133,XXX compute in chunks to avoid extra allocation,SATD
scikit-learn/sklearn/decomposition/_fastica.py,134,please don't vectorize.,not
scikit-learn/sklearn/decomposition/_fastica.py,473,Centering the columns (ie the variables),not
scikit-learn/sklearn/decomposition/_fastica.py,477,Whitening and preprocessing by PCA,not
scikit-learn/sklearn/decomposition/_fastica.py,481,see (6.33) p.140,not
scikit-learn/sklearn/decomposition/_fastica.py,484,see (13.6) p.267 Here X1 is white and data,not
scikit-learn/sklearn/decomposition/_fastica.py,485,in X has been projected onto a subspace by PCA,not
scikit-learn/sklearn/decomposition/_fastica.py,488,X must be casted to floats to avoid typing issues with numpy,not
scikit-learn/sklearn/decomposition/_fastica.py,489,2.0 and the line below,not
scikit-learn/sklearn/decomposition/_fastica.py,490,copy has been taken care of,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,37,Test gram schmidt orthonormalization,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,38,generate a random orthogonal  matrix,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,53,Test the FastICA algorithm on very simple data.,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,55,scipy.stats uses the global RNG:,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,57,Generate two sources:,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,64,Mixing angle,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,75,function as fun arg,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,96,Check that the mixing model described in the docstring holds:,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,102,Check to see if the sources have been estimated,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,103,in the wrong order,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,109,Check that we have estimated the original sources,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,117,Test FastICA class,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,142,test for issue #697,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,149,Test the FastICA algorithm on very simple data,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,150,(see test_non_square_fastica).,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,151,Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,155,Generate two sources:,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,162,Mixing matrix,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,166,Do fastICA with tolerance 0. to ensure failing convergence,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,174,Test the FastICA algorithm on very simple data.,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,178,Generate two sources:,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,186,Mixing matrix,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,198,Check that the mixing model described in the docstring holds:,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,203,Check to see if the sources have been estimated,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,204,in the wrong order,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,210,Check that we have estimated the original sources,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,217,Test FastICA.fit_transform,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,238,Test FastICA.inverse_transform,not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,255,"catch ""n_components ignored"" warning",not
scikit-learn/sklearn/decomposition/tests/test_fastica.py,262,reversibility test in non-reduction case,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,1,Author: Christian Osendorfer <osendorf@gmail.com>,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,2,Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,3,License: BSD3,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,16,Ignore warnings from switching to more power iterations in randomized_svd,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,19,Test FactorAnalysis ability to recover the data covariance structure,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,23,Some random settings for the generative model,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,25,"latent variable of dim 3, 20 of it",not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,27,using gamma to model different noise variance,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,28,per component,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,31,generate observations,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,32,"wlog, mean is 0",not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,56,Sample Covariance,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,59,Model Covariance,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,68,sign will not be equal,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,77,Test get_covariance and get_precision with n_components == n_features,not
scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py,78,with n_components < n_features and with n_components == 0,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,24,check the shape of fit.transform,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,28,check the equivalence of fit.transform and fit_transform,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,34,Test get_covariance and get_precision,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,41,test if we avoid numpy warnings for computing over empty arrays,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,43,anything > n_comps triggered it in 0.16,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,54,Check that PCA output has unit-variance,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,61,some low rank data with correlated features,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,65,the component-wise variance of the first 50 features is 3 times the,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,66,mean component-wise variance of the remaining 30 features,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,71,the component-wise variance is thus highly varying:,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,74,whiten the data while projecting to the lower dim subspace,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,75,make sure we keep an original across iterations.,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,78,test fit_transform,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,95,in that case the output components still have varying variances,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,97,"we always center, so no test for non-centering.",not
scikit-learn/sklearn/decomposition/tests/test_pca.py,168,compare to the Frobenius norm,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,172,Compare to the 2-norms of the score vectors,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,177,set the singular values and see what er get back,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,193,Test that the projection of data is correct,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,208,Test that the projection of data is correct,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,220,Test that the projection of data can be inverted,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,223,spherical data,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,224,make middle component relatively small,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,225,make a large mean,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,227,same check that we can find the original data from the transformed,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,228,signal (since the data is almost of rank n_components),not
scikit-learn/sklearn/decomposition/tests/test_pca.py,253,Ensures that solver-specific extreme inputs for the n_components,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,254,parameter raise errors,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,255,The smallest dimension,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,266,Additional case for arpack,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,292,Ensure that n_components == 'mle' doesn't raise error for auto/full,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,303,Ensure that n_components == 'mle' will raise an error for unsupported,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,304,solvers,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,316,Check automated dimensionality setting,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,327,TODO: explain what this is testing,SATD
scikit-learn/sklearn/decomposition/tests/test_pca.py,328,Or at least use explicit variable names...,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,341,TODO: explain what this is testing,SATD
scikit-learn/sklearn/decomposition/tests/test_pca.py,342,Or at least use explicit variable names...,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,369,row > col,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,370,row > col,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,371,row < col,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,383,Test that probabilistic PCA scoring yields a reasonable score,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,404,Check that probabilistic PCA selects the right model,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,422,Sanity check for the noise_variance_. For more details see,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,423,https://github.com/scikit-learn/scikit-learn/issues/7568,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,424,https://github.com/scikit-learn/scikit-learn/issues/8541,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,425,https://github.com/scikit-learn/scikit-learn/issues/8544,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,434,Check the consistency of score between solvers,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,443,"arpack raises ValueError for n_components == min(n_samples,  n_features)",not
scikit-learn/sklearn/decomposition/tests/test_pca.py,446,ensure that noise_variance_ is 0 in edge cases,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,447,"when n_components == min(n_samples, n_features)",not
scikit-learn/sklearn/decomposition/tests/test_pca.py,462,"case: n_components in (0,1) => 'full'",not
scikit-learn/sklearn/decomposition/tests/test_pca.py,464,case: max(X.shape) <= 500 => 'full',not
scikit-learn/sklearn/decomposition/tests/test_pca.py,466,case: n_components >= .8 * min(X.shape) => 'full',not
scikit-learn/sklearn/decomposition/tests/test_pca.py,468,n_components >= 1 and n_components < .8*min(X.shape) => 'randomized',not
scikit-learn/sklearn/decomposition/tests/test_pca.py,521,Ensure that PCA does not upscale the dtype when input is float32,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,536,the rtol is set such that the test passes on all platforms tested on,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,537,conda-forge: PR#15775,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,538,see: https://github.com/conda-forge/scikit-learn-feedstock/pull/113,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,543,Ensure that all int types will be upcast to float64,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,562,when n_components is the second highest cumulative sum of the,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,563,"explained_variance_ratio_, then n_components_ should equal the",not
scikit-learn/sklearn/decomposition/tests/test_pca.py,564,number of features in the dataset #15669,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,574,"Test error when tested rank not in [1, n_features - 1]",not
scikit-learn/sklearn/decomposition/tests/test_pca.py,584,Test rank associated with tiny eigenvalues are given a log-likelihood of,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,585,-inf. The inferred rank will be 1,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,597,Test 'mle' with pathological X: only one relevant feature should give a,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,598,rank of 1,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,608,Tests that an error is raised when the number of samples is smaller,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,609,than the number of features during an mle fit,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,621,non-regression test for issue,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,622,https://github.com/scikit-learn/scikit-learn/issues/16730,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,625,true X dim is ndim - 1,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,632,Make sure assess_dimension works properly on a matrix of rank 1,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,634,rank 1 matrix,not
scikit-learn/sklearn/decomposition/tests/test_pca.py,636,"except for rank 1, all eigenvalues are 0",not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,6,For testing internals,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,32,Test that initialization does not return negative values,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,72,Test NNDSVD error,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,73,Test that _initialize_nmf error is less than the standard deviation of,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,74,the entries in the matrix.,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,84,Test NNDSVD variants correctness,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,85,Test that the variants 'nndsvda' and 'nndsvdar' differ from basic,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,86,'nndsvd' only where the basic version has zeros.,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,98,ignore UserWarning raised when both solver='mu' and init='nndsvd',not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,101,Test that the decomposition does not contain negative values,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,116,Test that the fit is not too far away,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,125,Test that NMF.transform returns close values,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,136,Smoke test that checks if NMF.transform works with custom initialization,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,152,Test that NMF.inverse_transform returns close values,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,163,Smoke test for the case of more components than features.,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,170,Test that sparse matrices are accepted as input,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,193,Test that transform works on sparse data.  Issue #2124,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,208,"Test that the function is called in the same way, either directly",not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,209,or through the NMF class,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,233,Test parameters checking is public function,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,283,Compare _beta_divergence with the reference _beta_divergence_dense,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,289,initialization,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,306,"Test the function that computes np.dot(W, H), only where X is non zero.",not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,321,"test that both results have same values, in X_csr nonzero elements",not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,326,test that WH_safe and X_csr have the same sparse structure,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,334,Compare sparse and dense input in multiplicative update NMF,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,335,Also test continuity of the results with respect to beta_loss parameter,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,343,initialization,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,352,Reference with dense array X,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,359,Compare with sparse X,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,369,"Compare with almost same beta_loss, since some values have a specific",not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,370,"behavior, but the results should be continuous w.r.t beta_loss",not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,383,Test that an error is raised if beta_loss < 0 and X contains zeros.,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,384,Test that the output has not NaN values when the input contains zeros.,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,412,Test the effect of L1 and L2 regularizations,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,419,L1 regularization should increase the number of zeros,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,441,L2 regularization should decrease the mean of the coefficients,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,461,test that the objective function is decreasing at each iteration,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,469,initialization,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,479,not implemented,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,484,one more iteration starting from the previous results,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,498,Regression test for an underflow issue in _beta_divergence,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,519,Check that NMF preserves dtype (float32 and float64),not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,531,Check that the result of NMF is the same between float32 and float64,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,543,Check that an error is raise if custom H and/or W don't have the same,not
scikit-learn/sklearn/decomposition/tests/test_nmf.py,544,dtype as X.,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,17,Make an X that looks somewhat like a small tf-idf matrix.,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,37,"All elements are equal, but some elements are more equal than others.",not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,73,"We need a lot of components for the reconstruction to be ""almost",not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,74,"equal"" in all positions. XXX Test means or sums instead?",SATD
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,96,Assert that all the values are greater than 0,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,99,Assert that total explained variance is less than 1,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,102,Test that explained_variance is correct,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,120,Assert the 1st component is equal,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,127,Assert that 20 components has higher explained variance than 10,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,136,Check that the TruncatedSVD output has the correct singular values,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,144,Compare to the Frobenius norm,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,149,Compare to the 2-norms of the score vectors,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,156,Set the singular values and see what we get back,not
scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py,177,TruncatedSVD should be equal to PCA on centered data,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,1,Author: Vlad Niculae,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,32,Y is defined by : Y = UV + noise,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,34,Add noise,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,37,"SparsePCA can be a bit slow. To avoid having test times go up, we",not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,38,test different aspects of the code in the same test,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,48,test overcomplete decomposition,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,58,wide array,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,63,Test that CD gives similar results,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,74,wide array,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,79,Test multiple CPUs,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,88,Test that SparsePCA won't return NaN when there is 0 feature in all,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,89,samples.,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,91,wide array,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,99,tall array,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,125,test overcomplete decomposition,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,132,XXX: test always skipped,SATD
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,137,wide array,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,141,Test multiple CPUs,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,142,fake parallelism for win32,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,152,we can efficiently use parallelism,not
scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py,158,Test that CD gives similar results,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,77,subsampling factor,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,80,Compute a wavelet dictionary,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,91,check that the underlying model fails to converge,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,97,check that the underlying model converges w/o warnings,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,176,"used to test lars here too, but there's no guarantee the number of",not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,177,nonzero atoms is right.,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,181,regression test that parallel reconstruction works with n_jobs>1,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,318,test verbosity,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,376,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,397,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,413,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,426,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,437,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,449,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,468,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,476,random init,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,485,Non-regression test for:,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,486,https://github.com/scikit-learn/scikit-learn/issues/5956,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,487,Test that SparseCoder does not error by passing reading only,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,488,arrays to child processes,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,493,Ensure that `data` is >2M. Joblib memory maps arrays,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,494,if they are larger than 1MB. The 4 accounts for float32,not
scikit-learn/sklearn/decomposition/tests/test_dict_learning.py,495,data type,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,24,Histogram kernel implemented as a callable.,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,25,no kernel_params that we didn't ask for,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,30,histogram kernel produces singular matrix inside linalg.solve,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,31,XXX use a least-squares approximation?,SATD
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,34,transform fit data,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,42,"non-regression test: previously, gamma would be 0 by default,",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,43,forcing all eigenvalues to 0 under the poly kernel,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,46,transform new data,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,51,inverse transform,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,63,"X_fit_ needs to retain the old, unmodified copy of X",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,97,transform fit data,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,105,transform new data,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,110,inverse transform,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,111,X_pred2 = kpca.inverse_transform(X_pred_transformed),not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,112,assert X_pred2.shape == X_pred.shape),not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,120,"for a linear kernel, kernel PCA should find the same projection as PCA",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,121,modulo the sign (direction),not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,122,"fit only the first four components: fifth is near zero eigenvalue, so",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,123,can be trimmed due to roundoff error,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,145,n_components=None (default) => remove_zero_eig is True,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,165,"Assert that even with all np warnings on, there is no div by zero warning",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,170,"Fit, then transform",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,172,Do both at once,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,174,Compare,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,178,"There might be warnings about the kernel being badly conditioned,",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,179,but there should not be warnings about division by zero.,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,180,"(Numpy division by zero warning can have many message variants, but",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,181,at least we know that it is a RuntimeWarning so lets check only this),not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,220,Test if we can do a grid-search to find parameters to separate,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,221,circles with a perceptron model.,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,234,Test if we can do a grid-search to find parameters to separate,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,235,circles with a perceptron model using a precomputed kernel.,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,249,Test the linear separability of the first 2D KPCA transform,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,253,2D nested circles are not linearly separable,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,257,Project the circles data into the first 2 components of a RBF Kernel,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,258,PCA model.,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,259,Note that the gamma value is data dependent. If this test breaks,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,260,"and the gamma value has to be updated, the Kernel PCA example will",not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,261,have to be updated too.,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,266,The data is perfectly linearly separable in that space,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,275,create a pathological X leading to small non-zero eigenvalue,not
scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py,283,check that the small non-zero eigenvalue was correctly set to zero,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,24,Create 3 topics and each topic has 3 distinct words.,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,25,(Each word only belongs to a single topic.),not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,35,default prior parameter should be `1 / topics`,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,36,and verbose params should not affect result,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,50,Test LDA batch learning_offset (`fit` method with 'batch' learning),not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,60,Find top 3 words in each LDA component,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,66,Test LDA online learning (`fit` method with 'online' learning),not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,76,Find top 3 words in each LDA component,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,82,Test LDA online learning (`partial_fit` method),not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,83,(same as test_lda_batch),not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,99,Test LDA with dense input.,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,108,Find top 3 words in each LDA component,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,114,Test LDA transform.,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,115,Transform result cannot be negative and should be normalized,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,129,Test LDA fit_transform & transform,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,130,fit_transform and transform result should be the same,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,141,test `n_features` mismatch in `partial_fit`,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,156,test `_check_params` method,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,173,test pass dense matrix with sparse negative input.,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,182,test `perplexity` before `fit`,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,194,test `n_features` mismatch in partial_fit and transform,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,211,Test LDA batch training with multi CPU,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,226,Test LDA online training with multi CPU,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,242,test dimension mismatch in `perplexity` method,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,251,invalid samples,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,255,invalid topic number,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,263,Test LDA perplexity for batch training,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,264,perplexity should be lower after each iteration,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,286,Test LDA score for batch training,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,287,score should be higher after each iteration,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,304,Test LDA perplexity for sparse and dense input,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,305,score should be the same for both dense and sparse input,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,317,Test the relationship between LDA score and perplexity,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,330,Test that the perplexity computed during fit is consistent with what is,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,331,returned by the perplexity method,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,338,Perplexity computed at end of fit method,not
scikit-learn/sklearn/decomposition/tests/test_online_lda.py,341,Result of perplexity method on the train set,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,18,Incremental PCA on dense arrays.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,44,Incremental PCA on sparse arrays.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,75,Test that the projection of data is correct.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,82,Get the reconstruction of the generated data X,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,83,"Note that Xt has the same ""components"" as X, just separated",not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,84,This is what we want to ensure is recreated correctly,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,87,Normalize,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,90,"Make sure that the first element of Yt is ~1, this means",not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,91,the reconstruction worked as expected,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,96,Test that the projection of data can be inverted.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,99,spherical data,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,100,make middle component relatively small,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,101,make a large mean,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,103,same check that we can find the original data from the transformed,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,104,signal (since the data is almost of rank n_components),not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,112,Test that n_components is >=1 and <= n_features.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,123,Tests that n_components is also <= n_samples.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,132,Ensures that n_components == None is handled correctly,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,138,"First partial_fit call, ipca.n_components_ is inferred from",not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,139,min(X.shape),not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,143,"Second partial_fit call, ipca.n_components_ is inferred from",not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,144,ipca.components_ computed from the first partial_fit call,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,150,Test that components_ sign is stable over batch sizes.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,159,Decreasing number of components,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,163,Increasing number of components,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,167,Returning to original setting,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,173,Test that changing n_components will raise an error.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,185,Test that components_ sign is stable over batch sizes.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,201,Test that components_ values are stable over batch sizes.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,217,Test sample size in each batch is always larger or equal to n_components,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,234,Test that fit and partial_fit get equivalent results.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,237,spherical data,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,238,make middle component relatively small,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,239,make a large mean,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,241,same check that we can find the original data from the transformed,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,242,signal (since the data is almost of rank n_components),not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,246,Add one to make sure endpoint is included,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,254,Test that IncrementalPCA and PCA are approximate (to a sign flip).,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,264,Test that IncrementalPCA and PCA are approximate (to a sign flip).,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,277,Test that PCA and IncrementalPCA calculations match,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,294,Check that the IncrementalPCA output has the correct singular values,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,307,Compare to the Frobenius norm,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,315,Compare to the 2-norms of the score vectors,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,321,Set the singular values and see what we get back,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,345,Test that PCA and IncrementalPCA transforms match to sign flip.,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,366,Test to ensure float division is used in all versions of Python,not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,367,(non-regression test for issue #9489),not
scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py,375,Set n_samples_seen_ to be a floating point number instead of an int,not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,19,extra_(pre/post)args can be a callable to make it possible to get its,not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,20,value from the compiler,not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,32,Write test program,not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,38,"Compile, test program",not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,42,Link test program,not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,49,Run test program,not
scikit-learn/sklearn/_build_utils/pre_build_helpers.py,50,will raise a CalledProcessError if return code was non-zero,not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,3,"This code is adapted for a large part from the astropy openmp helpers, which",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,4,can be found at: https://github.com/astropy/astropy-helpers/blob/master/astropy_helpers/openmp_helpers.py  # noqa,not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,31,-fopenmp can't be passed as compile flag when using Apple-clang.,not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,32,OpenMP support has to be enabled during preprocessing.,not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,33,,not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,34,"For example, our macOS wheel build jobs use the following environment",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,35,"variables to build with Apple-clang and the brew installed ""libomp"":",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,36,,not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,37,"export CPPFLAGS=""$CPPFLAGS -Xpreprocessor -fopenmp""",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,38,"export CFLAGS=""$CFLAGS -I/usr/local/opt/libomp/include""",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,39,"export CXXFLAGS=""$CXXFLAGS -I/usr/local/opt/libomp/include""",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,40,"export LDFLAGS=""$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,41,"-L/usr/local/opt/libomp/lib -lomp""",not
scikit-learn/sklearn/_build_utils/openmp_helpers.py,43,Default flag for GCC and clang:,not
scikit-learn/sklearn/_build_utils/__init__.py,4,"author: Andy Mueller, Gael Varoquaux",not
scikit-learn/sklearn/_build_utils/__init__.py,5,license: BSD,not
scikit-learn/sklearn/_build_utils/__init__.py,20,The following places need to be in sync with regard to Cython version:,not
scikit-learn/sklearn/_build_utils/__init__.py,21,- .circleci config file,not
scikit-learn/sklearn/_build_utils/__init__.py,22,- sklearn/_build_utils/__init__.py,not
scikit-learn/sklearn/_build_utils/__init__.py,23,- advanced installation guide,not
scikit-learn/sklearn/_build_utils/__init__.py,34,Re-raise with more informative error message instead:,not
scikit-learn/sklearn/_build_utils/__init__.py,48,Fast fail before cythonization if compiler fails compiling basic test,not
scikit-learn/sklearn/_build_utils/__init__.py,49,code even without OpenMP,not
scikit-learn/sklearn/_build_utils/__init__.py,52,check simple compilation with OpenMP. If it fails scikit-learn will be,not
scikit-learn/sklearn/_build_utils/__init__.py,53,built without OpenMP and the test test_openmp_supported in the test suite,not
scikit-learn/sklearn/_build_utils/__init__.py,54,will fail.,not
scikit-learn/sklearn/_build_utils/__init__.py,55,`check_openmp_support` compiles a small test program to see if the,not
scikit-learn/sklearn/_build_utils/__init__.py,56,compilers are properly configured to build with OpenMP. This is expensive,not
scikit-learn/sklearn/_build_utils/__init__.py,57,and we only want to call this function once.,not
scikit-learn/sklearn/_build_utils/__init__.py,58,The result of this check is cached as a private attribute on the sklearn,not
scikit-learn/sklearn/_build_utils/__init__.py,59,module (only at build-time) to be used twice:,not
scikit-learn/sklearn/_build_utils/__init__.py,60,"- First to set the value of SKLEARN_OPENMP_PARALLELISM_ENABLED, the",not
scikit-learn/sklearn/_build_utils/__init__.py,61,cython build-time variable passed to the cythonize() call.,not
scikit-learn/sklearn/_build_utils/__init__.py,62,- Then in the build_ext subclass defined in the top-level setup.py file,not
scikit-learn/sklearn/_build_utils/__init__.py,63,to actually build the compiled extensions with OpenMP flags if needed.,not
scikit-learn/sklearn/_build_utils/__init__.py,70,earlier joblib versions don't account for CPU affinity,not
scikit-learn/sklearn/_build_utils/__init__.py,71,"constraints, and may over-estimate the number of available",not
scikit-learn/sklearn/_build_utils/__init__.py,72,CPU particularly in CI (cf loky#114),not
scikit-learn/sklearn/_build_utils/__init__.py,85,Lazy import because cython is not a runtime dependency.,not
scikit-learn/sklearn/_build_utils/__init__.py,91,"if the template is not updated, no need to output the cython file",not
scikit-learn/sklearn/_build_utils/deprecated_modules.py,6,TODO: Remove the whole file in 0.24,SATD
scikit-learn/sklearn/_build_utils/deprecated_modules.py,8,This is a set of 4-tuples consisting of,not
scikit-learn/sklearn/_build_utils/deprecated_modules.py,9,"(new_module_name, deprecated_path, correct_import_path, importee)",not
scikit-learn/sklearn/_build_utils/deprecated_modules.py,10,importee is used by test_import_deprecations to check for DeprecationWarnings,not
scikit-learn/sklearn/cross_decomposition/_pls.py,5,Author: Edouard Duchesnay <edouard.duchesnay@cea.fr>,not
scikit-learn/sklearn/cross_decomposition/_pls.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/cross_decomposition/_pls.py,46,Uses condition from scipy<1.3 in pinv2 which was changed in,not
scikit-learn/sklearn/cross_decomposition/_pls.py,47,"https://github.com/scipy/scipy/pull/10067. In scipy 1.3, the",not
scikit-learn/sklearn/cross_decomposition/_pls.py,48,condition was changed to depend on the largest singular value,not
scikit-learn/sklearn/cross_decomposition/_pls.py,56,Inner loop of the Wold algo.,not
scikit-learn/sklearn/cross_decomposition/_pls.py,58,1.1 Update u: the X weights,not
scikit-learn/sklearn/cross_decomposition/_pls.py,61,We use slower pinv2 (same as np.linalg.pinv) for stability,not
scikit-learn/sklearn/cross_decomposition/_pls.py,62,reasons,not
scikit-learn/sklearn/cross_decomposition/_pls.py,65,mode A,not
scikit-learn/sklearn/cross_decomposition/_pls.py,66,Mode A regress each X column on y_score,not
scikit-learn/sklearn/cross_decomposition/_pls.py,68,If y_score only has zeros x_weights will only have zeros. In,not
scikit-learn/sklearn/cross_decomposition/_pls.py,69,this case add an epsilon to converge to a more acceptable,not
scikit-learn/sklearn/cross_decomposition/_pls.py,70,solution,not
scikit-learn/sklearn/cross_decomposition/_pls.py,73,1.2 Normalize u,not
scikit-learn/sklearn/cross_decomposition/_pls.py,75,1.3 Update x_score: the X latent scores,not
scikit-learn/sklearn/cross_decomposition/_pls.py,77,2.1 Update y_weights,not
scikit-learn/sklearn/cross_decomposition/_pls.py,80,compute once pinv(Y),not
scikit-learn/sklearn/cross_decomposition/_pls.py,84,Mode A regress each Y column on x_score,not
scikit-learn/sklearn/cross_decomposition/_pls.py,86,2.2 Normalize y_weights,not
scikit-learn/sklearn/cross_decomposition/_pls.py,89,2.3 Update y_score: the Y latent scores,not
scikit-learn/sklearn/cross_decomposition/_pls.py,91,"y_score = np.dot(Y, y_weights) / np.dot(y_score.T, y_score) ## BUG",not
scikit-learn/sklearn/cross_decomposition/_pls.py,119,center,not
scikit-learn/sklearn/cross_decomposition/_pls.py,124,scale,not
scikit-learn/sklearn/cross_decomposition/_pls.py,280,copy since this will contains the residuals (deflated) matrices,not
scikit-learn/sklearn/cross_decomposition/_pls.py,303,Scale (in place),not
scikit-learn/sklearn/cross_decomposition/_pls.py,306,Residuals (deflated) matrices,not
scikit-learn/sklearn/cross_decomposition/_pls.py,309,Results matrices,not
scikit-learn/sklearn/cross_decomposition/_pls.py,318,"NIPALS algo: outer loop, over components",not
scikit-learn/sklearn/cross_decomposition/_pls.py,322,Yk constant,not
scikit-learn/sklearn/cross_decomposition/_pls.py,325,1) weights estimation (inner loop),not
scikit-learn/sklearn/cross_decomposition/_pls.py,326,-----------------------------------,not
scikit-learn/sklearn/cross_decomposition/_pls.py,328,Replace columns that are all close to zero with zeros,not
scikit-learn/sklearn/cross_decomposition/_pls.py,339,Forces sign stability of x_weights and y_weights,not
scikit-learn/sklearn/cross_decomposition/_pls.py,340,"Sign undeterminacy issue from svd if algorithm == ""svd""",not
scikit-learn/sklearn/cross_decomposition/_pls.py,341,and from platform dependent computation if algorithm == 'nipals',not
scikit-learn/sklearn/cross_decomposition/_pls.py,344,compute scores,not
scikit-learn/sklearn/cross_decomposition/_pls.py,351,test for null variance,not
scikit-learn/sklearn/cross_decomposition/_pls.py,355,2) Deflation (in place),not
scikit-learn/sklearn/cross_decomposition/_pls.py,356,----------------------,not
scikit-learn/sklearn/cross_decomposition/_pls.py,357,Possible memory footprint reduction may done here: in order to,not
scikit-learn/sklearn/cross_decomposition/_pls.py,358,avoid the allocation of a data chunk for the rank-one,not
scikit-learn/sklearn/cross_decomposition/_pls.py,359,"approximations matrix which is then subtracted to Xk, we suggest",not
scikit-learn/sklearn/cross_decomposition/_pls.py,360,to perform a column-wise deflation.,not
scikit-learn/sklearn/cross_decomposition/_pls.py,361,,not
scikit-learn/sklearn/cross_decomposition/_pls.py,362,- regress Xk's on x_score,not
scikit-learn/sklearn/cross_decomposition/_pls.py,364,- subtract rank-one approximations to obtain remainder matrix,not
scikit-learn/sklearn/cross_decomposition/_pls.py,367,"- regress Yk's on y_score, then subtract rank-one approx.",not
scikit-learn/sklearn/cross_decomposition/_pls.py,372,"- regress Yk's on x_score, then subtract rank-one approx.",not
scikit-learn/sklearn/cross_decomposition/_pls.py,376,"3) Store weights, scores and loadings # Notation:",not
scikit-learn/sklearn/cross_decomposition/_pls.py,377,T,not
scikit-learn/sklearn/cross_decomposition/_pls.py,378,U,not
scikit-learn/sklearn/cross_decomposition/_pls.py,379,W,not
scikit-learn/sklearn/cross_decomposition/_pls.py,380,C,not
scikit-learn/sklearn/cross_decomposition/_pls.py,381,P,not
scikit-learn/sklearn/cross_decomposition/_pls.py,382,Q,not
scikit-learn/sklearn/cross_decomposition/_pls.py,383,Such that: X = TP' + Err and Y = UQ' + Err,not
scikit-learn/sklearn/cross_decomposition/_pls.py,385,4) rotations from input space to transformed space (scores),not
scikit-learn/sklearn/cross_decomposition/_pls.py,386,T = X W(P'W)^-1 = XW* (W* : p x k matrix),not
scikit-learn/sklearn/cross_decomposition/_pls.py,387,U = Y C(Q'C)^-1 = YC* (W* : q x k matrix),not
scikit-learn/sklearn/cross_decomposition/_pls.py,401,FIXME what's with the if?,SATD
scikit-learn/sklearn/cross_decomposition/_pls.py,402,Estimate regression coefficient,not
scikit-learn/sklearn/cross_decomposition/_pls.py,403,Regress Y on T,not
scikit-learn/sklearn/cross_decomposition/_pls.py,404,"Y = TQ' + Err,",not
scikit-learn/sklearn/cross_decomposition/_pls.py,405,Then express in function of X,not
scikit-learn/sklearn/cross_decomposition/_pls.py,406,Y = X W(P'W)^-1Q' + Err = XB + Err,not
scikit-learn/sklearn/cross_decomposition/_pls.py,407,=> B = W*Q' (p x q),not
scikit-learn/sklearn/cross_decomposition/_pls.py,434,Normalize,not
scikit-learn/sklearn/cross_decomposition/_pls.py,437,Apply rotation,not
scikit-learn/sklearn/cross_decomposition/_pls.py,469,From pls space to original space,not
scikit-learn/sklearn/cross_decomposition/_pls.py,472,Denormalize,not
scikit-learn/sklearn/cross_decomposition/_pls.py,496,Normalize,not
scikit-learn/sklearn/cross_decomposition/_pls.py,893,copy since this will contains the centered data,not
scikit-learn/sklearn/cross_decomposition/_pls.py,906,Scale (in place),not
scikit-learn/sklearn/cross_decomposition/_pls.py,909,svd(X'Y),not
scikit-learn/sklearn/cross_decomposition/_pls.py,912,The arpack svds solver only works if the number of extracted,not
scikit-learn/sklearn/cross_decomposition/_pls.py,913,"components is smaller than rank(X) - 1. Hence, if we want to extract",not
scikit-learn/sklearn/cross_decomposition/_pls.py,914,"all the components (C.shape[1]), we have to use another one. Else,",not
scikit-learn/sklearn/cross_decomposition/_pls.py,915,let's use arpacks to compute only the interesting components.,not
scikit-learn/sklearn/cross_decomposition/_pls.py,920,Deterministic output,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,19,1) Canonical (symmetric) PLS (PLS 2 blocks canonical mode A),not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,20,===========================================================,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,21,Compare 2 algo.: nipals vs. svd,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,22,------------------------------,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,27,check equalities of loading (up to the sign of the second column),not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,38,Check PLS properties (with n_components=X.shape[1]),not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,39,---------------------------------------------------,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,53,Orthogonality of weights,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,54,~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,58,Orthogonality of latent scores,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,59,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,63,Check X = TP' and Y = UQ' (with (p == q) components),not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,64,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,65,"center scale X, Y",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,71,Check that rotations on training data lead to scores,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,72,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,82,Check that inverse_transform works,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,83,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,88,"""Non regression test"" on canonical PLS",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,89,--------------------------------------,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,90,The results were checked against the R-package plspm,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,98,"x_weights_sign_flip holds columns of 1 or -1, depending on sign flip",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,99,between R and python,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,120,x_weights = X.dot(x_rotation),not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,121,Hence R/python sign flip should be the same in x_weight and x_rotation,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,123,This test that R / python give the same result up to column,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,124,sign indeterminacy,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,133,"2) Regression PLS (PLS2): ""Non regression test""",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,134,===============================================,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,135,"The results were checked against the R-packages plspm, misOmics and pls",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,163,"x_loadings[:, i] = Xi.dot(x_weights[:, i]) \forall i",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,172,3) Another non-regression test of Canonical PLS on random dataset,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,173,=================================================================,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,174,The results were checked against the R-package plspm,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,178,2 latents vars:,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,260,Orthogonality of weights,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,261,~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,265,Orthogonality of latent scores,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,266,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,270,"4) Another ""Non regression test"" of PLS Regression (PLS2):",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,271,Checking behavior when the first column of Y is constant,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,272,===============================================,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,273,The results were compared against a modified version of plsreg2,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,274,from the R-package plsdepot,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,298,R/python sign flip should be the same in x_weight and x_rotation,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,301,This test that R / python give the same result up to column,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,302,sign indeterminacy,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,306,"For the PLSRegression with default parameters, it holds that",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,307,y_loadings==y_weights. In this case we only test that R/python,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,308,give the same result for the y_loadings irrespective of the sign,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,322,Let's check the PLSSVD doesn't return all possible component but just,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,323,the specified number,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,335,Ensure 1d Y is correctly interpreted,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,341,Compare 1d to column vector,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,348,"check that the ""copy"" keyword works",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,356,check that results are identical with copy,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,360,check also if passing Y,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,363,check that copy doesn't destroy,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,364,we do want to check exact equality here,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,367,also check that mean wasn't zero before (to make sure we didn't touch it),not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,372,We test scale=True parameter,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,373,This allows to check numerical stability over platforms as well,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,378,"causes X[:, -1].std() to be zero",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,381,From bug #2821,SATD
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,382,"Test with X2, T2 s.t. clf.x_score[:, 1] == 0, clf.y_score[:, 1] == 0",not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,383,This test robustness of algorithm when dealing with value close to 0,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,410,Scaling should be idempotent,not
scikit-learn/sklearn/cross_decomposition/tests/test_pls.py,429,sanity check for scale=True,not
scikit-learn/sklearn/_loss/glm_distribution.py,5,Author: Christian Lorentzen <lorentzen.ch@googlemail.com>,not
scikit-learn/sklearn/_loss/glm_distribution.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/_loss/glm_distribution.py,57,Note that currently supported distributions have +inf upper bound,not
scikit-learn/sklearn/_loss/glm_distribution.py,212,"We use a property with a setter, to update lower and",not
scikit-learn/sklearn/_loss/glm_distribution.py,213,upper bound when the power parameter is updated e.g. in grid,not
scikit-learn/sklearn/_loss/glm_distribution.py,214,search.,not
scikit-learn/sklearn/_loss/glm_distribution.py,220,Extreme Stable or Normal distribution,not
scikit-learn/sklearn/_loss/glm_distribution.py,226,Poisson or Compound Poisson distribution,not
scikit-learn/sklearn/_loss/glm_distribution.py,229,"Gamma, Positive Stable, Inverse Gaussian distributions",not
scikit-learn/sklearn/_loss/glm_distribution.py,231,pragma: no cover,not
scikit-learn/sklearn/_loss/glm_distribution.py,232,this branch should be unreachable.,not
scikit-learn/sklearn/_loss/glm_distribution.py,278,"'Extreme stable', y any realy number, y_pred > 0",not
scikit-learn/sklearn/_loss/glm_distribution.py,282,"Normal, y and y_pred can be any real number",not
scikit-learn/sklearn/_loss/glm_distribution.py,288,"Poisson and Compount poisson distribution, y >= 0, y_pred > 0",not
scikit-learn/sklearn/_loss/glm_distribution.py,293,"Gamma and Extreme stable distribution, y and y_pred > 0",not
scikit-learn/sklearn/_loss/glm_distribution.py,297,pragma: nocover,not
scikit-learn/sklearn/_loss/glm_distribution.py,298,Unreachable statement,not
scikit-learn/sklearn/_loss/glm_distribution.py,302,"'Extreme stable', y any realy number, y_pred > 0",not
scikit-learn/sklearn/_loss/glm_distribution.py,308,"Normal distribution, y and y_pred any real number",not
scikit-learn/sklearn/_loss/glm_distribution.py,314,Poisson distribution,not
scikit-learn/sklearn/_loss/glm_distribution.py,317,Gamma distribution,not
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,1,Authors: Christian Lorentzen <lorentzen.ch@gmail.com>,not
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,2,,not
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/_loss/tests/test_glm_distribution.py,97,make data positive,not
scikit-learn/sklearn/impute/_knn.py,1,Authors: Ashim Bhattarai <ashimb9@gmail.com>,not
scikit-learn/sklearn/impute/_knn.py,2,Thomas J Fan <thomasjpfan@gmail.com>,not
scikit-learn/sklearn/impute/_knn.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/impute/_knn.py,138,Get donors,not
scikit-learn/sklearn/impute/_knn.py,142,Get weight matrix from from distance matrix,not
scikit-learn/sklearn/impute/_knn.py,148,fill nans with zeros,not
scikit-learn/sklearn/impute/_knn.py,152,Retrieve donor values and calculate kNN average,not
scikit-learn/sklearn/impute/_knn.py,172,Check data integrity and calling arguments,not
scikit-learn/sklearn/impute/_knn.py,227,No missing values in X,not
scikit-learn/sklearn/impute/_knn.py,228,Remove columns where the training data is all nan,not
scikit-learn/sklearn/impute/_knn.py,235,Maps from indices from X to indices in dist matrix,not
scikit-learn/sklearn/impute/_knn.py,242,Find and impute missing by column,not
scikit-learn/sklearn/impute/_knn.py,245,column was all missing during training,not
scikit-learn/sklearn/impute/_knn.py,250,column has no missing values,not
scikit-learn/sklearn/impute/_knn.py,255,receivers_idx are indices in X,not
scikit-learn/sklearn/impute/_knn.py,258,distances for samples that needed imputation for column,not
scikit-learn/sklearn/impute/_knn.py,262,receivers with all nan distances impute with mean,not
scikit-learn/sklearn/impute/_knn.py,272,all receivers imputed with mean,not
scikit-learn/sklearn/impute/_knn.py,275,receivers with at least one defined distance,not
scikit-learn/sklearn/impute/_knn.py,289,process in fixed-memory chunks,not
scikit-learn/sklearn/impute/_knn.py,298,process_chunk modifies X in place. No return value.,not
scikit-learn/sklearn/impute/_iterative.py,305,"if no missing values, don't predict",not
scikit-learn/sklearn/impute/_iterative.py,309,get posterior samples if there is at least one missing value,not
scikit-learn/sklearn/impute/_iterative.py,315,two types of problems: (1) non-positive sigmas,not
scikit-learn/sklearn/impute/_iterative.py,316,(2) mus outside legal range of min_value and max_value,not
scikit-learn/sklearn/impute/_iterative.py,317,(results in inf sample),not
scikit-learn/sklearn/impute/_iterative.py,324,the rest can be sampled without statistical issues,not
scikit-learn/sklearn/impute/_iterative.py,341,update the feature,not
scikit-learn/sklearn/impute/_iterative.py,457,if a feature in the neighboorhood has only a single value,not
scikit-learn/sklearn/impute/_iterative.py,458,"(e.g., categorical feature), the std. dev. will be null and",not
scikit-learn/sklearn/impute/_iterative.py,459,np.corrcoef will raise a warning due to a division by zero,not
scikit-learn/sklearn/impute/_iterative.py,461,np.corrcoef is not defined for features with zero std,not
scikit-learn/sklearn/impute/_iterative.py,463,"ensures exploration, i.e. at least some probability of sampling",not
scikit-learn/sklearn/impute/_iterative.py,465,features are not their own neighbors,not
scikit-learn/sklearn/impute/_iterative.py,467,needs to sum to 1 for np.random.choice sampling,not
scikit-learn/sklearn/impute/_iterative.py,600,Edge case: a single feature. We return the initial ...,not
scikit-learn/sklearn/impute/_iterative.py,614,order in which to impute,not
scikit-learn/sklearn/impute/_iterative.py,615,note this is probably too slow for large feature data (d > 100000),not
scikit-learn/sklearn/impute/_iterative.py,616,and a better way would be good.,SATD
scikit-learn/sklearn/impute/_iterative.py,617,see: https://goo.gl/KyCNwj and subsequent comments,not
scikit-learn/sklearn/impute/__init__.py,8,Avoid errors in type checkers (e.g. mypy) for experimental estimators.,not
scikit-learn/sklearn/impute/__init__.py,9,TODO: remove this check once the estimator is no longer experimental.,SATD
scikit-learn/sklearn/impute/__init__.py,10,noqa,not
scikit-learn/sklearn/impute/_base.py,1,Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>,not
scikit-learn/sklearn/impute/_base.py,2,Sergey Feldman <sergeyfeldman@gmail.com>,not
scikit-learn/sklearn/impute/_base.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/impute/_base.py,35,Compute the most frequent value in array only,not
scikit-learn/sklearn/impute/_base.py,38,stats.mode raises a warning when input array contains objects due,not
scikit-learn/sklearn/impute/_base.py,39,to incapacity to detect NaNs. Irrelevant here since input array,not
scikit-learn/sklearn/impute/_base.py,40,has already been NaN-masked.,not
scikit-learn/sklearn/impute/_base.py,50,Compare to array + [extra_value] * n_repeat,not
scikit-learn/sklearn/impute/_base.py,58,Ties the breaks. Copy the behaviour of scipy.stats.mode,not
scikit-learn/sklearn/impute/_base.py,280,"default fill_value is 0 for numerical input and ""missing_value""",not
scikit-learn/sklearn/impute/_base.py,281,otherwise,not
scikit-learn/sklearn/impute/_base.py,290,fill_value should be numerical in case of numerical input,not
scikit-learn/sklearn/impute/_base.py,299,missing_values = 0 not allowed with sparse data as it would,not
scikit-learn/sklearn/impute/_base.py,300,force densification,not
scikit-learn/sklearn/impute/_base.py,325,"for constant strategy, self.statistcs_ is used to store",not
scikit-learn/sklearn/impute/_base.py,326,fill_value in each column,not
scikit-learn/sklearn/impute/_base.py,334,combine explicit and implicit zeros,not
scikit-learn/sklearn/impute/_base.py,359,Mean,not
scikit-learn/sklearn/impute/_base.py,362,"Avoid the warning ""Warning: converting a masked element to nan.""",not
scikit-learn/sklearn/impute/_base.py,368,Median,not
scikit-learn/sklearn/impute/_base.py,371,"Avoid the warning ""Warning: converting a masked element to nan.""",not
scikit-learn/sklearn/impute/_base.py,377,Most frequent,not
scikit-learn/sklearn/impute/_base.py,379,Avoid use of scipy.stats.mstats.mode due to the required,not
scikit-learn/sklearn/impute/_base.py,380,additional overhead and slow benchmarking performance.,not
scikit-learn/sklearn/impute/_base.py,381,See Issue 14325 and PR 14399 for full discussion.,not
scikit-learn/sklearn/impute/_base.py,383,To be able access the elements by columns,not
scikit-learn/sklearn/impute/_base.py,399,Constant,not
scikit-learn/sklearn/impute/_base.py,401,"for constant strategy, self.statistcs_ is used to store",not
scikit-learn/sklearn/impute/_base.py,402,fill_value in each column,not
scikit-learn/sklearn/impute/_base.py,424,Delete the invalid columns if strategy is not constant,not
scikit-learn/sklearn/impute/_base.py,428,same as np.isnan but also works for object dtypes,not
scikit-learn/sklearn/impute/_base.py,441,Do actual imputation,not
scikit-learn/sklearn/impute/_base.py,564,The imputer mask will be constructed with the same sparse format,not
scikit-learn/sklearn/impute/_base.py,565,as X.,not
scikit-learn/sklearn/impute/_base.py,614,missing_values = 0 not allowed with sparse data as it would,not
scikit-learn/sklearn/impute/_base.py,615,force densification,not
scikit-learn/sklearn/impute/tests/test_impute.py,16,make IterativeImputer available,not
scikit-learn/sklearn/impute/tests/test_impute.py,17,noqa,not
scikit-learn/sklearn/impute/tests/test_impute.py,50,Normal matrix,not
scikit-learn/sklearn/impute/tests/test_impute.py,57,Sparse matrix,not
scikit-learn/sklearn/impute/tests/test_impute.py,73,Verify the shapes of the imputed matrix for different strategies.,not
scikit-learn/sklearn/impute/tests/test_impute.py,111,check that error are raised when missing_values = 0 and input is sparse,not
scikit-learn/sklearn/impute/tests/test_impute.py,126,np.median([]) raises a TypeError for numpy >= 1.10.1,not
scikit-learn/sklearn/impute/tests/test_impute.py,132,np.mean([]) raises a RuntimeWarning for numpy >= 1.10.1,not
scikit-learn/sklearn/impute/tests/test_impute.py,138,"Test imputation using the mean and median strategies, when",not
scikit-learn/sklearn/impute/tests/test_impute.py,139,missing_values != 0.,not
scikit-learn/sklearn/impute/tests/test_impute.py,159,Create a matrix X with columns,not
scikit-learn/sklearn/impute/tests/test_impute.py,160,"- with only zeros,",not
scikit-learn/sklearn/impute/tests/test_impute.py,161,- with only missing values,not
scikit-learn/sklearn/impute/tests/test_impute.py,162,"- with zeros, missing values and values",not
scikit-learn/sklearn/impute/tests/test_impute.py,163,And a matrix X_true containing all true values,not
scikit-learn/sklearn/impute/tests/test_impute.py,176,Create the columns,not
scikit-learn/sklearn/impute/tests/test_impute.py,180,XXX unreached code as of v0.22,SATD
scikit-learn/sklearn/impute/tests/test_impute.py,191,Shuffle them the same way,not
scikit-learn/sklearn/impute/tests/test_impute.py,195,"Mean doesn't support columns containing NaNs, median does",not
scikit-learn/sklearn/impute/tests/test_impute.py,208,Test median imputation with sparse boundary cases,not
scikit-learn/sklearn/impute/tests/test_impute.py,210,odd: implicit zero,not
scikit-learn/sklearn/impute/tests/test_impute.py,211,odd: explicit nonzero,not
scikit-learn/sklearn/impute/tests/test_impute.py,212,even: average two zeros,not
scikit-learn/sklearn/impute/tests/test_impute.py,213,even: avg zero and neg,not
scikit-learn/sklearn/impute/tests/test_impute.py,214,even: avg zero and pos,not
scikit-learn/sklearn/impute/tests/test_impute.py,215,even: avg nonzeros,not
scikit-learn/sklearn/impute/tests/test_impute.py,216,even: avg negatives,not
scikit-learn/sklearn/impute/tests/test_impute.py,217,even: crossing neg and pos,not
scikit-learn/sklearn/impute/tests/test_impute.py,266,"Test imputation on non-numeric data using ""most_frequent"" and ""constant""",not
scikit-learn/sklearn/impute/tests/test_impute.py,267,strategy,not
scikit-learn/sklearn/impute/tests/test_impute.py,282,Test imputation using the most-frequent strategy.,not
scikit-learn/sklearn/impute/tests/test_impute.py,297,"scipy.stats.mode, used in SimpleImputer, doesn't return the first most",not
scikit-learn/sklearn/impute/tests/test_impute.py,298,frequent as promised in the doc but the lowest most frequent. When this,not
scikit-learn/sklearn/impute/tests/test_impute.py,299,"test will fail after an update of scipy, SimpleImputer will need to be",not
scikit-learn/sklearn/impute/tests/test_impute.py,300,updated to be consistent with the new (correct) behaviour,not
scikit-learn/sklearn/impute/tests/test_impute.py,306,Test imputation using the most-frequent strategy.,not
scikit-learn/sklearn/impute/tests/test_impute.py,330,Test imputation using the most frequent strategy on pandas df,not
scikit-learn/sklearn/impute/tests/test_impute.py,356,Verify that exceptions are raised on invalid fill_value type,not
scikit-learn/sklearn/impute/tests/test_impute.py,368,Test imputation using the constant strategy on integers,not
scikit-learn/sklearn/impute/tests/test_impute.py,392,Test imputation using the constant strategy on floats,not
scikit-learn/sklearn/impute/tests/test_impute.py,419,Test imputation using the constant strategy on objects,not
scikit-learn/sklearn/impute/tests/test_impute.py,443,Test imputation using the constant strategy on pandas df,not
scikit-learn/sklearn/impute/tests/test_impute.py,469,check we exit early when there is a single feature,not
scikit-learn/sklearn/impute/tests/test_impute.py,480,Test imputation within a pipeline + gridsearch.,not
scikit-learn/sklearn/impute/tests/test_impute.py,499,Test imputation with copy,not
scikit-learn/sklearn/impute/tests/test_impute.py,502,"copy=True, dense => copy",not
scikit-learn/sklearn/impute/tests/test_impute.py,509,"copy=True, sparse csr => copy",not
scikit-learn/sklearn/impute/tests/test_impute.py,517,"copy=False, dense => no copy",not
scikit-learn/sklearn/impute/tests/test_impute.py,524,"copy=False, sparse csc => no copy",not
scikit-learn/sklearn/impute/tests/test_impute.py,532,"copy=False, sparse csr => copy",not
scikit-learn/sklearn/impute/tests/test_impute.py,540,"Note: If X is sparse and if missing_values=0, then a (dense) copy of X is",not
scikit-learn/sklearn/impute/tests/test_impute.py,541,"made, even if copy=False.",not
scikit-learn/sklearn/impute/tests/test_impute.py,555,"with max_iter=0, only initial imputation is performed",not
scikit-learn/sklearn/impute/tests/test_impute.py,558,repeat but force n_iter_ to 0,not
scikit-learn/sklearn/impute/tests/test_impute.py,560,transformed should not be equal to initial imputation,not
scikit-learn/sklearn/impute/tests/test_impute.py,565,now they should be equal as only initial imputation is done,not
scikit-learn/sklearn/impute/tests/test_impute.py,603,this column should not be discarded by IterativeImputer,not
scikit-learn/sklearn/impute/tests/test_impute.py,650,check that types are correct for estimators,not
scikit-learn/sklearn/impute/tests/test_impute.py,658,check that each estimator is unique,not
scikit-learn/sklearn/impute/tests/test_impute.py,704,test that the values that are imputed using `sample_posterior=True`,not
scikit-learn/sklearn/impute/tests/test_impute.py,705,with boundaries (`min_value` and `max_value` are not None) are drawn,not
scikit-learn/sklearn/impute/tests/test_impute.py,706,from a distribution that looks gaussian via the Kolmogorov Smirnov test.,not
scikit-learn/sklearn/impute/tests/test_impute.py,707,note that starting from the wrong random seed will make this test fail,not
scikit-learn/sklearn/impute/tests/test_impute.py,708,because random sampling doesn't occur at all when the imputation,not
scikit-learn/sklearn/impute/tests/test_impute.py,709,"is outside of the (min_value, max_value) range",not
scikit-learn/sklearn/impute/tests/test_impute.py,721,generate multiple imputations for the single missing value,not
scikit-learn/sklearn/impute/tests/test_impute.py,732,we want to fail to reject null hypothesis,not
scikit-learn/sklearn/impute/tests/test_impute.py,733,null hypothesis: distributions are the same,not
scikit-learn/sklearn/impute/tests/test_impute.py,749,definitely no missing values in 0th column,not
scikit-learn/sklearn/impute/tests/test_impute.py,750,definitely missing value in 0th column,not
scikit-learn/sklearn/impute/tests/test_impute.py,759,"if there were no missing values at time of fit, then imputer will",not
scikit-learn/sklearn/impute/tests/test_impute.py,760,only use the initial imputer for that feature at transform,not
scikit-learn/sklearn/impute/tests/test_impute.py,773,"when sample_posterior=True, two transforms shouldn't be equal",not
scikit-learn/sklearn/impute/tests/test_impute.py,783,sufficient to assert that the means are not the same,not
scikit-learn/sklearn/impute/tests/test_impute.py,786,"when sample_posterior=False, and n_nearest_features=None",not
scikit-learn/sklearn/impute/tests/test_impute.py,787,and imputation_order is not random,not
scikit-learn/sklearn/impute/tests/test_impute.py,788,the two transforms should be identical even if rng are different,not
scikit-learn/sklearn/impute/tests/test_impute.py,821,should exclude the first column entirely,not
scikit-learn/sklearn/impute/tests/test_impute.py,823,fit and fit_transform should both be identical,not
scikit-learn/sklearn/impute/tests/test_impute.py,859,split up data in half,SATD
scikit-learn/sklearn/impute/tests/test_impute.py,883,a quarter is randomly missing,not
scikit-learn/sklearn/impute/tests/test_impute.py,888,split up data,not
scikit-learn/sklearn/impute/tests/test_impute.py,948,check that we catch a RuntimeWarning due to a division by zero when a,not
scikit-learn/sklearn/impute/tests/test_impute.py,949,feature is constant in the dataset,not
scikit-learn/sklearn/impute/tests/test_impute.py,953,simulate that a feature only contain one category during fit,not
scikit-learn/sklearn/impute/tests/test_impute.py,956,add some missing values,not
scikit-learn/sklearn/impute/tests/test_impute.py,985,check that passing scalar or array-like,not
scikit-learn/sklearn/impute/tests/test_impute.py,986,for min_value and max_value in IterativeImputer works,not
scikit-learn/sklearn/impute/tests/test_impute.py,1006,check that passing scalar or array-like,not
scikit-learn/sklearn/impute/tests/test_impute.py,1007,for min_value and max_value in IterativeImputer works,not
scikit-learn/sklearn/impute/tests/test_impute.py,1020,Test that None/inf and scalar/vector give the same imputation,not
scikit-learn/sklearn/impute/tests/test_impute.py,1045,check the imputing strategy when missing data are present in the,not
scikit-learn/sklearn/impute/tests/test_impute.py,1046,testing set only.,not
scikit-learn/sklearn/impute/tests/test_impute.py,1047,taken from: https://github.com/scikit-learn/scikit-learn/issues/14383,not
scikit-learn/sklearn/impute/tests/test_impute.py,1065,impute with the initial strategy: 'mean',not
scikit-learn/sklearn/impute/tests/test_impute.py,1122,convert the input to the right array format and right dtype,not
scikit-learn/sklearn/impute/tests/test_impute.py,1163,test for sparse input and missing_value == 0,not
scikit-learn/sklearn/impute/tests/test_impute.py,1171,convert the input to the right array format,not
scikit-learn/sklearn/impute/tests/test_impute.py,1196,check the format of the output with different sparse parameter,not
scikit-learn/sklearn/impute/tests/test_impute.py,1268,regression test for issue #11390. Comparison between incoherent dtype,not
scikit-learn/sklearn/impute/tests/test_impute.py,1269,for X and missing_values was not raising a proper error.,not
scikit-learn/sklearn/impute/tests/test_impute.py,1281,check that all features are dropped if there are no missing values when,not
scikit-learn/sklearn/impute/tests/test_impute.py,1282,features='missing-only' (#13491),not
scikit-learn/sklearn/impute/tests/test_impute.py,1293,Check that non missing values don't become explicit zeros in the mask,not
scikit-learn/sklearn/impute/tests/test_impute.py,1294,generated by missing indicator when X is sparse. (#13491),not
scikit-learn/sklearn/impute/tests/test_impute.py,1353,regression test for #15393,not
scikit-learn/sklearn/impute/tests/test_common.py,10,noqa,not
scikit-learn/sklearn/impute/tests/test_common.py,21,ConvergenceWarning will be raised by the IterativeImputer,not
scikit-learn/sklearn/impute/tests/test_common.py,25,[Non Regression Test for issue #13968] Missing value in test set should,not
scikit-learn/sklearn/impute/tests/test_common.py,26,not throw an error and return a finite dataset,not
scikit-learn/sklearn/impute/tests/test_common.py,33,ConvergenceWarning will be raised by the IterativeImputer,not
scikit-learn/sklearn/impute/tests/test_common.py,61,ConvergenceWarning will be raised by the IterativeImputer,not
scikit-learn/sklearn/impute/tests/test_common.py,89,ConvergenceWarning will be raised by the IterativeImputer,not
scikit-learn/sklearn/impute/tests/test_common.py,94,Test pandas IntegerArray with pd.NA,not
scikit-learn/sklearn/impute/tests/test_common.py,106,fit on numpy array,not
scikit-learn/sklearn/impute/tests/test_common.py,109,Creates dataframe with IntegerArrays with pd.NA,not
scikit-learn/sklearn/impute/tests/test_common.py,112,fit on pandas dataframe with IntegerArrays,not
scikit-learn/sklearn/impute/tests/test_knn.py,15,Verify the shapes of the imputed matrix for different weights and,not
scikit-learn/sklearn/impute/tests/test_knn.py,16,number of neighbors.,not
scikit-learn/sklearn/impute/tests/test_knn.py,29,Test imputation with default values and invalid input,not
scikit-learn/sklearn/impute/tests/test_knn.py,31,Test with inf present,not
scikit-learn/sklearn/impute/tests/test_knn.py,43,Test with inf present in matrix passed in transform(),not
scikit-learn/sklearn/impute/tests/test_knn.py,65,negative n_neighbors,not
scikit-learn/sklearn/impute/tests/test_knn.py,69,Test with missing_values=0 when NaN present,not
scikit-learn/sklearn/impute/tests/test_knn.py,87,Test with a metric type without NaN support,not
scikit-learn/sklearn/impute/tests/test_knn.py,115,Test with an imputable matrix and compare with different missing_values,not
scikit-learn/sklearn/impute/tests/test_knn.py,150,Test with an imputable matrix,not
scikit-learn/sklearn/impute/tests/test_knn.py,174,Test when there is not enough neighbors,not
scikit-learn/sklearn/impute/tests/test_knn.py,186,"Not enough neighbors, use column mean from training",not
scikit-learn/sklearn/impute/tests/test_knn.py,202,Test when data in fit() and transform() are different,not
scikit-learn/sklearn/impute/tests/test_knn.py,303,"Test with ""uniform"" weight (or unweighted)",not
scikit-learn/sklearn/impute/tests/test_knn.py,317,"Test with ""callable"" weight",not
scikit-learn/sklearn/impute/tests/test_knn.py,324,"Test with ""callable"" uniform weight",not
scikit-learn/sklearn/impute/tests/test_knn.py,344,"Test with ""distance"" weight",not
scikit-learn/sklearn/impute/tests/test_knn.py,350,Manual calculation,not
scikit-learn/sklearn/impute/tests/test_knn.py,366,NearestNeighbor calculation,not
scikit-learn/sklearn/impute/tests/test_knn.py,381,"Test with weights = ""distance"" and n_neighbors=2",not
scikit-learn/sklearn/impute/tests/test_knn.py,389,"neighbors are rows 1, 2, the nan_euclidean_distances are:",not
scikit-learn/sklearn/impute/tests/test_knn.py,404,Test with varying missingness patterns,not
scikit-learn/sklearn/impute/tests/test_knn.py,415,Get weights of donor neighbors,not
scikit-learn/sklearn/impute/tests/test_knn.py,425,Collect donor values,not
scikit-learn/sklearn/impute/tests/test_knn.py,429,Final imputed values,not
scikit-learn/sklearn/impute/tests/test_knn.py,461,Calculate weights,not
scikit-learn/sklearn/impute/tests/test_knn.py,467,Calculate weighted averages,not
scikit-learn/sklearn/impute/tests/test_knn.py,490,Define callable metric that returns the l1 norm:,not
scikit-learn/sklearn/impute/tests/test_knn.py,519,"Note that we use working_memory=0 to ensure that chunking is tested, even",not
scikit-learn/sklearn/impute/tests/test_knn.py,520,"for a small dataset. However, it should raise a UserWarning that we ignore.",not
scikit-learn/sklearn/impute/tests/test_knn.py,560,Samples with needed feature has nan distance,not
scikit-learn/sklearn/feature_extraction/image.py,6,Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,not
scikit-learn/sklearn/feature_extraction/image.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/feature_extraction/image.py,8,Olivier Grisel,not
scikit-learn/sklearn/feature_extraction/image.py,9,Vlad Niculae,not
scikit-learn/sklearn/feature_extraction/image.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/feature_extraction/image.py,28,,not
scikit-learn/sklearn/feature_extraction/image.py,29,From an image to a graph,not
scikit-learn/sklearn/feature_extraction/image.py,65,XXX: Why mask the image after computing the weights?,SATD
scikit-learn/sklearn/feature_extraction/image.py,205,,not
scikit-learn/sklearn/feature_extraction/image.py,206,From an image to a set of small image patches,not
scikit-learn/sklearn/feature_extraction/image.py,435,remove the color dimension if useless,not
scikit-learn/sklearn/feature_extraction/image.py,471,compute the dimensions of the patches array,not
scikit-learn/sklearn/feature_extraction/image.py,479,divide by the amount of overlap,not
scikit-learn/sklearn/feature_extraction/image.py,480,"XXX: is this the most efficient way? memory-wise yes, cpu wise?",SATD
scikit-learn/sklearn/feature_extraction/image.py,571,compute the dimensions of the patches array,not
scikit-learn/sklearn/feature_extraction/image.py,578,extract the patches,not
scikit-learn/sklearn/feature_extraction/_stop_words.py,1,"This list of English stop words is taken from the ""Glasgow Information",not
scikit-learn/sklearn/feature_extraction/_stop_words.py,2,"Retrieval Group"". The original list can be found at",not
scikit-learn/sklearn/feature_extraction/_stop_words.py,3,http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words,not
scikit-learn/sklearn/feature_extraction/_hash.py,1,Author: Lars Buitinck,not
scikit-learn/sklearn/feature_extraction/_hash.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/feature_extraction/_hash.py,104,"strangely, np.int16 instances are not instances of Integral,",not
scikit-learn/sklearn/feature_extraction/_hash.py,105,while np.int64 instances are...,not
scikit-learn/sklearn/feature_extraction/_hash.py,131,repeat input validation for grid search (which calls set_params),not
scikit-learn/sklearn/feature_extraction/_hash.py,168,also sorts the indices,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,1,Authors: Lars Buitinck,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,2,Dan Blanchard <dblanchard@ets.org>,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,19,single sample,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,136,Sanity check: Python's array has no way of explicitly requesting the,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,137,"signed 32-bit integers that scipy.sparse needs, so we use the next",not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,138,"best thing: typecode ""i"" (int). However, if that gives larger or",not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,139,"smaller integers than 32-bit ones, np.frombuffer screws up.",not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,153,Process everything as sparse regardless of setting,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,158,"XXX we could change values to an array.array as well, but it",SATD
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,159,would require (heuristic) conversion of dtype to typecode...,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,162,collect all the possible feature names and build sparse matrix at,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,163,same time,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,190,Sort everything if asked,not
scikit-learn/sklearn/feature_extraction/_dict_vectorizer.py,253,COO matrix is not subscriptable,not
scikit-learn/sklearn/feature_extraction/text.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/feature_extraction/text.py,2,Authors: Olivier Grisel <olivier.grisel@ensta.org>,not
scikit-learn/sklearn/feature_extraction/text.py,3,Mathieu Blondel <mathieu@mblondel.org>,not
scikit-learn/sklearn/feature_extraction/text.py,4,Lars Buitinck,not
scikit-learn/sklearn/feature_extraction/text.py,5,Robert Layton <robertlayton@gmail.com>,not
scikit-learn/sklearn/feature_extraction/text.py,6,Jochen Wersdörfer <jochen@wersdoerfer.de>,not
scikit-learn/sklearn/feature_extraction/text.py,7,Roman Sinayev <roman.sinayev@gmail.com>,not
scikit-learn/sklearn/feature_extraction/text.py,8,,not
scikit-learn/sklearn/feature_extraction/text.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/feature_extraction/text.py,134,"If `s` is ASCII-compatible, then it does not contain any accented",not
scikit-learn/sklearn/feature_extraction/text.py,135,characters and we can avoid an expensive list comprehension,not
scikit-learn/sklearn/feature_extraction/text.py,184,assume it's a collection,not
scikit-learn/sklearn/feature_extraction/text.py,226,handle stop words,not
scikit-learn/sklearn/feature_extraction/text.py,230,handle token n-grams,not
scikit-learn/sklearn/feature_extraction/text.py,235,no need to do any slicing for unigrams,not
scikit-learn/sklearn/feature_extraction/text.py,236,just iterate through the original tokens,not
scikit-learn/sklearn/feature_extraction/text.py,244,bind method outside of loop to reduce overhead,not
scikit-learn/sklearn/feature_extraction/text.py,257,normalize white spaces,not
scikit-learn/sklearn/feature_extraction/text.py,263,no need to do any slicing for unigrams,not
scikit-learn/sklearn/feature_extraction/text.py,264,iterate through the string,not
scikit-learn/sklearn/feature_extraction/text.py,270,bind method outside of loop to reduce overhead,not
scikit-learn/sklearn/feature_extraction/text.py,284,normalize white spaces,not
scikit-learn/sklearn/feature_extraction/text.py,290,bind method outside of loop to reduce overhead,not
scikit-learn/sklearn/feature_extraction/text.py,302,count a short word (w_len < n) only once,not
scikit-learn/sklearn/feature_extraction/text.py,317,accent stripping,not
scikit-learn/sklearn/feature_extraction/text.py,369,Stop words are were previously validated,not
scikit-learn/sklearn/feature_extraction/text.py,372,"NB: stop_words is validated, unlike self.stop_words",not
scikit-learn/sklearn/feature_extraction/text.py,389,Failed to check stop words consistency (e.g. because a custom,not
scikit-learn/sklearn/feature_extraction/text.py,390,preprocessor or tokenizer was used),not
scikit-learn/sklearn/feature_extraction/text.py,727,triggers a parameter validation,not
scikit-learn/sklearn/feature_extraction/text.py,1064,Calculate a mask based on document frequencies,not
scikit-learn/sklearn/feature_extraction/text.py,1078,maps old indices to new,not
scikit-learn/sklearn/feature_extraction/text.py,1098,Add a new value when a new vocabulary item is seen,not
scikit-learn/sklearn/feature_extraction/text.py,1118,Ignore out-of-vocabulary items for fixed_vocab=True,not
scikit-learn/sklearn/feature_extraction/text.py,1126,disable defaultdict behaviour,not
scikit-learn/sklearn/feature_extraction/text.py,1132,= 2**31 - 1,not
scikit-learn/sklearn/feature_extraction/text.py,1184,We intentionally don't call the transform method to make,not
scikit-learn/sklearn/feature_extraction/text.py,1185,fit_transform overridable without unwanted side effects in,not
scikit-learn/sklearn/feature_extraction/text.py,1186,TfidfVectorizer.,not
scikit-learn/sklearn/feature_extraction/text.py,1248,use the same matrix-building strategy as fit_transform,not
scikit-learn/sklearn/feature_extraction/text.py,1270,We need CSR format for fast row manipulations.,not
scikit-learn/sklearn/feature_extraction/text.py,1273,"We need to convert X to a matrix, so that the indexing",not
scikit-learn/sklearn/feature_extraction/text.py,1274,returns 2D objects,not
scikit-learn/sklearn/feature_extraction/text.py,1441,perform idf smoothing if required,not
scikit-learn/sklearn/feature_extraction/text.py,1445,log+1 instead of log makes sure terms with zero idf don't get,not
scikit-learn/sklearn/feature_extraction/text.py,1446,suppressed entirely.,not
scikit-learn/sklearn/feature_extraction/text.py,1482,"idf_ being a property, the automatic attributes detection",not
scikit-learn/sklearn/feature_extraction/text.py,1483,does not work as usual and we need to specify the attribute,not
scikit-learn/sklearn/feature_extraction/text.py,1484,name:,not
scikit-learn/sklearn/feature_extraction/text.py,1493,*= doesn't work,not
scikit-learn/sklearn/feature_extraction/text.py,1503,"if _idf_diag is not set, this will raise an attribute error,",not
scikit-learn/sklearn/feature_extraction/text.py,1504,"which means hasattr(self, ""idf_"") is False",not
scikit-learn/sklearn/feature_extraction/text.py,1744,Broadcast the TF-IDF parameters to the underlying transformer instance,not
scikit-learn/sklearn/feature_extraction/text.py,1745,for easy grid search and repr,not
scikit-learn/sklearn/feature_extraction/text.py,1842,X is already a transformed view of raw_documents so,not
scikit-learn/sklearn/feature_extraction/text.py,1843,we set copy to False,not
scikit-learn/sklearn/feature_extraction/text.py,1873,FIXME Remove copy parameter support in 0.24,SATD
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,80,check some classical latin accentuated symbols,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,89,check some arabic,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,90,alef with a hamza below: إ,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,91,simple alef: ا,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,94,mix letters accentuated and not,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,99,strings that are already decomposed,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,100,o with diaresis,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,104,combining marks by themselves,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,109,Multiple combining marks on one character,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,116,check some classical latin accentuated symbols,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,125,check some arabic,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,126,halef with a hamza below,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,127,halef has no direct ascii match,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,130,mix letters accentuated and not,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,156,with custom preprocessor,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,164,with custom tokenizer,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,187,"decode_error default to strict, so this should fail",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,188,"First, encode (as bytes) a unicode string.",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,192,"Then let the Analyzer try to decode it as ascii. It should fail,",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,193,because we have given it an incorrect encoding.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,269,Try a few of the supported types.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,334,fit on stopwords only,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,353,check normalization,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,356,this is robust to features with only zeros,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,373,check normalization,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,376,the lack of smoothing make IDF fragile in the presence of feature with,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,377,only zeros,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,400,raw documents as an iterator,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,405,test without vocabulary,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,412,build a vectorizer v1 with the same vocabulary as the one fitted by v1,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,415,compare that the two vectorizer give the same output on the test sample,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,426,stop word from the fixed list,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,429,stop word found automatically by the vectorizer DF thresholding,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,430,words that are high frequent across the complete corpus are likely,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,431,to be not informative (either real stop words of extraction,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,432,artifacts),not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,435,not present in the sample,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,441,test tf-idf,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,447,test tf-idf with new data,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,451,test tf alone,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,456,test idf transform with unlearned idf vector,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,461,test idf transform with incompatible n_features,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,470,L1-normalized term frequencies sum to one,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,473,test the direct tfidf vectorizer,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,474,(equivalent to term count vectorizer + tfidf transformer),not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,483,test the direct tfidf vectorizer with new data,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,487,test transform on unfitted vectorizer with empty vocabulary,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,492,ascii preprocessor?,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,501,error on bad strip_accents param,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,506,error with bad analyzer type,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,525,FIXME Remove copy parameter support in 0.24,SATD
scikit-learn/sklearn/feature_extraction/tests/test_text.py,545,By default the hashed values receive a random sign and l2 normalization,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,546,makes the feature values bounded,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,552,Check that the rows are normalized,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,556,Check vectorization with some non-default parameters,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,562,ngrams generate more non zeros,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,567,makes the feature values bounded,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,571,Check that the rows are normalized,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,579,test for Value error on unfitted/empty vocabulary,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,584,test for vocabulary learned from data,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,598,test for custom vocabulary,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,618,test bounded number of extracted features,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,626,Regression test: max_features didn't work correctly in 0.14.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,640,"The most common feature is ""the"", with frequency 7.",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,645,The most common feature should be the same,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,659,0.5 * 3 documents -> max_doc_count == 1.5,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,661,{ae} ignored,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,662,{bcdt} remain,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,668,{ae} ignored,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,669,{bcdt} remain,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,684,{bcdt} ignored,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,685,{ae} remain,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,689,0.8 * 3 documents -> min_doc_count == 2.4,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,691,{bcdet} ignored,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,692,{a} remains,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,698,by default multiple occurrences are counted as longs,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,706,"using boolean features, we can fetch the binary occurrence info",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,707,instead.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,713,check the ability to change the dtype,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,722,by default multiple occurrences are counted as longs,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,730,"using boolean features, we can fetch the binary occurrence info",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,731,instead.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,738,check the ability to change the dtype,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,747,raw documents,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,758,Test that inverse_transform also works with numpy arrays,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,766,raw documents,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,769,"label junk food as -1, the others as +1",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,772,split the dataset for model development and final evaluation,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,784,find the best parameters for both the feature extraction and the,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,785,classifier,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,788,Check that the best model found by grid search is 100% correct on the,SATD
scikit-learn/sklearn/feature_extraction/tests/test_text.py,789,held out evaluation set.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,793,on this toy dataset bigram representation which is used in the last of,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,794,the grid_search is considered the best estimator since they all converge,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,795,to 100% accuracy models,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,802,raw documents,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,805,"label junk food as -1, the others as +1",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,808,split the dataset for model development and final evaluation,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,821,find the best parameters for both the feature extraction and the,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,822,classifier,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,825,Check that the best model found by grid search is 100% correct on the,SATD
scikit-learn/sklearn/feature_extraction/tests/test_text.py,826,held out evaluation set.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,830,on this toy dataset bigram representation which is used in the last of,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,831,the grid_search is considered the best estimator since they all converge,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,832,to 100% accuracy models,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,841,raw documents,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,844,"label junk food as -1, the others as +1",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,856,tests that the count vectorizer works with cyrillic.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,871,No collisions on such a small dataset,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,874,"When norm is None and not alternate_sign, the tokens are counted up to",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,875,collisions,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,880,non regression smoke test for inheritance issues,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,938,ensure that vocabulary of type set is coerced to a list to,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,939,preserve iteration ordering after deserialization,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,969,Ensure that deleting the stop_words_ attribute doesn't affect transform,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1040,np.nan can appear when using pandas to load text fields from a csv file,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1041,with missing values.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1053,"Non-regression test: TfidfVectorizer used to ignore its ""binary"" param.",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1139,vectorizers could be initialized with invalid ngram range,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1140,test for raising error message,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1177,reset stop word validation,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1181,Only one warning per stop list,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1185,Test caching of inconsistency assessment,SATD
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1204,force indices and indptr to int64.,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1232,checks are cached,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1292,check if a custom exception from the analyzer is shown to the user,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1333,setting parameter and checking for corresponding warning messages,not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1350,"For vectorizers, n_features_in_ does not make sense",not
scikit-learn/sklearn/feature_extraction/tests/test_text.py,1357,TODO: Remove in 0.24,SATD
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,26,"mix byte and Unicode strings; note that ""foo"" is a duplicate in row 0",not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,33,iterable,not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,49,check the influence of the seed when computing the hashes,not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,50,import is here to avoid importing on pypy,not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,134,Test delayed input validation in fit (useful for grid search).,not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,142,Assert that no zeros are materialized in the output.,not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,165,check that some of the hashed tokens are added,not
scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py,166,with an opposite sign and cancel out,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,1,Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,2,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,23,Negative elements are the diagonal: the elements of the original,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,24,"image. Positive elements are the values of the gradient, they",not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,25,should all be equal on grad_x and grad_y,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,31,Checking that the function works with graphs containing no edges,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,34,Generating two convex parts with one vertex,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,35,"Thus, edges will be empty in _to_graph",not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,43,Checking that the function works whatever the type of mask is,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,48,Checking dtype of the graph,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,59,scipy deprecation inside face,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,64,Newer versions of scipy have face in misc,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,67,subsample by 4 to reduce run time,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,75,scipy deprecation inside face,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,80,Newer versions of scipy have face in misc,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,84,subsample by 4 to reduce run time,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,100,Newer versions of scipy have face in misc,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,124,make a collection of faces,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,185,Request patches of the same size as image,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,186,Should return just the single patch a.k.a. the image,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,195,this is 3185,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,320,test same patch size for all dimensions,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,331,width and height of the patch should be less than the image,not
scikit-learn/sklearn/feature_extraction/tests/test_image.py,339,TODO: Remove in 0.24,SATD
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,1,Authors: Lars Buitinck,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,2,Dan Blanchard <dblanchard@ets.org>,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,34,CSR matrices can't be compared for equality,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,47,make two feature dicts with two useful features and a bunch of useless,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,48,"ones, in terms of chi2",not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,101,Generate equal dictionaries with different memory layouts,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,108,check that the memory layout does not impact the resulting vocabulary,not
scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py,116,"For vectorizers, n_features_in_ does not make sense and does not exist.",not
scikit-learn/sklearn/compose/_column_transformer.py,6,Author: Andreas Mueller,not
scikit-learn/sklearn/compose/_column_transformer.py,7,Joris Van den Bossche,not
scikit-learn/sklearn/compose/_column_transformer.py,8,License: BSD,not
scikit-learn/sklearn/compose/_column_transformer.py,247,interleave the validated column specifiers,not
scikit-learn/sklearn/compose/_column_transformer.py,252,add transformer tuple for remainder,not
scikit-learn/sklearn/compose/_column_transformer.py,259,replace 'passthrough' with identity transformer and,not
scikit-learn/sklearn/compose/_column_transformer.py,260,skip in case of 'drop',not
scikit-learn/sklearn/compose/_column_transformer.py,278,validate names,not
scikit-learn/sklearn/compose/_column_transformer.py,281,validate estimators,not
scikit-learn/sklearn/compose/_column_transformer.py,318,Make it possible to check for reordered named columns on transform,not
scikit-learn/sklearn/compose/_column_transformer.py,341,Use Bunch object to improve autocomplete,not
scikit-learn/sklearn/compose/_column_transformer.py,379,transformers are fitted; excludes 'drop' cases,not
scikit-learn/sklearn/compose/_column_transformer.py,387,"FunctionTransformer is present in list of transformers,",not
scikit-learn/sklearn/compose/_column_transformer.py,388,"so get next transformer, but save original string",not
scikit-learn/sklearn/compose/_column_transformer.py,397,sanity check that transformers is exhausted,not
scikit-learn/sklearn/compose/_column_transformer.py,492,we use fit_transform to make sure to set sparse_output_ (for which we,not
scikit-learn/sklearn/compose/_column_transformer.py,493,need the transformed data) to have consistent output type in predict,not
scikit-learn/sklearn/compose/_column_transformer.py,519,TODO: this should be `feature_names_in_` when we start having it,SATD
scikit-learn/sklearn/compose/_column_transformer.py,525,set n_features_in_ attribute,not
scikit-learn/sklearn/compose/_column_transformer.py,535,All transformers are None,not
scikit-learn/sklearn/compose/_column_transformer.py,540,determine if concatenated output will be sparse or not,not
scikit-learn/sklearn/compose/_column_transformer.py,587,No column reordering allowed for named cols combined with remainder,not
scikit-learn/sklearn/compose/_column_transformer.py,588,"TODO: remove this mechanism in 0.24, once we enforce strict column",SATD
scikit-learn/sklearn/compose/_column_transformer.py,589,name order and count. See #14237 for details.,not
scikit-learn/sklearn/compose/_column_transformer.py,602,TODO: also call _check_n_features(reset=False) in 0.24,SATD
scikit-learn/sklearn/compose/_column_transformer.py,608,All transformers are None,not
scikit-learn/sklearn/compose/_column_transformer.py,625,since all columns should be numeric before stacking them,not
scikit-learn/sklearn/compose/_column_transformer.py,626,"in a sparse matrix, `check_array` is used for the",not
scikit-learn/sklearn/compose/_column_transformer.py,627,dtype conversion if necessary.,not
scikit-learn/sklearn/compose/_column_transformer.py,765,transformer_weights keyword is not passed through because the user,not
scikit-learn/sklearn/compose/_column_transformer.py,766,would need to know the automatically generated names of the transformers,not
scikit-learn/sklearn/compose/_column_transformer.py,782,TODO: remove in v0.24,SATD
scikit-learn/sklearn/compose/_target.py,1,Authors: Andreas Mueller <andreas.mueller@columbia.edu>,not
scikit-learn/sklearn/compose/_target.py,2,Guillaume Lemaitre <guillaume.lemaitre@inria.fr>,not
scikit-learn/sklearn/compose/_target.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/compose/_target.py,141,XXX: sample_weight is not currently passed to the,SATD
scikit-learn/sklearn/compose/_target.py,142,"transformer. However, if transformer starts using sample_weight, the",not
scikit-learn/sklearn/compose/_target.py,143,code should be modified accordingly. At the time to consider the,not
scikit-learn/sklearn/compose/_target.py,144,"sample_prop feature, it is also a good use case to be considered.",not
scikit-learn/sklearn/compose/_target.py,181,store the number of dimension of the target to predict an array of,not
scikit-learn/sklearn/compose/_target.py,182,similar shape at predict,not
scikit-learn/sklearn/compose/_target.py,185,"transformers are designed to modify X which is 2d dimensional, we",not
scikit-learn/sklearn/compose/_target.py,186,need to modify y accordingly.,not
scikit-learn/sklearn/compose/_target.py,193,transform y and convert back to 1d array if needed,not
scikit-learn/sklearn/compose/_target.py,195,FIXME: a FunctionTransformer can return a 1D array even when validate,SATD
scikit-learn/sklearn/compose/_target.py,196,"is set to True. Therefore, we need to check the number of dimension",not
scikit-learn/sklearn/compose/_target.py,197,first.,not
scikit-learn/sklearn/compose/_target.py,246,For consistency with other estimators we raise a AttributeError so,not
scikit-learn/sklearn/compose/_target.py,247,that hasattr() returns False the estimator isn't fitted.,not
scikit-learn/sklearn/compose/tests/test_target.py,30,provide a transformer and functions at the same time,not
scikit-learn/sklearn/compose/tests/test_target.py,38,fit with sample_weight with a regressor which does not support it,not
scikit-learn/sklearn/compose/tests/test_target.py,45,func is given but inverse_func is not,not
scikit-learn/sklearn/compose/tests/test_target.py,81,check the transformer output,not
scikit-learn/sklearn/compose/tests/test_target.py,88,check the regressor output,not
scikit-learn/sklearn/compose/tests/test_target.py,99,check the transformer output,not
scikit-learn/sklearn/compose/tests/test_target.py,105,check the regressor output,not
scikit-learn/sklearn/compose/tests/test_target.py,115,All transformer in scikit-learn expect 2D data. FunctionTransformer with,not
scikit-learn/sklearn/compose/tests/test_target.py,116,validate=False lift this constraint without checking that the input is a,not
scikit-learn/sklearn/compose/tests/test_target.py,117,2D vector. We check the consistency of the data shape using a 1D and 2D y,not
scikit-learn/sklearn/compose/tests/test_target.py,118,array.,not
scikit-learn/sklearn/compose/tests/test_target.py,125,consistency forward transform,not
scikit-learn/sklearn/compose/tests/test_target.py,129,consistency inverse transform,not
scikit-learn/sklearn/compose/tests/test_target.py,132,consistency of the regressor,not
scikit-learn/sklearn/compose/tests/test_target.py,146,Check consistency with transformer accepting only 2D array and a 1D/2D y,not
scikit-learn/sklearn/compose/tests/test_target.py,147,array.,not
scikit-learn/sklearn/compose/tests/test_target.py,153,consistency forward transform,not
scikit-learn/sklearn/compose/tests/test_target.py,154,create a 2D array and squeeze results,not
scikit-learn/sklearn/compose/tests/test_target.py,160,consistency inverse transform,not
scikit-learn/sklearn/compose/tests/test_target.py,163,consistency of the regressor,not
scikit-learn/sklearn/compose/tests/test_target.py,166,create a 2D array and squeeze results,not
scikit-learn/sklearn/compose/tests/test_target.py,176,Check consistency with transformer accepting only 2D array and a 2D y,not
scikit-learn/sklearn/compose/tests/test_target.py,177,array.,not
scikit-learn/sklearn/compose/tests/test_target.py,185,consistency forward transform,not
scikit-learn/sklearn/compose/tests/test_target.py,189,consistency inverse transform,not
scikit-learn/sklearn/compose/tests/test_target.py,192,consistency of the regressor,not
scikit-learn/sklearn/compose/tests/test_target.py,218,force that the function only return a 1D array,not
scikit-learn/sklearn/compose/tests/test_target.py,258,check that the target ``y`` passed to the transformer will always be a,not
scikit-learn/sklearn/compose/tests/test_target.py,259,"numpy array. Similarly, if ``X`` is passed as a list, we check that the",not
scikit-learn/sklearn/compose/tests/test_target.py,260,predictor receive as it is.,not
scikit-learn/sklearn/compose/tests/test_target.py,291,regression test for gh-issue #11618,not
scikit-learn/sklearn/compose/tests/test_target.py,292,check that we only call a single time fit for the transformer,not
scikit-learn/sklearn/compose/tests/test_target.py,303,"on the test below we force this to false, we make sure this is",not
scikit-learn/sklearn/compose/tests/test_target.py,304,actually passed to the regressor,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,33,1D Series -> 2D DataFrame,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,36,1D array -> 2D array,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,85,single column 1D / 2D,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,88,list-like,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,91,slice,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,94,boolean mask,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,104,callable that returns any of the allowed specifiers,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,116,test with transformer_weights,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,144,String keys: label based,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,146,scalar,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,148,list,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,151,slice,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,154,int keys: positional,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,156,scalar,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,158,list,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,162,slice,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,166,boolean mask,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,177,callable that returns any of the allowed specifiers,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,197,test with transformer_weights,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,209,test multiple columns,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,224,ensure pandas object is passes through,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,243,integer column spec + integer column names -> still use positional,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,260,test case that ensures that the column transformer does also work when,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,261,a given transformer doesn't have any columns to work on,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,289,including remainder,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,297,including remainder,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,304,no distinction between 1D and 2D,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,381,"this shouldn't fail, since boolean can be coerced into a numeric",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,382,See: https://github.com/scikit-learn/scikit-learn/issues/11912,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,395,this fails since strings `a` and `b` cannot be,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,396,coerced into a numeric.,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,402,above data has sparsity of 4 / 8 = 0.5,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,404,apply threshold even if all sparse,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,412,mixed -> sparsity of (4 + 2) / 8 = 0.75,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,431,if nothing is sparse -> no sparse,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,459,"if one transformer is dropped, test that name is still correct",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,464,"because fit is also doing transform, this raises already on fit",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,475,"if one transformer is dropped, test that name is still correct",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,479,"because fit is also doing transform, this raises already on fit",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,488,general invalid,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,494,invalid for arrays,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,500,transformed n_features does not match fitted n_features,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,508,"Should accept added columns, for now",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,563,invalid keyword parameters should raise an error message,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,633,check it are fitted transformers,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,654,raise correct error when not fitted,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,657,raise correct error when no feature names are available,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,663,working example,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,671,drop transformer,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,677,passthrough transformer,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,709,passthough transformer with a dataframe,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,757,one 'drop' -> ignore,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,767,all 'drop' -> return shape 0 array,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,775,'passthrough',not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,785,None itself / other string is not valid,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,802,default drop,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,811,specify passthrough,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,820,column order is not preserved (passed through added to end),not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,830,passthrough when all actual transformers are skipped,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,840,error on invalid arg,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,851,check default for make_column_transformer,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,859,test different ways that columns are specified with passthrough,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,878,test different ways that columns are specified with passthrough,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,905,second and third columns are doubled when remainder = DoubleTrans,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,938,columns are doubled when remainder = DoubleTrans,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,963,SparseMatrixTrans creates 3 features for each column. There is,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,964,"one column in ``transformers``, thus:",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,987,"SparseMatrixTrans creates 3 features for each column, thus:",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1106,assert that function gets the full array,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1123,assert that function gets the full dataframe,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1182,No error for added columns if ordering is identical,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1186,"No error should be raised, for now",not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1188,No 'columns' AttributeError when transform input is a numpy array,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1247,Regression test for #14510,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1248,Boolean array-like does not behave as boolean array with NumPy < 1.12,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1249,and sparse matrices as well,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1260,make sure n_features_in is what is passed as input to the column,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1261,transformer.,not
scikit-learn/sklearn/compose/tests/test_column_transformer.py,1303,Functional test for column transformer + column selector,not
scikit-learn/sklearn/gaussian_process/_gpc.py,3,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/_gpc.py,4,,not
scikit-learn/sklearn/gaussian_process/_gpc.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/_gpc.py,25,Values required for approximating the logistic sigmoid by,not
scikit-learn/sklearn/gaussian_process/_gpc.py,26,error functions. coefs are obtained via:,not
scikit-learn/sklearn/gaussian_process/_gpc.py,27,"x = np.array([0, 0.6, 2, 3.5, 4.5, np.inf])",not
scikit-learn/sklearn/gaussian_process/_gpc.py,28,b = logistic(x),not
scikit-learn/sklearn/gaussian_process/_gpc.py,29,"A = (erf(np.dot(x, self.lambdas)) + 1) / 2",not
scikit-learn/sklearn/gaussian_process/_gpc.py,30,"coefs = lstsq(A, b)[0]",not
scikit-learn/sklearn/gaussian_process/_gpc.py,175,Use an RBF kernel as default,not
scikit-learn/sklearn/gaussian_process/_gpc.py,185,Encode class labels and check that it is a binary classification,not
scikit-learn/sklearn/gaussian_process/_gpc.py,186,problem,not
scikit-learn/sklearn/gaussian_process/_gpc.py,200,Choose hyperparameters based on maximizing the log-marginal,not
scikit-learn/sklearn/gaussian_process/_gpc.py,201,likelihood (potentially starting from several initial values),not
scikit-learn/sklearn/gaussian_process/_gpc.py,211,First optimize starting from theta specified in kernel,not
scikit-learn/sklearn/gaussian_process/_gpc.py,216,Additional runs are performed from log-uniform chosen initial,not
scikit-learn/sklearn/gaussian_process/_gpc.py,217,theta,not
scikit-learn/sklearn/gaussian_process/_gpc.py,230,Select result from run with minimal (negative) log-marginal,not
scikit-learn/sklearn/gaussian_process/_gpc.py,231,likelihood,not
scikit-learn/sklearn/gaussian_process/_gpc.py,239,Precompute quantities required for predictions which are independent,not
scikit-learn/sklearn/gaussian_process/_gpc.py,240,of actual query points,not
scikit-learn/sklearn/gaussian_process/_gpc.py,263,"As discussed on Section 3.4.2 of GPML, for making hard binary",not
scikit-learn/sklearn/gaussian_process/_gpc.py,264,"decisions, it is enough to compute the MAP of the posterior and",not
scikit-learn/sklearn/gaussian_process/_gpc.py,265,pass it through the link function,not
scikit-learn/sklearn/gaussian_process/_gpc.py,266,K_star =k(x_star),not
scikit-learn/sklearn/gaussian_process/_gpc.py,267,"Algorithm 3.2,Line 4",not
scikit-learn/sklearn/gaussian_process/_gpc.py,288,Based on Algorithm 3.2 of GPML,not
scikit-learn/sklearn/gaussian_process/_gpc.py,289,K_star =k(x_star),not
scikit-learn/sklearn/gaussian_process/_gpc.py,290,Line 4,not
scikit-learn/sklearn/gaussian_process/_gpc.py,291,Line 5,not
scikit-learn/sklearn/gaussian_process/_gpc.py,292,Line 6 (compute np.diag(v.T.dot(v)) via einsum),not
scikit-learn/sklearn/gaussian_process/_gpc.py,295,Line 7:,not
scikit-learn/sklearn/gaussian_process/_gpc.py,296,"Approximate \int log(z) * N(z | f_star, var_f_star)",not
scikit-learn/sklearn/gaussian_process/_gpc.py,297,"Approximation is due to Williams & Barber, ""Bayesian Classification",not
scikit-learn/sklearn/gaussian_process/_gpc.py,298,"with Gaussian Processes"", Appendix A: Approximate the logistic",not
scikit-learn/sklearn/gaussian_process/_gpc.py,299,sigmoid by a linear combination of 5 error functions.,not
scikit-learn/sklearn/gaussian_process/_gpc.py,300,For information on how this integral can be computed see,not
scikit-learn/sklearn/gaussian_process/_gpc.py,301,blitiri.blogspot.de/2012/11/gaussian-integral-of-error-function.html,not
scikit-learn/sklearn/gaussian_process/_gpc.py,359,Compute log-marginal-likelihood Z and also store some temporaries,not
scikit-learn/sklearn/gaussian_process/_gpc.py,360,which can be reused for computing Z's gradient,not
scikit-learn/sklearn/gaussian_process/_gpc.py,367,Compute gradient based on Algorithm 5.1 of GPML,not
scikit-learn/sklearn/gaussian_process/_gpc.py,369,XXX: Get rid of the np.diag() in the next line,SATD
scikit-learn/sklearn/gaussian_process/_gpc.py,370,Line 7,not
scikit-learn/sklearn/gaussian_process/_gpc.py,371,Line 8,not
scikit-learn/sklearn/gaussian_process/_gpc.py,372,Line 9: (use einsum to compute np.diag(C.T.dot(C)))),not
scikit-learn/sklearn/gaussian_process/_gpc.py,374,third derivative,not
scikit-learn/sklearn/gaussian_process/_gpc.py,377,Line 11,not
scikit-learn/sklearn/gaussian_process/_gpc.py,378,Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C))),not
scikit-learn/sklearn/gaussian_process/_gpc.py,381,Line 13,not
scikit-learn/sklearn/gaussian_process/_gpc.py,382,Line 14,not
scikit-learn/sklearn/gaussian_process/_gpc.py,384,Line 15,not
scikit-learn/sklearn/gaussian_process/_gpc.py,395,Based on Algorithm 3.1 of GPML,not
scikit-learn/sklearn/gaussian_process/_gpc.py,397,"If warm_start are enabled, we reuse the last solution for the",not
scikit-learn/sklearn/gaussian_process/_gpc.py,398,"posterior mode as initialization; otherwise, we initialize with 0",not
scikit-learn/sklearn/gaussian_process/_gpc.py,405,Use Newton's iteration method to find mode of Laplace approximation,not
scikit-learn/sklearn/gaussian_process/_gpc.py,408,Line 4,not
scikit-learn/sklearn/gaussian_process/_gpc.py,411,Line 5,not
scikit-learn/sklearn/gaussian_process/_gpc.py,416,Line 6,not
scikit-learn/sklearn/gaussian_process/_gpc.py,418,Line 7,not
scikit-learn/sklearn/gaussian_process/_gpc.py,420,Line 8,not
scikit-learn/sklearn/gaussian_process/_gpc.py,423,Line 10: Compute log marginal likelihood in loop and use as,not
scikit-learn/sklearn/gaussian_process/_gpc.py,424,convergence criterion,not
scikit-learn/sklearn/gaussian_process/_gpc.py,428,Check if we have converged (log marginal likelihood does,not
scikit-learn/sklearn/gaussian_process/_gpc.py,429,not decrease),not
scikit-learn/sklearn/gaussian_process/_gpc.py,430,XXX: more complex convergence criterion,SATD
scikit-learn/sklearn/gaussian_process/_gpc.py,435,Remember solution for later warm-starts,not
scikit-learn/sklearn/gaussian_process/_gpc.py,784,use same theta for all sub-kernels,not
scikit-learn/sklearn/gaussian_process/_gpc.py,790,theta for compound kernel,not
scikit-learn/sklearn/gaussian_process/_gpr.py,3,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/_gpr.py,4,Modified by: Pete Green <p.l.green@liverpool.ac.uk>,not
scikit-learn/sklearn/gaussian_process/_gpr.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/_gpr.py,180,Use an RBF kernel as default,not
scikit-learn/sklearn/gaussian_process/_gpr.py,195,Normalize target value,not
scikit-learn/sklearn/gaussian_process/_gpr.py,200,Remove mean and make unit variance,not
scikit-learn/sklearn/gaussian_process/_gpr.py,220,Choose hyperparameters based on maximizing the log-marginal,not
scikit-learn/sklearn/gaussian_process/_gpr.py,221,likelihood (potentially starting from several initial values),not
scikit-learn/sklearn/gaussian_process/_gpr.py,231,First optimize starting from theta specified in kernel,not
scikit-learn/sklearn/gaussian_process/_gpr.py,236,Additional runs are performed from log-uniform chosen initial,not
scikit-learn/sklearn/gaussian_process/_gpr.py,237,theta,not
scikit-learn/sklearn/gaussian_process/_gpr.py,250,Select result from run with minimal (negative) log-marginal,not
scikit-learn/sklearn/gaussian_process/_gpr.py,251,likelihood,not
scikit-learn/sklearn/gaussian_process/_gpr.py,260,Precompute quantities required for predictions which are independent,not
scikit-learn/sklearn/gaussian_process/_gpr.py,261,of actual query points,not
scikit-learn/sklearn/gaussian_process/_gpr.py,265,Line 2,not
scikit-learn/sklearn/gaussian_process/_gpr.py,266,"self.L_ changed, self._K_inv needs to be recomputed",not
scikit-learn/sklearn/gaussian_process/_gpr.py,275,Line 3,not
scikit-learn/sklearn/gaussian_process/_gpr.py,322,Unfitted;predict based on GP prior,not
scikit-learn/sklearn/gaussian_process/_gpr.py,337,Predict based on GP posterior,not
scikit-learn/sklearn/gaussian_process/_gpr.py,339,Line 4 (y_mean = f_star),not
scikit-learn/sklearn/gaussian_process/_gpr.py,341,undo normalisation,not
scikit-learn/sklearn/gaussian_process/_gpr.py,345,Line 5,not
scikit-learn/sklearn/gaussian_process/_gpr.py,346,Line 6,not
scikit-learn/sklearn/gaussian_process/_gpr.py,348,undo normalisation,not
scikit-learn/sklearn/gaussian_process/_gpr.py,353,cache result of K_inv computation,not
scikit-learn/sklearn/gaussian_process/_gpr.py,355,compute inverse K_inv of K based on its Cholesky,not
scikit-learn/sklearn/gaussian_process/_gpr.py,356,decomposition L and its inverse L_inv,not
scikit-learn/sklearn/gaussian_process/_gpr.py,361,Compute variance of predictive distribution,not
scikit-learn/sklearn/gaussian_process/_gpr.py,366,Check if any of the variances is negative because of,not
scikit-learn/sklearn/gaussian_process/_gpr.py,367,numerical issues. If yes: set the variance to 0.,not
scikit-learn/sklearn/gaussian_process/_gpr.py,374,undo normalisation,not
scikit-learn/sklearn/gaussian_process/_gpr.py,466,Line 2,not
scikit-learn/sklearn/gaussian_process/_gpr.py,471,Support multi-dimensional output of self.y_train_,not
scikit-learn/sklearn/gaussian_process/_gpr.py,476,Line 3,not
scikit-learn/sklearn/gaussian_process/_gpr.py,478,Compute log-likelihood (compare line 7),not
scikit-learn/sklearn/gaussian_process/_gpr.py,482,sum over dimensions,not
scikit-learn/sklearn/gaussian_process/_gpr.py,484,compare Equation 5.9 from GPML,not
scikit-learn/sklearn/gaussian_process/_gpr.py,485,k: output-dimension,not
scikit-learn/sklearn/gaussian_process/_gpr.py,487,"Compute ""0.5 * trace(tmp.dot(K_gradient))"" without",not
scikit-learn/sklearn/gaussian_process/_gpr.py,488,constructing the full matrix tmp.dot(K_gradient) since only,not
scikit-learn/sklearn/gaussian_process/_gpr.py,489,its diagonal is required,not
scikit-learn/sklearn/gaussian_process/kernels.py,16,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/kernels.py,17,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/kernels.py,19,Note: this module is strongly inspired by the kernel module of the george,not
scikit-learn/sklearn/gaussian_process/kernels.py,20,package.,not
scikit-learn/sklearn/gaussian_process/kernels.py,84,A raw namedtuple is very memory efficient as it packs the attributes,not
scikit-learn/sklearn/gaussian_process/kernels.py,85,in a struct to get rid of the __dict__ of attributes in particular it,not
scikit-learn/sklearn/gaussian_process/kernels.py,86,does not copy the string for the keys on each instance.,not
scikit-learn/sklearn/gaussian_process/kernels.py,87,By deriving a namedtuple class just to introduce the __init__ method we,not
scikit-learn/sklearn/gaussian_process/kernels.py,88,would also reintroduce the __dict__ on the instance. By telling the,not
scikit-learn/sklearn/gaussian_process/kernels.py,89,Python interpreter that this subclass uses static __slots__ instead of,not
scikit-learn/sklearn/gaussian_process/kernels.py,90,dynamic attributes. Furthermore we don't need any additional slot in the,not
scikit-learn/sklearn/gaussian_process/kernels.py,91,subclass so we set __slots__ to the empty tuple.,not
scikit-learn/sklearn/gaussian_process/kernels.py,97,vector-valued parameter,not
scikit-learn/sklearn/gaussian_process/kernels.py,110,This is mainly a testing utility to check that two hyperparameters,not
scikit-learn/sklearn/gaussian_process/kernels.py,111,are equal.,not
scikit-learn/sklearn/gaussian_process/kernels.py,142,introspect the constructor arguments to find the model parameters,not
scikit-learn/sklearn/gaussian_process/kernels.py,143,to represent,not
scikit-learn/sklearn/gaussian_process/kernels.py,186,Simple optimisation to gain speed (inspect is slow),not
scikit-learn/sklearn/gaussian_process/kernels.py,192,nested objects case,not
scikit-learn/sklearn/gaussian_process/kernels.py,202,simple objects case,not
scikit-learn/sklearn/gaussian_process/kernels.py,274,vector-valued parameter,not
scikit-learn/sklearn/gaussian_process/kernels.py,1426,convert from upper-triangular matrix to square matrix,not
scikit-learn/sklearn/gaussian_process/kernels.py,1439,Hyperparameter l kept fixed,not
scikit-learn/sklearn/gaussian_process/kernels.py,1446,We need to recompute the pairwise dimension-wise distances,not
scikit-learn/sklearn/gaussian_process/kernels.py,1459,isotropic,not
scikit-learn/sklearn/gaussian_process/kernels.py,1596,general case; expensive to evaluate,not
scikit-learn/sklearn/gaussian_process/kernels.py,1598,strict zeros result in nan,not
scikit-learn/sklearn/gaussian_process/kernels.py,1605,convert from upper-triangular matrix to square matrix,not
scikit-learn/sklearn/gaussian_process/kernels.py,1611,Hyperparameter l kept fixed,not
scikit-learn/sklearn/gaussian_process/kernels.py,1615,We need to recompute the pairwise dimension-wise distances,not
scikit-learn/sklearn/gaussian_process/kernels.py,1635,approximate gradient numerically,not
scikit-learn/sklearn/gaussian_process/kernels.py,1636,helper function,not
scikit-learn/sklearn/gaussian_process/kernels.py,1783,gradient with respect to length_scale,not
scikit-learn/sklearn/gaussian_process/kernels.py,1788,l is kept fixed,not
scikit-learn/sklearn/gaussian_process/kernels.py,1791,gradient with respect to alpha,not
scikit-learn/sklearn/gaussian_process/kernels.py,1797,alpha is kept fixed,not
scikit-learn/sklearn/gaussian_process/kernels.py,1925,gradient with respect to length_scale,not
scikit-learn/sklearn/gaussian_process/kernels.py,1930,length_scale is kept fixed,not
scikit-learn/sklearn/gaussian_process/kernels.py,1932,gradient with respect to p,not
scikit-learn/sklearn/gaussian_process/kernels.py,1938,p is kept fixed,not
scikit-learn/sklearn/gaussian_process/kernels.py,2089,adapted from scipy/optimize/optimize.py for functions with 2d output,not
scikit-learn/sklearn/gaussian_process/kernels.py,2195,approximate gradient numerically,not
scikit-learn/sklearn/gaussian_process/kernels.py,2196,helper function,not
scikit-learn/sklearn/gaussian_process/kernels.py,2221,We have to fall back to slow way of computing diagonal,not
scikit-learn/sklearn/gaussian_process/__init__.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/gaussian_process/__init__.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/__init__.py,4,Vincent Dubourg <vincent.dubourg@gmail.com>,not
scikit-learn/sklearn/gaussian_process/__init__.py,5,"(mostly translation, see implementation details)",not
scikit-learn/sklearn/gaussian_process/__init__.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,4,Modified by: Pete Green <p.l.green@liverpool.ac.uk>,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,55,Test the interpolating property for different kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,64,Test the interpolating property for different kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,82,Test that hyperparameter-tuning improves log-marginal likelihood.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,90,Test that lml of optimized kernel is stored correctly.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,98,Test that lml of optimized kernel is stored correctly.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,108,Test that we are in local maximum after hyperparameter-optimization.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,121,Test that hyperparameter-optimization remains in bounds,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,135,Compare analytic and numeric gradient of log marginal likelihood.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,150,Test that GP prior has mean 0 and identical variances.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,157,"XXX: quite hacky, works only for current kernels",SATD
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,165,Test that statistics of samples drawn from GP are correct.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,172,More digits accuracy would require many more samples,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,179,Test that kernel parameters are unmodified when optimizer is None.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,190,Test that predicted std.-dev. is consistent with cov's diagonal.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,198,Test that GPR can identify meaningful anisotropic length-scales.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,199,We learn a function which varies in one dimension ten-times slower,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,200,than in the other. The corresponding length-scales should differ by at,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,201,least a factor 5,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,213,Test that an increasing number of random-starts of GP fitting only,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,214,increases the log marginal likelihood of the chosen theta.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,251,Fit non-normalizing GP on normalized y,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,255,Fit normalizing GP on unnormalized y,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,259,"Compare predicted mean, std-devs and covariances",not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,299,Here we utilise a larger variance version of the training data,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,302,Standard GP with normalize_y=True,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,309,'Gold standard' mean predictions from GPy,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,316,'Gold standard' std predictions from GPy,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,323,"Based on numerical experiments, it's reasonable to expect our",not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,324,GP's mean predictions to get within 7% of predictions of those,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,325,made by GPy.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,328,"Based on numerical experiments, it's reasonable to expect our",not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,329,GP's std predictions to get within 15% of predictions of those,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,330,made by GPy.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,335,Test that GPR can deal with multi-dimensional target values,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,338,Test for fixed kernel that first dimension of 2d GP equals the output,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,339,of 1d GP and that second dimension is twice as large,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,358,Standard deviation and covariance do not depend on output,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,366,Test hyperparameter optimization,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,379,Test that GPR can use externally defined optimizers.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,380,Define a dummy optimizer that simply tests 50 random hyperparameters,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,395,Checks that optimizer improved marginal likelihood,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,415,Test GPR can handle two different output-values for the same input.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,438,Test that GPR predictions without fit does not break by default.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,457,Test that self._K_inv is reset after a new fit,not
scikit-learn/sklearn/gaussian_process/tests/test_gpr.py,468,the value of K_inv should be independent of the first fit,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,27,multi-class,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,44,Check binary predict decision has also predicted probability above 0.5.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,51,Check binary predict decision has also predicted probability above 0.5.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,62,Test that hyperparameter-tuning improves log-marginal likelihood.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,70,Test that lml of optimized kernel is stored correctly.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,78,Test that clone_kernel=False has side-effects of kernel.theta.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,88,Test that we are in local maximum after hyperparameter-optimization.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,101,Compare analytic and numeric gradient of log marginal likelihood.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,115,Test that an increasing number of random-starts of GP fitting only,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,116,increases the log marginal likelihood of the chosen theta.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,137,Test that GPC can use externally defined optimizers.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,138,Define a dummy optimizer that simply tests 10 random hyperparameters,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,153,Checks that optimizer improved marginal likelihood,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,160,Test GPC for multi-class classification problems.,not
scikit-learn/sklearn/gaussian_process/tests/test_gpc.py,173,Test that multi-class GPC produces identical results with n_jobs>1.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,3,Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,4,License: BSD 3 clause,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,54,Compare analytic and numeric gradient of kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,75,skip non-basic kernels,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,79,Check that parameter vector theta of kernel is set correctly.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,83,Determine kernel parameters that contribute to theta,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,93,Check that values returned in theta are consistent with,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,94,hyperparameter values (being their logarithms),not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,98,Fixed kernel parameters must be excluded from theta and gradient.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,100,create copy with certain hyperparameter fixed,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,105,Check that theta and K_gradient are identical with the fixed,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,106,dimension left out,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,119,Check that values of theta are modified correctly,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,131,Identity is not satisfied on diagonal,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,134,Auto-correlation and cross-correlation should be consistent.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,142,Test that diag method of kernel returns consistent results.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,149,Adding kernels and multiplying kernels should be commutative.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,150,Check addition,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,154,Check multiplication,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,160,Anisotropic kernel should be consistent with isotropic kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,174,Check getting and setting via theta,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,184,Test stationarity of kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,191,Test whether kernels is for vectors or structured data,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,211,Check that hyperparameters of two kernels are equal,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,221,Test that sklearn's clone works correctly on kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,224,XXX: Should this be fixed?,SATD
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,225,This differs from the sklearn's estimators equality check.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,229,Check that all constructor parameters are equal.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,232,Check that all hyperparameters are equal.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,238,This test is to verify that using set_params does not,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,239,break clone on kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,240,"This used to break because in kernels such as the RBF, non-trivial",not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,241,logic that modified the length scale used to be in the constructor,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,242,See https://github.com/scikit-learn/scikit-learn/issues/6961,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,243,for more details.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,247,RationalQuadratic kernel is isotropic.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,253,XXX unreached code as of v0.22,SATD
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,267,Test consistency of Matern kernel for special values of nu.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,269,the diagonal elements of a matern kernel are 1,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,271,matern kernel for coef0==0.5 is equal to absolute exponential kernel,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,275,matern kernel with coef0==inf is equal to RBF kernel,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,280,"test that special cases of matern kernel (coef0 in [0.5, 1.5, 2.5])",not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,281,result in nearly identical results as the general case for coef0 in,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,282,"[0.5 + tiny, 1.5 + tiny, 2.5 + tiny]",not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,288,test that coef0==large is close to RBF,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,297,Check that GP kernels can also be used as pairwise kernels.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,299,Test auto-kernel,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,301,"For WhiteKernel: k(X) != k(X,X). This is assumed by",not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,302,pairwise_kernels,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,307,Test cross-kernel,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,315,Check that set_params()/get_params() is consistent with kernel.theta.,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,317,Test get_params(),not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,325,anisotropic kernels,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,333,Test set_params(),not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,335,arbitrary value,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,341,anisotropic kernels,not
scikit-learn/sklearn/gaussian_process/tests/test_kernels.py,354,Smoke-test for repr in kernels.,not
scikit-learn/sklearn/externals/conftest.py,1,Do not collect any tests in externals. This is more robust than using,not
scikit-learn/sklearn/externals/conftest.py,2,--ignore because --ignore needs a path and it is not convenient to pass in,not
scikit-learn/sklearn/externals/conftest.py,3,the externals path (very long install-dependent path in site-packages) when,not
scikit-learn/sklearn/externals/conftest.py,4,using --pyargs,not
scikit-learn/sklearn/externals/_pilutil.py,51,Modification of original scipy pilutil.py to make this module importable if,not
scikit-learn/sklearn/externals/_pilutil.py,52,"pillow is not installed. If pillow is not installed, functions will raise",not
scikit-learn/sklearn/externals/_pilutil.py,53,ImportError when called.,not
scikit-learn/sklearn/externals/_pilutil.py,299,"Mode 'P' means there is an indexed ""palette"".  If we leave the mode",not
scikit-learn/sklearn/externals/_pilutil.py,300,"as 'P', then when we do `a = array(im)` below, `a` will be a 2-D",not
scikit-learn/sklearn/externals/_pilutil.py,301,"containing the indices into the palette, and not a 3-D array",not
scikit-learn/sklearn/externals/_pilutil.py,302,containing the RGB or RGBA values.,not
scikit-learn/sklearn/externals/_pilutil.py,311,"Workaround for crash in PIL. When im is 1-bit, the call array(im)",SATD
scikit-learn/sklearn/externals/_pilutil.py,312,"can cause a seg. fault, or generate garbage. See",not
scikit-learn/sklearn/externals/_pilutil.py,313,https://github.com/scipy/scipy/issues/2138 and,not
scikit-learn/sklearn/externals/_pilutil.py,314,https://github.com/python-pillow/Pillow/issues/350.,not
scikit-learn/sklearn/externals/_pilutil.py,315,,not
scikit-learn/sklearn/externals/_pilutil.py,316,This converts im from a 1-bit image to an 8-bit image.,not
scikit-learn/sklearn/externals/_pilutil.py,369,columns show up first,not
scikit-learn/sklearn/externals/_pilutil.py,380,Becomes a mode='P' automagically.,not
scikit-learn/sklearn/externals/_pilutil.py,381,default gray-scale,not
scikit-learn/sklearn/externals/_pilutil.py,386,high input gives threshold for 1,not
scikit-learn/sklearn/externals/_pilutil.py,402,if here then 3-d array with a 3 or a 4 in the shape length.,not
scikit-learn/sklearn/externals/_pilutil.py,403,Check for 3 in datacube shape --- 'RGB' or 'YCbCr',not
scikit-learn/sklearn/externals/_pilutil.py,446,Here we know data and mode is correct,not
scikit-learn/sklearn/externals/_lobpcg.py,41,Used only when verbosity level > 10.,not
scikit-learn/sklearn/externals/_lobpcg.py,69,Assume 1!,not
scikit-learn/sklearn/externals/_lobpcg.py,105,Shared data!!!,not
scikit-learn/sklearn/externals/_lobpcg.py,110,VBV is a Cholesky factor from now on...,not
scikit-learn/sklearn/externals/_lobpcg.py,114,"blockVectorV = (cho_solve((VBV.T, True), blockVectorV.T)).T",not
scikit-learn/sklearn/externals/_lobpcg.py,117,"blockVectorBV = (cho_solve((VBV.T, True), blockVectorBV.T)).T",not
scikit-learn/sklearn/externals/_lobpcg.py,121,raise ValueError('Cholesky has failed'),not
scikit-learn/sklearn/externals/_lobpcg.py,312,Block size.,not
scikit-learn/sklearn/externals/_lobpcg.py,344,warn('The problem size is small compared to the block size.' \,not
scikit-learn/sklearn/externals/_lobpcg.py,345,' Using dense eigensolver instead of LOBPCG.'),not
scikit-learn/sklearn/externals/_lobpcg.py,353,Define the closed range of indices of eigenvalues to return.,not
scikit-learn/sklearn/externals/_lobpcg.py,365,Reverse order to be compatible with eigs() in 'LM' mode.,not
scikit-learn/sklearn/externals/_lobpcg.py,374,Apply constraints to X.,not
scikit-learn/sklearn/externals/_lobpcg.py,382,gramYBY is a dense array.,not
scikit-learn/sklearn/externals/_lobpcg.py,385,gramYBY is a Cholesky factor from now on...,not
scikit-learn/sklearn/externals/_lobpcg.py,392,,not
scikit-learn/sklearn/externals/_lobpcg.py,393,B-orthonormalize X.,not
scikit-learn/sklearn/externals/_lobpcg.py,396,,not
scikit-learn/sklearn/externals/_lobpcg.py,397,Compute the initial Ritz vectors: solve the eigenproblem.,not
scikit-learn/sklearn/externals/_lobpcg.py,411,,not
scikit-learn/sklearn/externals/_lobpcg.py,412,Active index set.,not
scikit-learn/sklearn/externals/_lobpcg.py,422,,not
scikit-learn/sklearn/externals/_lobpcg.py,423,Main iteration loop.,not
scikit-learn/sklearn/externals/_lobpcg.py,425,set during iteration,not
scikit-learn/sklearn/externals/_lobpcg.py,478,Apply preconditioner T to the active residuals.,not
scikit-learn/sklearn/externals/_lobpcg.py,481,,not
scikit-learn/sklearn/externals/_lobpcg.py,482,Apply constraints to the preconditioned residuals.,not
scikit-learn/sklearn/externals/_lobpcg.py,487,,not
scikit-learn/sklearn/externals/_lobpcg.py,488,B-orthogonalize the preconditioned residuals to X.,not
scikit-learn/sklearn/externals/_lobpcg.py,500,,not
scikit-learn/sklearn/externals/_lobpcg.py,501,B-orthonormalize the preconditioned residuals.,not
scikit-learn/sklearn/externals/_lobpcg.py,515,Function _b_orthonormalize returns None if Cholesky fails,not
scikit-learn/sklearn/externals/_lobpcg.py,523,,not
scikit-learn/sklearn/externals/_lobpcg.py,524,Perform the Rayleigh Ritz Procedure:,not
scikit-learn/sklearn/externals/_lobpcg.py,525,Compute symmetric Gram matrices:,not
scikit-learn/sklearn/externals/_lobpcg.py,537,"Once explicitGramFlag, forever explicitGramFlag.",not
scikit-learn/sklearn/externals/_lobpcg.py,540,Shared memory assingments to simplify the code,not
scikit-learn/sklearn/externals/_lobpcg.py,547,Common submatrices:,not
scikit-learn/sklearn/externals/_lobpcg.py,569,"Note: not documented, but leave it in here for now",not
scikit-learn/sklearn/externals/_lobpcg.py,599,try again after dropping the direction vectors P from RR,not
scikit-learn/sklearn/externals/_lobpcg.py,628,# Normalize eigenvectors!,not
scikit-learn/sklearn/externals/_lobpcg.py,629,"aux = np.sum( eigBlockVector.conj() * eigBlockVector, 0 )",not
scikit-learn/sklearn/externals/_lobpcg.py,630,eigVecNorms = np.sqrt( aux ),not
scikit-learn/sklearn/externals/_lobpcg.py,631,"eigBlockVector = eigBlockVector / eigVecNorms[np.newaxis, :]",not
scikit-learn/sklearn/externals/_lobpcg.py,632,"eigBlockVector, aux = _b_orthonormalize( B, eigBlockVector )",not
scikit-learn/sklearn/externals/_lobpcg.py,637,Compute Ritz vectors.,not
scikit-learn/sklearn/externals/_lobpcg.py,709,Future work: Need to add Postprocessing here:,not
scikit-learn/sklearn/externals/_lobpcg.py,710,"Making sure eigenvectors ""exactly"" satisfy the blockVectorY constrains?",not
scikit-learn/sklearn/externals/_lobpcg.py,711,"Making sure eigenvecotrs are ""exactly"" othonormalized by final ""exact"" RR",not
scikit-learn/sklearn/externals/_lobpcg.py,712,Computing the actual true residuals,not
scikit-learn/sklearn/externals/_arff.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/externals/_arff.py,2,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,3,Federal University of Rio Grande do Sul (UFRGS),not
scikit-learn/sklearn/externals/_arff.py,4,Connectionist Artificial Intelligence Laboratory (LIAC),not
scikit-learn/sklearn/externals/_arff.py,5,Renato de Pontes Pereira - rppereira@inf.ufrgs.br,not
scikit-learn/sklearn/externals/_arff.py,6,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,7,"Copyright (c) 2011 Renato de Pontes Pereira, renato.ppontes at gmail dot com",not
scikit-learn/sklearn/externals/_arff.py,8,,not
scikit-learn/sklearn/externals/_arff.py,9,"Permission is hereby granted, free of charge, to any person obtaining a copy",not
scikit-learn/sklearn/externals/_arff.py,10,"of this software and associated documentation files (the ""Software""), to deal",not
scikit-learn/sklearn/externals/_arff.py,11,"in the Software without restriction, including without limitation the rights",not
scikit-learn/sklearn/externals/_arff.py,12,"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",not
scikit-learn/sklearn/externals/_arff.py,13,"copies of the Software, and to permit persons to whom the Software is",not
scikit-learn/sklearn/externals/_arff.py,14,"furnished to do so, subject to the following conditions:",not
scikit-learn/sklearn/externals/_arff.py,15,,not
scikit-learn/sklearn/externals/_arff.py,16,The above copyright notice and this permission notice shall be included in,not
scikit-learn/sklearn/externals/_arff.py,17,all copies or substantial portions of the Software.,not
scikit-learn/sklearn/externals/_arff.py,18,,not
scikit-learn/sklearn/externals/_arff.py,19,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR",not
scikit-learn/sklearn/externals/_arff.py,20,"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,",not
scikit-learn/sklearn/externals/_arff.py,21,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE,not
scikit-learn/sklearn/externals/_arff.py,22,"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER",not
scikit-learn/sklearn/externals/_arff.py,23,"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",not
scikit-learn/sklearn/externals/_arff.py,24,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE,not
scikit-learn/sklearn/externals/_arff.py,25,SOFTWARE.,not
scikit-learn/sklearn/externals/_arff.py,26,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,157,CONSTANTS ===================================================================,not
scikit-learn/sklearn/externals/_arff.py,189,"a value is surrounded by "" or by ' or contains no quotables",not
scikit-learn/sklearn/externals/_arff.py,197,"This captures (value, error) groups. Because empty values are allowed,",not
scikit-learn/sklearn/externals/_arff.py,198,we cannot just look for empty values to handle syntax errors.,not
scikit-learn/sklearn/externals/_arff.py,199,"We presume the line has had ',' prepended...",not
scikit-learn/sklearn/externals/_arff.py,208,"This captures (key, value) groups and will have an empty key/value",not
scikit-learn/sklearn/externals/_arff.py,209,in case of syntax errors.,not
scikit-learn/sklearn/externals/_arff.py,210,It does not ensure that the line starts with '{' or ends with '}'.,not
scikit-learn/sklearn/externals/_arff.py,271,Fast path for trivial cases (unfortunately we have to handle missing,not
scikit-learn/sklearn/externals/_arff.py,272,values because of the empty string case :(.),not
scikit-learn/sklearn/externals/_arff.py,276,"_RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.",not
scikit-learn/sklearn/externals/_arff.py,285,an ARFF syntax error in sparse data,not
scikit-learn/sklearn/externals/_arff.py,291,an ARFF syntax error,not
scikit-learn/sklearn/externals/_arff.py,298,Constant value representing a dense matrix,not
scikit-learn/sklearn/externals/_arff.py,299,Constant value representing a sparse matrix in coordinate format,not
scikit-learn/sklearn/externals/_arff.py,300,Constant value representing a sparse matrix in list of,not
scikit-learn/sklearn/externals/_arff.py,301,dictionaries format,not
scikit-learn/sklearn/externals/_arff.py,302,Generator of dictionaries,not
scikit-learn/sklearn/externals/_arff.py,303,Generator of dictionaries,not
scikit-learn/sklearn/externals/_arff.py,306,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,308,COMPATIBILITY WITH PYTHON 3 =================================================,not
scikit-learn/sklearn/externals/_arff.py,315,COMPABILITY WITH PYTHON 2 ===================================================,not
scikit-learn/sklearn/externals/_arff.py,316,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,321,EXCEPTIONS ==================================================================,not
scikit-learn/sklearn/externals/_arff.py,413,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,415,INTERNAL ====================================================================,not
scikit-learn/sklearn/externals/_arff.py,446,Sparse decode,not
scikit-learn/sklearn/externals/_arff.py,447,See issue #52: nominals should take their first value when,not
scikit-learn/sklearn/externals/_arff.py,448,"unspecified in a sparse matrix. Naturally, this is consistent",not
scikit-learn/sklearn/externals/_arff.py,449,with EncodedNominalConversor.,not
scikit-learn/sklearn/externals/_arff.py,466,"XXX: int 0 is used for implicit values, not '0'",SATD
scikit-learn/sklearn/externals/_arff.py,545,conversor out of range,not
scikit-learn/sklearn/externals/_arff.py,563,Check if the rows are sorted,not
scikit-learn/sklearn/externals/_arff.py,570,Add empty rows if necessary,not
scikit-learn/sklearn/externals/_arff.py,605,conversor out of range,not
scikit-learn/sklearn/externals/_arff.py,651,Probably a scipy.sparse,not
scikit-learn/sklearn/externals/_arff.py,662,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,664,ADVANCED INTERFACE ==========================================================,not
scikit-learn/sklearn/externals/_arff.py,742,Verify the general structure of declaration,not
scikit-learn/sklearn/externals/_arff.py,747,Extracts the raw name and type,not
scikit-learn/sklearn/externals/_arff.py,750,Extracts the final name,not
scikit-learn/sklearn/externals/_arff.py,753,Extracts the final type,not
scikit-learn/sklearn/externals/_arff.py,763,"If not nominal, verify the type name",not
scikit-learn/sklearn/externals/_arff.py,773,Make sure this method is idempotent,not
scikit-learn/sklearn/externals/_arff.py,776,"If string, convert to a list of lines",not
scikit-learn/sklearn/externals/_arff.py,780,Create the return object,not
scikit-learn/sklearn/externals/_arff.py,789,Create the data helper object,not
scikit-learn/sklearn/externals/_arff.py,792,Read all lines,not
scikit-learn/sklearn/externals/_arff.py,797,Ignore empty lines,not
scikit-learn/sklearn/externals/_arff.py,803,DESCRIPTION -----------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,806,-----------------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,808,RELATION --------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,815,-----------------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,817,ATTRIBUTE -------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,844,-----------------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,846,DATA ------------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,852,-----------------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,854,COMMENT ---------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,857,-----------------------------------------------------------------,not
scikit-learn/sklearn/externals/_arff.py,859,Never found @DATA,not
scikit-learn/sklearn/externals/_arff.py,866,Ignore empty lines and comment lines.,not
scikit-learn/sklearn/externals/_arff.py,870,Alter the data object,not
scikit-learn/sklearn/externals/_arff.py,989,DESCRIPTION,not
scikit-learn/sklearn/externals/_arff.py,994,RELATION,not
scikit-learn/sklearn/externals/_arff.py,1001,ATTRIBUTES,not
scikit-learn/sklearn/externals/_arff.py,1007,Verify for bad object format,not
scikit-learn/sklearn/externals/_arff.py,1014,Verify for invalid types,not
scikit-learn/sklearn/externals/_arff.py,1018,Verify for bad object format,not
scikit-learn/sklearn/externals/_arff.py,1022,Verify attribute name is not used twice,not
scikit-learn/sklearn/externals/_arff.py,1033,DATA,not
scikit-learn/sklearn/externals/_arff.py,1042,=============================================================================,not
scikit-learn/sklearn/externals/_arff.py,1044,BASIC INTERFACE =============================================================,not
scikit-learn/sklearn/externals/_arff.py,1107,=============================================================================,not
scikit-learn/sklearn/covariance/_graph_lasso.py,5,Author: Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/covariance/_graph_lasso.py,6,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/_graph_lasso.py,7,Copyright: INRIA,not
scikit-learn/sklearn/covariance/_graph_lasso.py,23,mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast',not
scikit-learn/sklearn/covariance/_graph_lasso.py,24,type: ignore,not
scikit-learn/sklearn/covariance/_graph_lasso.py,29,Helper functions to compute the objective and dual objective functions,not
scikit-learn/sklearn/covariance/_graph_lasso.py,30,of the l1-penalized estimator,not
scikit-learn/sklearn/covariance/_graph_lasso.py,77,The g-lasso algorithm,not
scikit-learn/sklearn/covariance/_graph_lasso.py,185,"As a trivial regularization (Tikhonov like), we scale down the",not
scikit-learn/sklearn/covariance/_graph_lasso.py,186,"off-diagonal coefficients of our starting point: This is needed, as",not
scikit-learn/sklearn/covariance/_graph_lasso.py,187,in the cross-validation the cov_init can easily be,not
scikit-learn/sklearn/covariance/_graph_lasso.py,188,"ill-conditioned, and the CV loop blows. Beside, this takes",not
scikit-learn/sklearn/covariance/_graph_lasso.py,189,"conservative stand-point on the initial conditions, and it tends to",not
scikit-learn/sklearn/covariance/_graph_lasso.py,190,make the convergence go faster.,not
scikit-learn/sklearn/covariance/_graph_lasso.py,198,The different l1 regression solver have different numerical errors,not
scikit-learn/sklearn/covariance/_graph_lasso.py,204,"be robust to the max_iter=0 edge case, see:",not
scikit-learn/sklearn/covariance/_graph_lasso.py,205,https://github.com/scikit-learn/scikit-learn/issues/4134,not
scikit-learn/sklearn/covariance/_graph_lasso.py,207,set a sub_covariance buffer,not
scikit-learn/sklearn/covariance/_graph_lasso.py,211,To keep the contiguous matrix `sub_covariance` equal to,not
scikit-learn/sklearn/covariance/_graph_lasso.py,212,covariance_[indices != idx].T[indices != idx],not
scikit-learn/sklearn/covariance/_graph_lasso.py,213,we only need to update 1 column and 1 line when idx changes,not
scikit-learn/sklearn/covariance/_graph_lasso.py,223,Use coordinate descent,not
scikit-learn/sklearn/covariance/_graph_lasso.py,231,Use LARS,not
scikit-learn/sklearn/covariance/_graph_lasso.py,236,Update the precision matrix,not
scikit-learn/sklearn/covariance/_graph_lasso.py,392,Covariance does not make sense for a single feature,not
scikit-learn/sklearn/covariance/_graph_lasso.py,409,Cross-validation with GraphicalLasso,not
scikit-learn/sklearn/covariance/_graph_lasso.py,482,"Capture the errors, and move on",not
scikit-learn/sklearn/covariance/_graph_lasso.py,675,Covariance does not make sense for a single feature,not
scikit-learn/sklearn/covariance/_graph_lasso.py,686,"List of (alpha, scores, covs)",not
scikit-learn/sklearn/covariance/_graph_lasso.py,704,No need to see the convergence warnings on this grid:,not
scikit-learn/sklearn/covariance/_graph_lasso.py,705,they will always be points that will not converge,not
scikit-learn/sklearn/covariance/_graph_lasso.py,706,during the cross-validation,not
scikit-learn/sklearn/covariance/_graph_lasso.py,708,Compute the cross-validated loss on the current grid,not
scikit-learn/sklearn/covariance/_graph_lasso.py,710,"NOTE: Warm-restarting graphical_lasso_path has been tried,",not
scikit-learn/sklearn/covariance/_graph_lasso.py,711,and this did not allow to gain anything,not
scikit-learn/sklearn/covariance/_graph_lasso.py,712,(same execution time with or without).,not
scikit-learn/sklearn/covariance/_graph_lasso.py,725,Little danse to transform the list in what we need,not
scikit-learn/sklearn/covariance/_graph_lasso.py,732,Find the maximum (avoid using built in 'max' function to,not
scikit-learn/sklearn/covariance/_graph_lasso.py,733,have a fully-reproducible selection of the smallest alpha,not
scikit-learn/sklearn/covariance/_graph_lasso.py,734,in case of equality),not
scikit-learn/sklearn/covariance/_graph_lasso.py,747,Refine the grid,not
scikit-learn/sklearn/covariance/_graph_lasso.py,749,We do not need to go back: we have chosen,not
scikit-learn/sklearn/covariance/_graph_lasso.py,750,the highest value of alpha for which there are,not
scikit-learn/sklearn/covariance/_graph_lasso.py,751,non-zero coefficients,not
scikit-learn/sklearn/covariance/_graph_lasso.py,756,We have non-converged models on the upper bound of the,not
scikit-learn/sklearn/covariance/_graph_lasso.py,757,"grid, we need to refine the grid there",not
scikit-learn/sklearn/covariance/_graph_lasso.py,779,"Finally, compute the score with alpha = 0",not
scikit-learn/sklearn/covariance/_graph_lasso.py,789,Finally fit the model with the selected alpha,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,9,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,10,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,11,Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,12,,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,13,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,15,avoid division truncation,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,24,ShrunkCovariance estimator,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,146,"Not calling the parent object to fit, to avoid a potential",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,147,matrix inversion when setting the precision,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,160,Ledoit-Wolf estimator,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,196,"for only one feature, the result is the same whatever the shrinkage",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,207,optionally center data,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,211,A non-blocked version of the computation is present in the tests,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,212,in tests/test_covariance.py,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,214,number of blocks to split the covariance matrix into,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,219,"sum of the coefficients of <X2.T, X2>",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,220,"sum of the *squared* coefficients of <X.T, X>",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,221,starting block computation,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,242,use delta_ to compute beta,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,244,"delta is the sum of the squared coefficients of (<X.T,X> - mu*Id) / p",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,247,get final beta as the min between beta and delta,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,248,"We do this to prevent shrinking more than ""1"", which whould invert",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,249,the value of covariances,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,251,finally get shrinkage,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,294,"for only one feature, the result is the same whatever the shrinkage",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,307,get Ledoit-Wolf shrinkage,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,416,"Not calling the parent object to fit, to avoid computing the",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,417,covariance matrix (and potentially the precision),not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,432,OAS estimator,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,469,"for only one feature, the result is the same whatever the shrinkage",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,486,formula from Chen et al.'s **implementation**,not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,593,"Not calling the parent object to fit, to avoid computing the",not
scikit-learn/sklearn/covariance/_shrunk_covariance.py,594,covariance matrix (and potentially the precision),not
scikit-learn/sklearn/covariance/_elliptic_envelope.py,1,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/covariance/_elliptic_envelope.py,2,,not
scikit-learn/sklearn/covariance/_elliptic_envelope.py,3,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,6,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,7,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,8,Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,9,,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,10,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,12,avoid division truncation,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,165,set covariance,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,167,set precision,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,236,compute empirical covariance of the test set,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,239,compute log likelihood,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,275,compute the error,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,277,compute the error norm,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,285,optionally scale the error norm,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,288,finally get either the squared norm or the norm,not
scikit-learn/sklearn/covariance/_empirical_covariance.py,312,compute mahalanobis distances,not
scikit-learn/sklearn/covariance/_robust_covariance.py,7,Author: Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/covariance/_robust_covariance.py,8,,not
scikit-learn/sklearn/covariance/_robust_covariance.py,9,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/_robust_covariance.py,23,Minimum Covariance Determinant,not
scikit-learn/sklearn/covariance/_robust_covariance.py,24,Implementing of an algorithm by Rousseeuw & Van Driessen described in,not
scikit-learn/sklearn/covariance/_robust_covariance.py,25,"(A Fast Algorithm for the Minimum Covariance Determinant Estimator,",not
scikit-learn/sklearn/covariance/_robust_covariance.py,26,"1999, American Statistical Association and the American Society",not
scikit-learn/sklearn/covariance/_robust_covariance.py,27,"for Quality, TECHNOMETRICS)",not
scikit-learn/sklearn/covariance/_robust_covariance.py,28,XXX Is this really a public function? It's not listed in the docs or,SATD
scikit-learn/sklearn/covariance/_robust_covariance.py,29,exported by sklearn.covariance. Deprecate?,not
scikit-learn/sklearn/covariance/_robust_covariance.py,103,Initialisation,not
scikit-learn/sklearn/covariance/_robust_covariance.py,106,compute initial robust estimates from a random subset,not
scikit-learn/sklearn/covariance/_robust_covariance.py,109,get initial robust estimates from the function parameters,not
scikit-learn/sklearn/covariance/_robust_covariance.py,112,run a special iteration for that case (to get an initial support),not
scikit-learn/sklearn/covariance/_robust_covariance.py,116,compute new estimates,not
scikit-learn/sklearn/covariance/_robust_covariance.py,123,Iterative procedure for Minimum Covariance Determinant computation,not
scikit-learn/sklearn/covariance/_robust_covariance.py,125,"If the data already has singular covariance, calculate the precision,",not
scikit-learn/sklearn/covariance/_robust_covariance.py,126,as the loop below will not be entered.,not
scikit-learn/sklearn/covariance/_robust_covariance.py,133,save old estimates values,not
scikit-learn/sklearn/covariance/_robust_covariance.py,138,compute a new support from the full data set mahalanobis distances,not
scikit-learn/sklearn/covariance/_robust_covariance.py,142,compute new estimates,not
scikit-learn/sklearn/covariance/_robust_covariance.py,149,update remaining iterations for early stopping,not
scikit-learn/sklearn/covariance/_robust_covariance.py,154,"Check if best fit already found (det => 0, logdet => -inf)",not
scikit-learn/sklearn/covariance/_robust_covariance.py,157,Check convergence,not
scikit-learn/sklearn/covariance/_robust_covariance.py,159,c_step procedure converged,not
scikit-learn/sklearn/covariance/_robust_covariance.py,165,determinant has increased (should not happen),not
scikit-learn/sklearn/covariance/_robust_covariance.py,175,Check early stopping,not
scikit-learn/sklearn/covariance/_robust_covariance.py,279,compute `n_trials` location and shape estimates candidates in the subset,not
scikit-learn/sklearn/covariance/_robust_covariance.py,282,perform `n_trials` computations from random initial supports,not
scikit-learn/sklearn/covariance/_robust_covariance.py,290,perform computations from every given initial estimates,not
scikit-learn/sklearn/covariance/_robust_covariance.py,300,find the `n_best` best results among the `n_trials` ones,not
scikit-learn/sklearn/covariance/_robust_covariance.py,382,minimum breakdown value,not
scikit-learn/sklearn/covariance/_robust_covariance.py,388,1-dimensional case quick computation,not
scikit-learn/sklearn/covariance/_robust_covariance.py,389,"(Rousseeuw, P. J. and Leroy, A. M. (2005) References, in Robust",not
scikit-learn/sklearn/covariance/_robust_covariance.py,390,"Regression and Outlier Detection, John Wiley & Sons, chapter 4)",not
scikit-learn/sklearn/covariance/_robust_covariance.py,393,find the sample shortest halves,not
scikit-learn/sklearn/covariance/_robust_covariance.py,397,take the middle points' mean to get the robust location estimate,not
scikit-learn/sklearn/covariance/_robust_covariance.py,405,get precision matrix in an optimized way,not
scikit-learn/sklearn/covariance/_robust_covariance.py,413,get precision matrix in an optimized way,not
scikit-learn/sklearn/covariance/_robust_covariance.py,416,Starting FastMCD algorithm for p-dimensional case,not
scikit-learn/sklearn/covariance/_robust_covariance.py,418,1. Find candidate supports on subsets,not
scikit-learn/sklearn/covariance/_robust_covariance.py,419,a. split the set in subsets of size ~ 300,not
scikit-learn/sklearn/covariance/_robust_covariance.py,425,b. perform a total of 500 trials,not
scikit-learn/sklearn/covariance/_robust_covariance.py,427,"c. select 10 best (location, covariance) for each subset",not
scikit-learn/sklearn/covariance/_robust_covariance.py,436,The above is too big. Let's try with something much small,not
scikit-learn/sklearn/covariance/_robust_covariance.py,437,(and less optimal),not
scikit-learn/sklearn/covariance/_robust_covariance.py,454,2. Pool the candidate supports into a merged set,not
scikit-learn/sklearn/covariance/_robust_covariance.py,455,(possibly the full dataset),not
scikit-learn/sklearn/covariance/_robust_covariance.py,463,"find the best couples (location, covariance) on the merged set",not
scikit-learn/sklearn/covariance/_robust_covariance.py,472,"3. Finally get the overall best (locations, covariance) couple",not
scikit-learn/sklearn/covariance/_robust_covariance.py,474,"directly get the best couple (location, covariance)",not
scikit-learn/sklearn/covariance/_robust_covariance.py,482,select the best couple on the full dataset,SATD
scikit-learn/sklearn/covariance/_robust_covariance.py,495,"1. Find the 10 best couples (location, covariance)",not
scikit-learn/sklearn/covariance/_robust_covariance.py,496,considering two iterations,not
scikit-learn/sklearn/covariance/_robust_covariance.py,503,2. Select the best couple on the full dataset amongst the 10,SATD
scikit-learn/sklearn/covariance/_robust_covariance.py,645,check that the empirical covariance is full rank,not
scikit-learn/sklearn/covariance/_robust_covariance.py,649,compute and store raw estimates,not
scikit-learn/sklearn/covariance/_robust_covariance.py,658,get precision matrix in an optimized way,not
scikit-learn/sklearn/covariance/_robust_covariance.py,667,obtain consistency at normal models,not
scikit-learn/sklearn/covariance/_robust_covariance.py,669,re-weight estimator,not
scikit-learn/sklearn/covariance/_robust_covariance.py,700,Check that the covariance of the support data is not equal to 0.,not
scikit-learn/sklearn/covariance/_robust_covariance.py,701,Otherwise self.dist_ = 0 and thus correction = 0.,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,20,Sample data from a sparse multivariate normal,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,39,Check that the costs always decrease (doesn't hold if alpha == 0),not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,42,Check that the 2 approaches give similar results,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,46,Smoke test the estimator,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,52,"For a centered matrix, assume_centered could be chosen True or False",not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,53,Check that this returns indeed the same result for centered data,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,64,Hard-coded solution from R glasso package for alpha=1.0,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,65,(need to set penalize.diagonal to FALSE),not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,88,Hard-coded solution from Python skggm package,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,89,"obtained by calling `quic(emp_cov, lam=.1, tol=1e-8)`",not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,105,Small subset of rows to test the rank-deficient case,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,106,Need to choose samples such that none of the variances are zero,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,109,Hard-coded solution from R glasso package for alpha=0.01,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,132,Sample data from a sparse multivariate normal,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,140,"Capture stdout, to smoke test the verbose mode",not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,144,We need verbose very high so that Parallel prints on stdout,not
scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py,149,Smoke test with specified alphas,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,2,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,3,Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,4,,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,26,Tests Covariance module on a simple dataset.,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,27,test covariance fit from data,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,43,Mahalanobis distances computation test,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,47,test with n_features = 1,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,56,test with one sample,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,57,Create X with 1 sample and 5 features,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,64,test integer type,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,69,test centered case,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,76,Tests ShrunkCovariance module on a simple dataset.,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,77,compare shrunk covariance obtained from data and from MLE estimate,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,84,same test with shrinkage not provided,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,90,same test with shrinkage = 0 (<==> empirical_covariance),not
scikit-learn/sklearn/covariance/tests/test_covariance.py,95,test with n_features = 1,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,101,test shrinkage coeff on a simple data set (without saving precision),not
scikit-learn/sklearn/covariance/tests/test_covariance.py,108,Tests LedoitWolf module on a simple dataset.,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,109,test shrinkage coeff on a simple data set,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,122,compare shrunk covariance obtained from data and from MLE estimate,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,127,compare estimates given by LW and ShrunkCovariance,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,132,test with n_features = 1,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,142,test shrinkage coeff on a simple data set (without saving precision),not
scikit-learn/sklearn/covariance/tests/test_covariance.py,148,Same tests without assuming centered data,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,149,test shrinkage coeff on a simple data set,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,156,compare shrunk covariance obtained from data and from MLE estimate,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,160,compare estimates given by LW and ShrunkCovariance,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,165,test with n_features = 1,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,174,test with one sample,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,175,warning should be raised when using only 1 sample,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,182,test shrinkage coeff on a simple data set (without saving precision),not
scikit-learn/sklearn/covariance/tests/test_covariance.py,190,A simple implementation of the formulas from Ledoit & Wolf,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,192,The computation below achieves the following computations of the,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,193,"""O. Ledoit and M. Wolf, A Well-Conditioned Estimator for",not
scikit-learn/sklearn/covariance/tests/test_covariance.py,194,"Large-Dimensional Covariance Matrices""",not
scikit-learn/sklearn/covariance/tests/test_covariance.py,195,beta and delta are given in the beginning of section 3.2,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,212,Compare our blocked implementation to the naive implementation,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,222,test that ledoit_wolf doesn't error on data that is wider than block_size,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,224,use a number of features that is larger than the block-size,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,227,check that covariance is about diagonal (random normal noise),not
scikit-learn/sklearn/covariance/tests/test_covariance.py,231,check that the result is consistent with not splitting data into blocks.,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,237,Tests OAS module on a simple dataset.,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,238,test shrinkage coeff on a simple data set,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,244,compare shrunk covariance obtained from data and from MLE estimate,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,249,compare estimates given by OAS and ShrunkCovariance,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,254,test with n_features = 1,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,263,test shrinkage coeff on a simple data set (without saving precision),not
scikit-learn/sklearn/covariance/tests/test_covariance.py,269,Same tests without assuming centered data--------------------------------,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,270,test shrinkage coeff on a simple data set,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,275,compare shrunk covariance obtained from data and from MLE estimate,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,279,compare estimates given by OAS and ShrunkCovariance,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,284,test with n_features = 1,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,293,test with one sample,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,294,warning should be raised when using only 1 sample,not
scikit-learn/sklearn/covariance/tests/test_covariance.py,301,test shrinkage coeff on a simple data set (without saving precision),not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,1,Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,2,Gael Varoquaux <gael.varoquaux@normalesup.org>,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,3,Virgile Fritsch <virgile.fritsch@inria.fr>,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,4,,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,25,Tests the FastMCD algorithm implementation,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,26,Small data set,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,27,test without outliers (random independent normal data),not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,29,test with a contaminated data set (medium contamination),not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,31,test with a contaminated data set (strong contamination),not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,34,Medium data set,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,37,Large data set,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,40,1D data set,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,62,add some outliers,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,71,compute MCD by fitting an object,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,76,compare with the estimates learnt from the inliers,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,86,"Check that the code does not break with X.shape = (3, 1)",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,87,(i.e. n_support = n_samples),not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,95,Check that MCD completes when the covariance matrix is singular,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,96,i.e. one of the rows and columns are all zeros,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,99,Think of these as the values for X and Y -> 10 values between -5 and 5,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,101,Get the cartesian product of all possible coordinate pairs from above set,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,104,Add a third column that's all zeros to make our data a set of point,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,105,"within a plane, which means that the covariance matrix will be singular",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,108,The below line of code should raise an exception if the covariance matrix,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,109,"is singular. As a further test, since we have points in XYZ, the",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,110,principle components (Eigenvectors) of these directly relate to the,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,111,"geometry of the points. Since it's a plane, we should be able to test",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,112,that the Eigenvector that corresponds to the smallest Eigenvalue is the,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,113,"plane normal, specifically [0, 0, 1], since everything is in the XY plane",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,114,(as I've set it up above). To do this one would start by:,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,115,,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,116,"evals, evecs = np.linalg.eigh(mcd_fit.covariance_)",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,117,"normal = evecs[:, np.argmin(evals)]",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,118,,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,119,"After which we need to assert that our `normal` is equal to [0, 0, 1].",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,120,"Do note that there is floating point error associated with this, so it's",not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,121,best to subtract the two and then compare some small tolerance (e.g.,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,122,1e-12).,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,127,Check that MCD returns a ValueError with informative message when the,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,128,covariance of the support data is equal to 0.,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,140,Check that a warning is raised if we observe increasing determinants,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,141,during the c_step. In theory the sequence of determinants should be,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,142,decreasing. Increasing determinants are likely due to ill-conditioned,not
scikit-learn/sklearn/covariance/tests/test_robust_covariance.py,143,covariance matrices that result in poor precision matrices.,not
scikit-learn/sklearn/feature_selection/_from_model.py,1,"Authors: Gilles Louppe, Mathieu Blondel, Maheshakya Wijewardena",not
scikit-learn/sklearn/feature_selection/_from_model.py,2,License: BSD 3 clause,not
scikit-learn/sklearn/feature_selection/_from_model.py,43,determine default from estimator,not
scikit-learn/sklearn/feature_selection/_from_model.py,47,the natural default threshold is 0 when l1 penalty was used,not
scikit-learn/sklearn/feature_selection/_from_model.py,171,SelectFromModel can directly call on transform.,not
scikit-learn/sklearn/feature_selection/_from_model.py,261,For consistency with other estimators we raise a AttributeError so,not
scikit-learn/sklearn/feature_selection/_from_model.py,262,that hasattr() fails if the estimator isn't fitted.,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,1,Author: Nikolay Mayorov <n59_ru@hotmail.com>,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,2,License: 3-clause BSD,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,53,Here we rely on NearestNeighbors to select the fastest algorithm.,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,60,Algorithm is selected explicitly to allow passing an array as radius,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,61,later (not all algorithms support this).,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,129,Ignore points with unique labels.,not
scikit-learn/sklearn/feature_selection/_mutual_info.py,277,Add small noise to continuous features as advised in Kraskov et. al.,not
scikit-learn/sklearn/feature_selection/_rfe.py,1,Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>,not
scikit-learn/sklearn/feature_selection/_rfe.py,2,Vincent Michel <vincent.michel@inria.fr>,not
scikit-learn/sklearn/feature_selection/_rfe.py,3,Gilles Louppe <g.louppe@gmail.com>,not
scikit-learn/sklearn/feature_selection/_rfe.py,4,,not
scikit-learn/sklearn/feature_selection/_rfe.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/feature_selection/_rfe.py,154,Parameter step_score controls the calculation of self.scores_,not
scikit-learn/sklearn/feature_selection/_rfe.py,155,step_score is not exposed to users,not
scikit-learn/sklearn/feature_selection/_rfe.py,156,and is used when implementing RFECV,not
scikit-learn/sklearn/feature_selection/_rfe.py,157,self.scores_ will not be calculated when calling _fit through fit,not
scikit-learn/sklearn/feature_selection/_rfe.py,166,Initialization,not
scikit-learn/sklearn/feature_selection/_rfe.py,186,Elimination,not
scikit-learn/sklearn/feature_selection/_rfe.py,188,Remaining features,not
scikit-learn/sklearn/feature_selection/_rfe.py,191,Rank the remaining features,not
scikit-learn/sklearn/feature_selection/_rfe.py,198,Get coefs,not
scikit-learn/sklearn/feature_selection/_rfe.py,208,Get ranks,not
scikit-learn/sklearn/feature_selection/_rfe.py,214,for sparse case ranks is matrix,not
scikit-learn/sklearn/feature_selection/_rfe.py,217,Eliminate the worse features,not
scikit-learn/sklearn/feature_selection/_rfe.py,220,Compute step score on the previous selection iteration,not
scikit-learn/sklearn/feature_selection/_rfe.py,221,because 'estimator' must use features,not
scikit-learn/sklearn/feature_selection/_rfe.py,222,that have not been eliminated yet,not
scikit-learn/sklearn/feature_selection/_rfe.py,228,Set final attributes,not
scikit-learn/sklearn/feature_selection/_rfe.py,233,Compute step score when only n_features_to_select features left,not
scikit-learn/sklearn/feature_selection/_rfe.py,514,Initialization,not
scikit-learn/sklearn/feature_selection/_rfe.py,526,"Build an RFE object, which will evaluate and score each possible",not
scikit-learn/sklearn/feature_selection/_rfe.py,527,"feature count, down to self.min_features_to_select",not
scikit-learn/sklearn/feature_selection/_rfe.py,532,Determine the number of subsets of features by fitting across,not
scikit-learn/sklearn/feature_selection/_rfe.py,533,"the train folds and choosing the ""features_to_select"" parameter",not
scikit-learn/sklearn/feature_selection/_rfe.py,534,that gives the least averaged error across all folds.,not
scikit-learn/sklearn/feature_selection/_rfe.py,536,Note that joblib raises a non-picklable error for bound methods,not
scikit-learn/sklearn/feature_selection/_rfe.py,537,even if n_jobs is set to 1 with the default multiprocessing,not
scikit-learn/sklearn/feature_selection/_rfe.py,538,backend.,not
scikit-learn/sklearn/feature_selection/_rfe.py,539,This branching is done so that to,not
scikit-learn/sklearn/feature_selection/_rfe.py,540,make sure that user code that sets n_jobs to 1,not
scikit-learn/sklearn/feature_selection/_rfe.py,541,and provides bound methods as scorers is not broken with the,not
scikit-learn/sklearn/feature_selection/_rfe.py,542,addition of n_jobs parameter in version 0.18.,not
scikit-learn/sklearn/feature_selection/_rfe.py,561,Re-execute an elimination with best_k over the whole set,not
scikit-learn/sklearn/feature_selection/_rfe.py,568,Set final attributes,not
scikit-learn/sklearn/feature_selection/_rfe.py,575,"Fixing a normalization error, n is equal to get_n_splits(X, y) - 1",not
scikit-learn/sklearn/feature_selection/_rfe.py,576,"here, the scores are normalized by get_n_splits(X, y)",not
scikit-learn/sklearn/feature_selection/_variance_threshold.py,1,Author: Lars Buitinck,not
scikit-learn/sklearn/feature_selection/_variance_threshold.py,2,License: 3-clause BSD,not
scikit-learn/sklearn/feature_selection/_variance_threshold.py,71,sparse matrix,not
scikit-learn/sklearn/feature_selection/_variance_threshold.py,82,Use peak-to-peak to avoid numeric precision issues,not
scikit-learn/sklearn/feature_selection/_variance_threshold.py,83,for constant features,not
scikit-learn/sklearn/feature_selection/_base.py,1,-*- coding: utf-8 -*-,not
scikit-learn/sklearn/feature_selection/_base.py,4,"Authors: G. Varoquaux, A. Gramfort, L. Buitinck, J. Nothman",not
scikit-learn/sklearn/feature_selection/_base.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/feature_selection/_base.py,104,insert additional entries in indptr:,not
scikit-learn/sklearn/feature_selection/_base.py,105,e.g. if transform changed indptr from [0 2 6 7] to [0 2 3],not
scikit-learn/sklearn/feature_selection/_base.py,106,col_nonzeros here will be [2 0 1] so indptr becomes [0 2 2 3],not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,3,"Authors: V. Michel, B. Thirion, G. Varoquaux, A. Gramfort, E. Duchesnay.",not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,4,"L. Buitinck, A. Joly",not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,5,License: BSD 3 clause,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,29,XXX where should this function be called? fit? scoring functions,SATD
scikit-learn/sklearn/feature_selection/_univariate_selection.py,30,themselves?,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,36,,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,37,Scoring functions,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,40,The following function is a rewriting of scipy.stats.f_oneway,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,41,Contrary to the scipy.stats.f_oneway implementation it does not,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,42,copy the data while keeping the inputs unchanged.,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,117,flatten matrix to vector in sparse case,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,163,Reuse f_obs for chi-squared statistics,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,213,XXX: we might want to do some of the following in logspace instead for,SATD
scikit-learn/sklearn/feature_selection/_univariate_selection.py,214,numerical stability.,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,223,n_classes * n_features,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,284,compute centered values,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,285,"note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we",not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,286,need not center X,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,293,compute the scaled standard deviations via moments,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,299,compute the correlation,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,304,convert to p-value,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,311,,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,312,Base classes,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,371,,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,372,Specific filters,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,373,,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,442,Cater for NaNs,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,537,Request a stable sort. Mergesort takes more memory (~40MB per,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,538,megafeature on x86-64).,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,734,,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,735,Generic filter,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,736,,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,738,"TODO this class should fit on either p-values or scores,",SATD
scikit-learn/sklearn/feature_selection/_univariate_selection.py,739,depending on the mode.,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,807,Now perform some acrobatics to set the right named parameter in,not
scikit-learn/sklearn/feature_selection/_univariate_selection.py,808,the selector,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,26,,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,27,Test the score functions,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,30,Test that our f_oneway gives the same result as scipy.stats,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,41,Smoke test f_oneway on integers: that it does raise casting errors,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,42,with recent numpys,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,48,test that is gives the same result as with float,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,55,Test whether the F test yields meaningful results,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,56,on a simple simulated classification problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,75,Test whether the F test yields meaningful results,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,76,on a simple simulated regression problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,87,"with centering, compare with sparse",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,93,"again without centering, compare with sparse",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,101,Test whether f_regression returns the same value,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,102,for any numeric data_type,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,114,Test whether f_regression preserves dof according to 'center' argument,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,115,We use two centered variates so we have a simple relationship between,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,116,F-score with variates centering and F-score without variates centering.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,117,Create toy example,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,118,X has zero mean,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,122,have Y mean being null,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,127,value from statsmodels OLS,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,131,Test whether the F test yields meaningful results,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,132,on a simple simulated classification problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,148,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,149,gets the correct items in a simple classification problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,150,with the percentile heuristic,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,169,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,170,gets the correct items in a simple classification problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,171,with the percentile heuristic,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,193,Check other columns are empty,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,197,,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,198,Test univariate selection in classification settings,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,201,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,202,gets the correct items in a simple classification problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,203,with the k best heuristic,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,222,"Test whether k=""all"" correctly returns all features.",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,232,Test whether k=0 correctly returns no features.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,247,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,248,gets the correct items in a simple classification problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,249,"with the fdr, fwe and fpr heuristics",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,268,,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,269,Test univariate selection in regression settings,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,280,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,281,gets the correct items in a simple regression problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,282,with the percentile heuristic,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,299,Check inverse_transform respects dtype,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,305,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,306,selects all features when '100%' is asked.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,336,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,337,gets the correct items in a simple regression problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,338,with the k best heuristic,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,355,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,356,gets the correct items in a simple regression problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,357,"with the fpr, fdr or fwe heuristics",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,375,"Test boundary case, and always aim to select 1 feature.",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,411,Test that fdr heuristic actually has low FDR.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,418,Warnings can be raised when no features are selected,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,419,(low alpha or very noisy data),not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,436,"As per Benjamini-Hochberg, the expected false discovery rate",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,437,should be lower than alpha:,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,438,FDR = E(FP / (TP + FP)) <= alpha,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,444,Make sure that the empirical false discovery rate increases,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,445,with alpha:,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,451,Test whether the relative univariate feature selection,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,452,gets the correct items in a simple regression problem,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,453,with the fwe heuristic,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,470,Test whether SelectKBest actually selects k features in case of ties.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,471,"Prior to 0.11, SelectKBest would return more features than requested.",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,488,Test if SelectPercentile selects the right n_features in case of ties.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,505,Test whether k-best and percentiles work with tied pvalues from chi2.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,506,"chi2 will return the same p-values for the following features, but it",not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,507,will return different scores.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,523,Test whether k-best and percentiles works with multilabels with chi2.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,538,Test for stable sorting in k-best with tied scores.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,549,Assert that SelectKBest and SelectPercentile can handle NaNs.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,550,First feature has zero variance to confuse f_classif (ANOVA) and,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,551,make it return a NaN.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,586,Test that f_classif warns if a feature is constant throughout.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,596,Generate random uncorrelated data: a strict univariate test should,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,597,rejects all the features,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,621,Test in KBest mode.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,632,Test in Percentile mode.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,648,Test in KBest mode.,not
scikit-learn/sklearn/feature_selection/tests/test_feature_select.py,660,Test in Percentile mode.,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,18,Feature 0 is highly informative for class 1;,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,19,feature 1 is the same everywhere;,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,20,feature 2 is a bit informative for class 2.,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,34,Test Chi2 feature extraction,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,50,== doesn't work on scipy.sparse matrices,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,57,Check that chi2 works with a COO matrix,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,58,"(as returned by CountVectorizer, DictVectorizer)",not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,61,"if we got here without an exception, we're safe",not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,65,Check for proper error on negative numbers in the input X.,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,73,Unused feature should evaluate to NaN,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,74,and should issue no runtime warning,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,86,Test replacement for scipy.stats.chisquare against the original.,not
scikit-learn/sklearn/feature_selection/tests/test_chi2.py,91,call SciPy first because our version overwrites obs,not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,17,"Test VarianceThreshold with default setting, zero variance.",not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,30,Test VarianceThreshold with custom variance.,not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,40,Test that VarianceThreshold(0.0).fit eliminates features that have,not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,41,"the same value in every sample, even when floating point errors",not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,42,cause np.var not to be 0 for the feature.,not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,43,See #13691,not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,53,add single NaN and feature should still be included,not
scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py,55,make all values in feature NaN and feature should be rejected,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,14,In discrete case computations are straightforward and can be done,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,15,by hand on given vectors.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,27,For two continuous variables a good approach is to test on bivariate,SATD
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,28,"normal distribution, where mutual information is known.",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,30,"Mean of the distribution, irrelevant for mutual information.",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,33,Setup covariance matrix with correlation coeff. equal 0.5.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,42,True theoretical mutual information.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,51,"Theory and computed values won't be very close, assert that the",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,52,first figures after decimal point match.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,59,To test define a joint distribution as follows:,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,60,"p(x, y) = p(x) p(y | x)",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,61,X ~ Bernoulli(p),not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,62,"(Y | x = 0) ~ Uniform(-1, 1)",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,63,"(Y | x = 1) ~ Uniform(0, 2)",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,65,Use the following formula for mutual information:,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,66,I(X; Y) = H(Y) - H(Y | X),not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,67,Two entropies can be computed by hand:,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,68,H(Y) = -(1-p)/2 * ln((1-p)/2) - p/2*log(p/2) - 1/2*log(1/2),not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,69,H(Y | X) = ln(2),not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,71,"Now we need to implement sampling from out distribution, which is",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,72,done easily using conditional distribution logic.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,88,Assert the same tolerance.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,95,Test that adding unique label doesn't change MI.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,113,We are going test that feature ordering by MI matches our expectations.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,122,"Here X[:, 0] is the most informative feature, and X[:, 1] is weakly",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,123,informative.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,129,"We generate sample from multivariate normal distribution, using",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,130,transformation from initially uncorrelated variables. The zero,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,131,"variables after transformation is selected as the target vector,",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,132,"it has the strongest correlation with the variable 2, and",not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,133,the weakest correlation with the variable 1.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,153,Here the target is discrete and there are two continuous and one,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,154,discrete feature. The idea of this test is clear from the code.,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,167,Check that the continuous values have an higher MI with greater,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,168,n_neighbors,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,171,The n_neighbors should not have any effect on the discrete value,not
scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py,172,The MI should be the same,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,13,noqa,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,51,Test that SelectFromModel fits on a clone of the estimator.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,94,Test max_features parameter using various values,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,110,Test max_features against actual model.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,132,Test if max_features can break tie among feature importance,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,193,Ensure sample weights are passed to underlying estimator,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,198,Check with sample weights,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,219,"For the Lasso and related models, the threshold defaults to 1e-5",not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,237,Fit SelectFromModel a multi-class problem,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,246,Manually check that the norm is correctly performed,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,269,"check that if est doesn't have partial_fit, neither does SelectFromModel",not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,284,Test all possible combinations of the prefit parameter.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,286,Passing a prefit parameter with the selected model,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,287,and fitting a unfit model with prefit=False should give same results.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,297,Check that the model is rewritten if prefit=False and a fitted model is,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,298,passed,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,303,Check that prefit=True and calling fit raises a ValueError,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,315,Calculate the threshold from the estimator directly.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,323,Test that the threshold can be set without refitting the model.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,330,Set a higher threshold to filter out more features.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,336,Test that fit doesn't check for np.inf and np.nan values.,not
scikit-learn/sklearn/feature_selection/tests/test_from_model.py,349,Test that transform doesn't check for np.inf and np.nan values.,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,48,Check dtype matches,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,52,Check 1d list and other dtype:,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,56,Check wrong shape raises error,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,69,Check dtype matches,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,73,Check wrong shape raises error,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,83,Check dtype matches,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,89,Check 1d list and other dtype:,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,93,Check wrong shape raises error,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,104,Check dtype matches,not
scikit-learn/sklearn/feature_selection/tests/test_base.py,110,Check wrong shape raises error,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,74,Check if the supports are equal,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,85,dense model,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,93,sparse model,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,113,dense model,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,127,regression test: list should be supported,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,129,Test using the score function,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,132,non-regression test for missing worst feature:,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,137,All the noisy variable were filtered out,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,140,same in sparse,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,147,Test using a customized loss function,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,154,Test using a scorer,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,161,Test fix on grid_scores,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,167,"In the event of cross validation score ties, the expected behavior of",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,168,RFECV is to return the FEWEST features that maximize the CV score.,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,169,"Because test_scorer always returns 1.0 in this example, RFECV should",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,170,reduce the dimensionality to a single feature (i.e. n_features_ = 1),not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,173,"Same as the first two tests, but with step=2",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,187,Verifying that steps < 1 don't blow up.,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,199,regression test: list should be supported,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,201,Test using the score function,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,204,non-regression test for missing worst feature:,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,210,Check verbose=1 is producing an output.,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,232,regression test: list should be supported,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,234,Non-regression test for varying combinations of step and,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,235,min_features_to_select.,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,251,make sure that cross-validation is stratified,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,263,Test when floor(step * n_features) <= 0,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,268,"Test when step is between (0,1) and floor(step * n_features) > 0",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,273,Test when step is an integer,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,280,"In RFE, 'number_of_subsets_of_features'",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,281,= the number of iterations in '_fit',not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,282,= max(ranking_),not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,283,= 1 + (n_features + step - n_features_to_select - 1) // step,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,284,"After optimization #4534, this number",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,285,= 1 + np.ceil((n_features - n_features_to_select) / float(step)),not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,286,"This test case is to test their equivalence, refer to #4534 and #3824",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,294,RFE,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,295,"Case 1, n_features - n_features_to_select is divisible by step",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,296,"Case 2, n_features - n_features_to_select is not divisible by step",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,308,this number also equals to the maximum of ranking_,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,314,"In RFECV, 'fit' calls 'RFE._fit'",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,315,'number_of_subsets_of_features' of RFE,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,316,= the size of 'grid_scores' of RFECV,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,317,= the number of iterations of the for loop before optimization #4534,not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,319,"RFECV, n_features_to_select = 1",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,320,"Case 1, n_features - 1 is divisible by step",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,321,"Case 2, n_features - 1 is not divisible by step",not
scikit-learn/sklearn/feature_selection/tests/test_rfe.py,383,add nan and inf value to X,not
scikit-learn/maint_tools/test_docstrings.py,10,List of whitelisted modules and methods; regexp are supported.,not
scikit-learn/maint_tools/test_docstrings.py,38,$ to avoid match w/ predict_proba (regex),not
scikit-learn/maint_tools/test_docstrings.py,58,skip private classes,not
scikit-learn/maint_tools/test_docstrings.py,80,"We ignore following error code,",not
scikit-learn/maint_tools/test_docstrings.py,81,- RT02: The first line of the Returns section,not
scikit-learn/maint_tools/test_docstrings.py,82,"should contain only the type, ..",not
scikit-learn/maint_tools/test_docstrings.py,83,(as we may need refer to the name of the returned,not
scikit-learn/maint_tools/test_docstrings.py,84,object),not
scikit-learn/maint_tools/test_docstrings.py,85,- GL01: Docstring text (summary) should start in the line,not
scikit-learn/maint_tools/test_docstrings.py,86,"immediately after the opening quotes (not in the same line,",not
scikit-learn/maint_tools/test_docstrings.py,87,or leaving a blank line in between),not
scikit-learn/maint_tools/test_docstrings.py,92,Following codes are only taken into account for the,not
scikit-learn/maint_tools/test_docstrings.py,93,top level class docstrings:,not
scikit-learn/maint_tools/test_docstrings.py,94,- ES01: No extended summary found,not
scikit-learn/maint_tools/test_docstrings.py,95,- SA01: See Also section not found,not
scikit-learn/maint_tools/test_docstrings.py,96,- EX01: No examples section found,not
scikit-learn/maint_tools/test_docstrings.py,135,In particular we can't parse the signature of properties,not
scikit-learn/maint_tools/test_docstrings.py,201,"When applied to classes, detect class method. For functions",not
scikit-learn/maint_tools/test_docstrings.py,202,method = None.,not
scikit-learn/maint_tools/test_docstrings.py,203,TODO: this detection can be improved. Currently we assume that we have,SATD
scikit-learn/maint_tools/test_docstrings.py,204,class # methods if the second path element before last is in camel case.,not
scikit-learn/maint_tools/check_pxd_in_installation.py,27,A cython test file which cimports all modules corresponding to found,not
scikit-learn/maint_tools/check_pxd_in_installation.py,28,pxd files.,not
scikit-learn/maint_tools/check_pxd_in_installation.py,29,e.g. sklearn/tree/_utils.pxd becomes `cimport sklearn.tree._utils`,not
scikit-learn/maint_tools/check_pxd_in_installation.py,37,A basic setup file to build the test file.,not
scikit-learn/maint_tools/check_pxd_in_installation.py,38,We set the language to c++ and we use numpy.get_include() because,not
scikit-learn/maint_tools/check_pxd_in_installation.py,39,some modules require it.,not
scikit-learn/maint_tools/sort_whats_new.py,1,!/usr/bin/env python,not
scikit-learn/maint_tools/sort_whats_new.py,2,Sorts what's new entries with per-module headings.,not
scikit-learn/maint_tools/sort_whats_new.py,3,Pass what's new entries on stdin.,not
scikit-learn/maint_tools/sort_whats_new.py,20,discard headings and other non-entry lines,not
