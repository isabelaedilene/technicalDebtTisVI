file path,line #,comment,satd
spaCy/setup.py,1,!/usr/bin/env python,not
spaCy/setup.py,78,"On Mac, use libc++ because Apple deprecated use of",not
spaCy/setup.py,79,libstdc,not
spaCy/setup.py,82,g++ (used by unix compiler on mac) links to libstdc++ as a default lib.,not
spaCy/setup.py,83,See: https://stackoverflow.com/questions/1653047/avoid-linking-to-libstdc,not
spaCy/setup.py,87,By subclassing build_extensions we have the actual compiler that will be used which is really known only after finalize_options,not
spaCy/setup.py,88,http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used,not
spaCy/setup.py,168,???,not
spaCy/setup.py,169,Imported from patch from @mikepb,not
spaCy/setup.py,170,See Issue #267. Running blind here...,not
spaCy/fabfile.py,1,coding: utf-8,not
spaCy/bin/train_word_vectors.py,1,!/usr/bin/env python,not
spaCy/bin/train_word_vectors.py,24,This is to keep the input to the blank model (which doesn't,not
spaCy/bin/train_word_vectors.py,25,sentencize) from being too long. It works particularly well with,not
spaCy/bin/train_word_vectors.py,26,the output of [WikiExtractor](https://github.com/attardi/wikiextractor),not
spaCy/bin/load_reddit.py,1,coding: utf8,not
spaCy/bin/load_reddit.py,93,Python flushes standard streams on exit; redirect remaining output,not
spaCy/bin/load_reddit.py,94,to devnull to avoid another BrokenPipeError at shutdown,not
spaCy/bin/load_reddit.py,97,Python exits with error code 1 on EPIPE,not
spaCy/bin/cythonize.py,1,!/usr/bin/env python,not
spaCy/bin/cythonize.py,61,See Issue #791,not
spaCy/bin/cythonize.py,65,There are ways of installing Cython that don't result in a cython,not
spaCy/bin/cythonize.py,66,"executable on the path, see gh-2397.",not
spaCy/bin/ud/ud_train.py,1,flake8: noqa,not
spaCy/bin/ud/ud_train.py,36,,not
spaCy/bin/ud/ud_train.py,37,Data reading,not
spaCy/bin/ud/ud_train.py,38,,not
spaCy/bin/ud/ud_train.py,64,sd is spacy doc; cd is conllu doc,not
spaCy/bin/ud/ud_train.py,65,"cs is conllu sent, ct is conllu token",not
spaCy/bin/ud/ud_train.py,151,"Flatten the conll annotations, and adjust the head indices",not
spaCy/bin/ud/ud_train.py,160,Construct text if necessary,not
spaCy/bin/ud/ud_train.py,178,,not
spaCy/bin/ud/ud_train.py,179,Data transforms for spaCy,not
spaCy/bin/ud/ud_train.py,180,,not
spaCy/bin/ud/ud_train.py,195,,not
spaCy/bin/ud/ud_train.py,196,Evaluation,not
spaCy/bin/ud/ud_train.py,197,,not
spaCy/bin/ud/ud_train.py,295,"def get_sent_conllu(sent, sent_id):",not
spaCy/bin/ud/ud_train.py,296,"lines = [""# sent_id = {sent_id}"".format(sent_id=sent_id)]",not
spaCy/bin/ud/ud_train.py,331,,not
spaCy/bin/ud/ud_train.py,332,Initialization,not
spaCy/bin/ud/ud_train.py,333,,not
spaCy/bin/ud/ud_train.py,391,,not
spaCy/bin/ud/ud_train.py,392,Command line helpers,not
spaCy/bin/ud/ud_train.py,393,,not
spaCy/bin/ud/run_eval.py,15,All languages in spaCy - in UD format (note that Norwegian is 'no' instead of 'nb'),not
spaCy/bin/ud/run_eval.py,21,Non-parsing tasks that will be evaluated (works for default models),not
spaCy/bin/ud/run_eval.py,24,Tasks that will be evaluated if check_parse=True (does not work for default models),not
spaCy/bin/ud/run_eval.py,27,Minimum frequency an error should have to be printed,not
spaCy/bin/ud/run_eval.py,30,Maximum number of errors printed per category,not
spaCy/bin/ud/run_eval.py,80,assume the corpus is largely blinded when there are less than 1% unique tokens,not
spaCy/bin/ud/run_eval.py,100,ignore treebanks where the texts are not publicly available,not
spaCy/bin/ud/run_eval.py,104,check the tokens in the gold annotation to keep only the biggest treebank per language,not
spaCy/bin/ud/run_eval.py,122,STEP 1: tokenize text,not
spaCy/bin/ud/run_eval.py,129,STEP 2: record stats and timings,not
spaCy/bin/ud/run_eval.py,136,STEP 3: evaluate predicted tokens and features,not
spaCy/bin/ud/run_eval.py,144,STEP 4: format the scoring results,not
spaCy/bin/ud/run_eval.py,170,saving to CSV with ; seperator so blinding ; in the example output,not
spaCy/bin/ud/run_eval.py,180,STEP 5: print the formatted results to CSV,not
spaCy/bin/ud/run_eval.py,199,nested try blocks to ensure the code can continue with the next iteration after a failure,not
spaCy/bin/ud/run_eval.py,240,fetching all relevant treebank from the directory,not
spaCy/bin/ud/run_eval.py,247,multi-lang model,not
spaCy/bin/ud/run_eval.py,252,initialize all models with the multi-lang model,not
spaCy/bin/ud/run_eval.py,255,add default models if we don't want to evaluate parsing info,not
spaCy/bin/ud/run_eval.py,257,Norwegian is 'nb' in spaCy but 'no' in the UD corpora,not
spaCy/bin/ud/run_eval.py,263,language-specific trained models,not
spaCy/bin/ud/ud_run_test.py,1,flake8: noqa,not
spaCy/bin/ud/ud_run_test.py,21,"from spacy.morphology import Fused_begin, Fused_inside",not
spaCy/bin/ud/ud_run_test.py,41,,not
spaCy/bin/ud/ud_run_test.py,42,Data reading,not
spaCy/bin/ud/ud_run_test.py,43,,not
spaCy/bin/ud/ud_run_test.py,52,,not
spaCy/bin/ud/ud_run_test.py,53,Evaluation,not
spaCy/bin/ud/ud_run_test.py,54,,not
spaCy/bin/ud/ud_run_test.py,190,"Happy case: we get a perfect split, with each letter accounted for.",not
spaCy/bin/ud/ud_run_test.py,193,"Unideal, but at least lengths match.",not
spaCy/bin/ud/ud_run_test.py,203,"Let's say word is 6 long, and there are three subtokens. The orths",not
spaCy/bin/ud/ud_run_test.py,204,"*must* equal the original string. Arbitrarily, split [4, 1, 1]",not
spaCy/bin/ud/ud_run_test.py,261,,not
spaCy/bin/ud/ud_run_test.py,262,Initialization,not
spaCy/bin/ud/ud_run_test.py,263,,not
spaCy/bin/ud/conll17_ud_eval.py,1,!/usr/bin/env python,not
spaCy/bin/ud/conll17_ud_eval.py,2,flake8: noqa,not
spaCy/bin/ud/conll17_ud_eval.py,4,CoNLL 2017 UD Parsing evaluation script.,not
spaCy/bin/ud/conll17_ud_eval.py,5,,not
spaCy/bin/ud/conll17_ud_eval.py,6,"Compatible with Python 2.7 and 3.2+, can be used either as a module",not
spaCy/bin/ud/conll17_ud_eval.py,7,or a standalone executable.,not
spaCy/bin/ud/conll17_ud_eval.py,8,,not
spaCy/bin/ud/conll17_ud_eval.py,9,"Copyright 2017 Institute of Formal and Applied Linguistics (UFAL),",not
spaCy/bin/ud/conll17_ud_eval.py,10,"Faculty of Mathematics and Physics, Charles University, Czech Republic.",not
spaCy/bin/ud/conll17_ud_eval.py,11,,not
spaCy/bin/ud/conll17_ud_eval.py,12,Changelog:,not
spaCy/bin/ud/conll17_ud_eval.py,13,- [02 Jan 2017] Version 0.9: Initial release,not
spaCy/bin/ud/conll17_ud_eval.py,14,- [25 Jan 2017] Version 0.9.1: Fix bug in LCS alignment computation,not
spaCy/bin/ud/conll17_ud_eval.py,15,- [10 Mar 2017] Version 1.0: Add documentation and test,not
spaCy/bin/ud/conll17_ud_eval.py,16,Compare HEADs correctly using aligned words,not
spaCy/bin/ud/conll17_ud_eval.py,17,Allow evaluation with errorneous spaces in forms,not
spaCy/bin/ud/conll17_ud_eval.py,18,Compare forms in LCS case insensitively,not
spaCy/bin/ud/conll17_ud_eval.py,19,Detect cycles and multiple root nodes,not
spaCy/bin/ud/conll17_ud_eval.py,20,Compute AlignedAccuracy,not
spaCy/bin/ud/conll17_ud_eval.py,22,Command line usage,not
spaCy/bin/ud/conll17_ud_eval.py,23,------------------,not
spaCy/bin/ud/conll17_ud_eval.py,24,conll17_ud_eval.py [-v] [-w weights_file] gold_conllu_file system_conllu_file,not
spaCy/bin/ud/conll17_ud_eval.py,25,,not
spaCy/bin/ud/conll17_ud_eval.py,26,"- if no -v is given, only the CoNLL17 UD Shared Task evaluation LAS metrics",not
spaCy/bin/ud/conll17_ud_eval.py,27,is printed,not
spaCy/bin/ud/conll17_ud_eval.py,28,"- if -v is given, several metrics are printed (as precision, recall, F1 score,",not
spaCy/bin/ud/conll17_ud_eval.py,29,and in case the metric is computed on aligned words also accuracy on these):,not
spaCy/bin/ud/conll17_ud_eval.py,30,- Tokens: how well do the gold tokens match system tokens,not
spaCy/bin/ud/conll17_ud_eval.py,31,- Sentences: how well do the gold sentences match system sentences,not
spaCy/bin/ud/conll17_ud_eval.py,32,- Words: how well can the gold words be aligned to system words,not
spaCy/bin/ud/conll17_ud_eval.py,33,"- UPOS: using aligned words, how well does UPOS match",not
spaCy/bin/ud/conll17_ud_eval.py,34,"- XPOS: using aligned words, how well does XPOS match",not
spaCy/bin/ud/conll17_ud_eval.py,35,"- Feats: using aligned words, how well does FEATS match",not
spaCy/bin/ud/conll17_ud_eval.py,36,"- AllTags: using aligned words, how well does UPOS+XPOS+FEATS match",not
spaCy/bin/ud/conll17_ud_eval.py,37,"- Lemmas: using aligned words, how well does LEMMA match",not
spaCy/bin/ud/conll17_ud_eval.py,38,"- UAS: using aligned words, how well does HEAD match",not
spaCy/bin/ud/conll17_ud_eval.py,39,"- LAS: using aligned words, how well does HEAD+DEPREL(ignoring subtypes) match",not
spaCy/bin/ud/conll17_ud_eval.py,40,"- if weights_file is given (with lines containing deprel-weight pairs),",not
spaCy/bin/ud/conll17_ud_eval.py,41,one more metric is shown:,not
spaCy/bin/ud/conll17_ud_eval.py,42,"- WeightedLAS: as LAS, but each deprel (ignoring subtypes) has different weight",not
spaCy/bin/ud/conll17_ud_eval.py,44,API usage,not
spaCy/bin/ud/conll17_ud_eval.py,45,---------,not
spaCy/bin/ud/conll17_ud_eval.py,46,- load_conllu(file),not
spaCy/bin/ud/conll17_ud_eval.py,47,- loads CoNLL-U file from given file object to an internal representation,not
spaCy/bin/ud/conll17_ud_eval.py,48,- the file object should return str on both Python 2 and Python 3,not
spaCy/bin/ud/conll17_ud_eval.py,49,- raises UDError exception if the given file cannot be loaded,not
spaCy/bin/ud/conll17_ud_eval.py,50,"- evaluate(gold_ud, system_ud)",not
spaCy/bin/ud/conll17_ud_eval.py,51,- evaluate the given gold and system CoNLL-U files (loaded with load_conllu),not
spaCy/bin/ud/conll17_ud_eval.py,52,- raises UDError if the concatenated tokens of gold and system file do not match,not
spaCy/bin/ud/conll17_ud_eval.py,53,"- returns a dictionary with the metrics described above, each metrics having",not
spaCy/bin/ud/conll17_ud_eval.py,54,"four fields: precision, recall, f1 and aligned_accuracy (when using aligned",not
spaCy/bin/ud/conll17_ud_eval.py,55,"words, otherwise this is None)",not
spaCy/bin/ud/conll17_ud_eval.py,57,Description of token matching,not
spaCy/bin/ud/conll17_ud_eval.py,58,-----------------------------,not
spaCy/bin/ud/conll17_ud_eval.py,59,"In order to match tokens of gold file and system file, we consider the text",not
spaCy/bin/ud/conll17_ud_eval.py,60,resulting from concatenation of gold tokens and text resulting from,not
spaCy/bin/ud/conll17_ud_eval.py,61,"concatenation of system tokens. These texts should match -- if they do not,",not
spaCy/bin/ud/conll17_ud_eval.py,62,the evaluation fails.,not
spaCy/bin/ud/conll17_ud_eval.py,63,,not
spaCy/bin/ud/conll17_ud_eval.py,64,"If the texts do match, every token is represented as a range in this original",not
spaCy/bin/ud/conll17_ud_eval.py,65,"text, and tokens are equal only if their range is the same.",not
spaCy/bin/ud/conll17_ud_eval.py,67,Description of word matching,not
spaCy/bin/ud/conll17_ud_eval.py,68,----------------------------,not
spaCy/bin/ud/conll17_ud_eval.py,69,"When matching words of gold file and system file, we first match the tokens.",not
spaCy/bin/ud/conll17_ud_eval.py,70,"The words which are also tokens are matched as tokens, but words in multi-word",not
spaCy/bin/ud/conll17_ud_eval.py,71,tokens have to be handled differently.,not
spaCy/bin/ud/conll17_ud_eval.py,72,,not
spaCy/bin/ud/conll17_ud_eval.py,73,"To handle multi-word tokens, we start by finding ""multi-word spans"".",not
spaCy/bin/ud/conll17_ud_eval.py,74,Multi-word span is a span in the original text such that,not
spaCy/bin/ud/conll17_ud_eval.py,75,- it contains at least one multi-word token,not
spaCy/bin/ud/conll17_ud_eval.py,76,- all multi-word tokens in the span (considering both gold and system ones),not
spaCy/bin/ud/conll17_ud_eval.py,77,"are completely inside the span (i.e., they do not ""stick out"")",not
spaCy/bin/ud/conll17_ud_eval.py,78,- the multi-word span is as small as possible,not
spaCy/bin/ud/conll17_ud_eval.py,79,,not
spaCy/bin/ud/conll17_ud_eval.py,80,"For every multi-word span, we align the gold and system words completely",not
spaCy/bin/ud/conll17_ud_eval.py,81,inside this span using LCS on their FORMs. The words not intersecting,not
spaCy/bin/ud/conll17_ud_eval.py,82,(even partially) any multi-word span are then aligned as tokens.,not
spaCy/bin/ud/conll17_ud_eval.py,93,CoNLL-U column names,not
spaCy/bin/ud/conll17_ud_eval.py,96,UD Error is used when raising exceptions in this module,not
spaCy/bin/ud/conll17_ud_eval.py,100,Load given CoNLL-U file into internal representation,not
spaCy/bin/ud/conll17_ud_eval.py,102,Internal representation classes,not
spaCy/bin/ud/conll17_ud_eval.py,105,Characters of all the tokens in the whole file.,not
spaCy/bin/ud/conll17_ud_eval.py,106,Whitespace between tokens is not included.,not
spaCy/bin/ud/conll17_ud_eval.py,108,List of UDSpan instances with start&end indices into `characters`.,not
spaCy/bin/ud/conll17_ud_eval.py,110,List of UDWord instances.,not
spaCy/bin/ud/conll17_ud_eval.py,112,List of UDSpan instances with start&end indices into `characters`.,not
spaCy/bin/ud/conll17_ud_eval.py,117,"Note that self.end marks the first position **after the end** of span,",not
spaCy/bin/ud/conll17_ud_eval.py,118,"so we can use characters[start:end] or range(start, end).",not
spaCy/bin/ud/conll17_ud_eval.py,133,"Span of this word (or MWT, see below) within ud_representation.characters.",not
spaCy/bin/ud/conll17_ud_eval.py,135,"10 columns of the CoNLL-U file: ID, FORM, LEMMA,...",not
spaCy/bin/ud/conll17_ud_eval.py,137,is_multiword==True means that this word is part of a multi-word token.,not
spaCy/bin/ud/conll17_ud_eval.py,138,"In that case, self.span marks the span of the whole multi-word token.",not
spaCy/bin/ud/conll17_ud_eval.py,140,Reference to the UDWord instance representing the HEAD (or None if root).,not
spaCy/bin/ud/conll17_ud_eval.py,142,Let's ignore language-specific deprel subtypes.,not
spaCy/bin/ud/conll17_ud_eval.py,147,Load the CoNLL-U file,not
spaCy/bin/ud/conll17_ud_eval.py,157,Handle sentence start boundaries,not
spaCy/bin/ud/conll17_ud_eval.py,159,Skip comments,not
spaCy/bin/ud/conll17_ud_eval.py,162,Start a new sentence,not
spaCy/bin/ud/conll17_ud_eval.py,166,Add parent UDWord links and check there are no cycles,not
spaCy/bin/ud/conll17_ud_eval.py,184,Check there is a single root node,not
spaCy/bin/ud/conll17_ud_eval.py,189,End the sentence,not
spaCy/bin/ud/conll17_ud_eval.py,194,Read next token/word,not
spaCy/bin/ud/conll17_ud_eval.py,199,Skip empty nodes,not
spaCy/bin/ud/conll17_ud_eval.py,203,Delete spaces from FORM so gold.characters == system.characters,not
spaCy/bin/ud/conll17_ud_eval.py,204,even if one of them tokenizes the space.,not
spaCy/bin/ud/conll17_ud_eval.py,209,Save token,not
spaCy/bin/ud/conll17_ud_eval.py,214,Handle multi-word tokens to save word(s),not
spaCy/bin/ud/conll17_ud_eval.py,228,Basic tokens/words,not
spaCy/bin/ud/conll17_ud_eval.py,251,Evaluate the gold and system treebanks (loaded using load_conllu).,not
spaCy/bin/ud/conll17_ud_eval.py,279,We represent root parents in both gold and system data by '0'.,not
spaCy/bin/ud/conll17_ud_eval.py,280,"For gold data, we represent non-root parent by corresponding gold word.",not
spaCy/bin/ud/conll17_ud_eval.py,281,"For system data, we represent non-root parent by either gold word aligned",not
spaCy/bin/ud/conll17_ud_eval.py,282,"to parent system nodes, or by None if no gold words is aligned to the parent.",not
spaCy/bin/ud/conll17_ud_eval.py,304,avoid counting the same mistake twice,not
spaCy/bin/ud/conll17_ud_eval.py,310,avoid counting the same mistake twice,not
spaCy/bin/ud/conll17_ud_eval.py,346,Return score for whole aligned words,not
spaCy/bin/ud/conll17_ud_eval.py,368,We know gold_words[gi].is_multiword or system_words[si].is_multiword.,not
spaCy/bin/ud/conll17_ud_eval.py,369,"Find the start of the multiword span (gs, ss), so the multiword span is minimal.",not
spaCy/bin/ud/conll17_ud_eval.py,370,Initialize multiword_span_end characters index.,not
spaCy/bin/ud/conll17_ud_eval.py,375,if system_words[si].is_multiword,not
spaCy/bin/ud/conll17_ud_eval.py,381,Find the end of the multiword span,not
spaCy/bin/ud/conll17_ud_eval.py,382,(so both gi and si are pointing to the word following the multiword span end).,not
spaCy/bin/ud/conll17_ud_eval.py,410,"A: Multi-word tokens => align via LCS within the whole ""multiword span"".",not
spaCy/bin/ud/conll17_ud_eval.py,416,Store aligned words,not
spaCy/bin/ud/conll17_ud_eval.py,428,B: No multi-word token => align according to spans.,not
spaCy/bin/ud/conll17_ud_eval.py,442,Check that underlying character sequences do match,not
spaCy/bin/ud/conll17_ud_eval.py,456,Align words,not
spaCy/bin/ud/conll17_ud_eval.py,459,Compute the F1-scores,not
spaCy/bin/ud/conll17_ud_eval.py,483,Add WeightedLAS if weights are given,not
spaCy/bin/ud/conll17_ud_eval.py,497,Ignore comments and empty lines,not
spaCy/bin/ud/conll17_ud_eval.py,514,Load CoNLL-U files,not
spaCy/bin/ud/conll17_ud_eval.py,518,Load weights if requested,not
spaCy/bin/ud/conll17_ud_eval.py,524,Parse arguments,not
spaCy/bin/ud/conll17_ud_eval.py,537,Use verbose if weights are supplied,not
spaCy/bin/ud/conll17_ud_eval.py,541,Evaluate,not
spaCy/bin/ud/conll17_ud_eval.py,544,Print the evaluation,not
spaCy/bin/ud/conll17_ud_eval.py,566,"Tests, which can be executed with `python -m unittest conll17_ud_eval`.",not
spaCy/bin/ud/__init__.py,1,noqa: F401,not
spaCy/bin/ud/__init__.py,2,noqa: F401,not
spaCy/spacy/compat.py,1,coding: utf8,not
spaCy/spacy/compat.py,40,noqa: F401,not
spaCy/spacy/compat.py,42,noqa: F401,not
spaCy/spacy/compat.py,55,See: https://github.com/benjaminp/six/blob/master/six.py,not
spaCy/spacy/compat.py,62,noqa: F821,not
spaCy/spacy/compat.py,63,noqa: F821,not
spaCy/spacy/compat.py,64,noqa: F821,not
spaCy/spacy/compat.py,85,"Important: if no encoding is set, string becomes ""b'...'""",not
spaCy/spacy/compat.py,110,https://stackoverflow.com/q/26554135/6400719,not
spaCy/spacy/compat.py,112,this should only be on Py2.7 and windows,not
spaCy/spacy/compat.py,173,"We only want to unescape the unicode, so we first must protect the other",not
spaCy/spacy/compat.py,174,backslashes.,not
spaCy/spacy/compat.py,176,Now we remove that protection for the unicode.,not
spaCy/spacy/compat.py,179,Now we unescape by evaling the string with the AST. This can't execute,not
spaCy/spacy/compat.py,180,code -- it only does the representational level.,not
spaCy/spacy/scorer.py,1,coding: utf8,not
spaCy/spacy/scorer.py,61,catch ValueError: Only one class present in y_true.,not
spaCy/spacy/scorer.py,62,ROC AUC score is not defined in that case.,not
spaCy/spacy/scorer.py,171,binary multiclass,not
spaCy/spacy/scorer.py,174,other multiclass,not
spaCy/spacy/scorer.py,180,multilabel,not
spaCy/spacy/scorer.py,263,"None is indistinct, so we can't just add it to the set",not
spaCy/spacy/scorer.py,264,"Multiple (None, None) deps are possible",not
spaCy/spacy/scorer.py,278,Find all NER labels in gold and doc,not
spaCy/spacy/scorer.py,280,Set up all labels for per type scoring and prepare gold per type,not
spaCy/spacy/scorer.py,288,"Find all candidate labels, for all and per type",not
spaCy/spacy/scorer.py,300,Scores per ent,not
spaCy/spacy/scorer.py,304,Score for all ents,not
spaCy/spacy/scorer.py,350,,not
spaCy/spacy/scorer.py,351,,not
spaCy/spacy/scorer.py,352,The following implementation of roc_auc_score() is adapted from,not
spaCy/spacy/scorer.py,353,"scikit-learn, which is distributed under the following license:",not
spaCy/spacy/scorer.py,354,,not
spaCy/spacy/scorer.py,355,New BSD License,not
spaCy/spacy/scorer.py,356,,not
spaCy/spacy/scorer.py,357,Copyright (c) 2007–2019 The scikit-learn developers.,not
spaCy/spacy/scorer.py,358,All rights reserved.,not
spaCy/spacy/scorer.py,359,,not
spaCy/spacy/scorer.py,360,,not
spaCy/spacy/scorer.py,361,"Redistribution and use in source and binary forms, with or without",not
spaCy/spacy/scorer.py,362,"modification, are permitted provided that the following conditions are met:",not
spaCy/spacy/scorer.py,363,,not
spaCy/spacy/scorer.py,364,"a. Redistributions of source code must retain the above copyright notice,",not
spaCy/spacy/scorer.py,365,this list of conditions and the following disclaimer.,not
spaCy/spacy/scorer.py,366,b. Redistributions in binary form must reproduce the above copyright,not
spaCy/spacy/scorer.py,367,"notice, this list of conditions and the following disclaimer in the",not
spaCy/spacy/scorer.py,368,documentation and/or other materials provided with the distribution.,not
spaCy/spacy/scorer.py,369,c. Neither the name of the Scikit-learn Developers  nor the names of,not
spaCy/spacy/scorer.py,370,its contributors may be used to endorse or promote products,not
spaCy/spacy/scorer.py,371,derived from this software without specific prior written,not
spaCy/spacy/scorer.py,372,permission.,not
spaCy/spacy/scorer.py,373,,not
spaCy/spacy/scorer.py,374,,not
spaCy/spacy/scorer.py,375,"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""",not
spaCy/spacy/scorer.py,376,"AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE",not
spaCy/spacy/scorer.py,377,IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE,not
spaCy/spacy/scorer.py,378,ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR,not
spaCy/spacy/scorer.py,379,"ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL",not
spaCy/spacy/scorer.py,380,"DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR",not
spaCy/spacy/scorer.py,381,"SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER",not
spaCy/spacy/scorer.py,382,"CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT",not
spaCy/spacy/scorer.py,383,"LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY",not
spaCy/spacy/scorer.py,384,"OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH",not
spaCy/spacy/scorer.py,385,DAMAGE.,not
spaCy/spacy/scorer.py,478,Add an extra threshold position,not
spaCy/spacy/scorer.py,479,"to make sure that the curve starts at (0, 0)",not
spaCy/spacy/scorer.py,530,make y_true a boolean vector,not
spaCy/spacy/scorer.py,533,sort scores and corresponding truth values,not
spaCy/spacy/scorer.py,539,y_score typically has many tied values. Here we extract,not
spaCy/spacy/scorer.py,540,the indices associated with the distinct values. We also,not
spaCy/spacy/scorer.py,541,concatenate a value for the end of the curve.,not
spaCy/spacy/scorer.py,545,accumulate the true positives with decreasing threshold,not
spaCy/spacy/scorer.py,608,Reductions such as .sum used internally in np.trapz do not return a,not
spaCy/spacy/scorer.py,609,scalar by default for numpy.memmap instances contrary to,not
spaCy/spacy/scorer.py,610,regular numpy.ndarray instances.,not
spaCy/spacy/__main__.py,1,coding: utf8,not
spaCy/spacy/__main__.py,4,NB! This breaks in plac on Python 2!!,not
spaCy/spacy/__main__.py,5,from __future__ import unicode_literals,not
spaCy/spacy/util.py,1,coding: utf8,not
spaCy/spacy/util.py,72,Check if language is registered / entry point is available,not
spaCy/spacy/util.py,163,in data dir / shortcut,not
spaCy/spacy/util.py,166,installed as package,not
spaCy/spacy/util.py,168,path to model data directory,not
spaCy/spacy/util.py,170,Path or Path-like to model data,not
spaCy/spacy/util.py,196,Support language factories registered via entry points (e.g. custom,not
spaCy/spacy/util.py,197,"language subclass) while keeping top-level language identifier ""lang""",not
spaCy/spacy/util.py,262,compare package name against lowercase name,not
spaCy/spacy/util.py,276,use lowercase version to be safe,not
spaCy/spacy/util.py,277,Here we're importing the module just to find it. This is worryingly,not
spaCy/spacy/util.py,278,"indirect, but it's otherwise very difficult to find the package.",not
spaCy/spacy/util.py,288,https://stackoverflow.com/a/39662359/6400719,not
spaCy/spacy/util.py,292,Jupyter notebook or qtconsole,not
spaCy/spacy/util.py,294,Probably standard Python interpreter,not
spaCy/spacy/util.py,364,Handle deprecated data,not
spaCy/spacy/util.py,402,"This is implemented as functools.partial instead of a closure, to allow",not
spaCy/spacy/util.py,403,pickle to work.,not
spaCy/spacy/util.py,616,Check for end - 1 here because boundaries are inclusive,not
spaCy/spacy/util.py,627,Split to support file names like meta.json,not
spaCy/spacy/util.py,636,Split to support file names like meta.json,not
spaCy/spacy/util.py,647,Split to support file names like meta.json,not
spaCy/spacy/util.py,656,Split to support file names like meta.json,not
spaCy/spacy/util.py,709,We're using a helper function here to make it easier to change the,not
spaCy/spacy/util.py,710,"validator that's used (e.g. different draft implementation), without",not
spaCy/spacy/util.py,711,having to change it all across the codebase.,not
spaCy/spacy/util.py,712,"TODO: replace with (stable) Draft6Validator, if available",SATD
spaCy/spacy/util.py,738,"Error has suberrors, e.g. if schema uses anyOf",not
spaCy/spacy/util.py,750,Split to support file names like meta.json,not
spaCy/spacy/util.py,758,TODO: user warning?,SATD
spaCy/spacy/util.py,768,normalize words to remove all whitespace tokens,not
spaCy/spacy/util.py,770,align words with text,not
spaCy/spacy/util.py,809,"add dummy methods for to_bytes, from_bytes, to_disk and from_disk to",not
spaCy/spacy/util.py,810,allow serialization (see #1557),not
spaCy/spacy/language.py,1,coding: utf8,not
spaCy/spacy/language.py,72,"This is messy, but it's the minimal working fix to Issue #639.",not
spaCy/spacy/language.py,217,Conveniences to access pipeline components,not
spaCy/spacy/language.py,218,Shouldn't be used anymore!,not
spaCy/spacy/language.py,454,support list of names instead of spread,not
spaCy/spacy/language.py,500,"Allow dict of args to GoldParse, instead of GoldParse objects.",not
spaCy/spacy/language.py,544,TODO: document,SATD
spaCy/spacy/language.py,602,Populate vocab,not
spaCy/spacy/language.py,608,noqa: F841,not
spaCy/spacy/language.py,718,TODO: Having trouble with contextlib,SATD
spaCy/spacy/language.py,719,Workaround: these aren't actually context managers atm.,SATD
spaCy/spacy/language.py,787,contains functools.partial objects to easily create multiprocess worker.,not
spaCy/spacy/language.py,792,Allow component_cfg to overwrite the top-level kwargs.,not
spaCy/spacy/language.py,797,"Apply the function, but yield the doc",not
spaCy/spacy/language.py,804,"if n_process == 1, no processes are forked.",not
spaCy/spacy/language.py,809,"Track weakrefs of ""recent"" documents, so that we can see when they",not
spaCy/spacy/language.py,810,"expire from memory. When they do, we know we don't need old strings.",not
spaCy/spacy/language.py,811,"This way, we avoid maintaining an unbounded growth in string entries",not
spaCy/spacy/language.py,812,in the string store.,not
spaCy/spacy/language.py,815,"Keep track of the original string data, so that if we flush old strings,",not
spaCy/spacy/language.py,816,"we can recover the original ones. However, we only want to do this if we're",not
spaCy/spacy/language.py,817,"really adding strings, to save up-front costs.",not
spaCy/spacy/language.py,840,raw_texts is used later to stop iteration.,not
spaCy/spacy/language.py,842,for sending texts to worker,not
spaCy/spacy/language.py,844,for receiving byte-encoded docs from worker,not
spaCy/spacy/language.py,850,Sender sends texts to the workers.,not
spaCy/spacy/language.py,851,This is necessary to properly handle infinite length of texts.,not
spaCy/spacy/language.py,852,"(In this case, all data cannot be sent to the workers at once)",not
spaCy/spacy/language.py,854,send twice to make process busy,not
spaCy/spacy/language.py,875,Cycle channels not to break the order of docs.,not
spaCy/spacy/language.py,876,"The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.",not
spaCy/spacy/language.py,883,tell `sender` that one batch was consumed.,not
spaCy/spacy/language.py,953,Convert to list here in case exclude is (default) tuple,not
spaCy/spacy/language.py,1025,"NB: This decorator needs to live here, because it needs to write to",not
spaCy/spacy/language.py,1026,Language.factories. All other solutions would cause circular import.,not
spaCy/spacy/language.py,1063,TODO: Replace this once we handle vectors consistently as static,SATD
spaCy/spacy/language.py,1064,data,not
spaCy/spacy/language.py,1089,Important! Not deep copy -- we just want the container (but we also,not
spaCy/spacy/language.py,1090,want to support people providing arbitrarily typed nlp.pipeline,not
spaCy/spacy/language.py,1091,objects.),not
spaCy/spacy/language.py,1107,Don't change the pipeline if we're raising an error.,not
spaCy/spacy/language.py,1114,We added some args for pipe that __call__ doesn't expect.,not
spaCy/spacy/language.py,1141,"Connection does not accept unpickable objects, so send list.",not
spaCy/spacy/language.py,1159,cycle channels so that distribute the texts evenly,not
spaCy/spacy/glossary.py,1,coding: utf8,not
spaCy/spacy/glossary.py,21,POS tags,not
spaCy/spacy/glossary.py,22,Universal POS Tags,not
spaCy/spacy/glossary.py,23,http://universaldependencies.org/u/pos/,not
spaCy/spacy/glossary.py,44,POS tags (English),not
spaCy/spacy/glossary.py,45,OntoNotes 5 / Penn Treebank,not
spaCy/spacy/glossary.py,46,https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html,not
spaCy/spacy/glossary.py,102,POS Tags (German),not
spaCy/spacy/glossary.py,103,TIGER Treebank,not
spaCy/spacy/glossary.py,104,http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/tiger_introduction.pdf,not
spaCy/spacy/glossary.py,159,Noun chunks,not
spaCy/spacy/glossary.py,168,Dependency Labels (English),not
spaCy/spacy/glossary.py,169,ClearNLP / Universal Dependencies,not
spaCy/spacy/glossary.py,170,https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md,not
spaCy/spacy/glossary.py,241,Dependency labels (German),not
spaCy/spacy/glossary.py,242,TIGER Treebank,not
spaCy/spacy/glossary.py,243,http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/tiger_introduction.pdf,not
spaCy/spacy/glossary.py,244,currently missing: 'cc' (comparative complement) because of conflict,not
spaCy/spacy/glossary.py,245,with English labels,not
spaCy/spacy/glossary.py,288,Named Entity Recognition,not
spaCy/spacy/glossary.py,289,OntoNotes 5,not
spaCy/spacy/glossary.py,290,https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf,not
spaCy/spacy/glossary.py,310,Named Entity Recognition,not
spaCy/spacy/glossary.py,311,Wikipedia,not
spaCy/spacy/glossary.py,312,http://www.sciencedirect.com/science/article/pii/S0004370212000276,not
spaCy/spacy/glossary.py,313,https://pdfs.semanticscholar.org/5744/578cc243d92287f47448870bb426c66cc941.pdf,not
spaCy/spacy/glossary.py,316,https://github.com/ltgoslo/norne,not
spaCy/spacy/lemmatizer.py,1,coding: utf8,not
spaCy/spacy/lemmatizer.py,58,See Issue #435 for example of where this logic is requied.,not
spaCy/spacy/lemmatizer.py,87,This maps 'VBP' to base form -- probably just need 'IS_BASE',not
spaCy/spacy/lemmatizer.py,88,morphology,not
spaCy/spacy/lemmatizer.py,160,"Remove duplicates but preserve the ordering of applied ""rules""",not
spaCy/spacy/lemmatizer.py,162,"Put exceptions at the front of the list, so they get priority.",not
spaCy/spacy/lemmatizer.py,163,This is a dodgy heuristic -- but it's the best we can do until we get,not
spaCy/spacy/lemmatizer.py,164,"frequencies on this. We can at least prune out problematic exceptions,",not
spaCy/spacy/lemmatizer.py,165,if they shadow more frequent analyses.,not
spaCy/spacy/about.py,1,fmt: off,not
spaCy/spacy/lookups.py,1,coding: utf-8,not
spaCy/spacy/lookups.py,193,Assume a default size of 1M items,not
spaCy/spacy/lookups.py,245,"This can give a false positive, so we need to check it after",not
spaCy/spacy/errors.py,1,coding: utf8,not
spaCy/spacy/errors.py,15,fmt: off,not
spaCy/spacy/errors.py,572,fmt: on,not
spaCy/spacy/analysis.py,1,coding: utf8,not
spaCy/spacy/analysis.py,86,Support Span only for custom extension attributes,not
spaCy/spacy/analysis.py,91,first element is not doc/token/span,not
spaCy/spacy/analysis.py,94,"attr is something like ""doc""",not
spaCy/spacy/analysis.py,98,"attr is something like ""doc._""",not
spaCy/spacy/analysis.py,101,We don't check whether the attribute actually exists,not
spaCy/spacy/analysis.py,102,attr is something like doc._.x.y,not
spaCy/spacy/analysis.py,106,we can't validate those further,not
spaCy/spacy/analysis.py,107,"attr is something like ""token.pos_""",not
spaCy/spacy/analysis.py,109,attr is something like doc.x.y,not
spaCy/spacy/__init__.py,1,coding: utf8,not
spaCy/spacy/__init__.py,9,These are imported as part of the API,not
spaCy/spacy/_ml.py,1,coding: utf8,not
spaCy/spacy/_ml.py,33,Backwards compatibility with <2.2.2,not
spaCy/spacy/_ml.py,140,The dtype here matches what thinc is expecting -- which differs per,not
spaCy/spacy/_ml.py,141,platform (by int definition). This should be fixed once the problem,not
spaCy/spacy/_ml.py,142,is fixed on Thinc's side.,not
spaCy/spacy/_ml.py,199,Reuse the buffer,not
spaCy/spacy/_ml.py,204,"(o, p, f, i) --> (f, o, p, i)",not
spaCy/spacy/_ml.py,218,"(1, nF, nO, nP) += (nN, nF, nO, nP) where IDs (nN, nF) < 0",not
spaCy/spacy/_ml.py,250,nS ids. nW tokvecs. Exclude the padding array.,not
spaCy/spacy/_ml.py,251,"(nW, f, o, p)",not
spaCy/spacy/_ml.py,253,need nS vectors,not
spaCy/spacy/_ml.py,295,"Set an entry here, so that vectors are accessed by StaticVectors",not
spaCy/spacy/_ml.py,296,"(unideal, I know)",not
spaCy/spacy/_ml.py,300,This is a hack to avoid the problem in #3853.,SATD
spaCy/spacy/_ml.py,322,"Preserve prior tok2vec for backwards compat, in v2.2.2",not
spaCy/spacy/_ml.py,503,"Clip to range (-10, 10)",not
spaCy/spacy/_ml.py,675,TODO Make concatenate support lists,SATD
spaCy/spacy/_ml.py,767,context encoder,not
spaCy/spacy/_ml.py,803,pragma: no cover,not
spaCy/spacy/_ml.py,869,This needs to be here to avoid circular imports,not
spaCy/spacy/_ml.py,887,NB: If you change this implementation to instead modify,not
spaCy/spacy/_ml.py,888,"the docs in place, take care that the IDs reflect the original",SATD
spaCy/spacy/_ml.py,889,words. Currently we use the original docs to make the vectors,not
spaCy/spacy/_ml.py,890,"for the target, so we don't lose the original tokens. But if",not
spaCy/spacy/_ml.py,891,"you modified the docs in place here, you would.",SATD
spaCy/spacy/_ml.py,941,This assists in indexing; it's like looping over this dimension.,not
spaCy/spacy/_ml.py,942,Still consider this weird witch craft...But thanks to Mark Neumann,not
spaCy/spacy/_ml.py,943,for the tip.,not
spaCy/spacy/_ml.py,948,"Let's say I have a 2d array of indices, and a 3d table of data. What numpy",not
spaCy/spacy/_ml.py,949,incantation do I chant to get,not
spaCy/spacy/_ml.py,950,"output[i, j, k] == data[j, ids[i, j], k]?",not
spaCy/spacy/_ml.py,969,Find the zero vectors,not
spaCy/spacy/_ml.py,972,Add a small constant to avoid 0 vectors,not
spaCy/spacy/_ml.py,975,https://math.stackexchange.com/questions/1923613/partial-derivative-of-cosine-similarity,not
spaCy/spacy/_ml.py,983,"If the target was a zero vector, don't count it in the loss.",not
spaCy/spacy/displacy/templates.py,1,coding: utf8,not
spaCy/spacy/displacy/templates.py,5,Setting explicit height and max-width: none on the SVG is required for,not
spaCy/spacy/displacy/templates.py,6,Jupyter to render it properly in a cell,not
spaCy/spacy/displacy/__init__.py,1,coding: utf8,not
spaCy/spacy/displacy/__init__.py,59,return HTML rendered by IPython display(),not
spaCy/spacy/displacy/__init__.py,60,See #4840 for details on span wrapper to disable mathjax,not
spaCy/spacy/displacy/__init__.py,109,"Headers and status need to be bytes in Python 2, see #1227",not
spaCy/spacy/displacy/render.py,1,coding: utf8,not
spaCy/spacy/displacy/render.py,55,Create a random ID prefix to make sure parses don't receive the,not
spaCy/spacy/displacy/render.py,56,"same ID, even if they're identical",not
spaCy/spacy/tokens/underscore.py,1,coding: utf8,not
spaCy/spacy/tokens/underscore.py,19,"Assumption is that for doc values, _start and _end will both be None",not
spaCy/spacy/tokens/underscore.py,20,Span will set non-None values for _start and _end,not
spaCy/spacy/tokens/underscore.py,21,"Token will have _start be non-None, _end be None",not
spaCy/spacy/tokens/underscore.py,22,"This lets us key everything into the doc.user_data dictionary,",not
spaCy/spacy/tokens/underscore.py,23,"(see _get_key), and lets us use a single Underscore class.",not
spaCy/spacy/tokens/underscore.py,29,Hack to enable autocomplete on custom extensions,SATD
spaCy/spacy/tokens/underscore.py,41,Hack to port over docstrings of the original function,SATD
spaCy/spacy/tokens/underscore.py,42,See https://stackoverflow.com/q/27362727/6400719,not
spaCy/spacy/tokens/underscore.py,55,Handle mutable default arguments (see #2581),not
spaCy/spacy/tokens/underscore.py,119,"Extension is writable if it has a setter (getter + setter), if it has a",not
spaCy/spacy/tokens/underscore.py,120,"default value (or, if its default value is none, none of the other values",not
spaCy/spacy/tokens/underscore.py,121,should be set).,not
spaCy/spacy/tokens/_serialize.py,1,coding: utf8,not
spaCy/spacy/tokens/_serialize.py,58,Ensure ORTH is always attrs[0],not
spaCy/spacy/tokens/_serialize.py,82,this should never happen,not
spaCy/spacy/tokens/_serialize.py,139,this should never happen,not
spaCy/spacy/tokens/_serialize.py,176,this should never happen,not
spaCy/spacy/tokens/_serialize.py,204,"Compatibility, as we had named it this previously.",not
spaCy/spacy/tokens/__init__.py,1,coding: utf8,not
spaCy/spacy/matcher/_schemas.py,1,coding: utf8,not
spaCy/spacy/matcher/__init__.py,1,coding: utf8,not
spaCy/spacy/tests/test_lemmatizer.py,1,coding: utf8,not
spaCy/spacy/tests/test_lemmatizer.py,20,The update to the table should be reflected in the lemmatizer,not
spaCy/spacy/tests/test_lemmatizer.py,28,Make sure we have the previously saved lookup table,not
spaCy/spacy/tests/test_gold.py,1,coding: utf-8,not
spaCy/spacy/tests/test_gold.py,64,one-to-many,not
spaCy/spacy/tests/test_gold.py,76,many-to-one,not
spaCy/spacy/tests/test_gold.py,86,misaligned,not
spaCy/spacy/tests/test_gold.py,96,additional whitespace tokens in GoldParse words,not
spaCy/spacy/tests/test_gold.py,110,from issue #4791,not
spaCy/spacy/tests/test_gold.py,157,noqa: F841,not
spaCy/spacy/tests/test_gold.py,188,roundtrip to JSON,not
spaCy/spacy/tests/test_gold.py,207,roundtrip to JSONL train dicts,not
spaCy/spacy/tests/test_gold.py,226,roundtrip to JSONL tuples,not
spaCy/spacy/tests/test_gold.py,229,write to JSONL train dicts,not
spaCy/spacy/tests/test_gold.py,232,load and rewrite as JSONL tuples,not
spaCy/spacy/tests/test_gold.py,276,check symmetry,not
spaCy/spacy/tests/util.py,1,coding: utf-8,not
spaCy/spacy/tests/util.py,53,"if there are any other annotations, set them",not
spaCy/spacy/tests/util.py,75,"finally, set the entities",not
spaCy/spacy/tests/conftest.py,1,coding: utf-8,not
spaCy/spacy/tests/conftest.py,14,When using 'pytest --pyargs spacy' to test an installed copy of,not
spaCy/spacy/tests/conftest.py,15,"spacy, pytest skips running our pytest_addoption() hook. Later, when",not
spaCy/spacy/tests/conftest.py,16,"we call getoption(), pytest raises an error, because it doesn't",not
spaCy/spacy/tests/conftest.py,17,"recognize the option we're asking about. To avoid this, we need to",not
spaCy/spacy/tests/conftest.py,18,"pass a default value. We default to False, i.e., we act like all the",not
spaCy/spacy/tests/conftest.py,19,options weren't given.,not
spaCy/spacy/tests/conftest.py,27,Fixtures for language tokenizers (languages sorted alphabetically),not
spaCy/spacy/tests/test_pickles.py,1,coding: utf-8,not
spaCy/spacy/tests/test_language.py,1,coding: utf-8,not
spaCy/spacy/tests/test_language.py,33,Update with doc and gold objects,not
spaCy/spacy/tests/test_language.py,35,Update with text and dict,not
spaCy/spacy/tests/test_language.py,37,Update with doc object and dict,not
spaCy/spacy/tests/test_language.py,39,Update with text and gold object,not
spaCy/spacy/tests/test_language.py,41,Update badly,not
spaCy/spacy/tests/test_language.py,55,Evaluate with doc and gold objects,not
spaCy/spacy/tests/test_language.py,57,Evaluate with text and dict,not
spaCy/spacy/tests/test_language.py,59,Evaluate with doc object and dict,not
spaCy/spacy/tests/test_language.py,61,Evaluate with text and gold object,not
spaCy/spacy/tests/test_language.py,63,Evaluate badly,not
spaCy/spacy/tests/test_language.py,142,check if nlp.pipe can handle infinite length iterator properly.,not
spaCy/spacy/tests/test_tok2vec.py,1,coding: utf-8,not
spaCy/spacy/tests/test_tok2vec.py,17,"Make the words numbers, so that they're distnct",not
spaCy/spacy/tests/test_tok2vec.py,18,"across the batch, and easy to track.",not
spaCy/spacy/tests/test_tok2vec.py,25,This fails in Thinc v7.3.1. Need to push patch,not
spaCy/spacy/tests/test_scorer.py,1,coding: utf-8,not
spaCy/spacy/tests/test_scorer.py,47,Gold and Doc are identical,not
spaCy/spacy/tests/test_scorer.py,69,One dep is incorrect in Doc,not
spaCy/spacy/tests/test_scorer.py,94,Gold and Doc are identical,not
spaCy/spacy/tests/test_scorer.py,113,Doc has one missing and one extra entity,not
spaCy/spacy/tests/test_scorer.py,114,Entity type MONEY is not present in Doc,not
spaCy/spacy/tests/test_scorer.py,144,"Binary classification, toy tests from scikit-learn test suite",not
spaCy/spacy/tests/test_scorer.py,185,same result as above with ROCAUCScore wrapper,not
spaCy/spacy/tests/test_scorer.py,191,check that errors are raised in undefined cases and score is -inf,not
spaCy/spacy/tests/test_cli.py,1,coding: utf-8,not
spaCy/spacy/tests/test_cli.py,12,https://raw.githubusercontent.com/ohenrik/nb_news_ud_sm/master/original_data/no-ud-dev-ner.conllu,not
spaCy/spacy/tests/test_cli.py,52,fmt: off,not
spaCy/spacy/tests/test_cli.py,55,fmt: on,not
spaCy/spacy/tests/test_cli.py,118,fmt: off,not
spaCy/spacy/tests/test_cli.py,121,fmt: on,not
spaCy/spacy/tests/test_architectures.py,1,coding: utf8,not
spaCy/spacy/tests/test_json_schemas.py,1,coding: utf-8,not
spaCy/spacy/tests/test_displacy.py,1,coding: utf-8,not
spaCy/spacy/tests/test_displacy.py,71,Source: http://www.sobhe.ir/hazm/ – is this correct?,not
spaCy/spacy/tests/test_displacy.py,73,"These are (likely) wrong, but it's just for testing",not
spaCy/spacy/tests/test_displacy.py,74,needs to match lang.fa.tag_map,not
spaCy/spacy/tests/test_displacy.py,101,Restore,not
spaCy/spacy/tests/test_misc.py,1,coding: utf-8,not
spaCy/spacy/tests/test_misc.py,29,yield -- need to cleanup even if assertion fails,not
spaCy/spacy/tests/test_misc.py,30,https://github.com/pytest-dev/pytest/issues/2508#issuecomment-309934240,not
spaCy/spacy/tests/test_misc.py,33,Remove symlink only if it was created,not
spaCy/spacy/tests/test_misc.py,99,noqa: F401,not
spaCy/spacy/tests/test_misc.py,106,noqa: F401,not
spaCy/spacy/tests/regression/test_issue3555.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4054.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,21,"Problem: The dot is now properly split off, but the prefix/suffix rules",not
spaCy/spacy/tests/regression/test_issue2001-2500.py,22,are not applied again afterwards. This means that the quote will still be,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,23,attached to the remaining token.,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,51,Work around lemma corruption problem and set lemmas after tags,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,56,"We need to serialize both tag and lemma, since this is what causes the bug",not
spaCy/spacy/tests/regression/test_issue2001-2500.py,83,fix bug in labels with a 'b' character,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,86,maintain support for iob1 format,not
spaCy/spacy/tests/regression/test_issue2001-2500.py,89,maintain support for iob2 format,not
spaCy/spacy/tests/regression/test_issue3972.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3972.py,19,We should have a match for each of the two rules,not
spaCy/spacy/tests/regression/test_issue4707.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3962.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3962.py,33,"""jests at scars ,""",not
spaCy/spacy/tests/regression/test_issue3962.py,40,"head set to itself, being the new artificial root",not
spaCy/spacy/tests/regression/test_issue3962.py,46,head set to the new artificial root,not
spaCy/spacy/tests/regression/test_issue3962.py,49,We should still have 1 sentence,not
spaCy/spacy/tests/regression/test_issue3962.py,52,"""never felt a""",not
spaCy/spacy/tests/regression/test_issue3962.py,61,head set to ancestor,not
spaCy/spacy/tests/regression/test_issue3962.py,64,"We should still have 1 sentence as ""a"" can be attached to ""felt"" instead of ""wound""",not
spaCy/spacy/tests/regression/test_issue3962.py,92,"""jests at scars. They never""",not
spaCy/spacy/tests/regression/test_issue3962.py,99,"head set to itself, being the new artificial root (in sentence 1)",not
spaCy/spacy/tests/regression/test_issue3962.py,109,"head set to itself, being the new artificial root (in sentence 2)",not
spaCy/spacy/tests/regression/test_issue3962.py,113,head set to the new artificial head (in sentence 2),not
spaCy/spacy/tests/regression/test_issue3962.py,116,We should still have 2 sentences,not
spaCy/spacy/tests/regression/test_issue3839.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3869.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4725.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4725.py,11,ensures that this runs correctly and doesn't hang or crash because of the global vectors,not
spaCy/spacy/tests/regression/test_issue4002.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3611.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3611.py,18,preparing the data,not
spaCy/spacy/tests/regression/test_issue3611.py,24,set up the spacy model with a text categorizer component,not
spaCy/spacy/tests/regression/test_issue3611.py,36,training the network,not
spaCy/spacy/tests/regression/test_issue3625.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4529.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4674.py,1,coding: utf-8,not
spaCy/spacy/tests/regression/test_issue4674.py,28,dumping to file & loading back in,not
spaCy/spacy/tests/regression/test_issue3879.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3879.py,14,fails because of a FP match 'is a test',not
spaCy/spacy/tests/regression/test_issue3803.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4373.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4924.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3531.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4348.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4651.py,1,coding: utf-8,not
spaCy/spacy/tests/regression/test_issue4030.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4030.py,18,preparing the data,not
spaCy/spacy/tests/regression/test_issue4030.py,24,set up the spacy model with a text categorizer component,not
spaCy/spacy/tests/regression/test_issue4030.py,36,training the network,not
spaCy/spacy/tests/regression/test_issue4030.py,53,processing of an empty doc should result in 0.0 for all categories,not
spaCy/spacy/tests/regression/test_issue3521.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3521.py,10,"'not' and 'would' should be stopwords, also in their abbreviated forms",not
spaCy/spacy/tests/regression/test_issue1001-1500.py,1,coding: utf-8,not
spaCy/spacy/tests/regression/test_issue1001-1500.py,30,"For sanity, check it works when pipeline is clean.",not
spaCy/spacy/tests/regression/test_issue3001-3500.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3001-3500.py,79,Serializing then deserializing,not
spaCy/spacy/tests/regression/test_issue3001-3500.py,201,Add the OUT action. I wouldn't have thought this would be necessary...,not
spaCy/spacy/tests/regression/test_issue3001-3500.py,205,"Get into the state just before ""New""",not
spaCy/spacy/tests/regression/test_issue3001-3500.py,210,Check that B-GPE is valid.,not
spaCy/spacy/tests/regression/test_issue3001-3500.py,215,"If we have this test in Python 3, pytest chokes, as it can't print the",not
spaCy/spacy/tests/regression/test_issue3001-3500.py,216,string above in the xpass message.,not
spaCy/spacy/tests/regression/test_issue3001-3500.py,332,this crashed because of a padding error in layer.ops.unflatten in thinc,not
spaCy/spacy/tests/regression/test_issue4042.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4042.py,17,add ner pipe,not
spaCy/spacy/tests/regression/test_issue4042.py,23,Add entity ruler,not
spaCy/spacy/tests/regression/test_issue4042.py,30,"works fine with ""after""",not
spaCy/spacy/tests/regression/test_issue4042.py,53,add ner pipe,not
spaCy/spacy/tests/regression/test_issue4042.py,59,add a new label to the doc,not
spaCy/spacy/tests/regression/test_issue4042.py,66,reapply the NER - at this point it should resize itself,not
spaCy/spacy/tests/regression/test_issue4042.py,73,assert IO goes fine,not
spaCy/spacy/tests/regression/test_issue5082.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue5082.py,10,Ensure the 'merge_entities' pipeline does something sensible for the vectors of the merged tokens,not
spaCy/spacy/tests/regression/test_issue5048.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4402.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4402.py,21,assert that the data got split into 4 sentences,not
spaCy/spacy/tests/regression/test_issue4120.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4120.py,13,works,not
spaCy/spacy/tests/regression/test_issue4120.py,16,fixed,not
spaCy/spacy/tests/regression/test_issue4120.py,21,works,not
spaCy/spacy/tests/regression/test_issue4120.py,26,fixed,not
spaCy/spacy/tests/regression/test_issue3526.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4903.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4903.py,32,ensures that this runs correctly and doesn't hang or crash on Windows / macOS,not
spaCy/spacy/tests/regression/test_issue4590.py,1,coding: utf-8,not
spaCy/spacy/tests/regression/test_issue1-1000.py,1,coding: utf-8,not
spaCy/spacy/tests/regression/test_issue1-1000.py,89,"One token can only be part of one entity, so test that the matches",not
spaCy/spacy/tests/regression/test_issue1-1000.py,90,can't be added as entities,not
spaCy/spacy/tests/regression/test_issue1-1000.py,413,Skip test if pytest-timeout is not installed,not
spaCy/spacy/tests/regression/test_issue3882.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3549.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4190.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4190.py,13,Load default language,not
spaCy/spacy/tests/regression/test_issue4190.py,16,noqa: F841,not
spaCy/spacy/tests/regression/test_issue4190.py,17,Modify tokenizer,not
spaCy/spacy/tests/regression/test_issue4190.py,21,Save and Reload,not
spaCy/spacy/tests/regression/test_issue4190.py,25,This should be the modified tokenizer,not
spaCy/spacy/tests/regression/test_issue4190.py,35,Remove all exceptions where a single letter is followed by a period (e.g. 'h.'),not
spaCy/spacy/tests/regression/test_issue4272.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3880.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4367.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3951.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3540.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4313.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4313.py,22,add a new label to the doc,not
spaCy/spacy/tests/regression/test_issue4313.py,29,ensure the beam_parse still works with the new label,not
spaCy/spacy/tests/regression/test_issue3959.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue3959.py,19,usually this is already True when starting from proper models instead of blank English,not
spaCy/spacy/tests/regression/test_issue4278.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4267.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4267.py,18,assert that we have correct IOB annotations,not
spaCy/spacy/tests/regression/test_issue4267.py,24,add entity ruler and run again,not
spaCy/spacy/tests/regression/test_issue4267.py,33,assert that we still have correct IOB annotations,not
spaCy/spacy/tests/regression/test_issue4133.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4133.py,18,usually this is already True when starting from proper models instead of blank English,not
spaCy/spacy/tests/regression/test_issue4849.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4849.py,27,USING 1 PROCESS,not
spaCy/spacy/tests/regression/test_issue4849.py,33,USING 2 PROCESSES,not
spaCy/spacy/tests/regression/test_issue2501-3000.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue2501-3000.py,26,initialise weights,not
spaCy/spacy/tests/regression/test_issue2501-3000.py,125,A tree with a non-projective (i.e. crossing) arc,not
spaCy/spacy/tests/regression/test_issue2501-3000.py,126,"The arcs (0, 4) and (2, 9) cross.",not
spaCy/spacy/tests/regression/test_issue2501-3000.py,192,noqa: F841,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,37,We should run cleanup more than one time to actually cleanup data.,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,38,In first run — clean up only mark strings as «not hitted».,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,69,"TODO: Currently segfaulting, due to l_edge and r_edge misalignment",SATD
spaCy/spacy/tests/regression/test_issue1501-2000.py,70,def test_issue1537_model():,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,71,nlp = load_spacy('en'),not
spaCy/spacy/tests/regression/test_issue1501-2000.py,72,doc = nlp('The sky is blue. The man is pink. The dog is purple.'),not
spaCy/spacy/tests/regression/test_issue1501-2000.py,73,sents = [s.as_doc() for s in doc.sents],not
spaCy/spacy/tests/regression/test_issue1501-2000.py,74,print(list(sents[0].noun_chunks)),not
spaCy/spacy/tests/regression/test_issue1501-2000.py,75,print(list(sents[1].noun_chunks)),not
spaCy/spacy/tests/regression/test_issue1501-2000.py,241,should error out,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,254,we should see two overlapping matches here,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,279,Possibly related to #2675 and #2671?,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,290,"We could also assert length 1 here, but this is more conclusive, because",not
spaCy/spacy/tests/regression/test_issue1501-2000.py,291,the real problem here is that it returns a duplicate match for a match_id,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,292,that's not actually in the vocab!,not
spaCy/spacy/tests/regression/test_issue1501-2000.py,300,"{""IN"": [""EUR""]}}]",not
spaCy/spacy/tests/regression/test_issue1501-2000.py,331,Uncommenting this caused a segmentation fault,not
spaCy/spacy/tests/regression/test_issue5152.py,5,"Test that the comparison between a Span and a Token, goes well",not
spaCy/spacy/tests/regression/test_issue5152.py,6,There was a bug when the number of tokens in the span equaled the number of characters in the token (!),not
spaCy/spacy/tests/regression/test_issue5152.py,12,Talk about being,not
spaCy/spacy/tests/regression/test_issue5152.py,13,Talk about being,not
spaCy/spacy/tests/regression/test_issue5152.py,14,Talk of being,not
spaCy/spacy/tests/regression/test_issue5152.py,15,Let,not
spaCy/spacy/tests/regression/test_issue4528.py,1,coding: utf8,not
spaCy/spacy/tests/regression/test_issue4528.py,11,This is how extension attribute values are stored in the user data,not
spaCy/spacy/tests/morphology/test_morph_features.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_language.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,11,baseline assertions,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,15,dumping to file & loading back in,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,26,final assertions,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,50,check entities,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,57,check aliases,not
spaCy/spacy/tests/serialize/test_serialize_kb.py,64,check candidates & probabilities,not
spaCy/spacy/tests/serialize/test_serialize_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_vocab_strings.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_vocab_strings.py,75,Reported in #2153,not
spaCy/spacy/tests/serialize/test_serialize_extension_attrs.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_doc.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_doc.py,77,"Deserialize later, e.g. in a new process",not
spaCy/spacy/tests/serialize/test_serialize_pipeline.py,1,coding: utf-8,not
spaCy/spacy/tests/serialize/test_serialize_pipeline.py,118,See issue #1105,not
spaCy/spacy/tests/vocab_vectors/test_vocab_api.py,1,coding: utf-8,not
spaCy/spacy/tests/vocab_vectors/test_vocab_api.py,45,noqa: F841,not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,1,coding: utf-8,not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,95,decrease vector dimension (truncate),not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,105,increase vector dimension (pad with zeros),not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,160,not 1.0000002,not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,164,not 0.9999999,not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,313,noqa: F841,not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,314,noqa: F841,not
spaCy/spacy/tests/vocab_vectors/test_vectors.py,315,noqa: F841,not
spaCy/spacy/tests/vocab_vectors/test_lexeme.py,1,coding: utf-8,not
spaCy/spacy/tests/vocab_vectors/test_lookups.py,1,coding: utf-8,not
spaCy/spacy/tests/vocab_vectors/test_similarity.py,1,coding: utf-8,not
spaCy/spacy/tests/vocab_vectors/test_stringstore.py,1,coding: utf-8,not
spaCy/spacy/tests/matcher/test_matcher_logic.py,1,coding: utf-8,not
spaCy/spacy/tests/matcher/test_matcher_logic.py,160,should give two matches,not
spaCy/spacy/tests/matcher/test_matcher_logic.py,164,removing once should work,not
spaCy/spacy/tests/matcher/test_matcher_logic.py,167,should not return any maches anymore,not
spaCy/spacy/tests/matcher/test_matcher_logic.py,171,removing again should throw an error,not
spaCy/spacy/tests/matcher/test_matcher_api.py,1,coding: utf-8,not
spaCy/spacy/tests/matcher/test_matcher_api.py,9,noqa: F401,not
spaCy/spacy/tests/matcher/test_matcher_api.py,80,"New API: add(key: str, patterns: List[List[dict]], on_match: Callable)",not
spaCy/spacy/tests/matcher/test_matcher_api.py,374,"def test_dependency_matcher(dependency_matcher, text, heads, deps):",not
spaCy/spacy/tests/matcher/test_matcher_api.py,375,"doc = get_doc(dependency_matcher.vocab, text.split(), heads=heads, deps=deps)",not
spaCy/spacy/tests/matcher/test_matcher_api.py,376,matches = dependency_matcher(doc),not
spaCy/spacy/tests/matcher/test_matcher_api.py,377,"assert matches[0][1] == [[3, 1, 2]]",not
spaCy/spacy/tests/matcher/test_matcher_api.py,378,"assert matches[1][1] == [[4, 3, 3]]",not
spaCy/spacy/tests/matcher/test_matcher_api.py,379,"assert matches[2][1] == [[4, 3, 2]]",not
spaCy/spacy/tests/matcher/test_matcher_api.py,384,Potential mistake: pass in pattern instead of list of patterns,not
spaCy/spacy/tests/matcher/test_matcher_api.py,396,DEP requires is_parsed,not
spaCy/spacy/tests/matcher/test_matcher_api.py,404,"TAG, POS, LEMMA require is_tagged",not
spaCy/spacy/tests/matcher/test_matcher_api.py,413,TEXT/ORTH only require tokens,not
spaCy/spacy/tests/matcher/test_pattern_validation.py,1,coding: utf-8,not
spaCy/spacy/tests/matcher/test_pattern_validation.py,10,"(pattern, num errors with validation, num errors identified with minimal",not
spaCy/spacy/tests/matcher/test_pattern_validation.py,11,checks),not
spaCy/spacy/tests/matcher/test_pattern_validation.py,13,Bad patterns flagged in all cases,not
spaCy/spacy/tests/matcher/test_pattern_validation.py,20,Bad patterns flagged outside of Matcher,not
spaCy/spacy/tests/matcher/test_pattern_validation.py,22,Bad patterns not flagged with minimal checks,not
spaCy/spacy/tests/matcher/test_pattern_validation.py,29,Good patterns,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,1,coding: utf-8,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,13,intermediate phrase,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,18,initial token,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,23,initial phrase,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,28,final token,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,33,final phrase,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,67,"New API: add(key: str, patterns: List[List[dict]], on_match: Callable)",not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,80,match ID only gets added once,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,121,TEST2 is added alongside TEST,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,127,removing TEST does not remove the entry for TEST2,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,133,removing TEST2 removes all,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,221,DEP requires is_parsed,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,228,"TAG, POS, LEMMA require is_tagged",not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,236,TEXT/ORTH only require tokens,not
spaCy/spacy/tests/matcher/test_phrase_matcher.py,265,Potential mistake: pass in pattern instead of list of patterns,not
spaCy/spacy/tests/doc/test_add_entities.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_token_api.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_token_api.py,16,fmt: off,not
spaCy/spacy/tests/doc/test_token_api.py,21,fmt: on,not
spaCy/spacy/tests/doc/test_token_api.py,57,"TODO: Test more of these, esp. if a bug is found",SATD
spaCy/spacy/tests/doc/test_token_api.py,104,the structure of this sentence depends on the English annotation scheme,not
spaCy/spacy/tests/doc/test_token_api.py,118,the structure of this sentence depends on the English annotation scheme,not
spaCy/spacy/tests/doc/test_token_api.py,170,head token must be from the same document,not
spaCy/spacy/tests/doc/test_underscore.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_underscore.py,12,"reset the Underscore object after the test, to avoid having state copied across tests",not
spaCy/spacy/tests/doc/test_retokenize_split.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_retokenize_split.py,61,Not enough heads,not
spaCy/spacy/tests/doc/test_retokenize_split.py,66,Too many heads,not
spaCy/spacy/tests/doc/test_retokenize_split.py,73,Test entity IOB stays consistent after merging,not
spaCy/spacy/tests/doc/test_retokenize_split.py,88,fmt: off,not
spaCy/spacy/tests/doc/test_retokenize_split.py,94,fmt: on,not
spaCy/spacy/tests/doc/test_retokenize_split.py,150,Overwriting getter without setter,not
spaCy/spacy/tests/doc/test_retokenize_split.py,151,Overwriting method,not
spaCy/spacy/tests/doc/test_retokenize_split.py,152,Overwriting nonexistent attribute,not
spaCy/spacy/tests/doc/test_retokenize_split.py,153,Combination,not
spaCy/spacy/tests/doc/test_retokenize_split.py,154,Combination,not
spaCy/spacy/tests/doc/test_retokenize_split.py,155,Not a list of dicts,not
spaCy/spacy/tests/doc/test_pickle_doc.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_pickle_doc.py,19,noqa: F841,not
spaCy/spacy/tests/doc/test_span.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_span.py,15,fmt: off,not
spaCy/spacy/tests/doc/test_span.py,20,fmt: on,not
spaCy/spacy/tests/doc/test_span.py,90,test on manual sbd,not
spaCy/spacy/tests/doc/test_span.py,103,the & the -> the,not
spaCy/spacy/tests/doc/test_span.py,104,the & lazy -> dog (out of span),not
spaCy/spacy/tests/doc/test_span.py,105,lazy & the -> dog (out of span),not
spaCy/spacy/tests/doc/test_span.py,106,lazy & lazy -> lazy,not
spaCy/spacy/tests/doc/test_span.py,110,lazy & lazy -> lazy,not
spaCy/spacy/tests/doc/test_span.py,111,lazy & dog -> dog,not
spaCy/spacy/tests/doc/test_span.py,112,lazy & slept -> slept,not
spaCy/spacy/tests/doc/test_span.py,116,dog & dog -> dog,not
spaCy/spacy/tests/doc/test_span.py,117,dog & slept -> slept,not
spaCy/spacy/tests/doc/test_span.py,118,slept & dog -> slept,not
spaCy/spacy/tests/doc/test_span.py,119,slept & slept -> slept,not
spaCy/spacy/tests/doc/test_span.py,239,"First sentence, also tests start of sentence",not
spaCy/spacy/tests/doc/test_span.py,244,Second sentence,not
spaCy/spacy/tests/doc/test_span.py,250,"Third sentence ents, Also tests end of sentence",not
spaCy/spacy/tests/doc/test_span.py,258,Test filtering duplicates,not
spaCy/spacy/tests/doc/test_span.py,265,Test filtering overlaps with longest preference,not
spaCy/spacy/tests/doc/test_span.py,273,Test filtering overlaps with earlier preference for identical length,not
spaCy/spacy/tests/doc/test_morphanalysis.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_to_json.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_to_json.py,32,character offset!,not
spaCy/spacy/tests/doc/test_to_json.py,33,character offset!,not
spaCy/spacy/tests/doc/test_doc_api.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_doc_api.py,17,"Get the tokens in this order, so their ID ordering doesn't match the idx",not
spaCy/spacy/tests/doc/test_doc_api.py,149,Example that caused run-time error while parsing Reddit,not
spaCy/spacy/tests/doc/test_doc_api.py,150,fmt: off,not
spaCy/spacy/tests/doc/test_doc_api.py,155,fmt: on,not
spaCy/spacy/tests/doc/test_doc_api.py,176,fmt: off,not
spaCy/spacy/tests/doc/test_doc_api.py,180,fmt: on,not
spaCy/spacy/tests/doc/test_doc_api.py,266,Test creating doc from array with unknown values,not
spaCy/spacy/tests/doc/test_doc_api.py,270,Test serialization,not
spaCy/spacy/tests/doc/test_doc_api.py,278,fmt: off,not
spaCy/spacy/tests/doc/test_doc_api.py,280,fmt: on,not
spaCy/spacy/tests/doc/test_array.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_array.py,87,correct,not
spaCy/spacy/tests/doc/test_array.py,92,head before start,not
spaCy/spacy/tests/doc/test_array.py,99,head after end,not
spaCy/spacy/tests/doc/test_creation.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_creation.py,45,no whitespace in words,not
spaCy/spacy/tests/doc/test_creation.py,55,partial whitespace in words,not
spaCy/spacy/tests/doc/test_creation.py,65,non-standard whitespace tokens,not
spaCy/spacy/tests/doc/test_creation.py,75,mismatch between words and text,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,1,coding: utf-8,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,62,test both string and integer attributes and values,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,212,fmt: off,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,217,fmt: on,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,228,check looping is ok,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,233,Test entity IOB stays consistent after merging,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,250,Test that IOB stays consistent with provided IOB,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,260,"if no parse/heads, the first word in the span is the root and provides",not
spaCy/spacy/tests/doc/test_retokenize_merge.py,261,default values,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,282,"if there is a parse, span.root provides default values",not
spaCy/spacy/tests/doc/test_retokenize_merge.py,291,root of 'c d' is d,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,292,root is 'e f' is e,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,305,check that B is preserved if span[start] is B,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,322,fmt: off,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,328,fmt: on,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,343,fmt: off,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,349,fmt: on,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,364,Test regular merging,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,371,Test bulk merging,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,399,Test regular merging,not
spaCy/spacy/tests/doc/test_retokenize_merge.py,406,Test bulk merging,not
spaCy/spacy/tests/parser/test_ner.py,1,coding: utf-8,not
spaCy/spacy/tests/parser/test_ner.py,132,1. test normal behaviour,not
spaCy/spacy/tests/parser/test_ner.py,139,Add the OUT action,not
spaCy/spacy/tests/parser/test_ner.py,142,"Get into the state just before ""New""",not
spaCy/spacy/tests/parser/test_ner.py,147,Check that B-GPE is valid.,not
spaCy/spacy/tests/parser/test_ner.py,150,2. test blocking behaviour,not
spaCy/spacy/tests/parser/test_ner.py,155,"set ""New York"" to a blocked entity",not
spaCy/spacy/tests/parser/test_ner.py,160,Check that B-GPE is now invalid.,not
spaCy/spacy/tests/parser/test_ner.py,168,"we can only use U- for ""New""",not
spaCy/spacy/tests/parser/test_ner.py,172,"we can only use U- for ""York""",not
spaCy/spacy/tests/parser/test_ner.py,183,The untrained NER will predict O for each token,not
spaCy/spacy/tests/parser/test_ner.py,188,Check that a new ner can overwrite O,not
spaCy/spacy/tests/parser/test_ner.py,204,"1 : Entity Ruler - should set ""this"" to B and everything else to empty",not
spaCy/spacy/tests/parser/test_ner.py,210,2: untrained NER - should set everything else to O,not
spaCy/spacy/tests/parser/test_ner.py,227,1: untrained NER - should set everything to O,not
spaCy/spacy/tests/parser/test_ner.py,233,"2 : Entity Ruler - should set ""this"" to B and keep everything else O",not
spaCy/spacy/tests/parser/test_ner.py,248,"block ""Antti L Korhonen"" from being a named entity",not
spaCy/spacy/tests/parser/test_ner.py,263,Test the default number features,not
spaCy/spacy/tests/parser/test_ner.py,270,Test we can change it,not
spaCy/spacy/tests/parser/test_ner.py,279,Test the model runs,not
spaCy/spacy/tests/parser/test_parse_navigate.py,1,coding: utf-8,not
spaCy/spacy/tests/parser/test_parse_navigate.py,36,fmt: off,not
spaCy/spacy/tests/parser/test_parse_navigate.py,54,fmt: on,not
spaCy/spacy/tests/parser/test_parse.py,1,coding: utf-8,not
spaCy/spacy/tests/parser/test_parse.py,28,noqa: F841,not
spaCy/spacy/tests/parser/test_parse.py,36,"heads = [1, 0, 1, -2, -3, -1, -5]",not
spaCy/spacy/tests/parser/test_parse.py,82,right branching,not
spaCy/spacy/tests/parser/test_parse.py,117,left branching,not
spaCy/spacy/tests/parser/test_parse.py,154,fmt: off,not
spaCy/spacy/tests/parser/test_parse.py,158,fmt: on,not
spaCy/spacy/tests/parser/test_add_label.py,1,coding: utf8,not
spaCy/spacy/tests/parser/test_neural_parser.py,1,coding: utf8,not
spaCy/spacy/tests/parser/test_nonproj.py,1,coding: utf-8,not
spaCy/spacy/tests/parser/test_nonproj.py,93,fmt: off,not
spaCy/spacy/tests/parser/test_nonproj.py,99,fmt: on,not
spaCy/spacy/tests/parser/test_nonproj.py,112,fmt: off,not
spaCy/spacy/tests/parser/test_nonproj.py,132,if decoration is wrong such that there is no head with the desired label,not
spaCy/spacy/tests/parser/test_nonproj.py,133,the structure is kept and the label is undecorated,not
spaCy/spacy/tests/parser/test_nonproj.py,143,"if there are two potential new heads, the first one is chosen even if",not
spaCy/spacy/tests/parser/test_nonproj.py,144,"it""s wrong",not
spaCy/spacy/tests/parser/test_nonproj.py,155,fmt: on,not
spaCy/spacy/tests/parser/test_preset_sbd.py,1,coding: utf8,not
spaCy/spacy/tests/parser/test_preset_sbd.py,24,parser.add_label('right'),not
spaCy/spacy/tests/parser/test_arc_eager_oracle.py,1,coding: utf8,not
spaCy/spacy/tests/parser/test_arc_eager_oracle.py,72,Check gold moves is 0 cost,not
spaCy/spacy/tests/parser/test_space_attachment.py,1,coding: utf-8,not
spaCy/spacy/tests/parser/test_space_attachment.py,22,fmt: off,not
spaCy/spacy/tests/parser/test_space_attachment.py,28,fmt: on,not
spaCy/spacy/tests/parser/test_space_attachment.py,74,noqa: F841,not
spaCy/spacy/tests/parser/test_nn_beam.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_factories.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_tagger.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_entity_ruler.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_entity_ruler.py,141,invalid pattern raises error without validate,not
spaCy/spacy/tests/pipeline/test_entity_ruler.py,145,valid pattern is added without errors with validate,not
spaCy/spacy/tests/pipeline/test_entity_ruler.py,148,invalid pattern raises error with validate,not
spaCy/spacy/tests/pipeline/test_textcat.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,1,coding: utf-8,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,26,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,31,adding aliases,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,35,test the size of the corresponding KB,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,39,test retrieval of the entity vectors,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,44,test retrieval of prior probabilities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,55,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,60,adding aliases - should fail because one of the given IDs is not valid,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,71,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,76,adding aliases - should fail because the sum of the probabilities exceeds 1,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,85,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,90,adding aliases - should fail because the entities and probabilities vectors are not of equal length,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,101,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,104,this should fail because the kb's expected entity vector length is 3,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,113,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,118,adding aliases,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,122,test the size of the relevant candidates,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,127,test the content of the candidates,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,138,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,143,adding aliases,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,147,test the size of the relevant candidates,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,150,append an alias,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,153,test the size of the relevant candidates has been incremented,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,156,append the same alias-entity pair again should not work (will throw a warning),not
spaCy/spacy/tests/pipeline/test_entity_linker.py,160,test the size of the relevant candidates remained unchanged,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,168,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,173,adding aliases,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,177,append an alias - should fail because the entities and probabilities vectors are not of equal length,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,186,adding entities,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,190,adding aliases,not
spaCy/spacy/tests/pipeline/test_entity_linker.py,194,"set up pipeline with NER (Entity Ruler) and NEL (prior probability only, model not trained)",not
spaCy/spacy/tests/pipeline/test_entity_linker.py,213,test whether the entity links are preserved by the `as_doc()` function,not
spaCy/spacy/tests/pipeline/test_analysis.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_analysis.py,119,"The first argument here is the class itself, so we're accepting any here",not
spaCy/spacy/tests/pipeline/test_functions.py,1,coding: utf-8,not
spaCy/spacy/tests/pipeline/test_functions.py,11,fmt: off,not
spaCy/spacy/tests/pipeline/test_functions.py,16,fmt: on,not
spaCy/spacy/tests/pipeline/test_functions.py,23,"get_doc() doesn't set spaces, so the result is ""And a third .""",not
spaCy/spacy/tests/pipeline/test_sentencizer.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,53,The expected result here is that the duplicate punctuation gets merged,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,54,onto the same sentence and no one-token sentence is created for them.,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,61,We also want to make sure ¡ and ¿ aren't treated as sentence end,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,62,"markers, even though they're punctuation",not
spaCy/spacy/tests/pipeline/test_sentencizer.py,69,The Token.is_punct check ensures that quotes are handled as well,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,98,"Even thought it's not common, the punct_chars should be able to",not
spaCy/spacy/tests/pipeline/test_sentencizer.py,99,handle any tokens,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,129,fmt: off,not
spaCy/spacy/tests/pipeline/test_sentencizer.py,141,fmt: on,not
spaCy/spacy/tests/pipeline/test_pipe_methods.py,1,coding: utf8,not
spaCy/spacy/tests/pipeline/test_pipe_methods.py,137,Tagger always has the default coarse-grained label scheme,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,1,coding: utf-8,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,6,"Examples taken from the ""Big List of Naughty Strings""",not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,7,https://github.com/minimaxir/big-list-of-naughty-strings,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,11,ASCII punctuation,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,15,"Unicode additional control characters, byte order marks",not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,18,Unicode Symbols,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,30,Unicode Subscript/Superscript/Accents,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,35,Two-Byte Characters,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,45,Japanese Emoticons,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,56,Emoji,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,65,Regional Indicator Symbols,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,69,Unicode Numbers,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,72,Right-To-Left Strings,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,80,Trick Unicode,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,86,Zalgo Text,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,92,Unicode Upsidedown,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,95,Unicode font,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,104,File paths,not
spaCy/spacy/tests/tokenizer/test_naughty_strings.py,107,iOS Vulnerabilities,not
spaCy/spacy/tests/tokenizer/test_urls.py,1,coding: utf-8,not
spaCy/spacy/tests/tokenizer/test_urls.py,20,URL SHOULD_MATCH and SHOULD_NOT_MATCH patterns courtesy of https://mathiasbynens.be/demo/url-regex,not
spaCy/spacy/tests/tokenizer/test_urls.py,49,this is a legit domain name see: https://gist.github.com/dperini/729294 comment on 9/9/2014,not
spaCy/spacy/tests/tokenizer/test_urls.py,115,Punctuation we want to check is split away before the URL,not
spaCy/spacy/tests/tokenizer/test_urls.py,119,Punctuation we want to check is split away after the URL,not
spaCy/spacy/tests/tokenizer/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/tokenizer/test_whitespace.py,1,coding: utf-8,not
spaCy/spacy/tests/tokenizer/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/tokenizer/test_exceptions.py,9,Tweebo challenge (CMU),not
spaCy/spacy/tests/tokenizer/test_exceptions.py,46,"These break on narrow unicode builds, e.g. Windows",not
spaCy/spacy/tests/tokenizer/test_explain.py,1,coding: utf-8,not
spaCy/spacy/tests/tokenizer/test_explain.py,7,Only include languages with no external dependencies,not
spaCy/spacy/tests/tokenizer/test_explain.py,8,"""is"" seems to confuse importlib, so we're also excluding it for now",not
spaCy/spacy/tests/tokenizer/test_explain.py,9,"excluded: ja, ru, th, uk, vi, zh, is",not
spaCy/spacy/tests/lang/test_initialize.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/test_initialize.py,8,fmt: off,not
spaCy/spacy/tests/lang/test_initialize.py,9,Only include languages with no external dependencies,not
spaCy/spacy/tests/lang/test_initialize.py,10,"excluded: ja, ru, th, uk, vi, zh",not
spaCy/spacy/tests/lang/test_initialize.py,15,fmt: on,not
spaCy/spacy/tests/lang/test_initialize.py,22,Check for stray print statements (see #3342),not
spaCy/spacy/tests/lang/test_initialize.py,23,noqa: F841,not
spaCy/spacy/tests/lang/test_attrs.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ar/test_text.py,1,coding: utf8,not
spaCy/spacy/tests/lang/ar/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/pl/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/pl/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/gu/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/da/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/da/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/da/test_exceptions.py,61,note: skipping due to weirdness in UD_Danish-DDT,not
spaCy/spacy/tests/lang/da/test_exceptions.py,62,"(""Rotorhastigheden er 3400 o/m."", 5),",not
spaCy/spacy/tests/lang/da/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/lt/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/hy/test_tokenizer.py,6,TODO add test cases with valid punctuation signs.,SATD
spaCy/spacy/tests/lang/uk/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/uk/test_tokenizer_exc.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/tt/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/tt/test_tokenizer.py,42,"""?)"" => ""?)"" or ""? )""",not
spaCy/spacy/tests/lang/it/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/lb/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/lb/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/lb/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/fi/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/fi/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/nb/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/ga/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/ga/test_tokenizer.py,7,fmt: off,not
spaCy/spacy/tests/lang/ga/test_tokenizer.py,12,fmt: on,not
spaCy/spacy/tests/lang/de/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/de/test_parser.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/de/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/de/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/hu/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/hu/test_tokenizer.py,303,normal: default tests + 10% of extra tests,not
spaCy/spacy/tests/lang/hu/test_tokenizer.py,307,slow: remaining 90% of extra tests,not
spaCy/spacy/tests/lang/ru/test_lemmatizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ru/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ru/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ru/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ro/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/ro/test_tokenizer.py,15,number tests,not
spaCy/spacy/tests/lang/es/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/es/test_exception.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/zh/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/zh/test_tokenizer.py,8,fmt: off,not
spaCy/spacy/tests/lang/zh/test_tokenizer.py,22,fmt: on,not
spaCy/spacy/tests/lang/zh/test_tokenizer.py,49,reset user dict,not
spaCy/spacy/tests/lang/zh/test_tokenizer.py,56,"note: three spaces after ""I""",not
spaCy/spacy/tests/lang/zh/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/zh/test_serialize.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,10,A student read a book,not
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,17,The student read the best book,not
spaCy/spacy/tests/lang/sv/test_noun_chunks.py,24,The remorseless crooks had stolen the largest jewels that sunday,not
spaCy/spacy/tests/lang/sv/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/sv/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/sv/test_lex_attrs.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/sv/test_exceptions.py,1,coding: utf8,not
spaCy/spacy/tests/lang/sv/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/pt/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/sr/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/sr/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ml/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/id/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/id/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ca/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ca/test_exception.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ca/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/yo/test_text.py,1,coding: utf8,not
spaCy/spacy/tests/lang/th/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/en/test_noun_chunks.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_tagger.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_indices.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_parser.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_parser.py,22,fmt: off,not
spaCy/spacy/tests/lang/en/test_parser.py,27,fmt: on,not
spaCy/spacy/tests/lang/en/test_parser.py,54,fmt: off,not
spaCy/spacy/tests/lang/en/test_parser.py,59,fmt: on,not
spaCy/spacy/tests/lang/en/test_sbd.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_sbd.py,22,fmt: off,not
spaCy/spacy/tests/lang/en/test_sbd.py,29,fmt: on,not
spaCy/spacy/tests/lang/en/test_punct.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_customized_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_customized_tokenizer.py,56,the trailing '-' may cause Assertion Error,not
spaCy/spacy/tests/lang/en/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/en/test_prefix_suffix_infix.py,128,Re Issue #225,not
spaCy/spacy/tests/lang/fr/test_text.py,1,coding: utf8,not
spaCy/spacy/tests/lang/fr/test_exceptions.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/fr/test_exceptions.py,25,"u""K-POP"",",not
spaCy/spacy/tests/lang/fr/test_exceptions.py,26,"u""K-Pop"",",not
spaCy/spacy/tests/lang/fr/test_exceptions.py,27,"u""K-pop"",",not
spaCy/spacy/tests/lang/fr/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/el/test_text.py,1,coding: utf8,not
spaCy/spacy/tests/lang/el/test_exception.py,1,coding: utf8,not
spaCy/spacy/tests/lang/ja/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ja/test_tokenizer.py,7,fmt: off,not
spaCy/spacy/tests/lang/ja/test_tokenizer.py,31,fmt: on,not
spaCy/spacy/tests/lang/ja/test_tokenizer.py,53,"note: three spaces after ""I""",not
spaCy/spacy/tests/lang/ja/test_lemmatization.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ko/test_tokenizer.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ko/test_tokenizer.py,6,fmt: off,not
spaCy/spacy/tests/lang/ko/test_tokenizer.py,23,fmt: on,not
spaCy/spacy/tests/lang/ko/test_lemmatization.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/he/test_tokenizer.py,1,encoding: utf8,not
spaCy/spacy/tests/lang/eu/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/nl/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ur/test_text.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/ur/test_prefix_suffix_infix.py,1,coding: utf-8,not
spaCy/spacy/tests/lang/bn/test_tokenizer.py,1,coding: utf8,not
spaCy/spacy/tests/lang/bn/test_tokenizer.py,7,fmt: off,not
spaCy/spacy/tests/lang/bn/test_tokenizer.py,9,Punctuation tests,not
spaCy/spacy/tests/lang/bn/test_tokenizer.py,18,Abbreviations,not
spaCy/spacy/tests/lang/bn/test_tokenizer.py,21,fmt: on,not
spaCy/spacy/cli/_schemas.py,1,coding: utf-8,not
spaCy/spacy/cli/_schemas.py,5,"NB: This schema describes the new format of the training data, see #2928",not
spaCy/spacy/cli/validate.py,1,coding: utf8,not
spaCy/spacy/cli/convert.py,1,coding: utf8,not
spaCy/spacy/cli/convert.py,14,"Converters are matched by file extension except for ner/iob, which are",not
spaCy/spacy/cli/convert.py,15,"matched by file extension and content. To add a converter, add a new",not
spaCy/spacy/cli/convert.py,16,entry to this dict with the file extension mapped to the converter function,not
spaCy/spacy/cli/convert.py,17,imported from /converters.,not
spaCy/spacy/cli/convert.py,27,File types,not
spaCy/spacy/cli/convert.py,70,TODO: support msgpack via stdout in srsly?,SATD
spaCy/spacy/cli/convert.py,97,Use converter function to convert data,not
spaCy/spacy/cli/convert.py,109,Export data to a file,not
spaCy/spacy/cli/convert.py,122,Print to stdout,not
spaCy/spacy/cli/convert.py,130,guess format from the first 20 lines,not
spaCy/spacy/cli/profile.py,1,coding: utf8,not
spaCy/spacy/cli/pretrain.py,1,coding: utf8,not
spaCy/spacy/cli/pretrain.py,147,Load texts from file or stdin,not
spaCy/spacy/cli/pretrain.py,148,reading from a file,not
spaCy/spacy/cli/pretrain.py,158,reading from stdin,not
spaCy/spacy/cli/pretrain.py,173,Requires PyTorch. Experimental.,not
spaCy/spacy/cli/pretrain.py,174,Set to False for Chinese etc,not
spaCy/spacy/cli/pretrain.py,175,"If set to 1, use Mish activation.",not
spaCy/spacy/cli/pretrain.py,178,Load in pretrained weights,not
spaCy/spacy/cli/pretrain.py,182,Parse the epoch number from the given weight file,not
spaCy/spacy/cli/pretrain.py,185,Default weight file name so read epoch_start from it by cutting off 'model' and '.bin',not
spaCy/spacy/cli/pretrain.py,201,Without '--init-tok2vec' the '--epoch-start' argument is ignored,not
spaCy/spacy/cli/pretrain.py,251,Reshuffle the texts if texts were loaded from a file,not
spaCy/spacy/cli/pretrain.py,269,Don't want to return a cupy object here,not
spaCy/spacy/cli/pretrain.py,270,"The gradients are modified in-place by the BERT MLM,",not
spaCy/spacy/cli/pretrain.py,271,so we get an accurate loss,not
spaCy/spacy/cli/pretrain.py,313,The simplest way to implement this would be to vstack the,SATD
spaCy/spacy/cli/pretrain.py,314,"token.vector values, but that's a bit inefficient, especially on GPU.",not
spaCy/spacy/cli/pretrain.py,315,"Instead we fetch the index into the vectors table for each of our tokens,",not
spaCy/spacy/cli/pretrain.py,316,and look them up all at once. This prevents data copying.,not
spaCy/spacy/cli/pretrain.py,339,"This is annoying, but the parser etc have the flatten step after",not
spaCy/spacy/cli/pretrain.py,340,"the tok2vec. To load the weights in cleanly, we need to match",not
spaCy/spacy/cli/pretrain.py,341,the shape of the models' components exactly. So what we cann,not
spaCy/spacy/cli/pretrain.py,342,"""tok2vec"" has to be the same set of processes as what the components do.",not
spaCy/spacy/cli/download.py,1,coding: utf8,not
spaCy/spacy/cli/download.py,49,"if download subprocess doesn't return 0, exit",not
spaCy/spacy/cli/download.py,55,Only create symlink if the model is installed via a shortcut like 'en'.,not
spaCy/spacy/cli/download.py,56,There's no real advantage over an additional symlink for en_core_web_sm,not
spaCy/spacy/cli/download.py,57,"and if anything, it's more error prone and causes more confusion.",not
spaCy/spacy/cli/download.py,60,Get package path here because link uses,not
spaCy/spacy/cli/download.py,61,pip.get_installed_distributions() to check if model is a,not
spaCy/spacy/cli/download.py,62,"package, which fails if model was just installed via",not
spaCy/spacy/cli/download.py,63,subprocess,not
spaCy/spacy/cli/download.py,66,noqa: E722,not
spaCy/spacy/cli/download.py,67,"Dirty, but since spacy.download and the auto-linking is",not
spaCy/spacy/cli/download.py,68,"mostly a convenience wrapper, it's best to show a success",not
spaCy/spacy/cli/download.py,69,"message and loading instructions, even if linking fails.",not
spaCy/spacy/cli/download.py,77,"If a model is downloaded and then loaded within the same process, our",not
spaCy/spacy/cli/download.py,78,"is_package check currently fails, because pkg_resources.working_set",not
spaCy/spacy/cli/download.py,79,is not refreshed automatically (see #3923). We're trying to work,not
spaCy/spacy/cli/download.py,80,around this here be requiring the package explicitly.,not
spaCy/spacy/cli/download.py,90,noqa: E722,not
spaCy/spacy/cli/info.py,1,coding: utf8,not
spaCy/spacy/cli/info.py,68,exclude common cache directories and hidden directories,not
spaCy/spacy/cli/debug_data.py,1,coding: utf8,not
spaCy/spacy/cli/debug_data.py,16,Minimum number of expected occurrences of NER label in data to train new label,not
spaCy/spacy/cli/debug_data.py,18,Minimum number of expected occurrences of dependency labels,not
spaCy/spacy/cli/debug_data.py,20,Minimum number of expected examples to train a blank model,not
spaCy/spacy/cli/debug_data.py,26,fmt: off,not
spaCy/spacy/cli/debug_data.py,36,fmt: on,not
spaCy/spacy/cli/debug_data.py,56,Make sure all files and paths exists if they are needed,not
spaCy/spacy/cli/debug_data.py,66,Initialize the model and pipeline,not
spaCy/spacy/cli/debug_data.py,73,Update tag map with provided mapping,not
spaCy/spacy/cli/debug_data.py,78,TODO: Validate data format using the JSON schema,SATD
spaCy/spacy/cli/debug_data.py,79,TODO: update once the new format is ready,SATD
spaCy/spacy/cli/debug_data.py,80,TODO: move validation to GoldCorpus in order to be able to load from dir,SATD
spaCy/spacy/cli/debug_data.py,82,Create the gold corpus to be able to better analyze data,SATD
spaCy/spacy/cli/debug_data.py,110,Create all gold data here to avoid iterating over the train_docs constantly,not
spaCy/spacy/cli/debug_data.py,201,Get all unique NER labels present in the data,not
spaCy/spacy/cli/debug_data.py,395,profile sentence length,not
spaCy/spacy/cli/debug_data.py,404,check for documents with multiple sentences,not
spaCy/spacy/cli/debug_data.py,414,profile labels,not
spaCy/spacy/cli/debug_data.py,453,rare labels in train,not
spaCy/spacy/cli/debug_data.py,463,rare labels in projectivized train,not
spaCy/spacy/cli/debug_data.py,488,labels only in train,not
spaCy/spacy/cli/debug_data.py,496,labels only in dev,not
spaCy/spacy/cli/debug_data.py,511,multiple root labels,not
spaCy/spacy/cli/debug_data.py,521,"these should not happen, but just in case",not
spaCy/spacy/cli/debug_data.py,611,"""Illegal"" whitespace entity",not
spaCy/spacy/cli/debug_data.py,620,"punctuation entity: could be replaced by whitespace when training with noise,",not
spaCy/spacy/cli/debug_data.py,621,so add a warning to alert the user to this unexpected side effect.,not
spaCy/spacy/cli/package.py,1,coding: utf8,not
spaCy/spacy/cli/package.py,43,only print if user doesn't want to overwrite,not
spaCy/spacy/cli/link.py,1,coding: utf8,not
spaCy/spacy/cli/link.py,50,does a symlink exist?,not
spaCy/spacy/cli/link.py,51,"NB: It's important to check for is_symlink here and not for exists,",not
spaCy/spacy/cli/link.py,52,because invalid/outdated symlinks would return False otherwise.,not
spaCy/spacy/cli/link.py,54,does it exist otherwise?,not
spaCy/spacy/cli/link.py,55,"NB: Check this last because valid symlinks also ""exist"".",not
spaCy/spacy/cli/link.py,65,noqa: E722,not
spaCy/spacy/cli/link.py,66,"This is quite dirty, but just making sure other errors are caught.",not
spaCy/spacy/cli/init_model.py,1,coding: utf8,not
spaCy/spacy/cli/init_model.py,133,noqa: F841,not
spaCy/spacy/cli/init_model.py,145,"Decode as a little-endian string, so that we can do & 15 to get",not
spaCy/spacy/cli/init_model.py,146,the first 4 bits. See _parse_features.pyx,not
spaCy/spacy/cli/init_model.py,250,Take odd strings literally.,not
spaCy/spacy/cli/init_model.py,270,"If the clusterer has only seen the word a few times, its",not
spaCy/spacy/cli/init_model.py,271,cluster is unreliable.,not
spaCy/spacy/cli/init_model.py,276,Expand clusters with re-casing,not
spaCy/spacy/cli/train.py,1,coding: utf8,not
spaCy/spacy/cli/train.py,26,fmt: off,not
spaCy/spacy/cli/train.py,63,fmt: on,not
spaCy/spacy/cli/train.py,111,Make sure all files and paths exists if they are needed,not
spaCy/spacy/cli/train.py,140,Take dropout and batch size as generators of values -- dropout,not
spaCy/spacy/cli/train.py,141,"starts high and decays sharply, to force the optimizer to explore.",not
spaCy/spacy/cli/train.py,142,"Batch size starts at 1 and grows, so that we make updates quickly",not
spaCy/spacy/cli/train.py,143,at the beginning of training.,not
spaCy/spacy/cli/train.py,164,"Set up the base model and pipeline. If a base model is specified, load",not
spaCy/spacy/cli/train.py,165,the model and make sure the pipeline matches the pipeline setting. If,not
spaCy/spacy/cli/train.py,166,"training starts from a blank model, intitalize the language class.",not
spaCy/spacy/cli/train.py,248,Update tag map with provided mapping,not
spaCy/spacy/cli/train.py,255,Multitask objectives,not
spaCy/spacy/cli/train.py,268,Prepare training corpus,not
spaCy/spacy/cli/train.py,274,"Start with an existing model, use default optimizer",not
spaCy/spacy/cli/train.py,277,"Start with a blank model, call begin_training",not
spaCy/spacy/cli/train.py,290,Load in pretrained weights,not
spaCy/spacy/cli/train.py,295,Verify textcat config,not
spaCy/spacy/cli/train.py,379,fmt: off,not
spaCy/spacy/cli/train.py,383,fmt: on,not
spaCy/spacy/cli/train.py,425,"If raw text is available, perform 'rehearsal' updates,",not
spaCy/spacy/cli/train.py,426,which use unlabelled data to reduce overfitting.,not
spaCy/spacy/cli/train.py,457,Only evaluate on CPU in the first iteration (for,not
spaCy/spacy/cli/train.py,458,timing) if GPU is enabled,not
spaCy/spacy/cli/train.py,479,Update model meta.json,not
spaCy/spacy/cli/train.py,535,Early stopping,not
spaCy/spacy/cli/train.py,576,combine cpu and gpu speeds with the base model speeds,not
spaCy/spacy/cli/train.py,587,"if there were no speeds to update, overwrite with meta",not
spaCy/spacy/cli/train.py,593,note: beam speeds are not combined with the base model,not
spaCy/spacy/cli/train.py,636,These attrs are expected to be set by data. Others should,not
spaCy/spacy/cli/train.py,637,be set by calling the language functions.,not
spaCy/spacy/cli/train.py,683,remove per_type dicts from score list for max() comparison,not
spaCy/spacy/cli/__init__.py,1,noqa: F401,not
spaCy/spacy/cli/__init__.py,2,noqa: F401,not
spaCy/spacy/cli/__init__.py,3,noqa: F401,not
spaCy/spacy/cli/__init__.py,4,noqa: F401,not
spaCy/spacy/cli/__init__.py,5,noqa: F401,not
spaCy/spacy/cli/__init__.py,6,noqa: F401,not
spaCy/spacy/cli/__init__.py,7,noqa: F401,not
spaCy/spacy/cli/__init__.py,8,noqa: F401,not
spaCy/spacy/cli/__init__.py,9,noqa: F401,not
spaCy/spacy/cli/__init__.py,10,noqa: F401,not
spaCy/spacy/cli/__init__.py,11,noqa: F401,not
spaCy/spacy/cli/__init__.py,12,noqa: F401,not
spaCy/spacy/cli/evaluate.py,1,coding: utf8,not
spaCy/spacy/cli/converters/conllu2json.py,1,coding: utf8,not
spaCy/spacy/cli/converters/conllu2json.py,18,"by @dvsrepo, via #11 explosion/spacy-dev-resources",not
spaCy/spacy/cli/converters/conllu2json.py,19,by @katarkor,not
spaCy/spacy/cli/converters/conllu2json.py,31,Real-sized documents could be extracted using the comments on the,not
spaCy/spacy/cli/converters/conllu2json.py,32,conluu document,not
spaCy/spacy/cli/converters/conllu2json.py,79,noqa: E722,not
spaCy/spacy/cli/converters/conll_ner2json.py,1,coding: utf8,not
spaCy/spacy/cli/converters/conll_ner2json.py,41,"check for existing delimiters, which should be preserved",not
spaCy/spacy/cli/converters/conll_ner2json.py,54,do document segmentation with existing sentences,not
spaCy/spacy/cli/converters/conll_ner2json.py,58,do sentence segmentation with existing documents,not
spaCy/spacy/cli/converters/conll_ner2json.py,61,do both sentence segmentation and document segmentation according,not
spaCy/spacy/cli/converters/conll_ner2json.py,62,to options,not
spaCy/spacy/cli/converters/conll_ner2json.py,64,sentence segmentation required for document segmentation,not
spaCy/spacy/cli/converters/conll_ner2json.py,76,provide warnings for problematic data,not
spaCy/spacy/cli/converters/jsonl2json.py,1,coding: utf8,not
spaCy/spacy/cli/converters/jsonl2json.py,40,Trim whitespace,not
spaCy/spacy/cli/converters/iob2json.py,1,coding: utf8,not
spaCy/spacy/cli/converters/__init__.py,1,noqa: F401,not
spaCy/spacy/cli/converters/__init__.py,2,noqa: F401,not
spaCy/spacy/cli/converters/__init__.py,3,noqa: F401,not
spaCy/spacy/cli/converters/__init__.py,4,noqa: F401,not
spaCy/spacy/ml/_wire.py,6,pragma: no cover,not
spaCy/spacy/ml/tok2vec.py,35,"For backwards compatibility with models before the architecture registry,",not
spaCy/spacy/ml/tok2vec.py,36,we have to be careful to get exactly the same model structure. One subtle,not
spaCy/spacy/ml/tok2vec.py,37,"trick is that when we define concatenation with the operator, the operator",not
spaCy/spacy/ml/tok2vec.py,38,"is actually binary associative. So when we write (a | b | c), we're actually",not
spaCy/spacy/ml/tok2vec.py,39,"getting concatenate(concatenate(a, b), c). That's why the implementation",not
spaCy/spacy/ml/tok2vec.py,40,is a bit ugly here.,SATD
spaCy/spacy/ml/__init__.py,1,coding: utf8,not
spaCy/spacy/ml/__init__.py,4,noqa: F401,not
spaCy/spacy/ml/__init__.py,5,noqa: F401,not
spaCy/spacy/ml/_legacy_tok2vec.py,1,coding: utf8,not
spaCy/spacy/ml/_legacy_tok2vec.py,16,Circular imports :(,not
spaCy/spacy/ml/_legacy_tok2vec.py,89,Work around thinc API limitations :(. TODO: Revise in Thinc 7,SATD
spaCy/spacy/ml/_legacy_tok2vec.py,107,pragma: no cover,not
spaCy/spacy/pipeline/hooks.py,1,coding: utf8,not
spaCy/spacy/pipeline/entityruler.py,1,coding: utf8,not
spaCy/spacy/pipeline/entityruler.py,106,check for end - 1 here because boundaries are inclusive,not
spaCy/spacy/pipeline/entityruler.py,199,disable the nlp components after this one in case they hadn't been initialized / deserialised yet,not
spaCy/spacy/pipeline/entityruler.py,378,user wants to save only JSONL,not
spaCy/spacy/pipeline/functions.py,1,coding: utf8,not
spaCy/spacy/pipeline/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/norm_exceptions.py,5,These exceptions are used to add NORM values based on a token's ORTH value.,not
spaCy/spacy/lang/norm_exceptions.py,6,Individual languages can also add their own exceptions and overwrite them -,not
spaCy/spacy/lang/norm_exceptions.py,7,"for example, British vs. American spelling in English.",not
spaCy/spacy/lang/norm_exceptions.py,9,Norms are only set if no alternative is provided in the tokenizer exceptions.,not
spaCy/spacy/lang/norm_exceptions.py,10,Note that this does not change any other token attributes. Its main purpose,not
spaCy/spacy/lang/norm_exceptions.py,11,is to normalise the word representations so that equivalent tokens receive,not
spaCy/spacy/lang/norm_exceptions.py,12,"similar representations. For example: $ and € are very different, but they're",not
spaCy/spacy/lang/norm_exceptions.py,13,"both currency symbols. By normalising currency symbols to $, all symbols are",not
spaCy/spacy/lang/norm_exceptions.py,14,"seen as similar, no matter how common they are in the training data.",not
spaCy/spacy/lang/char_classes.py,1,coding: utf8,not
spaCy/spacy/lang/char_classes.py,20,from the final table in: https://en.wikipedia.org/wiki/CJK_Unified_Ideographs,not
spaCy/spacy/lang/char_classes.py,31,Latin standard,not
spaCy/spacy/lang/char_classes.py,40,"letters with diacritics - French, German, Icelandic, Spanish",not
spaCy/spacy/lang/char_classes.py,49,"letters with diacritics - Catalan, Czech, Latin, Latvian, Lithuanian, Polish, Slovak, Turkish, Welsh",not
spaCy/spacy/lang/char_classes.py,66,"special characters - Khoisan, Pan-Nigerian, Pinyin, Romanian",not
spaCy/spacy/lang/char_classes.py,67,those that are a combination of both upper and lower letters are only included in the group _latin_extendedB,not
spaCy/spacy/lang/char_classes.py,88,"special characters - Uighur, Uralic Phonetic",not
spaCy/spacy/lang/char_classes.py,97,"special characters - phonetic, Mayan, Medieval",not
spaCy/spacy/lang/char_classes.py,114,special characters - phonetic Teuthonista and Sakha,not
spaCy/spacy/lang/char_classes.py,118,"phonetic letters - Greek, Latin, Cyrillic",not
spaCy/spacy/lang/char_classes.py,122,letters with multiple diacritics - Vietnamese,not
spaCy/spacy/lang/char_classes.py,147,all lower latin classes,not
spaCy/spacy/lang/char_classes.py,164,all upper latin classes,not
spaCy/spacy/lang/char_classes.py,179,all latin classes,not
spaCy/spacy/lang/char_classes.py,245,"These expressions contain various unicode variations, including characters",not
spaCy/spacy/lang/char_classes.py,246,"used in Chinese (see #1333, #1340, #1351) – unless there are cross-language",not
spaCy/spacy/lang/char_classes.py,247,"conflicts, spaCy's base tokenizer should handle all of those by default",not
spaCy/spacy/lang/char_classes.py,254,"Various symbols like dingbats, but also emoji",not
spaCy/spacy/lang/char_classes.py,255,Details: https://www.compart.com/en/unicode/category/So,not
spaCy/spacy/lang/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/tokenizer_exceptions.py,10,URL validation regex courtesy of: https://mathiasbynens.be/demo/url-regex,not
spaCy/spacy/lang/tokenizer_exceptions.py,11,"and https://gist.github.com/dperini/729294 (Diego Perini, MIT License)",not
spaCy/spacy/lang/tokenizer_exceptions.py,12,A few mods to this regex to account for use cases represented in test_urls,not
spaCy/spacy/lang/tokenizer_exceptions.py,14,fmt: off,not
spaCy/spacy/lang/tokenizer_exceptions.py,16,protocol identifier (mods: make optional and expand schemes),not
spaCy/spacy/lang/tokenizer_exceptions.py,17,(see: https://www.iana.org/assignments/uri-schemes/uri-schemes.xhtml),not
spaCy/spacy/lang/tokenizer_exceptions.py,19,mailto:user or user:pass authentication,not
spaCy/spacy/lang/tokenizer_exceptions.py,22,IP address exclusion,not
spaCy/spacy/lang/tokenizer_exceptions.py,23,private & local networks,not
spaCy/spacy/lang/tokenizer_exceptions.py,27,IP address dotted notation octets,not
spaCy/spacy/lang/tokenizer_exceptions.py,28,excludes loopback network 0.0.0.0,not
spaCy/spacy/lang/tokenizer_exceptions.py,29,excludes reserved space >= 224.0.0.0,not
spaCy/spacy/lang/tokenizer_exceptions.py,30,excludes network & broadcast addresses,not
spaCy/spacy/lang/tokenizer_exceptions.py,31,(first & last IP address of each class),not
spaCy/spacy/lang/tokenizer_exceptions.py,32,"MH: Do we really need this? Seems excessive, and seems to have caused",not
spaCy/spacy/lang/tokenizer_exceptions.py,33,Issue #957,not
spaCy/spacy/lang/tokenizer_exceptions.py,38,host & domain names,not
spaCy/spacy/lang/tokenizer_exceptions.py,39,"mods: match is case-sensitive, so include [A-Z]",not
spaCy/spacy/lang/tokenizer_exceptions.py,40,noqa,not
spaCy/spacy/lang/tokenizer_exceptions.py,47,TLD identifier,not
spaCy/spacy/lang/tokenizer_exceptions.py,48,mods: use ALPHA_LOWER instead of a wider range so that this doesn't match,not
spaCy/spacy/lang/tokenizer_exceptions.py,49,"strings like ""lower.Upper"", which can be split on ""."" by infixes in some",not
spaCy/spacy/lang/tokenizer_exceptions.py,50,languages,not
spaCy/spacy/lang/tokenizer_exceptions.py,53,port number,not
spaCy/spacy/lang/tokenizer_exceptions.py,55,resource path,not
spaCy/spacy/lang/tokenizer_exceptions.py,58,fmt: on,not
spaCy/spacy/lang/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/lex_attrs.py,44,can be overwritten by lang with list of number words,not
spaCy/spacy/lang/lex_attrs.py,113,"can be overwritten by lang with list of currency words, e.g. dollar, euro",not
spaCy/spacy/lang/lex_attrs.py,125,"We're looking for things that function in text like URLs. So, valid URL",not
spaCy/spacy/lang/lex_attrs.py,126,"or not, anything they say http:// is going to be good.",not
spaCy/spacy/lang/lex_attrs.py,134,prevent matches on e-mail addresses – check after splitting the text,not
spaCy/spacy/lang/lex_attrs.py,135,to still allow URLs containing an '@' character (see #1715),not
spaCy/spacy/lang/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/bg/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/bg/stop_words.py,5,Source: https://github.com/Alir3z4/stop-words,not
spaCy/spacy/lang/bg/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/bg/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ar/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/ar/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/ar/tokenizer_exceptions.py,10,Time,not
spaCy/spacy/lang/ar/tokenizer_exceptions.py,20,Scientific abv.,not
spaCy/spacy/lang/ar/tokenizer_exceptions.py,37,Other abv.,not
spaCy/spacy/lang/ar/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/ar/punctuation.py,13,Arabic is written from Right-To-Left,not
spaCy/spacy/lang/ar/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ar/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/ar/examples.py,1,coding: utf8,not
spaCy/spacy/lang/sq/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/sq/stop_words.py,5,Source: https://github.com/andrixh/index-albanian,not
spaCy/spacy/lang/sq/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/sq/examples.py,1,coding: utf8,not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,1,-*- coding: utf-8 -*-,not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,5,The following list consists of:,not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,6,- exceptions generated from polish_srx_rules [1],not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,7,(https://github.com/milekpl/polish_srx_rules),not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,8,- abbreviations parsed from Wikipedia,not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,9,- some manually added exceptions,not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,10,,not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,11,"[1] M. Miłkowski and J. Lipski,",not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,12,"""Using SRX Standard for Sentence Segmentation,"" in LTC 2009,",not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,13,"Lecture Notes in Artificial Intelligence 6562,",not
spaCy/spacy/lang/pl/_tokenizer_exceptions_list.py,14,"Z. Vetulani, Ed. Berlin Heidelberg: Springer-Verlag, 2011, pp. 172–182.",not
spaCy/spacy/lang/pl/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/pl/stop_words.py,5,sources: https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt and https://github.com/stopwords-iso/stopwords-pl,not
spaCy/spacy/lang/pl/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/pl/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/pl/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/pl/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/pl/tag_map.py,24,fmt: off,not
spaCy/spacy/lang/pl/tag_map.py,600,UD,not
spaCy/spacy/lang/pl/tag_map.py,1649,fmt: on,not
spaCy/spacy/lang/pl/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/pl/examples.py,1,coding: utf8,not
spaCy/spacy/lang/mr/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/mr/stop_words.py,5,"Source: https://github.com/stopwords-iso/stopwords-mr/blob/master/stopwords-mr.txt, https://github.com/6/stopwords-json/edit/master/dist/mr.json",not
spaCy/spacy/lang/mr/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/cs/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/cs/stop_words.py,5,Source: https://github.com/Alir3z4/stop-words,not
spaCy/spacy/lang/cs/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/kn/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/kn/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/kn/examples.py,1,coding: utf8,not
spaCy/spacy/lang/sk/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/sk/stop_words.py,5,Source: https://github.com/Ardevop-sk/stopwords-sk,not
spaCy/spacy/lang/sk/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/sk/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/sk/tag_map.py,7,Source https://universaldependencies.org/tagset-conversion/sk-snk-uposf.html,not
spaCy/spacy/lang/sk/tag_map.py,8,fmt: off,not
spaCy/spacy/lang/sk/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/sk/examples.py,1,coding: utf8,not
spaCy/spacy/lang/gu/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/gu/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/gu/examples.py,1,coding: utf8,not
spaCy/spacy/lang/da/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/da/norm_exceptions.py,9,Sources:,not
spaCy/spacy/lang/da/norm_exceptions.py,10,1: https://dsn.dk/retskrivning/om-retskrivningsordbogen/mere-om-retskrivningsordbogen-2012/endrede-stave-og-ordformer/,not
spaCy/spacy/lang/da/norm_exceptions.py,11,2: http://www.tjerry-korrektur.dk/ord-med-flere-stavemaader/,not
spaCy/spacy/lang/da/norm_exceptions.py,14,Alternative spelling,not
spaCy/spacy/lang/da/norm_exceptions.py,15,1,not
spaCy/spacy/lang/da/norm_exceptions.py,16,2,not
spaCy/spacy/lang/da/norm_exceptions.py,18,1,not
spaCy/spacy/lang/da/norm_exceptions.py,19,1,not
spaCy/spacy/lang/da/norm_exceptions.py,20,1,not
spaCy/spacy/lang/da/norm_exceptions.py,21,1,not
spaCy/spacy/lang/da/norm_exceptions.py,22,1,not
spaCy/spacy/lang/da/norm_exceptions.py,23,1,not
spaCy/spacy/lang/da/norm_exceptions.py,24,1,not
spaCy/spacy/lang/da/norm_exceptions.py,25,1,not
spaCy/spacy/lang/da/norm_exceptions.py,26,1,not
spaCy/spacy/lang/da/norm_exceptions.py,27,1,not
spaCy/spacy/lang/da/norm_exceptions.py,28,2,not
spaCy/spacy/lang/da/norm_exceptions.py,29,1,not
spaCy/spacy/lang/da/norm_exceptions.py,30,2,not
spaCy/spacy/lang/da/norm_exceptions.py,31,1,not
spaCy/spacy/lang/da/norm_exceptions.py,32,1,not
spaCy/spacy/lang/da/norm_exceptions.py,33,1,not
spaCy/spacy/lang/da/norm_exceptions.py,34,2,not
spaCy/spacy/lang/da/norm_exceptions.py,35,2,not
spaCy/spacy/lang/da/norm_exceptions.py,36,1,not
spaCy/spacy/lang/da/norm_exceptions.py,37,1,not
spaCy/spacy/lang/da/norm_exceptions.py,38,1,not
spaCy/spacy/lang/da/norm_exceptions.py,39,2,not
spaCy/spacy/lang/da/norm_exceptions.py,40,1,not
spaCy/spacy/lang/da/norm_exceptions.py,41,1,not
spaCy/spacy/lang/da/norm_exceptions.py,42,1,not
spaCy/spacy/lang/da/norm_exceptions.py,43,1,not
spaCy/spacy/lang/da/norm_exceptions.py,44,1,not
spaCy/spacy/lang/da/norm_exceptions.py,45,1,not
spaCy/spacy/lang/da/norm_exceptions.py,46,1,not
spaCy/spacy/lang/da/norm_exceptions.py,47,1,not
spaCy/spacy/lang/da/norm_exceptions.py,48,1,not
spaCy/spacy/lang/da/norm_exceptions.py,49,1,not
spaCy/spacy/lang/da/norm_exceptions.py,50,1,not
spaCy/spacy/lang/da/norm_exceptions.py,51,1,not
spaCy/spacy/lang/da/norm_exceptions.py,52,1,not
spaCy/spacy/lang/da/norm_exceptions.py,53,2,not
spaCy/spacy/lang/da/norm_exceptions.py,54,2,not
spaCy/spacy/lang/da/norm_exceptions.py,55,1,not
spaCy/spacy/lang/da/norm_exceptions.py,56,1,not
spaCy/spacy/lang/da/norm_exceptions.py,57,2,not
spaCy/spacy/lang/da/norm_exceptions.py,58,2,not
spaCy/spacy/lang/da/norm_exceptions.py,59,1,not
spaCy/spacy/lang/da/norm_exceptions.py,60,1,not
spaCy/spacy/lang/da/norm_exceptions.py,61,1,not
spaCy/spacy/lang/da/norm_exceptions.py,62,1,not
spaCy/spacy/lang/da/norm_exceptions.py,63,1,not
spaCy/spacy/lang/da/norm_exceptions.py,64,1,not
spaCy/spacy/lang/da/norm_exceptions.py,65,1,not
spaCy/spacy/lang/da/norm_exceptions.py,66,1,not
spaCy/spacy/lang/da/norm_exceptions.py,67,2,not
spaCy/spacy/lang/da/norm_exceptions.py,68,1,not
spaCy/spacy/lang/da/norm_exceptions.py,69,2,not
spaCy/spacy/lang/da/norm_exceptions.py,70,1,not
spaCy/spacy/lang/da/norm_exceptions.py,71,1,not
spaCy/spacy/lang/da/norm_exceptions.py,72,1,not
spaCy/spacy/lang/da/norm_exceptions.py,73,1,not
spaCy/spacy/lang/da/norm_exceptions.py,74,1,not
spaCy/spacy/lang/da/norm_exceptions.py,75,1,not
spaCy/spacy/lang/da/norm_exceptions.py,76,1,not
spaCy/spacy/lang/da/norm_exceptions.py,77,2,not
spaCy/spacy/lang/da/norm_exceptions.py,78,1,not
spaCy/spacy/lang/da/norm_exceptions.py,79,1,not
spaCy/spacy/lang/da/norm_exceptions.py,80,1,not
spaCy/spacy/lang/da/norm_exceptions.py,81,1,not
spaCy/spacy/lang/da/norm_exceptions.py,82,1,not
spaCy/spacy/lang/da/norm_exceptions.py,83,1,not
spaCy/spacy/lang/da/norm_exceptions.py,84,1,not
spaCy/spacy/lang/da/norm_exceptions.py,85,1,not
spaCy/spacy/lang/da/norm_exceptions.py,86,1,not
spaCy/spacy/lang/da/norm_exceptions.py,87,2,not
spaCy/spacy/lang/da/norm_exceptions.py,88,1,not
spaCy/spacy/lang/da/norm_exceptions.py,89,1,not
spaCy/spacy/lang/da/norm_exceptions.py,90,1,not
spaCy/spacy/lang/da/norm_exceptions.py,91,1,not
spaCy/spacy/lang/da/norm_exceptions.py,92,1,not
spaCy/spacy/lang/da/norm_exceptions.py,93,1,not
spaCy/spacy/lang/da/norm_exceptions.py,94,1,not
spaCy/spacy/lang/da/norm_exceptions.py,95,1,not
spaCy/spacy/lang/da/norm_exceptions.py,96,1,not
spaCy/spacy/lang/da/norm_exceptions.py,97,1,not
spaCy/spacy/lang/da/norm_exceptions.py,98,2,not
spaCy/spacy/lang/da/norm_exceptions.py,99,1,not
spaCy/spacy/lang/da/norm_exceptions.py,100,1,not
spaCy/spacy/lang/da/norm_exceptions.py,101,1,not
spaCy/spacy/lang/da/norm_exceptions.py,102,1,not
spaCy/spacy/lang/da/norm_exceptions.py,103,1,not
spaCy/spacy/lang/da/norm_exceptions.py,104,1,not
spaCy/spacy/lang/da/norm_exceptions.py,105,1,not
spaCy/spacy/lang/da/norm_exceptions.py,106,1,not
spaCy/spacy/lang/da/norm_exceptions.py,107,1,not
spaCy/spacy/lang/da/norm_exceptions.py,108,1,not
spaCy/spacy/lang/da/norm_exceptions.py,109,1,not
spaCy/spacy/lang/da/norm_exceptions.py,110,1,not
spaCy/spacy/lang/da/norm_exceptions.py,111,1,not
spaCy/spacy/lang/da/norm_exceptions.py,112,1,not
spaCy/spacy/lang/da/norm_exceptions.py,113,2,not
spaCy/spacy/lang/da/norm_exceptions.py,114,1,not
spaCy/spacy/lang/da/norm_exceptions.py,115,1,not
spaCy/spacy/lang/da/norm_exceptions.py,116,1,not
spaCy/spacy/lang/da/norm_exceptions.py,117,1,not
spaCy/spacy/lang/da/norm_exceptions.py,118,2,not
spaCy/spacy/lang/da/norm_exceptions.py,119,1,not
spaCy/spacy/lang/da/norm_exceptions.py,120,1,not
spaCy/spacy/lang/da/norm_exceptions.py,121,1,not
spaCy/spacy/lang/da/norm_exceptions.py,122,1,not
spaCy/spacy/lang/da/norm_exceptions.py,123,1,not
spaCy/spacy/lang/da/norm_exceptions.py,124,1,not
spaCy/spacy/lang/da/norm_exceptions.py,125,1,not
spaCy/spacy/lang/da/norm_exceptions.py,126,1,not
spaCy/spacy/lang/da/norm_exceptions.py,127,1,not
spaCy/spacy/lang/da/norm_exceptions.py,128,1,not
spaCy/spacy/lang/da/norm_exceptions.py,129,1,not
spaCy/spacy/lang/da/norm_exceptions.py,130,2,not
spaCy/spacy/lang/da/norm_exceptions.py,131,2,not
spaCy/spacy/lang/da/norm_exceptions.py,132,2,not
spaCy/spacy/lang/da/norm_exceptions.py,133,1,not
spaCy/spacy/lang/da/norm_exceptions.py,134,1,not
spaCy/spacy/lang/da/norm_exceptions.py,135,2,not
spaCy/spacy/lang/da/norm_exceptions.py,136,1,not
spaCy/spacy/lang/da/norm_exceptions.py,137,1,not
spaCy/spacy/lang/da/norm_exceptions.py,138,1,not
spaCy/spacy/lang/da/norm_exceptions.py,139,2,not
spaCy/spacy/lang/da/norm_exceptions.py,140,1,not
spaCy/spacy/lang/da/norm_exceptions.py,141,1,not
spaCy/spacy/lang/da/norm_exceptions.py,142,1,not
spaCy/spacy/lang/da/norm_exceptions.py,143,1,not
spaCy/spacy/lang/da/norm_exceptions.py,144,1,not
spaCy/spacy/lang/da/norm_exceptions.py,145,1,not
spaCy/spacy/lang/da/norm_exceptions.py,146,1,not
spaCy/spacy/lang/da/norm_exceptions.py,147,1,not
spaCy/spacy/lang/da/norm_exceptions.py,148,1,not
spaCy/spacy/lang/da/norm_exceptions.py,149,1,not
spaCy/spacy/lang/da/norm_exceptions.py,150,1,not
spaCy/spacy/lang/da/norm_exceptions.py,151,1,not
spaCy/spacy/lang/da/norm_exceptions.py,152,1,not
spaCy/spacy/lang/da/norm_exceptions.py,153,1,not
spaCy/spacy/lang/da/norm_exceptions.py,154,1,not
spaCy/spacy/lang/da/norm_exceptions.py,155,1,not
spaCy/spacy/lang/da/norm_exceptions.py,156,1,not
spaCy/spacy/lang/da/norm_exceptions.py,157,1,not
spaCy/spacy/lang/da/norm_exceptions.py,158,1,not
spaCy/spacy/lang/da/norm_exceptions.py,159,2,not
spaCy/spacy/lang/da/norm_exceptions.py,160,1,not
spaCy/spacy/lang/da/norm_exceptions.py,161,1,not
spaCy/spacy/lang/da/norm_exceptions.py,162,1,not
spaCy/spacy/lang/da/norm_exceptions.py,163,1,not
spaCy/spacy/lang/da/norm_exceptions.py,164,1,not
spaCy/spacy/lang/da/norm_exceptions.py,165,2,not
spaCy/spacy/lang/da/norm_exceptions.py,166,2,not
spaCy/spacy/lang/da/norm_exceptions.py,167,1,not
spaCy/spacy/lang/da/norm_exceptions.py,168,1,not
spaCy/spacy/lang/da/norm_exceptions.py,169,1,not
spaCy/spacy/lang/da/norm_exceptions.py,170,1,not
spaCy/spacy/lang/da/norm_exceptions.py,171,1,not
spaCy/spacy/lang/da/norm_exceptions.py,172,1,not
spaCy/spacy/lang/da/norm_exceptions.py,173,1,not
spaCy/spacy/lang/da/norm_exceptions.py,174,1,not
spaCy/spacy/lang/da/norm_exceptions.py,175,1,not
spaCy/spacy/lang/da/norm_exceptions.py,176,1,not
spaCy/spacy/lang/da/norm_exceptions.py,177,1,not
spaCy/spacy/lang/da/norm_exceptions.py,178,1,not
spaCy/spacy/lang/da/norm_exceptions.py,179,1,not
spaCy/spacy/lang/da/norm_exceptions.py,180,1,not
spaCy/spacy/lang/da/norm_exceptions.py,181,1,not
spaCy/spacy/lang/da/norm_exceptions.py,182,1,not
spaCy/spacy/lang/da/norm_exceptions.py,183,1,not
spaCy/spacy/lang/da/norm_exceptions.py,184,1,not
spaCy/spacy/lang/da/norm_exceptions.py,185,1,not
spaCy/spacy/lang/da/norm_exceptions.py,186,1,not
spaCy/spacy/lang/da/norm_exceptions.py,187,1,not
spaCy/spacy/lang/da/norm_exceptions.py,188,1,not
spaCy/spacy/lang/da/norm_exceptions.py,189,2,not
spaCy/spacy/lang/da/norm_exceptions.py,190,2,not
spaCy/spacy/lang/da/norm_exceptions.py,191,1,not
spaCy/spacy/lang/da/norm_exceptions.py,192,1,not
spaCy/spacy/lang/da/norm_exceptions.py,193,1,not
spaCy/spacy/lang/da/norm_exceptions.py,194,2,not
spaCy/spacy/lang/da/norm_exceptions.py,195,1,not
spaCy/spacy/lang/da/norm_exceptions.py,196,2,not
spaCy/spacy/lang/da/norm_exceptions.py,197,1,not
spaCy/spacy/lang/da/norm_exceptions.py,198,1,not
spaCy/spacy/lang/da/norm_exceptions.py,199,2,not
spaCy/spacy/lang/da/norm_exceptions.py,200,1,not
spaCy/spacy/lang/da/norm_exceptions.py,201,1,not
spaCy/spacy/lang/da/norm_exceptions.py,202,1,not
spaCy/spacy/lang/da/norm_exceptions.py,203,1,not
spaCy/spacy/lang/da/norm_exceptions.py,204,2,not
spaCy/spacy/lang/da/norm_exceptions.py,205,1,not
spaCy/spacy/lang/da/norm_exceptions.py,206,1,not
spaCy/spacy/lang/da/norm_exceptions.py,207,1,not
spaCy/spacy/lang/da/norm_exceptions.py,208,1,not
spaCy/spacy/lang/da/norm_exceptions.py,209,1,not
spaCy/spacy/lang/da/norm_exceptions.py,210,1,not
spaCy/spacy/lang/da/norm_exceptions.py,211,1,not
spaCy/spacy/lang/da/norm_exceptions.py,212,1,not
spaCy/spacy/lang/da/norm_exceptions.py,213,1,not
spaCy/spacy/lang/da/norm_exceptions.py,214,2,not
spaCy/spacy/lang/da/norm_exceptions.py,215,1,not
spaCy/spacy/lang/da/norm_exceptions.py,216,1,not
spaCy/spacy/lang/da/norm_exceptions.py,217,1,not
spaCy/spacy/lang/da/norm_exceptions.py,218,1,not
spaCy/spacy/lang/da/norm_exceptions.py,219,1,not
spaCy/spacy/lang/da/norm_exceptions.py,220,2,not
spaCy/spacy/lang/da/norm_exceptions.py,221,1,not
spaCy/spacy/lang/da/norm_exceptions.py,222,1,not
spaCy/spacy/lang/da/norm_exceptions.py,223,1,not
spaCy/spacy/lang/da/norm_exceptions.py,224,1,not
spaCy/spacy/lang/da/norm_exceptions.py,225,1,not
spaCy/spacy/lang/da/norm_exceptions.py,226,1,not
spaCy/spacy/lang/da/norm_exceptions.py,227,1,not
spaCy/spacy/lang/da/norm_exceptions.py,228,1,not
spaCy/spacy/lang/da/norm_exceptions.py,229,1,not
spaCy/spacy/lang/da/norm_exceptions.py,230,1,not
spaCy/spacy/lang/da/norm_exceptions.py,231,1,not
spaCy/spacy/lang/da/norm_exceptions.py,232,2,not
spaCy/spacy/lang/da/norm_exceptions.py,233,1,not
spaCy/spacy/lang/da/norm_exceptions.py,234,1,not
spaCy/spacy/lang/da/norm_exceptions.py,235,1,not
spaCy/spacy/lang/da/norm_exceptions.py,236,1,not
spaCy/spacy/lang/da/norm_exceptions.py,237,1,not
spaCy/spacy/lang/da/norm_exceptions.py,238,2,not
spaCy/spacy/lang/da/norm_exceptions.py,239,1,not
spaCy/spacy/lang/da/norm_exceptions.py,240,2,not
spaCy/spacy/lang/da/norm_exceptions.py,241,1,not
spaCy/spacy/lang/da/norm_exceptions.py,242,2,not
spaCy/spacy/lang/da/norm_exceptions.py,243,1,not
spaCy/spacy/lang/da/norm_exceptions.py,244,1,not
spaCy/spacy/lang/da/norm_exceptions.py,245,1,not
spaCy/spacy/lang/da/norm_exceptions.py,246,1,not
spaCy/spacy/lang/da/norm_exceptions.py,247,1,not
spaCy/spacy/lang/da/norm_exceptions.py,248,1,not
spaCy/spacy/lang/da/norm_exceptions.py,249,1,not
spaCy/spacy/lang/da/norm_exceptions.py,250,1,not
spaCy/spacy/lang/da/norm_exceptions.py,251,1,not
spaCy/spacy/lang/da/norm_exceptions.py,252,1,not
spaCy/spacy/lang/da/norm_exceptions.py,253,1,not
spaCy/spacy/lang/da/norm_exceptions.py,254,1,not
spaCy/spacy/lang/da/norm_exceptions.py,255,1,not
spaCy/spacy/lang/da/norm_exceptions.py,256,1,not
spaCy/spacy/lang/da/norm_exceptions.py,257,1,not
spaCy/spacy/lang/da/norm_exceptions.py,258,1,not
spaCy/spacy/lang/da/norm_exceptions.py,259,1,not
spaCy/spacy/lang/da/norm_exceptions.py,260,1,not
spaCy/spacy/lang/da/norm_exceptions.py,261,1,not
spaCy/spacy/lang/da/norm_exceptions.py,262,1,not
spaCy/spacy/lang/da/norm_exceptions.py,263,2,not
spaCy/spacy/lang/da/norm_exceptions.py,264,1,not
spaCy/spacy/lang/da/norm_exceptions.py,265,2,not
spaCy/spacy/lang/da/norm_exceptions.py,266,1,not
spaCy/spacy/lang/da/norm_exceptions.py,267,2,not
spaCy/spacy/lang/da/norm_exceptions.py,268,1,not
spaCy/spacy/lang/da/norm_exceptions.py,269,1,not
spaCy/spacy/lang/da/norm_exceptions.py,270,1,not
spaCy/spacy/lang/da/norm_exceptions.py,271,1,not
spaCy/spacy/lang/da/norm_exceptions.py,272,1,not
spaCy/spacy/lang/da/norm_exceptions.py,273,1,not
spaCy/spacy/lang/da/norm_exceptions.py,274,1,not
spaCy/spacy/lang/da/norm_exceptions.py,275,2,not
spaCy/spacy/lang/da/norm_exceptions.py,276,1,not
spaCy/spacy/lang/da/norm_exceptions.py,277,1,not
spaCy/spacy/lang/da/norm_exceptions.py,278,2,not
spaCy/spacy/lang/da/norm_exceptions.py,279,1,not
spaCy/spacy/lang/da/norm_exceptions.py,280,1,not
spaCy/spacy/lang/da/norm_exceptions.py,281,1,not
spaCy/spacy/lang/da/norm_exceptions.py,282,1,not
spaCy/spacy/lang/da/norm_exceptions.py,283,1,not
spaCy/spacy/lang/da/norm_exceptions.py,284,1,not
spaCy/spacy/lang/da/norm_exceptions.py,285,1,not
spaCy/spacy/lang/da/norm_exceptions.py,286,1,not
spaCy/spacy/lang/da/norm_exceptions.py,287,1,not
spaCy/spacy/lang/da/norm_exceptions.py,288,1,not
spaCy/spacy/lang/da/norm_exceptions.py,289,1,not
spaCy/spacy/lang/da/norm_exceptions.py,290,1,not
spaCy/spacy/lang/da/norm_exceptions.py,291,1,not
spaCy/spacy/lang/da/norm_exceptions.py,292,2,not
spaCy/spacy/lang/da/norm_exceptions.py,293,1,not
spaCy/spacy/lang/da/norm_exceptions.py,294,1,not
spaCy/spacy/lang/da/norm_exceptions.py,295,1,not
spaCy/spacy/lang/da/norm_exceptions.py,296,1,not
spaCy/spacy/lang/da/norm_exceptions.py,297,1,not
spaCy/spacy/lang/da/norm_exceptions.py,298,1,not
spaCy/spacy/lang/da/norm_exceptions.py,299,1,not
spaCy/spacy/lang/da/norm_exceptions.py,300,1,not
spaCy/spacy/lang/da/norm_exceptions.py,301,1,not
spaCy/spacy/lang/da/norm_exceptions.py,302,1,not
spaCy/spacy/lang/da/norm_exceptions.py,303,1,not
spaCy/spacy/lang/da/norm_exceptions.py,304,1,not
spaCy/spacy/lang/da/norm_exceptions.py,305,1,not
spaCy/spacy/lang/da/norm_exceptions.py,306,1,not
spaCy/spacy/lang/da/norm_exceptions.py,307,1,not
spaCy/spacy/lang/da/norm_exceptions.py,308,1,not
spaCy/spacy/lang/da/norm_exceptions.py,309,1,not
spaCy/spacy/lang/da/norm_exceptions.py,310,1,not
spaCy/spacy/lang/da/norm_exceptions.py,311,1,not
spaCy/spacy/lang/da/norm_exceptions.py,312,1,not
spaCy/spacy/lang/da/norm_exceptions.py,313,1,not
spaCy/spacy/lang/da/norm_exceptions.py,314,1,not
spaCy/spacy/lang/da/norm_exceptions.py,315,1,not
spaCy/spacy/lang/da/norm_exceptions.py,316,1,not
spaCy/spacy/lang/da/norm_exceptions.py,317,1,not
spaCy/spacy/lang/da/norm_exceptions.py,318,1,not
spaCy/spacy/lang/da/norm_exceptions.py,319,1,not
spaCy/spacy/lang/da/norm_exceptions.py,320,1,not
spaCy/spacy/lang/da/norm_exceptions.py,321,1,not
spaCy/spacy/lang/da/norm_exceptions.py,322,2,not
spaCy/spacy/lang/da/norm_exceptions.py,323,2,not
spaCy/spacy/lang/da/norm_exceptions.py,324,1,not
spaCy/spacy/lang/da/norm_exceptions.py,325,1,not
spaCy/spacy/lang/da/norm_exceptions.py,326,1,not
spaCy/spacy/lang/da/norm_exceptions.py,327,1,not
spaCy/spacy/lang/da/norm_exceptions.py,328,1,not
spaCy/spacy/lang/da/norm_exceptions.py,329,1,not
spaCy/spacy/lang/da/norm_exceptions.py,330,1,not
spaCy/spacy/lang/da/norm_exceptions.py,331,2,not
spaCy/spacy/lang/da/norm_exceptions.py,332,2,not
spaCy/spacy/lang/da/norm_exceptions.py,333,1,not
spaCy/spacy/lang/da/norm_exceptions.py,334,1,not
spaCy/spacy/lang/da/norm_exceptions.py,335,1,not
spaCy/spacy/lang/da/norm_exceptions.py,336,2,not
spaCy/spacy/lang/da/norm_exceptions.py,337,2,not
spaCy/spacy/lang/da/norm_exceptions.py,338,2,not
spaCy/spacy/lang/da/norm_exceptions.py,339,1,not
spaCy/spacy/lang/da/norm_exceptions.py,340,1,not
spaCy/spacy/lang/da/norm_exceptions.py,341,1,not
spaCy/spacy/lang/da/norm_exceptions.py,342,1,not
spaCy/spacy/lang/da/norm_exceptions.py,343,1,not
spaCy/spacy/lang/da/norm_exceptions.py,344,1,not
spaCy/spacy/lang/da/norm_exceptions.py,345,1,not
spaCy/spacy/lang/da/norm_exceptions.py,346,1,not
spaCy/spacy/lang/da/norm_exceptions.py,347,2,not
spaCy/spacy/lang/da/norm_exceptions.py,348,1,not
spaCy/spacy/lang/da/norm_exceptions.py,349,1,not
spaCy/spacy/lang/da/norm_exceptions.py,350,1,not
spaCy/spacy/lang/da/norm_exceptions.py,351,1,not
spaCy/spacy/lang/da/norm_exceptions.py,352,2,not
spaCy/spacy/lang/da/norm_exceptions.py,353,1,not
spaCy/spacy/lang/da/norm_exceptions.py,354,1,not
spaCy/spacy/lang/da/norm_exceptions.py,355,1,not
spaCy/spacy/lang/da/norm_exceptions.py,356,1,not
spaCy/spacy/lang/da/norm_exceptions.py,357,2,not
spaCy/spacy/lang/da/norm_exceptions.py,358,1,not
spaCy/spacy/lang/da/norm_exceptions.py,359,1,not
spaCy/spacy/lang/da/norm_exceptions.py,360,1,not
spaCy/spacy/lang/da/norm_exceptions.py,361,1,not
spaCy/spacy/lang/da/norm_exceptions.py,362,1,not
spaCy/spacy/lang/da/norm_exceptions.py,363,2,not
spaCy/spacy/lang/da/norm_exceptions.py,364,2,not
spaCy/spacy/lang/da/norm_exceptions.py,365,1,not
spaCy/spacy/lang/da/norm_exceptions.py,366,1,not
spaCy/spacy/lang/da/norm_exceptions.py,367,2,not
spaCy/spacy/lang/da/norm_exceptions.py,368,1,not
spaCy/spacy/lang/da/norm_exceptions.py,369,1,not
spaCy/spacy/lang/da/norm_exceptions.py,370,1,not
spaCy/spacy/lang/da/norm_exceptions.py,371,1,not
spaCy/spacy/lang/da/norm_exceptions.py,372,1,not
spaCy/spacy/lang/da/norm_exceptions.py,373,1,not
spaCy/spacy/lang/da/norm_exceptions.py,374,1,not
spaCy/spacy/lang/da/norm_exceptions.py,375,1,not
spaCy/spacy/lang/da/norm_exceptions.py,376,1,not
spaCy/spacy/lang/da/norm_exceptions.py,377,2,not
spaCy/spacy/lang/da/norm_exceptions.py,378,1,not
spaCy/spacy/lang/da/norm_exceptions.py,379,1,not
spaCy/spacy/lang/da/norm_exceptions.py,380,2,not
spaCy/spacy/lang/da/norm_exceptions.py,381,2,not
spaCy/spacy/lang/da/norm_exceptions.py,382,1,not
spaCy/spacy/lang/da/norm_exceptions.py,383,1,not
spaCy/spacy/lang/da/norm_exceptions.py,384,1,not
spaCy/spacy/lang/da/norm_exceptions.py,385,1,not
spaCy/spacy/lang/da/norm_exceptions.py,386,2,not
spaCy/spacy/lang/da/norm_exceptions.py,387,1,not
spaCy/spacy/lang/da/norm_exceptions.py,388,1,not
spaCy/spacy/lang/da/norm_exceptions.py,389,1,not
spaCy/spacy/lang/da/norm_exceptions.py,390,1,not
spaCy/spacy/lang/da/norm_exceptions.py,391,1,not
spaCy/spacy/lang/da/norm_exceptions.py,392,1,not
spaCy/spacy/lang/da/norm_exceptions.py,393,1,not
spaCy/spacy/lang/da/norm_exceptions.py,394,1,not
spaCy/spacy/lang/da/norm_exceptions.py,395,1,not
spaCy/spacy/lang/da/norm_exceptions.py,396,1,not
spaCy/spacy/lang/da/norm_exceptions.py,397,2,not
spaCy/spacy/lang/da/norm_exceptions.py,398,1,not
spaCy/spacy/lang/da/norm_exceptions.py,399,1,not
spaCy/spacy/lang/da/norm_exceptions.py,400,1,not
spaCy/spacy/lang/da/norm_exceptions.py,401,1,not
spaCy/spacy/lang/da/norm_exceptions.py,402,1,not
spaCy/spacy/lang/da/norm_exceptions.py,403,1,not
spaCy/spacy/lang/da/norm_exceptions.py,404,1,not
spaCy/spacy/lang/da/norm_exceptions.py,405,1,not
spaCy/spacy/lang/da/norm_exceptions.py,406,1,not
spaCy/spacy/lang/da/norm_exceptions.py,407,1,not
spaCy/spacy/lang/da/norm_exceptions.py,408,1,not
spaCy/spacy/lang/da/norm_exceptions.py,409,1,not
spaCy/spacy/lang/da/norm_exceptions.py,410,1,not
spaCy/spacy/lang/da/norm_exceptions.py,411,1,not
spaCy/spacy/lang/da/norm_exceptions.py,412,1,not
spaCy/spacy/lang/da/norm_exceptions.py,413,1,not
spaCy/spacy/lang/da/norm_exceptions.py,414,1,not
spaCy/spacy/lang/da/norm_exceptions.py,415,1,not
spaCy/spacy/lang/da/norm_exceptions.py,416,1,not
spaCy/spacy/lang/da/norm_exceptions.py,417,1,not
spaCy/spacy/lang/da/norm_exceptions.py,418,1,not
spaCy/spacy/lang/da/norm_exceptions.py,419,1,not
spaCy/spacy/lang/da/norm_exceptions.py,420,1,not
spaCy/spacy/lang/da/norm_exceptions.py,421,1,not
spaCy/spacy/lang/da/norm_exceptions.py,422,1,not
spaCy/spacy/lang/da/norm_exceptions.py,423,1,not
spaCy/spacy/lang/da/norm_exceptions.py,424,1,not
spaCy/spacy/lang/da/norm_exceptions.py,425,1,not
spaCy/spacy/lang/da/norm_exceptions.py,426,1,not
spaCy/spacy/lang/da/norm_exceptions.py,427,2,not
spaCy/spacy/lang/da/norm_exceptions.py,428,1,not
spaCy/spacy/lang/da/norm_exceptions.py,429,1,not
spaCy/spacy/lang/da/norm_exceptions.py,430,1,not
spaCy/spacy/lang/da/norm_exceptions.py,431,1,not
spaCy/spacy/lang/da/norm_exceptions.py,432,1,not
spaCy/spacy/lang/da/norm_exceptions.py,433,2,not
spaCy/spacy/lang/da/norm_exceptions.py,434,2,not
spaCy/spacy/lang/da/norm_exceptions.py,435,1,not
spaCy/spacy/lang/da/norm_exceptions.py,436,1,not
spaCy/spacy/lang/da/norm_exceptions.py,437,2,not
spaCy/spacy/lang/da/norm_exceptions.py,438,1,not
spaCy/spacy/lang/da/norm_exceptions.py,439,1,not
spaCy/spacy/lang/da/norm_exceptions.py,440,1,not
spaCy/spacy/lang/da/norm_exceptions.py,441,1,not
spaCy/spacy/lang/da/norm_exceptions.py,442,1,not
spaCy/spacy/lang/da/norm_exceptions.py,443,1,not
spaCy/spacy/lang/da/norm_exceptions.py,444,1,not
spaCy/spacy/lang/da/norm_exceptions.py,445,1,not
spaCy/spacy/lang/da/norm_exceptions.py,446,2,not
spaCy/spacy/lang/da/norm_exceptions.py,447,1,not
spaCy/spacy/lang/da/norm_exceptions.py,448,1,not
spaCy/spacy/lang/da/norm_exceptions.py,449,1,not
spaCy/spacy/lang/da/norm_exceptions.py,450,1,not
spaCy/spacy/lang/da/norm_exceptions.py,451,1,not
spaCy/spacy/lang/da/norm_exceptions.py,452,1,not
spaCy/spacy/lang/da/norm_exceptions.py,453,1,not
spaCy/spacy/lang/da/norm_exceptions.py,454,1,not
spaCy/spacy/lang/da/norm_exceptions.py,455,1,not
spaCy/spacy/lang/da/norm_exceptions.py,456,1,not
spaCy/spacy/lang/da/norm_exceptions.py,457,1,not
spaCy/spacy/lang/da/norm_exceptions.py,458,1,not
spaCy/spacy/lang/da/norm_exceptions.py,459,1,not
spaCy/spacy/lang/da/norm_exceptions.py,460,1,not
spaCy/spacy/lang/da/norm_exceptions.py,461,1,not
spaCy/spacy/lang/da/norm_exceptions.py,462,1,not
spaCy/spacy/lang/da/norm_exceptions.py,463,1,not
spaCy/spacy/lang/da/norm_exceptions.py,464,1,not
spaCy/spacy/lang/da/norm_exceptions.py,465,1,not
spaCy/spacy/lang/da/norm_exceptions.py,466,1,not
spaCy/spacy/lang/da/norm_exceptions.py,467,2,not
spaCy/spacy/lang/da/norm_exceptions.py,468,2,not
spaCy/spacy/lang/da/norm_exceptions.py,469,1,not
spaCy/spacy/lang/da/norm_exceptions.py,470,1,not
spaCy/spacy/lang/da/norm_exceptions.py,471,1,not
spaCy/spacy/lang/da/norm_exceptions.py,472,2,not
spaCy/spacy/lang/da/norm_exceptions.py,473,2,not
spaCy/spacy/lang/da/norm_exceptions.py,474,1,not
spaCy/spacy/lang/da/norm_exceptions.py,475,1,not
spaCy/spacy/lang/da/norm_exceptions.py,476,1,not
spaCy/spacy/lang/da/norm_exceptions.py,477,1,not
spaCy/spacy/lang/da/norm_exceptions.py,478,1,not
spaCy/spacy/lang/da/norm_exceptions.py,479,1,not
spaCy/spacy/lang/da/norm_exceptions.py,480,1,not
spaCy/spacy/lang/da/norm_exceptions.py,481,1,not
spaCy/spacy/lang/da/norm_exceptions.py,482,1,not
spaCy/spacy/lang/da/norm_exceptions.py,483,1,not
spaCy/spacy/lang/da/norm_exceptions.py,484,1,not
spaCy/spacy/lang/da/norm_exceptions.py,485,1,not
spaCy/spacy/lang/da/norm_exceptions.py,486,1,not
spaCy/spacy/lang/da/norm_exceptions.py,487,1,not
spaCy/spacy/lang/da/norm_exceptions.py,488,1,not
spaCy/spacy/lang/da/norm_exceptions.py,489,1,not
spaCy/spacy/lang/da/norm_exceptions.py,490,2,not
spaCy/spacy/lang/da/norm_exceptions.py,491,1,not
spaCy/spacy/lang/da/norm_exceptions.py,492,1,not
spaCy/spacy/lang/da/norm_exceptions.py,493,1,not
spaCy/spacy/lang/da/norm_exceptions.py,494,1,not
spaCy/spacy/lang/da/norm_exceptions.py,495,1,not
spaCy/spacy/lang/da/norm_exceptions.py,496,1,not
spaCy/spacy/lang/da/norm_exceptions.py,497,1,not
spaCy/spacy/lang/da/norm_exceptions.py,498,2,not
spaCy/spacy/lang/da/norm_exceptions.py,499,1,not
spaCy/spacy/lang/da/norm_exceptions.py,500,1,not
spaCy/spacy/lang/da/norm_exceptions.py,501,1,not
spaCy/spacy/lang/da/norm_exceptions.py,502,1,not
spaCy/spacy/lang/da/norm_exceptions.py,503,1,not
spaCy/spacy/lang/da/norm_exceptions.py,504,1,not
spaCy/spacy/lang/da/norm_exceptions.py,505,1,not
spaCy/spacy/lang/da/norm_exceptions.py,506,1,not
spaCy/spacy/lang/da/norm_exceptions.py,507,2,not
spaCy/spacy/lang/da/norm_exceptions.py,508,1,not
spaCy/spacy/lang/da/norm_exceptions.py,509,1,not
spaCy/spacy/lang/da/norm_exceptions.py,510,1,not
spaCy/spacy/lang/da/norm_exceptions.py,511,1,not
spaCy/spacy/lang/da/norm_exceptions.py,512,2,not
spaCy/spacy/lang/da/norm_exceptions.py,513,1,not
spaCy/spacy/lang/da/norm_exceptions.py,514,1,not
spaCy/spacy/lang/da/norm_exceptions.py,515,1,not
spaCy/spacy/lang/da/norm_exceptions.py,516,1,not
spaCy/spacy/lang/da/norm_exceptions.py,517,1,not
spaCy/spacy/lang/da/norm_exceptions.py,518,2,not
spaCy/spacy/lang/da/norm_exceptions.py,519,1,not
spaCy/spacy/lang/da/morph_rules.py,1,coding: utf8,not
spaCy/spacy/lang/da/morph_rules.py,6,Source: Danish Universal Dependencies and http://fjern-uv.dk/pronom.php,not
spaCy/spacy/lang/da/morph_rules.py,8,Note: The Danish Universal Dependencies specify Case=Acc for all instances,not
spaCy/spacy/lang/da/morph_rules.py,9,"of ""den""/""det"" even when the case is in fact ""Nom"". In the rules below, Case",not
spaCy/spacy/lang/da/morph_rules.py,10,"is left unspecified for ""den"" and ""det"".",not
spaCy/spacy/lang/da/morph_rules.py,21,Case=Nom|Gender=Com|Number=Sing|Person=1|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,29,Case=Acc|Gender=Com|Number=Sing|Person=1|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,37,Gender=Com|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,45,Gender=Neut|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,53,Gender=Com|Number=Sing|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs|Style=Form,not
spaCy/spacy/lang/da/morph_rules.py,61,Gender=Neut|Number=Sing|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs|Style=Form,not
spaCy/spacy/lang/da/morph_rules.py,69,Case=Nom|Gender=Com|Number=Sing|Person=2|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,77,Case=Acc|Gender=Com|Number=Sing|Person=2|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,85,Gender=Com|Number=Sing|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,93,Gender=Neut|Number=Sing|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,101,Case=Nom|Gender=Com|Number=Sing|Person=3|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,109,Case=Nom|Gender=Com|Number=Sing|Person=3|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,116,"Case=Acc|Gender=Com|Number=Sing|Person=3|PronType=Prs, See note above.",not
spaCy/spacy/lang/da/morph_rules.py,123,Case=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs See note above.,not
spaCy/spacy/lang/da/morph_rules.py,131,Case=Acc|Gender=Com|Number=Sing|Person=3|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,139,Case=Acc|Gender=Com|Number=Sing|Person=3|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,148,Gender=Com|Number=Sing|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs|Reflex=Yes,not
spaCy/spacy/lang/da/morph_rules.py,157,Gender=Neut|Number=Sing|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs|Reflex=Yes,not
spaCy/spacy/lang/da/morph_rules.py,165,Case=Nom|Gender=Com|Number=Plur|Person=1|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,173,Case=Acc|Gender=Com|Number=Plur|Person=1|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,180,Number=Plur|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,187,Number=Plur|Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs|Style=Form,not
spaCy/spacy/lang/da/morph_rules.py,195,Case=Nom|Gender=Com|Number=Plur|Person=2|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,203,Case=Acc|Gender=Com|Number=Plur|Person=2|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,210,Number=Plur|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,217,Case=Nom|Number=Plur|Person=3|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,224,Case=Acc|Number=Plur|Person=3|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,232,Number=Plur|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs|Reflex=Yes,not
spaCy/spacy/lang/da/morph_rules.py,238,Number[psor]=Plur|Person=1|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,245,Case=Nom|Gender=Com|Person=2|Polite=Form|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,252,Case=Acc|Gender=Com|Person=2|Polite=Form|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,258,Person=2|Polite=Form|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,264,Number[psor]=Plur|Person=2|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,271,Case=Acc|Person=3|PronType=Prs|Reflex=Yes,not
spaCy/spacy/lang/da/morph_rules.py,277,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,283,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,289,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,295,Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/morph_rules.py,301,Number[psor]=Plur|Person=3|Poss=Yes|PronType=Prs,not
spaCy/spacy/lang/da/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/da/stop_words.py,4,Source: Handpicked by Jens Dahl Møllerhøj.,not
spaCy/spacy/lang/da/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/da/tokenizer_exceptions.py,14,"Abbreviations for weekdays ""søn."" (for ""søndag"") as well as ""Tor."" and ""Tors.""",not
spaCy/spacy/lang/da/tokenizer_exceptions.py,15,"(for ""torsdag"") are left out because they are ambiguous. The same is the case",not
spaCy/spacy/lang/da/tokenizer_exceptions.py,16,"for abbreviations ""jul."" and ""Jul."" (""juli"").",not
spaCy/spacy/lang/da/tokenizer_exceptions.py,60,Specified case only,not
spaCy/spacy/lang/da/tokenizer_exceptions.py,575,Dates,not
spaCy/spacy/lang/da/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/da/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/da/lex_attrs.py,7,Source http://fjern-uv.dk/tal.php,not
spaCy/spacy/lang/da/lex_attrs.py,23,Source: http://www.duda.dk/video/dansk/grammatik/talord/talord.html,not
spaCy/spacy/lang/da/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/da/examples.py,1,coding: utf8,not
spaCy/spacy/lang/lt/morph_rules.py,1,coding: utf8,not
spaCy/spacy/lang/lt/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,11,"""G."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,12,"""J. E."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,13,"""J. Em."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,14,"""J.E."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,15,"""J.Em."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,16,"""K."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,17,"""N."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,18,"""V."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,19,"""Vt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,20,"""a."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,21,"""a.k."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,22,"""a.s."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,23,"""adv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,24,"""akad."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,25,"""aklg."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,26,"""akt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,27,"""al."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,28,"""ang."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,29,"""angl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,30,"""aps."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,31,"""apskr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,32,"""apyg."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,33,"""arbat."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,34,"""asist."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,35,"""asm."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,36,"""asm.k."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,37,"""asmv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,38,"""atk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,39,"""atsak."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,40,"""atsisk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,41,"""atsisk.sąsk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,42,"""atv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,43,"""aut."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,44,"""avd."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,45,"""b.k."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,46,"""baud."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,47,"""biol."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,48,"""bkl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,49,"""bot."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,50,"""bt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,51,"""buv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,52,"""ch."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,53,"""chem."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,54,"""corp."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,55,"""d."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,56,"""dab."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,57,"""dail."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,58,"""dek."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,59,"""deš."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,60,"""dir."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,61,"""dirig."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,62,"""doc."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,63,"""dol."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,64,"""dr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,65,"""drp."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,66,"""dvit."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,67,"""dėst."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,68,"""dš."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,69,"""dž."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,70,"""e.b."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,71,"""e.bankas"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,72,"""e.p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,73,"""e.parašas"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,74,"""e.paštas"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,75,"""e.v."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,76,"""e.valdžia"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,77,"""egz."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,78,"""eil."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,79,"""ekon."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,80,"""el."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,81,"""el.bankas"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,82,"""el.p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,83,"""el.parašas"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,84,"""el.paštas"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,85,"""el.valdžia"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,86,"""etc."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,87,"""ež."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,88,"""fak."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,89,"""faks."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,90,"""feat."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,91,"""filol."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,92,"""filos."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,93,"""g."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,94,"""gen."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,95,"""geol."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,96,"""gerb."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,97,"""gim."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,98,"""gr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,99,"""gv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,100,"""gyd."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,101,"""gyv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,102,"""habil."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,103,"""inc."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,104,"""insp."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,105,"""inž."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,106,"""ir pan."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,107,"""ir t. t."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,108,"""isp."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,109,"""istor."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,110,"""it."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,111,"""just."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,112,"""k."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,113,"""k. a."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,114,"""k.a."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,115,"""kab."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,116,"""kand."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,117,"""kart."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,118,"""kat."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,119,"""ketv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,120,"""kh."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,121,"""kl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,122,"""kln."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,123,"""km."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,124,"""kn."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,125,"""koresp."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,126,"""kpt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,127,"""kr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,128,"""kt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,129,"""kub."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,130,"""kun."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,131,"""kv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,132,"""kyš."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,133,"""l. e. p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,134,"""l.e.p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,135,"""lenk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,136,"""liet."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,137,"""lot."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,138,"""lt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,139,"""ltd."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,140,"""ltn."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,141,"""m."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,142,"""m.e.."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,143,"""m.m."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,144,"""mat."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,145,"""med."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,146,"""mgnt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,147,"""mgr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,148,"""min."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,149,"""mjr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,150,"""ml."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,151,"""mln."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,152,"""mlrd."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,153,"""mob."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,154,"""mok."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,155,"""moksl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,156,"""mokyt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,157,"""mot."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,158,"""mr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,159,"""mst."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,160,"""mstl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,161,"""mėn."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,162,"""nkt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,163,"""no."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,164,"""nr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,165,"""ntk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,166,"""nuotr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,167,"""op."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,168,"""org."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,169,"""orig."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,170,"""p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,171,"""p.d."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,172,"""p.m.e."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,173,"""p.s."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,174,"""pab."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,175,"""pan."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,176,"""past."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,177,"""pav."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,178,"""pavad."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,179,"""per."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,180,"""perd."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,181,"""pirm."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,182,"""pl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,183,"""plg."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,184,"""plk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,185,"""pr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,186,"""pr.Kr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,187,"""pranc."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,188,"""proc."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,189,"""prof."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,190,"""prom."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,191,"""prot."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,192,"""psl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,193,"""pss."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,194,"""pvz."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,195,"""pšt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,196,"""r."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,197,"""raj."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,198,"""red."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,199,"""rez."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,200,"""rež."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,201,"""rus."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,202,"""rš."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,203,"""s."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,204,"""sav."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,205,"""saviv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,206,"""sek."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,207,"""sekr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,208,"""sen."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,209,"""sh."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,210,"""sk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,211,"""skg."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,212,"""skv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,213,"""skyr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,214,"""sp."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,215,"""spec."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,216,"""sr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,217,"""st."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,218,"""str."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,219,"""stud."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,220,"""sąs."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,221,"""t."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,222,"""t. p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,223,"""t. y."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,224,"""t.p."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,225,"""t.t."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,226,"""t.y."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,227,"""techn."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,228,"""tel."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,229,"""teol."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,230,"""th."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,231,"""tir."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,232,"""trit."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,233,"""trln."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,234,"""tšk."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,235,"""tūks."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,236,"""tūkst."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,237,"""up."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,238,"""upl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,239,"""v.s."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,240,"""vad."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,241,"""val."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,242,"""valg."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,243,"""ved."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,244,"""vert."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,245,"""vet."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,246,"""vid."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,247,"""virš."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,248,"""vlsč."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,249,"""vnt."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,250,"""vok."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,251,"""vs."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,252,"""vtv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,253,"""vv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,254,"""vyr."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,255,"""vyresn."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,256,"""zool."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,257,"""Įn"",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,258,"""įl."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,259,"""š.m."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,260,"""šnek."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,261,"""šv."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,262,"""švč."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,263,"""ž.ū."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,264,"""žin."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,265,"""žml."",",not
spaCy/spacy/lang/lt/tokenizer_exceptions.py,266,"""žr."",",not
spaCy/spacy/lang/lt/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/lt/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/lt/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/lt/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/lt/examples.py,1,coding: utf8,not
spaCy/spacy/lang/vi/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/vi/stop_words.py,4,Source: https://github.com/stopwords/vietnamese-stopwords,not
spaCy/spacy/lang/vi/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/vi/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/vi/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/vi/__init__.py,15,for pickling,not
spaCy/spacy/lang/vi/__init__.py,26,override defaults,not
spaCy/spacy/lang/hy/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/af/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/af/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-af,not
spaCy/spacy/lang/af/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/uk/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/uk/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/uk/lemmatizer.py,1,coding: utf8,not
spaCy/spacy/lang/uk/lemmatizer.py,29,Skip unchangeable pos,not
spaCy/spacy/lang/uk/lemmatizer.py,36,Skip suggested parse variant for unknown word for pymorphy,not
spaCy/spacy/lang/uk/lemmatizer.py,55,VERB,not
spaCy/spacy/lang/uk/lemmatizer.py,116,Can also be an ADV - unchangeable,not
spaCy/spacy/lang/uk/lemmatizer.py,117,Can also be a SCONJ - both unchangeable ones,not
spaCy/spacy/lang/uk/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/uk/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/uk/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/uk/examples.py,1,coding: utf8,not
spaCy/spacy/lang/uk/examples.py,15,Serhiy Zhadan,not
spaCy/spacy/lang/uk/examples.py,17,wikipedia,not
spaCy/spacy/lang/uk/examples.py,19,blyznets_viktor_semenovych/zemlia_svitliachkiv,not
spaCy/spacy/lang/uk/examples.py,21,Hryhorij Czubaj,not
spaCy/spacy/lang/uk/examples.py,22,homographs,not
spaCy/spacy/lang/tt/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/tt/stop_words.py,4,Tatar stopwords are from https://github.com/aliiae/stopwords-tt,not
spaCy/spacy/lang/tt/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/tt/tokenizer_exceptions.py,9,Weekdays abbreviations,not
spaCy/spacy/lang/tt/tokenizer_exceptions.py,17,Months abbreviations,not
spaCy/spacy/lang/tt/tokenizer_exceptions.py,30,Number abbreviations,not
spaCy/spacy/lang/tt/tokenizer_exceptions.py,40,"""etc."" abbreviations",not
spaCy/spacy/lang/tt/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/tt/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/tt/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/tt/examples.py,1,coding: utf8,not
spaCy/spacy/lang/it/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/it/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/it/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/it/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/it/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/it/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ta/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/ta/norm_exceptions.py,5,Regional words normal,not
spaCy/spacy/lang/ta/norm_exceptions.py,6,Sri Lanka - wikipeadia,not
spaCy/spacy/lang/ta/norm_exceptions.py,41,Srilankan and indian,not
spaCy/spacy/lang/ta/norm_exceptions.py,52,relationship,not
spaCy/spacy/lang/ta/norm_exceptions.py,62,difference words,not
spaCy/spacy/lang/ta/norm_exceptions.py,71,regular spokens,not
spaCy/spacy/lang/ta/norm_exceptions.py,78,Portugeese formal words,not
spaCy/spacy/lang/ta/norm_exceptions.py,93,Dutch formal words,not
spaCy/spacy/lang/ta/norm_exceptions.py,98,English formal words,not
spaCy/spacy/lang/ta/norm_exceptions.py,105,Arabic formal words,not
spaCy/spacy/lang/ta/norm_exceptions.py,112,"Persian, Hindustanian and hindi formal words",not
spaCy/spacy/lang/ta/norm_exceptions.py,122,English words used in text conversations,not
spaCy/spacy/lang/ta/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/ta/stop_words.py,5,Stop words,not
spaCy/spacy/lang/ta/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ta/lex_attrs.py,57,"20-89 ,90-899,900-99999 and above have different suffixes",not
spaCy/spacy/lang/ta/lex_attrs.py,59,text without numeral suffixes,not
spaCy/spacy/lang/ta/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/ta/examples.py,1,coding: utf8,not
spaCy/spacy/lang/lv/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/lv/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-lv,not
spaCy/spacy/lang/lv/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/tr/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/tr/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-tr,not
spaCy/spacy/lang/tr/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/tr/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/tr/lex_attrs.py,7,"Thirteen, fifteen etc. are written separate: on üç",not
spaCy/spacy/lang/tr/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/tr/examples.py,1,coding: utf8,not
spaCy/spacy/lang/tl/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/tl/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/tl/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/tl/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/lb/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/lb/norm_exceptions.py,4,TODO,SATD
spaCy/spacy/lang/lb/norm_exceptions.py,5,norm execptions: find a possibility to deal with the zillions of spelling,not
spaCy/spacy/lang/lb/norm_exceptions.py,6,"variants (vläicht = vlaicht, vleicht, viläicht, viläischt, etc. etc.)",not
spaCy/spacy/lang/lb/norm_exceptions.py,7,here one could include the most common spelling mistakes,SATD
spaCy/spacy/lang/lb/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/lb/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/lb/tokenizer_exceptions.py,6,TODO,SATD
spaCy/spacy/lang/lb/tokenizer_exceptions.py,7,"treat other apostrophes within words as part of the word: [op d'mannst], [fir d'éischt] (= exceptions)",not
spaCy/spacy/lang/lb/tokenizer_exceptions.py,11,translate / delete what is not necessary,not
spaCy/spacy/lang/lb/tokenizer_exceptions.py,30,to be extended,not
spaCy/spacy/lang/lb/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/lb/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/lb/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/lb/tag_map.py,7,TODO: tag map is still using POS tags from an internal training set.,SATD
spaCy/spacy/lang/lb/tag_map.py,8,These POS tags have to be modified to match those from Universal Dependencies,not
spaCy/spacy/lang/lb/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/lb/examples.py,1,coding: utf8,not
spaCy/spacy/lang/fi/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/fi/stop_words.py,5,Source https://github.com/stopwords-iso/stopwords-fi/blob/master/stopwords-fi.txt,not
spaCy/spacy/lang/fi/stop_words.py,6,Reformatted with some minor corrections,not
spaCy/spacy/lang/fi/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/fi/tokenizer_exceptions.py,10,Source https://www.cs.tut.fi/~jkorpela/kielenopas/5.5.html,not
spaCy/spacy/lang/fi/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/fi/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/fi/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/fi/examples.py,1,coding: utf8,not
spaCy/spacy/lang/nb/morph_rules.py,1,encoding: utf8,not
spaCy/spacy/lang/nb/morph_rules.py,6,This dict includes all the PRON and DET tag combinations found in the,not
spaCy/spacy/lang/nb/morph_rules.py,7,"dataset developed by Schibsted, Nasjonalbiblioteket and LTG (to be published",not
spaCy/spacy/lang/nb/morph_rules.py,8,autumn 2018) and the rarely used polite form.,not
spaCy/spacy/lang/nb/morph_rules.py,28,"polite form, not sure about the tag",not
spaCy/spacy/lang/nb/morph_rules.py,132,"polite form, not sure about the tag",not
spaCy/spacy/lang/nb/morph_rules.py,259,"polite form, not sure about the tag",not
spaCy/spacy/lang/nb/morph_rules.py,320,"polite form, not sure about the tag",not
spaCy/spacy/lang/nb/morph_rules.py,381,"polite form, not sure about the tag",not
spaCy/spacy/lang/nb/morph_rules.py,656,"same wordform and pos (verb), have to specify the exact features in order to not mix them up",not
spaCy/spacy/lang/nb/morph_rules.py,665,copied from the English morph_rules.py,not
spaCy/spacy/lang/nb/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/nb/syntax_iterators.py,21,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/nb/syntax_iterators.py,29,Prevent nested chunks from being produced,not
spaCy/spacy/lang/nb/syntax_iterators.py,41,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/nb/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/nb/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/nb/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/nb/punctuation.py,9,Punctuation adapted from Danish,not
spaCy/spacy/lang/nb/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/nb/tag_map.py,8,Tags are a combination of POS and morphological features from a,not
spaCy/spacy/lang/nb/tag_map.py,9,"https://github.com/ltgoslo/norne developed by Schibsted, Nasjonalbiblioteket and LTG. The",not
spaCy/spacy/lang/nb/tag_map.py,10,data format is .conllu and follows the Universal Dependencies annotation.,not
spaCy/spacy/lang/nb/tag_map.py,11,(There are some annotation differences compared to this dataset:,not
spaCy/spacy/lang/nb/tag_map.py,12,https://github.com/UniversalDependencies/UD_Norwegian-Bokmaal,not
spaCy/spacy/lang/nb/tag_map.py,13,mainly in the way determiners and pronouns are tagged).,not
spaCy/spacy/lang/nb/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/nb/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ga/irish_morphology_helpers.py,1,coding: utf8,not
spaCy/spacy/lang/ga/irish_morphology_helpers.py,5,fmt: off,not
spaCy/spacy/lang/ga/irish_morphology_helpers.py,10,fmt: on,not
spaCy/spacy/lang/ga/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/ga/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/ga/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/ga/tag_map.py,4,fmt: off,not
spaCy/spacy/lang/ga/tag_map.py,369,fmt: on,not
spaCy/spacy/lang/ga/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/de/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/de/norm_exceptions.py,4,"Here we only want to include the absolute most common words. Otherwise,",not
spaCy/spacy/lang/de/norm_exceptions.py,5,this list would get impossibly long for German – especially considering the,not
spaCy/spacy/lang/de/norm_exceptions.py,6,"old vs. new spelling rules, and all possible cases.",not
spaCy/spacy/lang/de/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/de/syntax_iterators.py,11,this iterator extracts spans headed by NOUNs starting from the left-most,not
spaCy/spacy/lang/de/syntax_iterators.py,12,syntactic dependent until the NOUN itself for close apposition and,not
spaCy/spacy/lang/de/syntax_iterators.py,13,"measurement construction, the span is sometimes extended to the right of",not
spaCy/spacy/lang/de/syntax_iterators.py,14,"the NOUN. Example: ""eine Tasse Tee"" (a cup (of) tea) returns ""eine Tasse Tee""",not
spaCy/spacy/lang/de/syntax_iterators.py,15,"and not just ""eine Tasse"", same for ""das Thema Familie"".",not
spaCy/spacy/lang/de/syntax_iterators.py,30,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/de/syntax_iterators.py,41,try to extend the span to the right,not
spaCy/spacy/lang/de/syntax_iterators.py,42,to capture close apposition/measurement constructions,not
spaCy/spacy/lang/de/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/de/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/de/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/de/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/de/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/de/examples.py,1,coding: utf8,not
spaCy/spacy/lang/is/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/is/stop_words.py,5,Source: https://github.com/Xangis/extra-stopwords,not
spaCy/spacy/lang/is/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/hu/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/hu/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/hu/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/hu/punctuation.py,8,removing ° from the special icons to keep e.g. 99° as one token,not
spaCy/spacy/lang/hu/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/hu/examples.py,1,coding: utf8,not
spaCy/spacy/lang/hr/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/hr/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-hr,not
spaCy/spacy/lang/hr/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/hr/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ru/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/ru/norm_exceptions.py,6,Slang,not
spaCy/spacy/lang/ru/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,10,Weekdays abbreviations,not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,22,Months abbreviations,not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,27,"{ORTH: ""март"", LEMMA: ""март"", NORM: ""март""},",not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,30,"{ORTH: ""май"", LEMMA: ""май"", NORM: ""май""},",not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,32,"{ORTH: ""июнь"", LEMMA: ""июнь"", NORM: ""июнь""},",not
spaCy/spacy/lang/ru/tokenizer_exceptions.py,34,"{ORTH: ""июль"", LEMMA: ""июль"", NORM: ""июль""},",not
spaCy/spacy/lang/ru/lemmatizer.py,1,coding: utf8,not
spaCy/spacy/lang/ru/lemmatizer.py,32,Skip unchangeable pos,not
spaCy/spacy/lang/ru/lemmatizer.py,39,Skip suggested parse variant for unknown word for pymorphy,not
spaCy/spacy/lang/ru/lemmatizer.py,58,VERB,not
spaCy/spacy/lang/ru/lemmatizer.py,119,Can also be an ADV - unchangeable,not
spaCy/spacy/lang/ru/lemmatizer.py,120,Can also be a SCONJ - both unchangeable ones,not
spaCy/spacy/lang/ru/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ru/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/ru/tag_map.py,7,fmt: off,not
spaCy/spacy/lang/ru/tag_map.py,746,fmt: on,not
spaCy/spacy/lang/ru/__init__.py,1,encoding: utf8,not
spaCy/spacy/lang/ru/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ru/examples.py,14,Translations from English:,not
spaCy/spacy/lang/ru/examples.py,19,Native Russian sentences:,not
spaCy/spacy/lang/ru/examples.py,20,Colloquial:,not
spaCy/spacy/lang/ru/examples.py,21,Typical polite refusal,not
spaCy/spacy/lang/ru/examples.py,22,From a tour guide speech,not
spaCy/spacy/lang/ru/examples.py,23,Examples of Bookish Russian:,not
spaCy/spacy/lang/ru/examples.py,24,"Quote from ""The Golden Calf""",not
spaCy/spacy/lang/ru/examples.py,26,"Quotes from ""Ivan Vasilievich changes his occupation""",not
spaCy/spacy/lang/ru/examples.py,29,Quotes from Dostoevsky:,not
spaCy/spacy/lang/ru/examples.py,33,Quotes from Chekhov:,not
spaCy/spacy/lang/ru/examples.py,35,Quotes from Turgenev:,not
spaCy/spacy/lang/ru/examples.py,38,Quotes from newspapers:,not
spaCy/spacy/lang/ru/examples.py,39,Komsomolskaya Pravda:,not
spaCy/spacy/lang/ru/examples.py,42,Argumenty i Facty:,not
spaCy/spacy/lang/ro/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/ro/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-ro,not
spaCy/spacy/lang/ro/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/ro/tokenizer_exceptions.py,11,Source: https://en.wiktionary.org/wiki/Category:Romanian_abbreviations,not
spaCy/spacy/lang/ro/tokenizer_exceptions.py,49,below: from UD_Romanian-RRT:,not
spaCy/spacy/lang/ro/tokenizer_exceptions.py,92,note: does not distinguish capitalized-only exceptions from others,not
spaCy/spacy/lang/ro/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/ro/punctuation.py,35,UD_Romanian-RRT closed class prefixes,not
spaCy/spacy/lang/ro/punctuation.py,36,POS: ADP|AUX|CCONJ|DET|NUM|PART|PRON|SCONJ,not
spaCy/spacy/lang/ro/punctuation.py,70,UD_Romanian-RRT closed class suffixes without NUM,not
spaCy/spacy/lang/ro/punctuation.py,71,POS: ADP|AUX|CCONJ|DET|PART|PRON|SCONJ,not
spaCy/spacy/lang/ro/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ro/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/ro/__init__.py,16,Lemma data note:,not
spaCy/spacy/lang/ro/__init__.py,17,Original pairs downloaded from http://www.lexiconista.com/datasets/lemmatization/,not
spaCy/spacy/lang/ro/__init__.py,18,Replaced characters using cedillas with the correct ones (ș and ț),not
spaCy/spacy/lang/ro/examples.py,1,coding: utf8,not
spaCy/spacy/lang/te/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/te/stop_words.py,4,Source: https://github.com/Xangis/extra-stopwords (MIT License),not
spaCy/spacy/lang/te/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/te/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/te/examples.py,1,coding: utf8,not
spaCy/spacy/lang/es/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/es/syntax_iterators.py,12,"['nunmod', 'det', 'appos', 'fixed']",not
spaCy/spacy/lang/es/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/es/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/es/tokenizer_exceptions.py,27,Times,not
spaCy/spacy/lang/es/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/es/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/es/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/es/tag_map.py,7,fmt: off,not
spaCy/spacy/lang/es/tag_map.py,313,fmt: on,not
spaCy/spacy/lang/es/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/es/examples.py,1,coding: utf8,not
spaCy/spacy/lang/zh/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/zh/stop_words.py,5,stop words as whitespace-separated list,not
spaCy/spacy/lang/zh/stop_words.py,6,"Chinese stop words,maybe not enough",not
spaCy/spacy/lang/zh/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/zh/lex_attrs.py,89,fmt: off,not
spaCy/spacy/lang/zh/lex_attrs.py,94,fmt: on,not
spaCy/spacy/lang/zh/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/zh/tag_map.py,7,The Chinese part-of-speech tagger uses the OntoNotes 5 version of the Penn,not
spaCy/spacy/lang/zh/tag_map.py,8,Treebank tag set. We also map the tags to the simpler Universal Dependencies,not
spaCy/spacy/lang/zh/tag_map.py,9,v2 tag set.,not
spaCy/spacy/lang/zh/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/zh/__init__.py,26,segment a short text to have jieba initialize its cache in advance,not
spaCy/spacy/lang/zh/__init__.py,80,remove relevant settings from config so they're not also saved in,not
spaCy/spacy/lang/zh/__init__.py,81,Language.meta,not
spaCy/spacy/lang/zh/__init__.py,102,split into individual characters,not
spaCy/spacy/lang/zh/__init__.py,312,override defaults,not
spaCy/spacy/lang/zh/examples.py,1,coding: utf8,not
spaCy/spacy/lang/zh/examples.py,12,from https://zh.wikipedia.org/wiki/汉语,not
spaCy/spacy/lang/fa/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/fa/syntax_iterators.py,22,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/fa/syntax_iterators.py,30,Prevent nested chunks from being produced,not
spaCy/spacy/lang/fa/syntax_iterators.py,42,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/fa/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/fa/stop_words.py,5,Stop words from HAZM package,not
spaCy/spacy/lang/fa/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/fa/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/fa/punctuation.py,13,"4% -> [""4"", ""%""]",not
spaCy/spacy/lang/fa/punctuation.py,14,Persian is written from Right-To-Left,not
spaCy/spacy/lang/fa/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/fa/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/fa/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/fa/examples.py,1,coding: utf8,not
spaCy/spacy/lang/fa/generate_verbs_exc.py,1,coding: utf8,not
spaCy/spacy/lang/fa/generate_verbs_exc.py,609,"Below code is a modified version of HAZM package's verb conjugator,",not
spaCy/spacy/lang/fa/generate_verbs_exc.py,610,with soem extra verbs (Anything in hazm and not in here? compare needed!),not
spaCy/spacy/lang/fa/generate_verbs_exc.py,617,special case of '#هست':,not
spaCy/spacy/lang/lij/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/lij/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/lij/tokenizer_exceptions.py,28,"Prefix + prepositions with à (e.g. ""sott'a-o"")",not
spaCy/spacy/lang/lij/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/lij/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/lij/examples.py,1,coding: utf8,not
spaCy/spacy/lang/si/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/si/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/si/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/si/examples.py,1,coding: utf8,not
spaCy/spacy/lang/sv/morph_rules.py,1,coding: utf8,not
spaCy/spacy/lang/sv/morph_rules.py,7,Used the table of pronouns at https://sv.wiktionary.org/wiki/deras,not
spaCy/spacy/lang/sv/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/sv/syntax_iterators.py,22,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/sv/syntax_iterators.py,30,Prevent nested chunks from being produced,not
spaCy/spacy/lang/sv/syntax_iterators.py,42,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/sv/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,9,Verbs,not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,29,"Abbreviations for weekdays ""sön."" (for ""söndag"" / ""söner"")",not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,30,are left out because they are ambiguous. The same is the case,not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,31,"for abbreviations ""jul."" and ""Jul."" (""juli"" / ""jul"").",not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,73,Specific case abbreviations only,not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,145,Add abbreviation for trailing punctuation too. If the abbreviation already has a trailing punctuation - skip it.,not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,155,"Sentences ending in ""i."" (as in ""... peka i.""), ""m."" (as in ""...än 2000 m.""),",not
spaCy/spacy/lang/sv/tokenizer_exceptions.py,156,should be tokenized as two separate tokens.,not
spaCy/spacy/lang/sv/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/sv/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/sv/tag_map.py,8,Tag mappings according to https://universaldependencies.org/tagset-conversion/sv-suc-uposf.html,not
spaCy/spacy/lang/sv/tag_map.py,9,for https://github.com/UniversalDependencies/UD_Swedish-Talbanken,not
spaCy/spacy/lang/sv/tag_map.py,12,"inte, också, så, bara, nu",not
spaCy/spacy/lang/sv/tag_map.py,13,"t.ex., ca, t_ex, bl.a., s_k",not
spaCy/spacy/lang/sv/tag_map.py,14,"mer, tidigare, mindre, vidare, mera",not
spaCy/spacy/lang/sv/tag_map.py,15,"mycket, helt, ofta, länge, långt",not
spaCy/spacy/lang/sv/tag_map.py,16,"över-, in-",not
spaCy/spacy/lang/sv/tag_map.py,17,"minst, mest, högst, främst, helst",not
spaCy/spacy/lang/sv/tag_map.py,20,"det, detta",not
spaCy/spacy/lang/sv/tag_map.py,21,"ett, något, inget, vart, vartannat",not
spaCy/spacy/lang/sv/tag_map.py,22,allt,not
spaCy/spacy/lang/sv/tag_map.py,23,"de, dessa, bägge, dom",not
spaCy/spacy/lang/sv/tag_map.py,24,"några, inga",not
spaCy/spacy/lang/sv/tag_map.py,25,alla,not
spaCy/spacy/lang/sv/tag_map.py,26,samma,not
spaCy/spacy/lang/sv/tag_map.py,27,vardera,not
spaCy/spacy/lang/sv/tag_map.py,28,"varje, varenda",not
spaCy/spacy/lang/sv/tag_map.py,29,"den, denna",not
spaCy/spacy/lang/sv/tag_map.py,30,"en, någon, ingen, var, varannan",not
spaCy/spacy/lang/sv/tag_map.py,31,all,not
spaCy/spacy/lang/sv/tag_map.py,32,"när, där, hur, som, då",not
spaCy/spacy/lang/sv/tag_map.py,33,vilket,not
spaCy/spacy/lang/sv/tag_map.py,34,vilka,not
spaCy/spacy/lang/sv/tag_map.py,35,vilken,not
spaCy/spacy/lang/sv/tag_map.py,36,som,not
spaCy/spacy/lang/sv/tag_map.py,37,"vad, vilket",not
spaCy/spacy/lang/sv/tag_map.py,39,vilka,not
spaCy/spacy/lang/sv/tag_map.py,40,"vilken, vem",not
spaCy/spacy/lang/sv/tag_map.py,41,"vars, vilkas, Vems",not
spaCy/spacy/lang/sv/tag_map.py,42,att,not
spaCy/spacy/lang/sv/tag_map.py,43,"Jo, ja, nej, fan, visst",not
spaCy/spacy/lang/sv/tag_map.py,44,"ev, S:t, Kungl, Kungl., Teol",not
spaCy/spacy/lang/sv/tag_map.py,45,äldres,not
spaCy/spacy/lang/sv/tag_map.py,48,"större, högre, mindre, bättre, äldre",not
spaCy/spacy/lang/sv/tag_map.py,50,"enskildes, sjukes, andres",not
spaCy/spacy/lang/sv/tag_map.py,51,"enskilde, sjuke, andre, unge, ene",not
spaCy/spacy/lang/sv/tag_map.py,52,eget,not
spaCy/spacy/lang/sv/tag_map.py,54,"annat, svårt, möjligt, nytt, sådant",not
spaCy/spacy/lang/sv/tag_map.py,57,"ogiftas, ungas, frånskildas, efterkommandes, färgblindas",not
spaCy/spacy/lang/sv/tag_map.py,58,"olika, andra, många, stora, vissa",not
spaCy/spacy/lang/sv/tag_map.py,59,"flera, sådana, fler, få, samtliga",not
spaCy/spacy/lang/sv/tag_map.py,61,"bra, ena, enda, nästa, ringa",not
spaCy/spacy/lang/sv/tag_map.py,63,"hela, nya, andra, svenska, ekonomiska",not
spaCy/spacy/lang/sv/tag_map.py,64,"fri-, låg-, sexual-",not
spaCy/spacy/lang/sv/tag_map.py,65,egen,not
spaCy/spacy/lang/sv/tag_map.py,66,enskilds,not
spaCy/spacy/lang/sv/tag_map.py,67,"stor, annan, själv, sådan, viss",not
spaCy/spacy/lang/sv/tag_map.py,69,"störste, främste, äldste, minste",not
spaCy/spacy/lang/sv/tag_map.py,70,flesta,not
spaCy/spacy/lang/sv/tag_map.py,74,"bästa, största, närmaste, viktigaste, högsta",not
spaCy/spacy/lang/sv/tag_map.py,77,"störst, bäst, tidigast, högst, fattigast",not
spaCy/spacy/lang/sv/tag_map.py,78,"och, eller, som, än, men",not
spaCy/spacy/lang/sv/tag_map.py,80,"., ?, :, !, ...",not
spaCy/spacy/lang/sv/tag_map.py,81,",, -, :, *, ;",not
spaCy/spacy/lang/sv/tag_map.py,82,"godo, fjol, fullo, somras, måtto",not
spaCy/spacy/lang/sv/tag_map.py,83,"kr, %, s., dr, kap.",not
spaCy/spacy/lang/sv/tag_map.py,85,"yrkes-, barn-, hem-, fack-, vatten-",not
spaCy/spacy/lang/sv/tag_map.py,88,"barnens, årens, u-ländernas, företagens, århundradenas",not
spaCy/spacy/lang/sv/tag_map.py,89,"barnen, u-länderna, åren, länderna, könen",not
spaCy/spacy/lang/sv/tag_map.py,90,"slags, års, barns, länders, tusentals",not
spaCy/spacy/lang/sv/tag_map.py,91,"barn, år, fall, länder, problem",not
spaCy/spacy/lang/sv/tag_map.py,94,"äktenskapets, samhällets, barnets, 1800-talets, 1960-talets",not
spaCy/spacy/lang/sv/tag_map.py,97,"äktenskapet, samhället, barnet, stället, hemmet",not
spaCy/spacy/lang/sv/tag_map.py,98,"års, slags, lands, havs, företags",not
spaCy/spacy/lang/sv/tag_map.py,99,"år, arbete, barn, sätt, äktenskap",not
spaCy/spacy/lang/sv/tag_map.py,100,"PCB-, Syd-",not
spaCy/spacy/lang/sv/tag_map.py,101,"dags, rätta",not
spaCy/spacy/lang/sv/tag_map.py,102,"far-, kibbutz-, röntgen-, barna-, hälso-",not
spaCy/spacy/lang/sv/tag_map.py,105,"föräldrarnas, kvinnornas, elevernas, kibbutzernas, makarnas",not
spaCy/spacy/lang/sv/tag_map.py,108,"kvinnorna, föräldrarna, makarna, männen, hyrorna",not
spaCy/spacy/lang/sv/tag_map.py,109,"människors, kvinnors, dagars, tiders, månaders",not
spaCy/spacy/lang/sv/tag_map.py,110,"procent, människor, kvinnor, miljoner, kronor",not
spaCy/spacy/lang/sv/tag_map.py,111,"kvinnans, världens, familjens, dagens, jordens",not
spaCy/spacy/lang/sv/tag_map.py,112,"familjen, kvinnan, mannen, världen, skolan",not
spaCy/spacy/lang/sv/tag_map.py,113,"sorts, medelålders, makes, kvinnas, veckas",not
spaCy/spacy/lang/sv/tag_map.py,114,"del, tid, dag, fråga, man",not
spaCy/spacy/lang/sv/tag_map.py,115,", ), (",not
spaCy/spacy/lang/sv/tag_map.py,117,avlidnes,not
spaCy/spacy/lang/sv/tag_map.py,119,"taget, sett, särskilt, förbjudet, ökat",not
spaCy/spacy/lang/sv/tag_map.py,120,"försäkrades, anställdas",not
spaCy/spacy/lang/sv/tag_map.py,123,"särskilda, gifta, ökade, handikappade, skilda",not
spaCy/spacy/lang/sv/tag_map.py,125,"ökade, gifta, nämnda, nedärvda, dolda",not
spaCy/spacy/lang/sv/tag_map.py,127,"särskild, ökad, beredd, gift, oförändrad",not
spaCy/spacy/lang/sv/tag_map.py,130,"studerandes, sammanboendes, dubbelarbetandes",not
spaCy/spacy/lang/sv/tag_map.py,133,"följande, beroende, nuvarande, motsvarande, liknande",not
spaCy/spacy/lang/sv/tag_map.py,134,"ut, upp, in, till, med",not
spaCy/spacy/lang/sv/tag_map.py,136,"F, N, Liechtenstein, Danmark, DK",not
spaCy/spacy/lang/sv/tag_map.py,137,"Sveriges, EEC:s, Guds, Stockholms, Kristi",not
spaCy/spacy/lang/sv/tag_map.py,138,"Sverige, EEC, Stockholm, USA, ATP",not
spaCy/spacy/lang/sv/tag_map.py,139,"Göteborgs-, Nord-, Väst-",not
spaCy/spacy/lang/sv/tag_map.py,140,denne,not
spaCy/spacy/lang/sv/tag_map.py,141,"det, detta, detsamma",not
spaCy/spacy/lang/sv/tag_map.py,142,"något, allt, mycket, annat, ingenting",not
spaCy/spacy/lang/sv/tag_map.py,143,"dem, varandra, varann",not
spaCy/spacy/lang/sv/tag_map.py,144,"de, bägge",not
spaCy/spacy/lang/sv/tag_map.py,145,"dessa, dom, båda, den, bådadera",not
spaCy/spacy/lang/sv/tag_map.py,146,"andra, alla, många, sådana, några",not
spaCy/spacy/lang/sv/tag_map.py,147,"sig, sej",not
spaCy/spacy/lang/sv/tag_map.py,148,"oss, er, eder",not
spaCy/spacy/lang/sv/tag_map.py,149,vi,not
spaCy/spacy/lang/sv/tag_map.py,150,"dig, mig, henne, honom, Er",not
spaCy/spacy/lang/sv/tag_map.py,151,"du, han, hon, jag, ni",not
spaCy/spacy/lang/sv/tag_map.py,152,"den, denna, densamma",not
spaCy/spacy/lang/sv/tag_map.py,153,man,not
spaCy/spacy/lang/sv/tag_map.py,154,"en, var, någon, ingen, Varannan",not
spaCy/spacy/lang/sv/tag_map.py,155,"i, av, på, för, till",not
spaCy/spacy/lang/sv/tag_map.py,156,f,not
spaCy/spacy/lang/sv/tag_map.py,158,"sitt, vårt, ditt, mitt, ert",not
spaCy/spacy/lang/sv/tag_map.py,159,"sina, våra, dina, mina",not
spaCy/spacy/lang/sv/tag_map.py,160,"deras, dess, hans, hennes, varandras",not
spaCy/spacy/lang/sv/tag_map.py,161,"sin, vår, din, min, er",not
spaCy/spacy/lang/sv/tag_map.py,162,"2, 17, 20, 1, 18",not
spaCy/spacy/lang/sv/tag_map.py,165,ett,not
spaCy/spacy/lang/sv/tag_map.py,166,"två, tre, 1, 20, 2",not
spaCy/spacy/lang/sv/tag_map.py,167,"ett-, 1950-, två-, tre-, 1700-",not
spaCy/spacy/lang/sv/tag_map.py,169,en,not
spaCy/spacy/lang/sv/tag_map.py,171,förste,not
spaCy/spacy/lang/sv/tag_map.py,173,"första, andra, tredje, fjärde, femte",not
spaCy/spacy/lang/sv/tag_map.py,174,"att, om, innan, eftersom, medan",not
spaCy/spacy/lang/sv/tag_map.py,175,"companionship, vice, versa, family, capita",not
spaCy/spacy/lang/sv/tag_map.py,176,jfr,not
spaCy/spacy/lang/sv/tag_map.py,177,"se, Diskutera, låt, Läs, Gå",not
spaCy/spacy/lang/sv/tag_map.py,178,tas,not
spaCy/spacy/lang/sv/tag_map.py,179,"vara, få, ha, bli, kunna",not
spaCy/spacy/lang/sv/tag_map.py,180,"användas, finnas, göras, tas, ses",not
spaCy/spacy/lang/sv/tag_map.py,181,"vare, Gånge",not
spaCy/spacy/lang/sv/tag_map.py,182,"vore, finge",not
spaCy/spacy/lang/sv/tag_map.py,184,"är, har, kan, får, måste",not
spaCy/spacy/lang/sv/tag_map.py,185,"finns, kallas, behövs, beräknas, används",not
spaCy/spacy/lang/sv/tag_map.py,186,"skulle, var, hade, kunde, fick",not
spaCy/spacy/lang/sv/tag_map.py,187,"fanns, gjordes, höjdes, användes, infördes",not
spaCy/spacy/lang/sv/tag_map.py,188,läs-,not
spaCy/spacy/lang/sv/tag_map.py,189,"varit, fått, blivit, haft, kommit",not
spaCy/spacy/lang/sv/tag_map.py,190,"nämnts, gjorts, förändrats, sagts, framhållits",not
spaCy/spacy/lang/sv/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/sv/__init__.py,10,Punctuation stolen from Danish,not
spaCy/spacy/lang/sv/examples.py,1,coding: utf8,not
spaCy/spacy/lang/pt/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/pt/norm_exceptions.py,4,These exceptions are used to add NORM values based on a token's ORTH value.,not
spaCy/spacy/lang/pt/norm_exceptions.py,5,Individual languages can also add their own exceptions and overwrite them -,not
spaCy/spacy/lang/pt/norm_exceptions.py,6,"for example, British vs. American spelling in English.",not
spaCy/spacy/lang/pt/norm_exceptions.py,8,Norms are only set if no alternative is provided in the tokenizer exceptions.,not
spaCy/spacy/lang/pt/norm_exceptions.py,9,Note that this does not change any other token attributes. Its main purpose,not
spaCy/spacy/lang/pt/norm_exceptions.py,10,is to normalise the word representations so that equivalent tokens receive,not
spaCy/spacy/lang/pt/norm_exceptions.py,11,"similar representations. For example: $ and € are very different, but they're",not
spaCy/spacy/lang/pt/norm_exceptions.py,12,"both currency symbols. By normalising currency symbols to $, all symbols are",not
spaCy/spacy/lang/pt/norm_exceptions.py,13,"seen as similar, no matter how common they are in the training data.",not
spaCy/spacy/lang/pt/norm_exceptions.py,17,Real,not
spaCy/spacy/lang/pt/norm_exceptions.py,18,Real,not
spaCy/spacy/lang/pt/norm_exceptions.py,19,Cruzado,not
spaCy/spacy/lang/pt/norm_exceptions.py,20,Cruzado,not
spaCy/spacy/lang/pt/norm_exceptions.py,21,Cruzado Novo,not
spaCy/spacy/lang/pt/norm_exceptions.py,22,Cruzado Novo,not
spaCy/spacy/lang/pt/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/pt/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/pt/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/pt/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/pt/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/pt/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/pt/examples.py,1,coding: utf8,not
spaCy/spacy/lang/sl/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/sl/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-sl,not
spaCy/spacy/lang/sl/stop_words.py,6,TODO: probably needs to be tidied up – the list seems to have month names in,SATD
spaCy/spacy/lang/sl/stop_words.py,7,"it, which shouldn't be considered stop words.",not
spaCy/spacy/lang/sl/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/sr/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/sr/norm_exceptions.py,6,Slang,not
spaCy/spacy/lang/sr/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,10,Weekdays abbreviations,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,18,Months abbreviations,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,43,common abbreviations,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,45,without dot,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,59,with dot,not
spaCy/spacy/lang/sr/tokenizer_exceptions.py,84,with qoute,not
spaCy/spacy/lang/sr/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/sr/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/sr/examples.py,1,coding: utf8,not
spaCy/spacy/lang/sr/examples.py,14,Translations from English,not
spaCy/spacy/lang/sr/examples.py,20,Serbian common and slang,not
spaCy/spacy/lang/ml/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/ml/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ml/lex_attrs.py,7,reference 2: https://www.omniglot.com/language/numbers/malayalam.htm,not
spaCy/spacy/lang/ml/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/ml/examples.py,1,coding: utf8,not
spaCy/spacy/lang/id/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/id/norm_exceptions.py,4,Daftar kosakata yang sering salah dieja,not
spaCy/spacy/lang/id/norm_exceptions.py,5,https://id.wikipedia.org/wiki/Wikipedia:Daftar_kosakata_bahasa_Indonesia_yang_sering_salah_dieja,not
spaCy/spacy/lang/id/norm_exceptions.py,7,Slang and abbreviations,not
spaCy/spacy/lang/id/norm_exceptions.py,35,Daftar kosakata yang sering salah dieja,not
spaCy/spacy/lang/id/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/id/syntax_iterators.py,21,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/id/syntax_iterators.py,29,Prevent nested chunks from being produced,not
spaCy/spacy/lang/id/syntax_iterators.py,41,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/id/_tokenizer_exceptions_list.py,1,coding: utf8,not
spaCy/spacy/lang/id/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/id/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/id/tokenizer_exceptions.py,7,Daftar singkatan dan Akronim dari:,not
spaCy/spacy/lang/id/tokenizer_exceptions.py,8,https://id.wiktionary.org/wiki/Wiktionary:Daftar_singkatan_dan_akronim_bahasa_Indonesia#A,not
spaCy/spacy/lang/id/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/id/punctuation.py,31,hashtag,not
spaCy/spacy/lang/id/punctuation.py,38,disabled: variable width currency variable,not
spaCy/spacy/lang/id/punctuation.py,39,"r""(?<={c})(?:[0-9]+)"".format(c=CURRENCY),",not
spaCy/spacy/lang/id/punctuation.py,42,disabled: variable width HTML_SUFFIX variable,not
spaCy/spacy/lang/id/punctuation.py,43,"r""(?<=[0-9{a}]{h})(?:[\.,:-])"".format(a=ALPHA, h=HTML_SUFFIX),",not
spaCy/spacy/lang/id/punctuation.py,51,disabled: variable width units variable,not
spaCy/spacy/lang/id/punctuation.py,52,"r""(?<={u})[\/-](?=[0-9])"".format(u=UNITS),",not
spaCy/spacy/lang/id/punctuation.py,53,disabled: variable width months variable,not
spaCy/spacy/lang/id/punctuation.py,54,"r""(?<={m})[\/-](?=[0-9])"".format(m=MONTHS),",not
spaCy/spacy/lang/id/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/id/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/id/tag_map.py,8,POS explanations for indonesian available from https://www.aclweb.org/anthology/Y12-1014,not
spaCy/spacy/lang/id/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/id/examples.py,1,coding: utf8,not
spaCy/spacy/lang/et/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/et/stop_words.py,5,Source: https://github.com/stopwords-iso/stopwords-et,not
spaCy/spacy/lang/et/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/xx/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/xx/examples.py,1,coding: utf8,not
spaCy/spacy/lang/xx/examples.py,12,combined examples from de/en/es/fr/it/nl/pl/pt/ru,not
spaCy/spacy/lang/xx/examples.py,69,Translations from English:,not
spaCy/spacy/lang/xx/examples.py,74,Native Russian sentences:,not
spaCy/spacy/lang/xx/examples.py,75,Colloquial:,not
spaCy/spacy/lang/xx/examples.py,76,Typical polite refusal,not
spaCy/spacy/lang/xx/examples.py,77,From a tour guide speech,not
spaCy/spacy/lang/xx/examples.py,78,Examples of Bookish Russian:,not
spaCy/spacy/lang/xx/examples.py,79,"Quote from ""The Golden Calf""",not
spaCy/spacy/lang/xx/examples.py,81,"Quotes from ""Ivan Vasilievich changes his occupation""",not
spaCy/spacy/lang/xx/examples.py,84,Quotes from Dostoevsky:,not
spaCy/spacy/lang/xx/examples.py,88,Quotes from Chekhov:,not
spaCy/spacy/lang/xx/examples.py,90,Quotes from Turgenev:,not
spaCy/spacy/lang/xx/examples.py,93,Quotes from newspapers:,not
spaCy/spacy/lang/xx/examples.py,94,Komsomolskaya Pravda:,not
spaCy/spacy/lang/xx/examples.py,97,Argumenty i Facty:,not
spaCy/spacy/lang/ca/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/ca/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/ca/tokenizer_exceptions.py,31,Times,not
spaCy/spacy/lang/ca/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/ca/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ca/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/ca/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/ca/examples.py,1,coding: utf8,not
spaCy/spacy/lang/yo/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/yo/stop_words.py,4,stop words as whitespace-separated list.,not
spaCy/spacy/lang/yo/stop_words.py,5,Source: https://raw.githubusercontent.com/dohliam/more-stoplists/master/yo/yo.txt,not
spaCy/spacy/lang/yo/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/yo/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/yo/examples.py,1,coding: utf8,not
spaCy/spacy/lang/yo/examples.py,12,1. https://yo.wikipedia.org/wiki/Wikipedia:%C3%80y%E1%BB%8Dk%C3%A0_p%C3%A0t%C3%A0k%C3%AC,not
spaCy/spacy/lang/yo/examples.py,13,2.https://yo.wikipedia.org/wiki/Oj%C3%BAew%C3%A9_%C3%80k%E1%BB%8D%CC%81k%E1%BB%8D%CC%81,not
spaCy/spacy/lang/yo/examples.py,14,3. https://www.bbc.com/yoruba,not
spaCy/spacy/lang/th/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/th/norm_exceptions.py,6,Conjugation and Diversion invalid to Tonal form (ผันอักษรและเสียงไม่ตรงกับรูปวรรณยุกต์),not
spaCy/spacy/lang/th/norm_exceptions.py,9,Misspelled because of being lazy or hustle (สะกดผิดเพราะขี้เกียจพิมพ์ หรือเร่งรีบ),not
spaCy/spacy/lang/th/norm_exceptions.py,12,Strange (ให้ดูแปลกตา),not
spaCy/spacy/lang/th/norm_exceptions.py,48,Misspelled to express emotions (คำที่สะกดผิดเพื่อแสดงอารมณ์),not
spaCy/spacy/lang/th/norm_exceptions.py,72,Reduce rough words or Avoid to software filter (คำที่สะกดผิดเพื่อลดความหยาบของคำ หรืออาจใช้หลีกเลี่ยงการกรองคำหยาบของซอฟต์แวร์),not
spaCy/spacy/lang/th/norm_exceptions.py,93,Imitate words (คำเลียนเสียง โดยส่วนใหญ่จะเพิ่มทัณฑฆาต หรือซ้ำตัวอักษร),not
spaCy/spacy/lang/th/norm_exceptions.py,98,Acronym (แบบคำย่อ),not
spaCy/spacy/lang/th/tokenizer_exceptions.py,1,encoding: utf8,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,8,หน่วยงานรัฐ / government agency,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,102,มหาวิทยาลัย / สถานศึกษา / university / college,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,110,ยศ / rank,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,229,วุฒิ / bachelor degree,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,303,ปี / เวลา / year / time,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,309,ระยะทาง / distance,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,316,น้ำหนัก / weight,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,325,ปริมาตร / volume,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,333,พื้นที่ / area,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,338,เดือน / month,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,351,เพศ / gender,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,356,ที่อยู่ / address,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,361,สรรพนาม / pronoun,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,366,การเมือง / politic,not
spaCy/spacy/lang/th/tokenizer_exceptions.py,383,ทั่วไป / general,not
spaCy/spacy/lang/th/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/th/tag_map.py,1,encoding: utf8,not
spaCy/spacy/lang/th/tag_map.py,7,Source: Korakot Chaovavanich,not
spaCy/spacy/lang/th/tag_map.py,8,https://www.facebook.com/photo.php?fbid=390564854695031&set=p.390564854695031&type=3&permPage=1&ifg=1,not
spaCy/spacy/lang/th/tag_map.py,10,NOUN,not
spaCy/spacy/lang/th/tag_map.py,19,VERB,not
spaCy/spacy/lang/th/tag_map.py,22,PRON,not
spaCy/spacy/lang/th/tag_map.py,25,ADJ,not
spaCy/spacy/lang/th/tag_map.py,30,ADV,not
spaCy/spacy/lang/th/tag_map.py,36,INT,not
spaCy/spacy/lang/th/tag_map.py,38,PRON,not
spaCy/spacy/lang/th/tag_map.py,43,DET,not
spaCy/spacy/lang/th/tag_map.py,52,TODO: resolve duplicate (see below),SATD
spaCy/spacy/lang/th/tag_map.py,53,"""DCNM"": {POS: DET},",not
spaCy/spacy/lang/th/tag_map.py,54,NUM,not
spaCy/spacy/lang/th/tag_map.py,59,AUX,not
spaCy/spacy/lang/th/tag_map.py,66,ADP,not
spaCy/spacy/lang/th/tag_map.py,69,CCONJ,not
spaCy/spacy/lang/th/tag_map.py,72,SCONJ,not
spaCy/spacy/lang/th/tag_map.py,77,PART,not
spaCy/spacy/lang/th/tag_map.py,85,PUNCT,not
spaCy/spacy/lang/th/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/en/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/en/norm_exceptions.py,6,Slang and abbreviations,not
spaCy/spacy/lang/en/norm_exceptions.py,15,US vs. UK spelling,not
spaCy/spacy/lang/en/morph_rules.py,1,coding: utf8,not
spaCy/spacy/lang/en/morph_rules.py,6,Several entries here look pretty suspicious. These will get the POS SCONJ,not
spaCy/spacy/lang/en/morph_rules.py,7,"given the tag IN, when an adpositional reading seems much more likely for",not
spaCy/spacy/lang/en/morph_rules.py,8,a lot of these prepositions. I'm not sure what I was running in 04395ffa4,not
spaCy/spacy/lang/en/morph_rules.py,9,when I did this? It doesn't seem right.,not
spaCy/spacy/lang/en/morph_rules.py,15,"""of"",",not
spaCy/spacy/lang/en/morph_rules.py,16,"""for"",",not
spaCy/spacy/lang/en/morph_rules.py,17,"""before"",",not
spaCy/spacy/lang/en/morph_rules.py,18,"""in"",",not
spaCy/spacy/lang/en/morph_rules.py,20,"""after"",",not
spaCy/spacy/lang/en/morph_rules.py,23,"""with"",",not
spaCy/spacy/lang/en/morph_rules.py,25,"""to"",",not
spaCy/spacy/lang/en/morph_rules.py,26,"""by"",",not
spaCy/spacy/lang/en/morph_rules.py,27,"""on"",",not
spaCy/spacy/lang/en/morph_rules.py,28,"""about"",",not
spaCy/spacy/lang/en/morph_rules.py,32,"""from"",",not
spaCy/spacy/lang/en/morph_rules.py,34,"""until"",",not
spaCy/spacy/lang/en/morph_rules.py,37,"""without"",",not
spaCy/spacy/lang/en/morph_rules.py,38,"""at"",",not
spaCy/spacy/lang/en/morph_rules.py,39,"""into"",",not
spaCy/spacy/lang/en/morph_rules.py,41,"""over"",",not
spaCy/spacy/lang/en/morph_rules.py,45,"""beyond"",",not
spaCy/spacy/lang/en/morph_rules.py,50,"""then"",",not
spaCy/spacy/lang/en/morph_rules.py,54,"""below"",",not
spaCy/spacy/lang/en/morph_rules.py,55,"""against"",",not
spaCy/spacy/lang/en/morph_rules.py,58,"""toward"",",not
spaCy/spacy/lang/en/morph_rules.py,70,"""towards"",",not
spaCy/spacy/lang/en/morph_rules.py,76,This seems kind of wrong too?,not
spaCy/spacy/lang/en/morph_rules.py,77,"_relative_pronouns = [""this"", ""that"", ""those"", ""these""]",not
spaCy/spacy/lang/en/morph_rules.py,80,"""DT"": {word: {""POS"": ""PRON""} for word in _relative_pronouns},",not
spaCy/spacy/lang/en/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/en/syntax_iterators.py,22,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/en/syntax_iterators.py,30,Prevent nested chunks from being produced,not
spaCy/spacy/lang/en/syntax_iterators.py,42,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/en/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/en/stop_words.py,5,Stop words,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,28,Pronouns,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,140,"W-words, relative pronouns, prepositions etc.",not
spaCy/spacy/lang/en/tokenizer_exceptions.py,216,Verbs,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,300,Other contractions with trailing apostrophe,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,319,Other contractions with leading apostrophe,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,333,Times,not
spaCy/spacy/lang/en/tokenizer_exceptions.py,348,Rest,not
spaCy/spacy/lang/en/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/en/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/en/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/en/examples.py,1,coding: utf8,not
spaCy/spacy/lang/fr/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/fr/syntax_iterators.py,21,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/fr/syntax_iterators.py,29,Prevent nested chunks from being produced,not
spaCy/spacy/lang/fr/syntax_iterators.py,41,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/fr/_tokenizer_exceptions_list.py,1,coding: utf8,not
spaCy/spacy/lang/fr/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,11,not using the large _tokenizer_exceptions_list by default as it slows down the tokenizer,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,12,from ._tokenizer_exceptions_list import FR_BASE_EXCEPTIONS,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,96,", TAG: ""VERB""},",not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,105,", TAG: ""VERB""},",not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,140,"loop through the elison and hyphen characters, and try to substitute the ones that weren't used in the original list",not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,416,catching cases like faux-vampire,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,420,putting the - first in the [] range avoids having to use a backslash,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,427,catching cases like entr'abat,not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,436,"catching cases like saut-de-ski, pet-en-l'air",not
spaCy/spacy/lang/fr/tokenizer_exceptions.py,458,URLs,not
spaCy/spacy/lang/fr/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/fr/punctuation.py,30,"°C. -> [""°C"", "".""]",not
spaCy/spacy/lang/fr/punctuation.py,31,"4% -> [""4"", ""%""]",not
spaCy/spacy/lang/fr/lemmatizer.py,1,coding: utf8,not
spaCy/spacy/lang/fr/lemmatizer.py,48,See Issue #435 for example of where this logic is requied.,not
spaCy/spacy/lang/fr/lemmatizer.py,77,This maps 'VBP' to base form -- probably just need 'IS_BASE',not
spaCy/spacy/lang/fr/lemmatizer.py,78,morphology,not
spaCy/spacy/lang/fr/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/fr/lex_attrs.py,29,Might require more work?,not
spaCy/spacy/lang/fr/lex_attrs.py,30,See this discussion: https://github.com/explosion/spaCy/pull/1161,not
spaCy/spacy/lang/fr/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/fr/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/fr/examples.py,1,coding: utf8,not
spaCy/spacy/lang/el/norm_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/el/norm_exceptions.py,5,These exceptions are used to add NORM values based on a token's ORTH value.,not
spaCy/spacy/lang/el/norm_exceptions.py,6,Norms are only set if no alternative is provided in the tokenizer exceptions.,not
spaCy/spacy/lang/el/syntax_iterators.py,1,coding: utf8,not
spaCy/spacy/lang/el/syntax_iterators.py,11,"It follows the logic of the noun chunks finder of English language,",not
spaCy/spacy/lang/el/syntax_iterators.py,12,adjusted to some Greek language special characteristics.,not
spaCy/spacy/lang/el/syntax_iterators.py,13,obj tag corrects some DEP tagger mistakes.,not
spaCy/spacy/lang/el/syntax_iterators.py,14,Further improvement of the models will eliminate the need for this tag.,not
spaCy/spacy/lang/el/syntax_iterators.py,16,Ensure works on both Doc and Span.,not
spaCy/spacy/lang/el/syntax_iterators.py,25,Prevent nested chunks from being produced,not
spaCy/spacy/lang/el/syntax_iterators.py,33,check for patterns such as γραμμή παραγωγής,not
spaCy/spacy/lang/el/syntax_iterators.py,46,covers the case: έχει όμορφα και έξυπνα παιδιά,not
spaCy/spacy/lang/el/syntax_iterators.py,50,"If the head is an NP, and we're coordinated to it, we're an NP",not
spaCy/spacy/lang/el/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/el/stop_words.py,5,Stop words,not
spaCy/spacy/lang/el/stop_words.py,6,Link to greek stop words: https://www.translatum.gr/forum/index.php?topic=3550.0?topic=3550.0,not
spaCy/spacy/lang/el/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/el/punctuation.py,1,-*- coding: utf-8 -*-,not
spaCy/spacy/lang/el/punctuation.py,29,90%,not
spaCy/spacy/lang/el/punctuation.py,30,'12'-13,not
spaCy/spacy/lang/el/punctuation.py,31,-12.13,not
spaCy/spacy/lang/el/punctuation.py,32,'αβγ',not
spaCy/spacy/lang/el/punctuation.py,33,αβγ',not
spaCy/spacy/lang/el/punctuation.py,35,όνομα*,not
spaCy/spacy/lang/el/punctuation.py,51,12+,not
spaCy/spacy/lang/el/punctuation.py,52,12',not
spaCy/spacy/lang/el/punctuation.py,53,a',not
spaCy/spacy/lang/el/punctuation.py,54,12.,not
spaCy/spacy/lang/el/punctuation.py,55,12.,not
spaCy/spacy/lang/el/punctuation.py,56,12),not
spaCy/spacy/lang/el/punctuation.py,57,12),not
spaCy/spacy/lang/el/punctuation.py,59,12&,not
spaCy/spacy/lang/el/punctuation.py,66,όνομα-,not
spaCy/spacy/lang/el/punctuation.py,70,"πρώτος-δεύτερος , πρώτος-δεύτερος-τρίτος",not
spaCy/spacy/lang/el/punctuation.py,72,13mg,not
spaCy/spacy/lang/el/punctuation.py,73,1.2m,not
spaCy/spacy/lang/el/punctuation.py,81,"1/2 , 1-2 , 1*2",not
spaCy/spacy/lang/el/punctuation.py,82,name1/name2/name3,not
spaCy/spacy/lang/el/punctuation.py,83,"10.9 , 10.9.9 , 10.9-6",not
spaCy/spacy/lang/el/punctuation.py,84,"10,11,12",not
spaCy/spacy/lang/el/punctuation.py,85,1ης-2,not
spaCy/spacy/lang/el/punctuation.py,86,"15/2 , 15/2/17 , 2017/2/15",not
spaCy/spacy/lang/el/punctuation.py,88,abc@cde-fgh.a,not
spaCy/spacy/lang/el/punctuation.py,89,abc-abc,not
spaCy/spacy/lang/el/lemmatizer.py,1,coding: utf8,not
spaCy/spacy/lang/el/lex_attrs.py,1,-*- coding: utf-8 -*-,not
spaCy/spacy/lang/el/tag_map_fine.py,1,coding: utf8,not
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,1,coding: utf8,not
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,12,get words based on the Wiktionary dump,not
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,13,check only for specific parts,not
spaCy/spacy/lang/el/get_pos_from_wiktionary.py,15,==={{κύριο όνομα|el}}===,not
spaCy/spacy/lang/el/__init__.py,1,-*- coding: utf-8 -*-,not
spaCy/spacy/lang/el/examples.py,1,-*- coding: utf-8 -*-,not
spaCy/spacy/lang/ja/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/ja/stop_words.py,4,This list was created by taking the top 2000 words from a Wikipedia dump and,not
spaCy/spacy/lang/ja/stop_words.py,5,filtering out everything that wasn't hiragana. ー (one) was also added.,not
spaCy/spacy/lang/ja/stop_words.py,6,Considered keeping some non-hiragana words but too many place names were,not
spaCy/spacy/lang/ja/stop_words.py,7,present.,not
spaCy/spacy/lang/ja/tag_map.py,1,encoding: utf8,not
spaCy/spacy/lang/ja/tag_map.py,9,Explanation of Unidic tags:,not
spaCy/spacy/lang/ja/tag_map.py,10,https://www.gavo.t.u-tokyo.ac.jp/~mine/japanese/nlp+slp/UNIDIC_manual.pdf,not
spaCy/spacy/lang/ja/tag_map.py,11,Universal Dependencies Mapping:,not
spaCy/spacy/lang/ja/tag_map.py,12,http://universaldependencies.org/ja/overview/morphology.html,not
spaCy/spacy/lang/ja/tag_map.py,13,http://universaldependencies.org/ja/pos/all.html,not
spaCy/spacy/lang/ja/tag_map.py,16,this includes characters used to represent sounds like ドレミ,not
spaCy/spacy/lang/ja/tag_map.py,19,"this is for Greek and Latin characters used as sumbols, as in math",not
spaCy/spacy/lang/ja/tag_map.py,22,this is specifically for unicode full-width space,not
spaCy/spacy/lang/ja/tag_map.py,24,This is used when sequential half-width spaces are present,not
spaCy/spacy/lang/ja/tag_map.py,30,"XXX ADJ if alone, AUX otherwise",SATD
spaCy/spacy/lang/ja/tag_map.py,34,の as in 走るのが速い,not
spaCy/spacy/lang/ja/tag_map.py,35,verb ending て,not
spaCy/spacy/lang/ja/tag_map.py,36,"ばかり, つつ after a verb",not
spaCy/spacy/lang/ja/tag_map.py,38,XXX: might need refinement,SATD
spaCy/spacy/lang/ja/tag_map.py,40,"がち, チック",not
spaCy/spacy/lang/ja/tag_map.py,41,-らしい,not
spaCy/spacy/lang/ja/tag_map.py,42,-じみ,not
spaCy/spacy/lang/ja/tag_map.py,43,"XXX see 名詞,普通名詞,サ変可能,*",SATD
spaCy/spacy/lang/ja/tag_map.py,46,"-後, -過ぎ",not
spaCy/spacy/lang/ja/tag_map.py,49,"XXX VERB if alone, AUX otherwise",SATD
spaCy/spacy/lang/ja/tag_map.py,53,text art,not
spaCy/spacy/lang/ja/tag_map.py,54,kaomoji,not
spaCy/spacy/lang/ja/tag_map.py,56,open bracket,not
spaCy/spacy/lang/ja/tag_map.py,57,close bracket,not
spaCy/spacy/lang/ja/tag_map.py,58,period or other EOS marker,not
spaCy/spacy/lang/ja/tag_map.py,59,comma,not
spaCy/spacy/lang/ja/tag_map.py,60,general proper noun,not
spaCy/spacy/lang/ja/tag_map.py,61,person's name,not
spaCy/spacy/lang/ja/tag_map.py,62,surname,not
spaCy/spacy/lang/ja/tag_map.py,63,first name,not
spaCy/spacy/lang/ja/tag_map.py,64,place name,not
spaCy/spacy/lang/ja/tag_map.py,65,country name,not
spaCy/spacy/lang/ja/tag_map.py,67,includes Chinese numerals,not
spaCy/spacy/lang/ja/tag_map.py,68,XXX: sometimes VERB in UDv2; suru-verb noun,SATD
spaCy/spacy/lang/ja/tag_map.py,71,ex: 下手,not
spaCy/spacy/lang/ja/tag_map.py,73,XXX: sometimes ADJ in UDv2,SATD
spaCy/spacy/lang/ja/tag_map.py,76,counter / unit,not
spaCy/spacy/lang/ja/tag_map.py,78,XXX this has exceptions based on literal token,SATD
spaCy/spacy/lang/ja/__init__.py,1,encoding: utf8,not
spaCy/spacy/lang/ja/__init__.py,15,"Handling for multiple spaces in a row is somewhat awkward, this simplifies",not
spaCy/spacy/lang/ja/__init__.py,16,the flow by creating a dummy with the same interface.,not
spaCy/spacy/lang/ja/__init__.py,43,this is only used for consecutive ascii spaces,not
spaCy/spacy/lang/ja/__init__.py,47,TODO: This is a first take. The rules here are crude approximations.,SATD
spaCy/spacy/lang/ja/__init__.py,48,"For many of these, full dependencies are needed to properly resolve",not
spaCy/spacy/lang/ja/__init__.py,49,PoS mappings.,not
spaCy/spacy/lang/ja/__init__.py,71,"If there's more than one space, spaces after the first become tokens",not
spaCy/spacy/lang/ja/__init__.py,85,see #2901,not
spaCy/spacy/lang/ja/__init__.py,96,if there's no lemma info (it's an unk) just use the surface,not
spaCy/spacy/lang/ja/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ko/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/ko/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ko/lex_attrs.py,10,Native Korean number system,not
spaCy/spacy/lang/ko/lex_attrs.py,29,Sino-Korean number system,not
spaCy/spacy/lang/ko/tag_map.py,1,encoding: utf8,not
spaCy/spacy/lang/ko/tag_map.py,7,은전한닢(mecab-ko-dic)의 품사 태그를 universal pos tag로 대응시킴,not
spaCy/spacy/lang/ko/tag_map.py,8,https://docs.google.com/spreadsheets/d/1-9blXKjtjeKZqsf4NzHeYJCrr49-nXeRF6D80udfcwY/edit#gid=589544265,not
spaCy/spacy/lang/ko/tag_map.py,9,https://universaldependencies.org/u/pos/,not
spaCy/spacy/lang/ko/tag_map.py,11,"J.{1,2} 조사",not
spaCy/spacy/lang/ko/tag_map.py,19,보조사,not
spaCy/spacy/lang/ko/tag_map.py,20,접속 조사,not
spaCy/spacy/lang/ko/tag_map.py,21,접속 부사,not
spaCy/spacy/lang/ko/tag_map.py,22,일반 부사,not
spaCy/spacy/lang/ko/tag_map.py,23,관형사,not
spaCy/spacy/lang/ko/tag_map.py,24,접두사,not
spaCy/spacy/lang/ko/tag_map.py,25,XS. 접미사,not
spaCy/spacy/lang/ko/tag_map.py,29,어근,not
spaCy/spacy/lang/ko/tag_map.py,30,"E.{1,2} 어미",not
spaCy/spacy/lang/ko/tag_map.py,36,감탄사,not
spaCy/spacy/lang/ko/tag_map.py,37,동사,not
spaCy/spacy/lang/ko/tag_map.py,38,형용사,not
spaCy/spacy/lang/ko/tag_map.py,39,보조 용언,not
spaCy/spacy/lang/ko/tag_map.py,40,긍정 지정사(이다),not
spaCy/spacy/lang/ko/tag_map.py,41,부정 지정사(아니다),not
spaCy/spacy/lang/ko/tag_map.py,42,일반 명사(general noun),not
spaCy/spacy/lang/ko/tag_map.py,43,의존 명사,not
spaCy/spacy/lang/ko/tag_map.py,44,의존 명사(단위: unit),not
spaCy/spacy/lang/ko/tag_map.py,45,고유 명사(proper noun),not
spaCy/spacy/lang/ko/tag_map.py,46,대명사,not
spaCy/spacy/lang/ko/tag_map.py,47,수사(numerals),not
spaCy/spacy/lang/ko/tag_map.py,48,숫자,not
spaCy/spacy/lang/ko/tag_map.py,49,"S.{1,2} 부호",not
spaCy/spacy/lang/ko/tag_map.py,50,문장 부호,not
spaCy/spacy/lang/ko/tag_map.py,51,period or other EOS marker,not
spaCy/spacy/lang/ko/tag_map.py,53,"comma, etc.",not
spaCy/spacy/lang/ko/tag_map.py,54,open bracket,not
spaCy/spacy/lang/ko/tag_map.py,55,close bracket,not
spaCy/spacy/lang/ko/tag_map.py,56,기타 기호,not
spaCy/spacy/lang/ko/tag_map.py,57,외국어,not
spaCy/spacy/lang/ko/tag_map.py,58,한자,not
spaCy/spacy/lang/ko/__init__.py,1,encoding: utf8,not
spaCy/spacy/lang/ko/__init__.py,26,fmt: on,not
spaCy/spacy/lang/ko/__init__.py,53,stem(어간) or pre-final(선어말 어미),not
spaCy/spacy/lang/ko/__init__.py,59,"품사 태그(POS)[0], 의미 부류(semantic class)[1],	종성 유무(jongseong)[2], 읽기(reading)[3],",not
spaCy/spacy/lang/ko/__init__.py,60,"타입(type)[4], 첫번째 품사(start pos)[5],	마지막 품사(end pos)[6], 표현(expression)[7], *",not
spaCy/spacy/lang/ko/examples.py,1,coding: utf8,not
spaCy/spacy/lang/hi/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/hi/stop_words.py,5,"Source: https://github.com/taranjeet/hindi-tokenizer/blob/master/stopwords.txt, https://data.mendeley.com/datasets/bsr3frvvjc/1#file-a21d5092-99d7-45d8-b044-3ae9edd391c6",not
spaCy/spacy/lang/hi/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/hi/lex_attrs.py,8,fmt: off,not
spaCy/spacy/lang/hi/lex_attrs.py,16,fmt: on,not
spaCy/spacy/lang/hi/lex_attrs.py,18,reference 1:https://en.wikipedia.org/wiki/Indian_numbering_system,not
spaCy/spacy/lang/hi/lex_attrs.py,19,reference 2: https://blogs.transparent.com/hindi/hindi-numbers-1-100/,not
spaCy/spacy/lang/hi/lex_attrs.py,60,"normalise base exceptions,  e.g. punctuation or currency symbols",not
spaCy/spacy/lang/hi/lex_attrs.py,63,"set stem word as norm,  if available,  adapted from:",not
spaCy/spacy/lang/hi/lex_attrs.py,64,http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf,not
spaCy/spacy/lang/hi/lex_attrs.py,65,http://research.variancia.com/hindi_stemmer/,not
spaCy/spacy/lang/hi/lex_attrs.py,66,https://github.com/taranjeet/hindi-tokenizer/blob/master/HindiTokenizer.py#L142,not
spaCy/spacy/lang/hi/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/hi/examples.py,1,coding: utf8,not
spaCy/spacy/lang/he/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/he/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/he/examples.py,1,coding: utf8,not
spaCy/spacy/lang/eu/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/eu/stop_words.py,4,Source: https://github.com/stopwords-iso/stopwords-eu,not
spaCy/spacy/lang/eu/stop_words.py,5,https://www.ranks.nl/stopwords/basque,not
spaCy/spacy/lang/eu/stop_words.py,6,https://www.mustgo.com/worldlanguages/basque/,not
spaCy/spacy/lang/eu/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/eu/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/eu/lex_attrs.py,6,Source http://mylanguages.org/basque_numbers.php,not
spaCy/spacy/lang/eu/lex_attrs.py,35,source https://www.google.com/intl/ur/inputtools/try/,not
spaCy/spacy/lang/eu/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/eu/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/eu/examples.py,1,coding: utf8,not
spaCy/spacy/lang/nl/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/nl/stop_words.py,4,The original stop words list (added in f46ffe3) was taken from,not
spaCy/spacy/lang/nl/stop_words.py,5,http://www.damienvanholten.com/downloads/dutch-stop-words.txt,not
spaCy/spacy/lang/nl/stop_words.py,6,and consisted of about 100 tokens.,not
spaCy/spacy/lang/nl/stop_words.py,7,In order to achieve parity with some of the better-supported,not
spaCy/spacy/lang/nl/stop_words.py,8,"languages, e.g., English, French, and German, this original list has been",not
spaCy/spacy/lang/nl/stop_words.py,9,extended with 200 additional tokens. The main source of inspiration was,not
spaCy/spacy/lang/nl/stop_words.py,10,https://raw.githubusercontent.com/stopwords-iso/stopwords-nl/master/stopwords-nl.txt.,not
spaCy/spacy/lang/nl/stop_words.py,11,"However, quite a bit of manual editing has taken place as well.",SATD
spaCy/spacy/lang/nl/stop_words.py,12,Tokens whose status as a stop word is not entirely clear were admitted or,not
spaCy/spacy/lang/nl/stop_words.py,13,rejected by deferring to their counterparts in the stop words lists for English,not
spaCy/spacy/lang/nl/stop_words.py,14,"and French. Similarly, those lists were used to identify and fill in gaps so",not
spaCy/spacy/lang/nl/stop_words.py,15,that -- in principle -- each token contained in the English stop words list,not
spaCy/spacy/lang/nl/stop_words.py,16,should have a Dutch counterpart here.,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,1,coding: utf8,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,6,Extensive list of both common and uncommon dutch abbreviations copied from,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,7,"github.com/diasks2/pragmatic_segmenter, a Ruby library for rule-based",not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,8,"sentence boundary detection (MIT, Copyright 2015 Kevin S. Dias).",not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,9,Source file: https://github.com/diasks2/pragmatic_segmenter/blob/master/lib/pragmatic_segmenter/languages/dutch.rb,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,10,(Last commit: 4d1477b),not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,12,Main purpose of such an extensive list: considerably improved sentence,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,13,segmentation.,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,15,Note: This list has been copied over largely as-is. Some of the abbreviations,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,16,are extremely domain-specific. Tokenizer performance may benefit from some,not
spaCy/spacy/lang/nl/tokenizer_exceptions.py,17,"slight pruning, although no performance regression has been observed so far.",not
spaCy/spacy/lang/nl/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/nl/punctuation.py,14,Copied from `de` package. Main purpose is to ensure that hyphens are not,not
spaCy/spacy/lang/nl/punctuation.py,15,split on.,not
spaCy/spacy/lang/nl/lemmatizer.py,1,coding: utf8,not
spaCy/spacy/lang/nl/lemmatizer.py,9,"Note: CGN does not distinguish AUX verbs, so we treat AUX as VERB.",not
spaCy/spacy/lang/nl/lemmatizer.py,41,"Difference 1: self.rules is assumed to be non-None, so no",not
spaCy/spacy/lang/nl/lemmatizer.py,42,'is None' check required.,not
spaCy/spacy/lang/nl/lemmatizer.py,43,String lowercased from the get-go. All lemmatization results in,not
spaCy/spacy/lang/nl/lemmatizer.py,44,"lowercased strings. For most applications, this shouldn't pose",not
spaCy/spacy/lang/nl/lemmatizer.py,45,"any problems, and it keeps the exceptions indexes small. If this",not
spaCy/spacy/lang/nl/lemmatizer.py,46,"creates problems for proper nouns, we can introduce a check for",not
spaCy/spacy/lang/nl/lemmatizer.py,47,"univ_pos == ""PROPN"".",not
spaCy/spacy/lang/nl/lemmatizer.py,52,"Because PROPN not in self.univ_pos_name_variants, proper names",not
spaCy/spacy/lang/nl/lemmatizer.py,53,"are not lemmatized. They are lowercased, however.",not
spaCy/spacy/lang/nl/lemmatizer.py,55,if string in self.lemma_index.get(univ_pos),not
spaCy/spacy/lang/nl/lemmatizer.py,58,string is already lemma,not
spaCy/spacy/lang/nl/lemmatizer.py,63,string is irregular token contained in exceptions index.,not
spaCy/spacy/lang/nl/lemmatizer.py,69,string corresponds to key in lookup table,not
spaCy/spacy/lang/nl/lemmatizer.py,78,Back-off through remaining return value candidates.,not
spaCy/spacy/lang/nl/lemmatizer.py,95,Overrides parent method so that a lowercased version of the string is,not
spaCy/spacy/lang/nl/lemmatizer.py,96,used to search the lookup table. This is necessary because our lookup,not
spaCy/spacy/lang/nl/lemmatizer.py,97,table consists entirely of lowercase keys.,not
spaCy/spacy/lang/nl/lemmatizer.py,106,Reimplemented to focus more on application of suffix rules and to return,not
spaCy/spacy/lang/nl/lemmatizer.py,107,as early as possible.,not
spaCy/spacy/lang/nl/lemmatizer.py,109,"returns (forms, is_known: bool)",not
spaCy/spacy/lang/nl/lemmatizer.py,117,True = Is known (is lemma),not
spaCy/spacy/lang/nl/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/nl/lex_attrs.py,26,This only does the most basic check for whether a token is a digit,not
spaCy/spacy/lang/nl/lex_attrs.py,27,or matches one of the number words. In order to handle numbers like,not
spaCy/spacy/lang/nl/lex_attrs.py,28,"""drieëntwintig"", more work is required.",not
spaCy/spacy/lang/nl/lex_attrs.py,29,See this discussion: https://github.com/explosion/spaCy/pull/1177,not
spaCy/spacy/lang/nl/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/nl/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/nl/examples.py,1,coding: utf8,not
spaCy/spacy/lang/ur/stop_words.py,1,encoding: utf8,not
spaCy/spacy/lang/ur/stop_words.py,4,Source: collected from different resource on internet,not
spaCy/spacy/lang/ur/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/ur/lex_attrs.py,1,coding: utf8,not
spaCy/spacy/lang/ur/lex_attrs.py,6,Source https://quizlet.com/4271889/1-100-urdu-number-wordsurdu-numerals-flash-cards/,not
spaCy/spacy/lang/ur/lex_attrs.py,7,http://www.urduword.com/lessons.php?lesson=numbers,not
spaCy/spacy/lang/ur/lex_attrs.py,8,https://en.wikibooks.org/wiki/Urdu/Vocabulary/Numbers,not
spaCy/spacy/lang/ur/lex_attrs.py,9,https://www.urdu-english.com/lessons/beginner/numbers,not
spaCy/spacy/lang/ur/lex_attrs.py,24,source https://www.google.com/intl/ur/inputtools/try/,not
spaCy/spacy/lang/ur/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/ur/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/ur/examples.py,1,coding: utf8,not
spaCy/spacy/lang/bn/morph_rules.py,1,coding: utf8,not
spaCy/spacy/lang/bn/stop_words.py,1,coding: utf8,not
spaCy/spacy/lang/bn/tokenizer_exceptions.py,1,coding=utf-8,not
spaCy/spacy/lang/bn/punctuation.py,1,coding: utf8,not
spaCy/spacy/lang/bn/tag_map.py,1,coding: utf8,not
spaCy/spacy/lang/bn/__init__.py,1,coding: utf8,not
spaCy/spacy/lang/bn/examples.py,1,coding: utf8,not
spaCy/examples/deep_learning_keras.py,67,Sentiment has a native slot for a single float.,not
spaCy/examples/deep_learning_keras.py,68,"For arbitrary data storage, there's:",not
spaCy/examples/deep_learning_keras.py,69,doc.user_data['my_data'] = y,not
spaCy/examples/deep_learning_keras.py,195,Unzips into two lists,not
spaCy/examples/deep_learning_keras.py,217,Shape,not
spaCy/examples/deep_learning_keras.py,219,General NN config,not
spaCy/examples/deep_learning_keras.py,223,Training params,not
spaCy/examples/vectors_tensorboard.py,1,!/usr/bin/env python,not
spaCy/examples/vectors_tensorboard.py,2,coding: utf8,not
spaCy/examples/vectors_tensorboard.py,57,Store vector data in a tensorflow variable,not
spaCy/examples/vectors_tensorboard.py,60,Write a tab-separated file that contains information about the vectors for visualization,not
spaCy/examples/vectors_tensorboard.py,61,,not
spaCy/examples/vectors_tensorboard.py,62,Reference: https://www.tensorflow.org/programmers_guide/embedding#metadata,not
spaCy/examples/vectors_tensorboard.py,64,Define columns in the first row,not
spaCy/examples/vectors_tensorboard.py,66,Write out a row for each vector that we add to the tensorflow variable we created,not
spaCy/examples/vectors_tensorboard.py,69,https://github.com/tensorflow/tensorflow/issues/9094,not
spaCy/examples/vectors_tensorboard.py,73,Store vector data and metadata,not
spaCy/examples/vectors_tensorboard.py,89,Link the embeddings into the config,not
spaCy/examples/vectors_tensorboard.py,95,Tell the projector about the configured embeddings and metadata file,not
spaCy/examples/vectors_tensorboard.py,98,Save session and print run command to the output,not
spaCy/examples/vectors_fast_text.py,1,!/usr/bin/env python,not
spaCy/examples/vectors_fast_text.py,2,coding: utf8,not
spaCy/examples/vectors_fast_text.py,28,create empty language class – this is required if you're planning to,not
spaCy/examples/vectors_fast_text.py,29,save the model to disk and load it back later (models always need a,not
spaCy/examples/vectors_fast_text.py,30,"""lang"" setting). Use 'xx' for blank multi-language class.",not
spaCy/examples/vectors_fast_text.py,41,add the vectors to the vocab,not
spaCy/examples/vectors_fast_text.py,42,test the vectors and similarity,not
spaCy/examples/load_from_docbin.py,1,coding: utf-8,not
spaCy/examples/streamlit_spacy.py,1,coding: utf-8,not
spaCy/examples/streamlit_spacy.py,72,Double newlines seem to mess with the rendering,not
spaCy/examples/streamlit_spacy.py,86,Newlines seem to mess with the rendering,not
spaCy/examples/keras_parikh_entailment/spacy_hook.py,50,the extra +1 is for a zero vector representing sentence-final padding,not
spaCy/examples/keras_parikh_entailment/spacy_hook.py,53,create random vectors for OOV tokens,not
spaCy/examples/keras_parikh_entailment/__main__.py,17,workaround for keras/tensorflow bug,SATD
spaCy/examples/keras_parikh_entailment/__main__.py,18,see https://github.com/tensorflow/tensorflow/issues/3388,not
spaCy/examples/keras_parikh_entailment/__main__.py,66,remove the embedding matrix.  We can reconstruct it.,not
spaCy/examples/keras_parikh_entailment/__main__.py,115,"per Parikh, ignore - SNLI entries",not
spaCy/examples/keras_parikh_entailment/__main__.py,130,skip odd spaces from tokenizer,not
spaCy/examples/keras_parikh_entailment/__main__.py,140,"if we don't have a vector, pick an OOV entry",not
spaCy/examples/keras_parikh_entailment/__main__.py,143,there must be a simpler way of generating padded arrays from lists...,not
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,1,Semantic entailment/similarity with decomposable attention (using spaCy and Keras),not
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,2,Practical state-of-the-art textual entailment with spaCy and Keras,not
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,15,embeddings (projected),not
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,21,step 1: attend,not
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,33,step 2: compare,not
spaCy/examples/keras_parikh_entailment/keras_decomposable_attention.py,39,step 3: aggregate,not
spaCy/examples/information_extraction/parse_subtrees.py,1,!/usr/bin/env python,not
spaCy/examples/information_extraction/parse_subtrees.py,2,coding: utf8,not
spaCy/examples/information_extraction/parse_subtrees.py,38,"The easiest way is to find the head of the subtree you want, and then use",not
spaCy/examples/information_extraction/parse_subtrees.py,39,"the `.subtree`, `.children`, `.lefts` and `.rights` iterators. `.subtree`",not
spaCy/examples/information_extraction/parse_subtrees.py,40,is the one that does what you're asking for most directly:,not
spaCy/examples/information_extraction/parse_subtrees.py,45,It'd probably be better for `word.subtree` to return a `Span` object,SATD
spaCy/examples/information_extraction/parse_subtrees.py,46,instead of a generator over the tokens. If you want the `Span` you can,not
spaCy/examples/information_extraction/parse_subtrees.py,47,get it via the `.right_edge` and `.left_edge` properties. The `Span`,not
spaCy/examples/information_extraction/parse_subtrees.py,48,"object is nice because you can easily get a vector, merge it, etc.",SATD
spaCy/examples/information_extraction/parse_subtrees.py,54,"You might also want to select a head, and then select a start and end",not
spaCy/examples/information_extraction/parse_subtrees.py,55,position by walking along its children. You could then take the,not
spaCy/examples/information_extraction/parse_subtrees.py,56,"`.left_edge` and `.right_edge` of those tokens, and use it to calculate",not
spaCy/examples/information_extraction/parse_subtrees.py,57,a span.,not
spaCy/examples/information_extraction/parse_subtrees.py,63,Expected output:,not
spaCy/examples/information_extraction/parse_subtrees.py,64,to show you how computers understand language,SATD
spaCy/examples/information_extraction/parse_subtrees.py,65,how computers understand language,SATD
spaCy/examples/information_extraction/parse_subtrees.py,66,to show you how computers understand language | show,SATD
spaCy/examples/information_extraction/parse_subtrees.py,67,how computers understand language | understand,SATD
spaCy/examples/information_extraction/entity_relations.py,1,!/usr/bin/env python,not
spaCy/examples/information_extraction/entity_relations.py,2,coding: utf8,not
spaCy/examples/information_extraction/entity_relations.py,40,Filter a sequence of spans so they don't contain overlaps,not
spaCy/examples/information_extraction/entity_relations.py,41,For spaCy 2.1.4+: this function is available as spacy.util.filter_spans(),not
spaCy/examples/information_extraction/entity_relations.py,47,Check for end - 1 here because boundaries are inclusive,not
spaCy/examples/information_extraction/entity_relations.py,56,Merge entities and noun chunks into one token,not
spaCy/examples/information_extraction/entity_relations.py,78,Expected output:,not
spaCy/examples/information_extraction/entity_relations.py,79,Net income      MONEY   $9.4 million,not
spaCy/examples/information_extraction/entity_relations.py,80,the prior year  MONEY   $2.7 million,not
spaCy/examples/information_extraction/entity_relations.py,81,Revenue         MONEY   twelve billion dollars,not
spaCy/examples/information_extraction/entity_relations.py,82,a loss          MONEY   1b,not
spaCy/examples/information_extraction/phrase_matcher.py,1,!/usr/bin/env python,not
spaCy/examples/information_extraction/phrase_matcher.py,2,coding: utf8,not
spaCy/examples/training/pretrain_textcat.py,43,Partition off part of the train data for evaluation,not
spaCy/examples/training/pretrain_textcat.py,133,get names of other pipes to disable them during training,not
spaCy/examples/training/pretrain_textcat.py,136,only train textcat,not
spaCy/examples/training/pretrain_textcat.py,143,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/pretrain_textcat.py,149,evaluate on the dev data split off in load_data(),not
spaCy/examples/training/pretrain_textcat.py,152,print a simple table,not
spaCy/examples/training/rehearsal.py,57,Avoid use of Adam when resuming training. I don't understand this well,not
spaCy/examples/training/rehearsal.py,58,"yet, but I'm getting weird results from Adam. Try commenting out the",not
spaCy/examples/training/rehearsal.py,59,"nlp.update(), and using Adam -- you'll find the models drift apart.",not
spaCy/examples/training/rehearsal.py,60,"I guess Adam is losing precision, introducing gradient noise?",not
spaCy/examples/training/rehearsal.py,65,get names of other pipes to disable them during training,not
spaCy/examples/training/rehearsal.py,75,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/conllu.py,61,,not
spaCy/examples/training/conllu.py,62,Data reading,not
spaCy/examples/training/conllu.py,63,,not
spaCy/examples/training/conllu.py,89,sd is spacy doc; cd is conllu doc,not
spaCy/examples/training/conllu.py,90,"cs is conllu sent, ct is conllu token",not
spaCy/examples/training/conllu.py,161,"Flatten the conll annotations, and adjust the head indices",not
spaCy/examples/training/conllu.py,167,Construct text if necessary,not
spaCy/examples/training/conllu.py,179,,not
spaCy/examples/training/conllu.py,180,Data transforms for spaCy,not
spaCy/examples/training/conllu.py,181,,not
spaCy/examples/training/conllu.py,196,,not
spaCy/examples/training/conllu.py,197,Evaluation,not
spaCy/examples/training/conllu.py,198,,not
spaCy/examples/training/conllu.py,260,"def get_sent_conllu(sent, sent_id):",not
spaCy/examples/training/conllu.py,261,"lines = [""# sent_id = {sent_id}"".format(sent_id=sent_id)]",not
spaCy/examples/training/conllu.py,293,,not
spaCy/examples/training/conllu.py,294,Initialization,not
spaCy/examples/training/conllu.py,295,,not
spaCy/examples/training/conllu.py,318,Replace labels that didn't make the frequency cutoff,not
spaCy/examples/training/conllu.py,328,,not
spaCy/examples/training/conllu.py,329,Command line helpers,not
spaCy/examples/training/conllu.py,330,,not
spaCy/examples/training/train_ner.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_ner.py,2,coding: utf8,not
spaCy/examples/training/train_ner.py,22,training data,not
spaCy/examples/training/train_ner.py,37,load existing spaCy model,not
spaCy/examples/training/train_ner.py,40,create blank Language class,not
spaCy/examples/training/train_ner.py,43,create the built-in pipeline components and add them to the pipeline,not
spaCy/examples/training/train_ner.py,44,nlp.create_pipe works for built-ins that are registered with spaCy,not
spaCy/examples/training/train_ner.py,48,"otherwise, get it so we can add labels",not
spaCy/examples/training/train_ner.py,52,add labels,not
spaCy/examples/training/train_ner.py,57,get names of other pipes to disable them during training,not
spaCy/examples/training/train_ner.py,60,only train NER,not
spaCy/examples/training/train_ner.py,61,reset and initialize the weights randomly – but only if we're,not
spaCy/examples/training/train_ner.py,62,training a new model,not
spaCy/examples/training/train_ner.py,68,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_ner.py,73,batch of texts,not
spaCy/examples/training/train_ner.py,74,batch of annotations,not
spaCy/examples/training/train_ner.py,75,dropout - make it harder to memorise data,not
spaCy/examples/training/train_ner.py,80,test the trained model,not
spaCy/examples/training/train_ner.py,86,save model to output directory,not
spaCy/examples/training/train_ner.py,94,test the saved model,not
spaCy/examples/training/train_ner.py,106,Expected output:,not
spaCy/examples/training/train_ner.py,107,"Entities [('Shaka Khan', 'PERSON')]",not
spaCy/examples/training/train_ner.py,108,"Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),",not
spaCy/examples/training/train_ner.py,109,"('Khan', 'PERSON', 1), ('?', '', 2)]",not
spaCy/examples/training/train_ner.py,110,"Entities [('London', 'LOC'), ('Berlin', 'LOC')]",not
spaCy/examples/training/train_ner.py,111,"Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),",not
spaCy/examples/training/train_ner.py,112,"('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]",not
spaCy/examples/training/create_kb.py,1,!/usr/bin/env python,not
spaCy/examples/training/create_kb.py,2,coding: utf8,not
spaCy/examples/training/create_kb.py,24,Q2146908 (Russ Cochran): American golfer,not
spaCy/examples/training/create_kb.py,25,Q7381115 (Russ Cochran): publisher,not
spaCy/examples/training/create_kb.py,38,load existing spaCy model,not
spaCy/examples/training/create_kb.py,41,check the length of the nlp vectors,not
spaCy/examples/training/create_kb.py,48,You can change the dimension of vectors in your KB by using an encoder that changes the dimensionality.,not
spaCy/examples/training/create_kb.py,49,"For simplicity, we'll just use the original vector dimension here instead.",not
spaCy/examples/training/create_kb.py,53,set up the data,not
spaCy/examples/training/create_kb.py,63,"set the entities, can also be done by calling `kb.add_entity` for each entity",not
spaCy/examples/training/create_kb.py,66,"adding aliases, the entities need to be defined in the KB beforehand",not
spaCy/examples/training/create_kb.py,70,the sum of these probabilities should not exceed 1,not
spaCy/examples/training/create_kb.py,73,test the trained model,not
spaCy/examples/training/create_kb.py,77,save model to output directory,not
spaCy/examples/training/create_kb.py,93,test the saved model,not
spaCy/examples/training/create_kb.py,94,always reload a knowledge base with the same vocab instance!,not
spaCy/examples/training/create_kb.py,112,Expected output:,not
spaCy/examples/training/create_kb.py,113,"2 kb entities: ['Q2146908', 'Q7381115']",not
spaCy/examples/training/create_kb.py,114,1 kb aliases: ['Russ Cochran'],not
spaCy/examples/training/train_new_entity_type.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_new_entity_type.py,2,coding: utf8,not
spaCy/examples/training/train_new_entity_type.py,38,new entity label,not
spaCy/examples/training/train_new_entity_type.py,41,training data,not
spaCy/examples/training/train_new_entity_type.py,42,"Note: If you're using an existing model, make sure to mix in examples of",not
spaCy/examples/training/train_new_entity_type.py,43,"other entity types that spaCy correctly recognized before. Otherwise, your",not
spaCy/examples/training/train_new_entity_type.py,44,"model might learn the new type, but ""forget"" what it previously knew.",not
spaCy/examples/training/train_new_entity_type.py,45,https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting,not
spaCy/examples/training/train_new_entity_type.py,75,load existing spaCy model,not
spaCy/examples/training/train_new_entity_type.py,78,create blank Language class,not
spaCy/examples/training/train_new_entity_type.py,80,Add entity recognizer to model if it's not in the pipeline,not
spaCy/examples/training/train_new_entity_type.py,81,nlp.create_pipe works for built-ins that are registered with spaCy,not
spaCy/examples/training/train_new_entity_type.py,85,"otherwise, get it, so we can add labels to it",not
spaCy/examples/training/train_new_entity_type.py,89,add new entity label to entity recognizer,not
spaCy/examples/training/train_new_entity_type.py,90,Adding extraneous labels shouldn't mess anything up,SATD
spaCy/examples/training/train_new_entity_type.py,97,get names of other pipes to disable them during training,not
spaCy/examples/training/train_new_entity_type.py,100,only train NER,not
spaCy/examples/training/train_new_entity_type.py,102,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_new_entity_type.py,112,test the trained model,not
spaCy/examples/training/train_new_entity_type.py,119,save model to output directory,not
spaCy/examples/training/train_new_entity_type.py,124,rename model,not
spaCy/examples/training/train_new_entity_type.py,128,test the saved model,not
spaCy/examples/training/train_new_entity_type.py,131,Check the classes have loaded back consistently,not
spaCy/examples/training/train_textcat.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_textcat.py,2,coding: utf8,not
spaCy/examples/training/train_textcat.py,36,load existing spaCy model,not
spaCy/examples/training/train_textcat.py,39,create blank Language class,not
spaCy/examples/training/train_textcat.py,42,add the text classifier to the pipeline if it doesn't exist,not
spaCy/examples/training/train_textcat.py,43,nlp.create_pipe works for built-ins that are registered with spaCy,not
spaCy/examples/training/train_textcat.py,49,"otherwise, get it, so we can add labels to it",not
spaCy/examples/training/train_textcat.py,53,add label to text classifier,not
spaCy/examples/training/train_textcat.py,57,load the IMDB dataset,not
spaCy/examples/training/train_textcat.py,69,get names of other pipes to disable them during training,not
spaCy/examples/training/train_textcat.py,72,only train textcat,not
spaCy/examples/training/train_textcat.py,82,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_textcat.py,89,evaluate on the dev data split off in load_data(),not
spaCy/examples/training/train_textcat.py,92,print a simple table,not
spaCy/examples/training/train_textcat.py,100,test the trained model,not
spaCy/examples/training/train_textcat.py,110,test the saved model,not
spaCy/examples/training/train_textcat.py,119,Partition off part of the train data for evaluation,not
spaCy/examples/training/train_textcat.py,131,True positives,not
spaCy/examples/training/train_textcat.py,132,False positives,not
spaCy/examples/training/train_textcat.py,133,False negatives,not
spaCy/examples/training/train_textcat.py,134,True negatives,not
spaCy/examples/training/train_parser.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_parser.py,2,coding: utf8,not
spaCy/examples/training/train_parser.py,20,training data,not
spaCy/examples/training/train_parser.py,47,load existing spaCy model,not
spaCy/examples/training/train_parser.py,50,create blank Language class,not
spaCy/examples/training/train_parser.py,53,add the parser to the pipeline if it doesn't exist,not
spaCy/examples/training/train_parser.py,54,nlp.create_pipe works for built-ins that are registered with spaCy,not
spaCy/examples/training/train_parser.py,58,"otherwise, get it, so we can add labels to it",not
spaCy/examples/training/train_parser.py,62,add labels to the parser,not
spaCy/examples/training/train_parser.py,67,get names of other pipes to disable them during training,not
spaCy/examples/training/train_parser.py,70,only train parser,not
spaCy/examples/training/train_parser.py,75,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_parser.py,82,test the trained model,not
spaCy/examples/training/train_parser.py,87,save model to output directory,not
spaCy/examples/training/train_parser.py,95,test the saved model,not
spaCy/examples/training/train_parser.py,105,expected result:,not
spaCy/examples/training/train_parser.py,106,[,not
spaCy/examples/training/train_parser.py,107,"('I', 'nsubj', 'like'),",not
spaCy/examples/training/train_parser.py,108,"('like', 'ROOT', 'like'),",not
spaCy/examples/training/train_parser.py,109,"('securities', 'dobj', 'like'),",not
spaCy/examples/training/train_parser.py,110,"('.', 'punct', 'like')",not
spaCy/examples/training/train_parser.py,111,],not
spaCy/examples/training/train_entity_linker.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_entity_linker.py,2,coding: utf8,not
spaCy/examples/training/train_entity_linker.py,31,Q2146908 (Russ Cochran): American golfer,not
spaCy/examples/training/train_entity_linker.py,32,Q7381115 (Russ Cochran): publisher,not
spaCy/examples/training/train_entity_linker.py,53,training data,not
spaCy/examples/training/train_entity_linker.py,67,create blank English model with correct vocab,not
spaCy/examples/training/train_entity_linker.py,72,"Add a sentencizer component. Alternatively, add a dependency parser for higher accuracy.",not
spaCy/examples/training/train_entity_linker.py,75,"Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.",not
spaCy/examples/training/train_entity_linker.py,76,"Note that in a realistic application, an actual NER algorithm should be used instead.",not
spaCy/examples/training/train_entity_linker.py,82,Create the Entity Linker component and add it to the pipeline.,not
spaCy/examples/training/train_entity_linker.py,84,use only the predicted EL score and not the prior probability (for demo purposes),not
spaCy/examples/training/train_entity_linker.py,93,Convert the texts to docs to make sure we have doc.ents set for the training examples.,not
spaCy/examples/training/train_entity_linker.py,94,Also ensure that the annotated examples correspond to known identifiers in the knowlege base.,not
spaCy/examples/training/train_entity_linker.py,113,get names of other pipes to disable them during training,not
spaCy/examples/training/train_entity_linker.py,116,only train entity linker,not
spaCy/examples/training/train_entity_linker.py,117,reset and initialize the weights randomly,not
spaCy/examples/training/train_entity_linker.py,122,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_entity_linker.py,127,batch of texts,not
spaCy/examples/training/train_entity_linker.py,128,batch of annotations,not
spaCy/examples/training/train_entity_linker.py,129,dropout - make it harder to memorise data,not
spaCy/examples/training/train_entity_linker.py,135,test the trained model,not
spaCy/examples/training/train_entity_linker.py,138,save model to output directory,not
spaCy/examples/training/train_entity_linker.py,147,test the saved model,not
spaCy/examples/training/train_entity_linker.py,155,apply the entity linker which will now make predictions for the 'Russ Cochran' entities,not
spaCy/examples/training/train_entity_linker.py,165,Expected output (can be shuffled):,not
spaCy/examples/training/train_entity_linker.py,167,"Entities[('Russ Cochran', 'PERSON', 'Q7381115')]",not
spaCy/examples/training/train_entity_linker.py,168,"Tokens[('Russ', 'PERSON', 'Q7381115'), ('Cochran', 'PERSON', 'Q7381115'), (""his"", '', ''), ('reprints', '', ''), ('include', '', ''), ('The', '', ''), ('Complete', '', ''), ('EC', '', ''), ('Library', '', ''), ('.', '', '')]",not
spaCy/examples/training/train_entity_linker.py,170,"Entities[('Russ Cochran', 'PERSON', 'Q7381115')]",not
spaCy/examples/training/train_entity_linker.py,171,"Tokens[('Russ', 'PERSON', 'Q7381115'), ('Cochran', 'PERSON', 'Q7381115'), ('has', '', ''), ('been', '', ''), ('publishing', '', ''), ('comic', '', ''), ('art', '', ''), ('.', '', '')]",not
spaCy/examples/training/train_entity_linker.py,173,"Entities[('Russ Cochran', 'PERSON', 'Q2146908')]",not
spaCy/examples/training/train_entity_linker.py,174,"Tokens[('Russ', 'PERSON', 'Q2146908'), ('Cochran', 'PERSON', 'Q2146908'), ('captured', '', ''), ('his', '', ''), ('first', '', ''), ('major', '', ''), ('title', '', ''), ('with', '', ''), ('his', '', ''), ('son', '', ''), ('as', '', ''), ('caddie', '', ''), ('.', '', '')]",not
spaCy/examples/training/train_entity_linker.py,176,"Entities[('Russ Cochran', 'PERSON', 'Q2146908')]",not
spaCy/examples/training/train_entity_linker.py,177,"Tokens[('Russ', 'PERSON', 'Q2146908'), ('Cochran', 'PERSON', 'Q2146908'), ('was', '', ''), ('a', '', ''), ('member', '', ''), ('of', '', ''), ('University', '', ''), ('of', '', ''), ('Kentucky', '', ''), (""'s"", '', ''), ('golf', '', ''), ('team', '', ''), ('.', '', '')]",not
spaCy/examples/training/train_tagger.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_tagger.py,2,coding: utf8,not
spaCy/examples/training/train_tagger.py,23,You need to define a mapping from your data's part-of-speech tag names to the,not
spaCy/examples/training/train_tagger.py,24,"Universal Part-of-Speech tag set, as spaCy includes an enum of these tags.",not
spaCy/examples/training/train_tagger.py,25,See here for the Universal Tag Set:,not
spaCy/examples/training/train_tagger.py,26,http://universaldependencies.github.io/docs/u/pos/index.html,not
spaCy/examples/training/train_tagger.py,27,"You may also specify morphological features for your tags, from the universal",not
spaCy/examples/training/train_tagger.py,28,scheme.,not
spaCy/examples/training/train_tagger.py,31,"Usually you'll read this in, of course. Data formats vary. Ensure your",not
spaCy/examples/training/train_tagger.py,32,strings are unicode and that the number of tags assigned matches spaCy's,not
spaCy/examples/training/train_tagger.py,33,"tokenization. If not, you can always add a 'words' key to the annotations",not
spaCy/examples/training/train_tagger.py,34,"that specifies the gold-standard tokenization, e.g.:",not
spaCy/examples/training/train_tagger.py,35,"(""Eatblueham"", {'words': ['Eat', 'blue', 'ham'], 'tags': ['V', 'J', 'N']})",not
spaCy/examples/training/train_tagger.py,53,add the tagger to the pipeline,not
spaCy/examples/training/train_tagger.py,54,nlp.create_pipe works for built-ins that are registered with spaCy,not
spaCy/examples/training/train_tagger.py,56,Add the tags. This needs to be done before you start training.,not
spaCy/examples/training/train_tagger.py,65,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_tagger.py,72,test the trained model,not
spaCy/examples/training/train_tagger.py,77,save model to output directory,not
spaCy/examples/training/train_tagger.py,85,test the save model,not
spaCy/examples/training/train_tagger.py,95,Expected output:,not
spaCy/examples/training/train_tagger.py,96,[,not
spaCy/examples/training/train_tagger.py,97,"('I', 'N', 'NOUN'),",not
spaCy/examples/training/train_tagger.py,98,"('like', 'V', 'VERB'),",not
spaCy/examples/training/train_tagger.py,99,"('blue', 'J', 'ADJ'),",not
spaCy/examples/training/train_tagger.py,100,"('eggs', 'N', 'NOUN')",not
spaCy/examples/training/train_tagger.py,101,],not
spaCy/examples/training/ner_multitask_objective.py,72,batch of texts,not
spaCy/examples/training/ner_multitask_objective.py,73,batch of annotations,not
spaCy/examples/training/ner_multitask_objective.py,74,dropout - make it harder to memorise data,not
spaCy/examples/training/ner_multitask_objective.py,75,callable to update weights,not
spaCy/examples/training/ner_multitask_objective.py,80,test the trained model,not
spaCy/examples/training/train_intent_parser.py,1,!/usr/bin/env python,not
spaCy/examples/training/train_intent_parser.py,2,coding: utf-8,not
spaCy/examples/training/train_intent_parser.py,29,"training data: texts, heads and dependency labels",not
spaCy/examples/training/train_intent_parser.py,30,"for no relation, we simply chose an arbitrary dependency label, e.g. '-'",not
spaCy/examples/training/train_intent_parser.py,35,index of token head,not
spaCy/examples/training/train_intent_parser.py,66,"attach ""flowers"" to store!",not
spaCy/examples/training/train_intent_parser.py,110,load existing spaCy model,not
spaCy/examples/training/train_intent_parser.py,113,create blank Language class,not
spaCy/examples/training/train_intent_parser.py,116,"We'll use the built-in dependency parser class, but we want to create a",not
spaCy/examples/training/train_intent_parser.py,117,fresh instance – just in case.,not
spaCy/examples/training/train_intent_parser.py,129,only train parser,not
spaCy/examples/training/train_intent_parser.py,134,batch up the examples using spaCy's minibatch,not
spaCy/examples/training/train_intent_parser.py,141,test the trained model,not
spaCy/examples/training/train_intent_parser.py,144,save model to output directory,not
spaCy/examples/training/train_intent_parser.py,152,test the saved model,not
spaCy/examples/training/train_intent_parser.py,173,Expected output:,not
spaCy/examples/training/train_intent_parser.py,174,find a hotel with good wifi,not
spaCy/examples/training/train_intent_parser.py,175,[,not
spaCy/examples/training/train_intent_parser.py,176,"('find', 'ROOT', 'find'),",not
spaCy/examples/training/train_intent_parser.py,177,"('hotel', 'PLACE', 'find'),",not
spaCy/examples/training/train_intent_parser.py,178,"('good', 'QUALITY', 'wifi'),",not
spaCy/examples/training/train_intent_parser.py,179,"('wifi', 'ATTRIBUTE', 'hotel')",not
spaCy/examples/training/train_intent_parser.py,180,],not
spaCy/examples/training/train_intent_parser.py,181,find me the cheapest gym near work,not
spaCy/examples/training/train_intent_parser.py,182,[,not
spaCy/examples/training/train_intent_parser.py,183,"('find', 'ROOT', 'find'),",not
spaCy/examples/training/train_intent_parser.py,184,"('cheapest', 'QUALITY', 'gym'),",not
spaCy/examples/training/train_intent_parser.py,185,"('gym', 'PLACE', 'find'),",not
spaCy/examples/training/train_intent_parser.py,186,"('near', 'ATTRIBUTE', 'gym'),",not
spaCy/examples/training/train_intent_parser.py,187,"('work', 'LOCATION', 'near')",not
spaCy/examples/training/train_intent_parser.py,188,],not
spaCy/examples/training/train_intent_parser.py,189,show me the best hotel in berlin,not
spaCy/examples/training/train_intent_parser.py,190,[,not
spaCy/examples/training/train_intent_parser.py,191,"('show', 'ROOT', 'show'),",not
spaCy/examples/training/train_intent_parser.py,192,"('best', 'QUALITY', 'hotel'),",not
spaCy/examples/training/train_intent_parser.py,193,"('hotel', 'PLACE', 'show'),",not
spaCy/examples/training/train_intent_parser.py,194,"('berlin', 'LOCATION', 'hotel')",not
spaCy/examples/training/train_intent_parser.py,195,],not
spaCy/examples/training/textcat_example_data/textcatjsonl_to_trainjson.py,15,Load model with tokenizer + sentencizer only,not
spaCy/examples/pipeline/custom_attr_methods.py,1,!/usr/bin/env python,not
spaCy/examples/pipeline/custom_attr_methods.py,2,coding: utf-8,not
spaCy/examples/pipeline/custom_attr_methods.py,26,start off with blank English class,not
spaCy/examples/pipeline/custom_attr_methods.py,37,"add entity manually for demo purposes, to make it work without a model",not
spaCy/examples/pipeline/custom_attr_methods.py,47,generate filename from first six non-punct tokens,not
spaCy/examples/pipeline/custom_attr_methods.py,49,render markup,not
spaCy/examples/pipeline/custom_attr_methods.py,55,save to file,not
spaCy/examples/pipeline/custom_attr_methods.py,75,Expected output:,not
spaCy/examples/pipeline/custom_attr_methods.py,76,Text 1: Peach emoji is where it has always been.,not
spaCy/examples/pipeline/custom_attr_methods.py,77,Text 2: Peach is the superior emoji.,not
spaCy/examples/pipeline/custom_attr_methods.py,78,"Overlapping tokens: [Peach, emoji, is, .]",not
spaCy/examples/pipeline/fix_space_entities.py,1,!/usr/bin/env python,not
spaCy/examples/pipeline/fix_space_entities.py,2,coding: utf8,not
spaCy/examples/pipeline/fix_space_entities.py,20,"Sets 'O' tag (0 is None, so I is 1, O is 2)",not
spaCy/examples/pipeline/custom_component_countries_api.py,1,!/usr/bin/env python,not
spaCy/examples/pipeline/custom_component_countries_api.py,2,coding: utf8,not
spaCy/examples/pipeline/custom_component_countries_api.py,25,"For simplicity, we start off with only the blank English Language class",not
spaCy/examples/pipeline/custom_component_countries_api.py,26,and no model or pre-defined pipeline loaded.,not
spaCy/examples/pipeline/custom_component_countries_api.py,28,initialise component,not
spaCy/examples/pipeline/custom_component_countries_api.py,29,add it to the pipeline,not
spaCy/examples/pipeline/custom_component_countries_api.py,31,pipeline contains component name,not
spaCy/examples/pipeline/custom_component_countries_api.py,32,Doc contains countries,not
spaCy/examples/pipeline/custom_component_countries_api.py,40,country data,not
spaCy/examples/pipeline/custom_component_countries_api.py,41,entities,not
spaCy/examples/pipeline/custom_component_countries_api.py,50,"component name, will show up in the pipeline",not
spaCy/examples/pipeline/custom_component_countries_api.py,57,Make request once on initialisation and store the data,not
spaCy/examples/pipeline/custom_component_countries_api.py,59,make sure requests raises an error if it fails,not
spaCy/examples/pipeline/custom_component_countries_api.py,62,Convert API response to dict keyed by country name for easy lookup,not
spaCy/examples/pipeline/custom_component_countries_api.py,63,This could also be extended using the alternative and foreign language,not
spaCy/examples/pipeline/custom_component_countries_api.py,64,names provided by the API,not
spaCy/examples/pipeline/custom_component_countries_api.py,66,get entity label ID,not
spaCy/examples/pipeline/custom_component_countries_api.py,68,Set up the PhraseMatcher with Doc patterns for each country name,not
spaCy/examples/pipeline/custom_component_countries_api.py,73,Register attribute on the Token. We'll be overwriting this based on,not
spaCy/examples/pipeline/custom_component_countries_api.py,74,"the matches, so we're only setting a default value, not a getter.",not
spaCy/examples/pipeline/custom_component_countries_api.py,75,"If no default value is set, it defaults to None.",not
spaCy/examples/pipeline/custom_component_countries_api.py,81,Register attributes on Doc and Span via a getter that checks if one of,not
spaCy/examples/pipeline/custom_component_countries_api.py,82,the contained tokens is set to is_country == True.,not
spaCy/examples/pipeline/custom_component_countries_api.py,92,keep the spans for later so we can merge them afterwards,not
spaCy/examples/pipeline/custom_component_countries_api.py,94,Generate Span representing the entity & set label,not
spaCy/examples/pipeline/custom_component_countries_api.py,97,Set custom attribute on each token of the entity,not
spaCy/examples/pipeline/custom_component_countries_api.py,98,"Can be extended with other data returned by the API, like",not
spaCy/examples/pipeline/custom_component_countries_api.py,99,"currencies, country code, flag, calling code etc.",not
spaCy/examples/pipeline/custom_component_countries_api.py,105,Overwrite doc.ents and add entity – be careful not to replace!,not
spaCy/examples/pipeline/custom_component_countries_api.py,108,Iterate over all spans and merge them into one token. This is done,not
spaCy/examples/pipeline/custom_component_countries_api.py,109,"after setting the entities – otherwise, it would cause mismatched",not
spaCy/examples/pipeline/custom_component_countries_api.py,110,indices!,not
spaCy/examples/pipeline/custom_component_countries_api.py,112,don't forget to return the Doc!,not
spaCy/examples/pipeline/custom_component_countries_api.py,125,Expected output:,not
spaCy/examples/pipeline/custom_component_countries_api.py,126,Pipeline ['rest_countries'],not
spaCy/examples/pipeline/custom_component_countries_api.py,127,Doc has countries True,not
spaCy/examples/pipeline/custom_component_countries_api.py,128,"Colombia Bogotá [4.0, -72.0] https://restcountries.eu/data/col.svg",not
spaCy/examples/pipeline/custom_component_countries_api.py,129,"Czech Republic Prague [49.75, 15.5] https://restcountries.eu/data/cze.svg",not
spaCy/examples/pipeline/custom_component_countries_api.py,130,"Entities [('Colombia', 'GPE'), ('Czech Republic', 'GPE')]",not
spaCy/examples/pipeline/multi_processing.py,1,!/usr/bin/env python,not
spaCy/examples/pipeline/multi_processing.py,2,coding: utf8,not
spaCy/examples/pipeline/multi_processing.py,32,load spaCy model,not
spaCy/examples/pipeline/multi_processing.py,36,load and pre-process the IMBD dataset,not
spaCy/examples/pipeline/multi_processing.py,51,return None in case same batch is called again,not
spaCy/examples/pipeline/multi_processing.py,63,"True-case, i.e. try to normalize sentence-initial capitals.",not
spaCy/examples/pipeline/multi_processing.py,64,Only do this if the lower-cased form is more probable.,not
spaCy/examples/pipeline/custom_sentence_segmentation.py,31,We're not checking for is_title here to ignore arbitrary titlecased,not
spaCy/examples/pipeline/custom_sentence_segmentation.py,32,tokens within sentences,not
spaCy/examples/pipeline/custom_sentence_segmentation.py,33,elif token.is_title:,not
spaCy/examples/pipeline/custom_sentence_segmentation.py,34,return True,not
spaCy/examples/pipeline/custom_component_entities.py,1,!/usr/bin/env python,not
spaCy/examples/pipeline/custom_component_entities.py,2,coding: utf8,not
spaCy/examples/pipeline/custom_component_entities.py,27,"For simplicity, we start off with only the blank English Language class",not
spaCy/examples/pipeline/custom_component_entities.py,28,and no model or pre-defined pipeline loaded.,not
spaCy/examples/pipeline/custom_component_entities.py,30,set default companies if none are set via args,not
spaCy/examples/pipeline/custom_component_entities.py,31,etc.,not
spaCy/examples/pipeline/custom_component_entities.py,32,initialise component,not
spaCy/examples/pipeline/custom_component_entities.py,33,add last to the pipeline,not
spaCy/examples/pipeline/custom_component_entities.py,36,pipeline contains component name,not
spaCy/examples/pipeline/custom_component_entities.py,37,company names from the list are merged,not
spaCy/examples/pipeline/custom_component_entities.py,38,Doc contains tech orgs,not
spaCy/examples/pipeline/custom_component_entities.py,39,"""Alphabet Inc."" is a tech org",not
spaCy/examples/pipeline/custom_component_entities.py,40,"""is"" is not",not
spaCy/examples/pipeline/custom_component_entities.py,41,all orgs are entities,not
spaCy/examples/pipeline/custom_component_entities.py,51,"component name, will show up in the pipeline",not
spaCy/examples/pipeline/custom_component_entities.py,58,get entity label ID,not
spaCy/examples/pipeline/custom_component_entities.py,60,"Set up the PhraseMatcher – it can now take Doc objects as patterns,",not
spaCy/examples/pipeline/custom_component_entities.py,61,"so even if the list of companies is long, it's very efficient",not
spaCy/examples/pipeline/custom_component_entities.py,66,Register attribute on the Token. We'll be overwriting this based on,not
spaCy/examples/pipeline/custom_component_entities.py,67,"the matches, so we're only setting a default value, not a getter.",not
spaCy/examples/pipeline/custom_component_entities.py,70,Register attributes on Doc and Span via a getter that checks if one of,not
spaCy/examples/pipeline/custom_component_entities.py,71,the contained tokens is set to is_tech_org == True.,not
spaCy/examples/pipeline/custom_component_entities.py,81,keep the spans for later so we can merge them afterwards,not
spaCy/examples/pipeline/custom_component_entities.py,83,Generate Span representing the entity & set label,not
spaCy/examples/pipeline/custom_component_entities.py,86,Set custom attribute on each token of the entity,not
spaCy/examples/pipeline/custom_component_entities.py,89,Overwrite doc.ents and add entity – be careful not to replace!,not
spaCy/examples/pipeline/custom_component_entities.py,92,Iterate over all spans and merge them into one token. This is done,not
spaCy/examples/pipeline/custom_component_entities.py,93,"after setting the entities – otherwise, it would cause mismatched",not
spaCy/examples/pipeline/custom_component_entities.py,94,indices!,not
spaCy/examples/pipeline/custom_component_entities.py,96,don't forget to return the Doc!,not
spaCy/examples/pipeline/custom_component_entities.py,109,Expected output:,not
spaCy/examples/pipeline/custom_component_entities.py,110,Pipeline ['tech_companies'],not
spaCy/examples/pipeline/custom_component_entities.py,111,"Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']",not
spaCy/examples/pipeline/custom_component_entities.py,112,Doc has_tech_org True,not
spaCy/examples/pipeline/custom_component_entities.py,113,Token 0 is_tech_org True,not
spaCy/examples/pipeline/custom_component_entities.py,114,Token 1 is_tech_org False,not
spaCy/examples/pipeline/custom_component_entities.py,115,"Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]",not
