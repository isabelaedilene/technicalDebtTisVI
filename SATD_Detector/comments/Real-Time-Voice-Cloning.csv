file path,line #,comment,satd
Real-Time-Voice-Cloning/synthesizer_train.py,38,Was 5000,not
Real-Time-Voice-Cloning/synthesizer_train.py,40,Was 10000,not
Real-Time-Voice-Cloning/synthesizer_train.py,42,Was 100000,not
Real-Time-Voice-Cloning/vocoder_train.py,42,Process the arguments,not
Real-Time-Voice-Cloning/vocoder_train.py,53,Run the training,not
Real-Time-Voice-Cloning/encoder_train.py,41,Process the arguments,not
Real-Time-Voice-Cloning/encoder_train.py,44,Run the training,not
Real-Time-Voice-Cloning/synthesizer_preprocess_embeds.py,23,Preprocess the dataset,not
Real-Time-Voice-Cloning/encoder_preprocess.py,42,Process the arguments,not
Real-Time-Voice-Cloning/encoder_preprocess.py,49,Preprocess the datasets,not
Real-Time-Voice-Cloning/synthesizer_preprocess_audio.py,29,Process the arguments,not
Real-Time-Voice-Cloning/synthesizer_preprocess_audio.py,33,Create directories,not
Real-Time-Voice-Cloning/synthesizer_preprocess_audio.py,37,Preprocess the dataset,not
Real-Time-Voice-Cloning/demo_toolbox.py,30,Launch the toolbox,not
Real-Time-Voice-Cloning/demo_cli.py,15,Info & args,not
Real-Time-Voice-Cloning/demo_cli.py,39,Print some environment information (for debugging purposes),not
Real-Time-Voice-Cloning/demo_cli.py,59,Load the models one by one.,not
Real-Time-Voice-Cloning/demo_cli.py,66,Run a test,not
Real-Time-Voice-Cloning/demo_cli.py,68,Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's,not
Real-Time-Voice-Cloning/demo_cli.py,69,"sampling rate, which may differ.",not
Real-Time-Voice-Cloning/demo_cli.py,70,"If you're unfamiliar with digital audio, know that it is encoded as an array of floats",not
Real-Time-Voice-Cloning/demo_cli.py,71,"(or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.",not
Real-Time-Voice-Cloning/demo_cli.py,72,"The sampling rate is the number of values (samples) recorded per second, it is set to",not
Real-Time-Voice-Cloning/demo_cli.py,73,16000 for the encoder. Creating an array of length <sampling_rate> will always correspond,not
Real-Time-Voice-Cloning/demo_cli.py,74,to an audio of 1 second.,not
Real-Time-Voice-Cloning/demo_cli.py,78,Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance,not
Real-Time-Voice-Cloning/demo_cli.py,79,"returns, but here we're going to make one ourselves just for the sake of showing that it's",not
Real-Time-Voice-Cloning/demo_cli.py,80,possible.,not
Real-Time-Voice-Cloning/demo_cli.py,82,"Embeddings are L2-normalized (this isn't important here, but if you want to make your own",not
Real-Time-Voice-Cloning/demo_cli.py,83,embeddings it will be).,not
Real-Time-Voice-Cloning/demo_cli.py,85,The synthesizer can handle multiple inputs with batching. Let's create another embedding to,not
Real-Time-Voice-Cloning/demo_cli.py,86,illustrate that,not
Real-Time-Voice-Cloning/demo_cli.py,92,"The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We",not
Real-Time-Voice-Cloning/demo_cli.py,93,can concatenate the mel spectrograms to a single one.,not
Real-Time-Voice-Cloning/demo_cli.py,95,The vocoder can take a callback function to display the generation. More on that later. For,not
Real-Time-Voice-Cloning/demo_cli.py,96,now we'll simply hide it like this:,not
Real-Time-Voice-Cloning/demo_cli.py,99,"For the sake of making this test short, we'll pass a short target length. The target length",not
Real-Time-Voice-Cloning/demo_cli.py,100,is the length of the wav segments that are processed in parallel. E.g. for audio sampled,not
Real-Time-Voice-Cloning/demo_cli.py,101,"at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of",not
Real-Time-Voice-Cloning/demo_cli.py,102,"0.5 seconds which will all be generated together. The parameters here are absurdly short, and",not
Real-Time-Voice-Cloning/demo_cli.py,103,that has a detrimental effect on the quality of the audio. The default parameters are,not
Real-Time-Voice-Cloning/demo_cli.py,104,recommended in general.,not
Real-Time-Voice-Cloning/demo_cli.py,110,Interactive speech generation,not
Real-Time-Voice-Cloning/demo_cli.py,119,Get the reference audio filepath,not
Real-Time-Voice-Cloning/demo_cli.py,125,Computing the embedding,not
Real-Time-Voice-Cloning/demo_cli.py,126,"First, we load the wav using the function that the speaker encoder provides. This is",not
Real-Time-Voice-Cloning/demo_cli.py,127,important: there is preprocessing that must be applied.,not
Real-Time-Voice-Cloning/demo_cli.py,129,The following two methods are equivalent:,not
Real-Time-Voice-Cloning/demo_cli.py,130,- Directly load from the filepath:,not
Real-Time-Voice-Cloning/demo_cli.py,132,- If the wav is already loaded:,not
Real-Time-Voice-Cloning/demo_cli.py,137,Then we derive the embedding. There are many functions and parameters that the,not
Real-Time-Voice-Cloning/demo_cli.py,138,speaker encoder interfaces. These are mostly for in-depth research. You will typically,not
Real-Time-Voice-Cloning/demo_cli.py,139,only use this function (with its default parameters):,not
Real-Time-Voice-Cloning/demo_cli.py,144,Generating the spectrogram,not
Real-Time-Voice-Cloning/demo_cli.py,147,"The synthesizer works in batch, so you need to put your data in a list or numpy array",not
Real-Time-Voice-Cloning/demo_cli.py,150,"If you know what the attention layer alignments are, you can retrieve them here by",not
Real-Time-Voice-Cloning/demo_cli.py,151,passing return_alignments=True,not
Real-Time-Voice-Cloning/demo_cli.py,157,Generating the waveform,not
Real-Time-Voice-Cloning/demo_cli.py,159,Synthesizing the waveform is fairly straightforward. Remember that the longer the,not
Real-Time-Voice-Cloning/demo_cli.py,160,"spectrogram, the more time-efficient the vocoder.",not
Real-Time-Voice-Cloning/demo_cli.py,164,Post-generation,not
Real-Time-Voice-Cloning/demo_cli.py,165,"There's a bug with sounddevice that makes the audio cut one second earlier, so we",not
Real-Time-Voice-Cloning/demo_cli.py,166,pad it.,not
Real-Time-Voice-Cloning/demo_cli.py,169,Play the audio (non-blocking),not
Real-Time-Voice-Cloning/demo_cli.py,174,Save it on the disk,not
Real-Time-Voice-Cloning/toolbox/ui.py,12,"from sklearn.manifold import TSNE         # You can try with TSNE if you like, I prefer UMAP",not
Real-Time-Voice-Cloning/toolbox/ui.py,65,Embedding,not
Real-Time-Voice-Cloning/toolbox/ui.py,66,Clear the plot,not
Real-Time-Voice-Cloning/toolbox/ui.py,71,Draw the embed,not
Real-Time-Voice-Cloning/toolbox/ui.py,83,Spectrogram,not
Real-Time-Voice-Cloning/toolbox/ui.py,84,Draw the spectrogram,not
Real-Time-Voice-Cloning/toolbox/ui.py,88,"spec_ax.figure.colorbar(mappable=im, shrink=0.65, orientation=""horizontal"",",not
Real-Time-Voice-Cloning/toolbox/ui.py,89,spec_ax=spec_ax),not
Real-Time-Voice-Cloning/toolbox/ui.py,105,Display a message if there aren't enough points,not
Real-Time-Voice-Cloning/toolbox/ui.py,112,Compute the projections,not
Real-Time-Voice-Cloning/toolbox/ui.py,120,reducer = TSNE(),not
Real-Time-Voice-Cloning/toolbox/ui.py,131,"self.umap_ax.set_title(""UMAP projections"")",not
Real-Time-Voice-Cloning/toolbox/ui.py,134,Draw the plot,not
Real-Time-Voice-Cloning/toolbox/ui.py,211,Select a random dataset,not
Real-Time-Voice-Cloning/toolbox/ui.py,236,Select a random speaker,not
Real-Time-Voice-Cloning/toolbox/ui.py,242,Select a random utterance,not
Real-Time-Voice-Cloning/toolbox/ui.py,272,Encoder,not
Real-Time-Voice-Cloning/toolbox/ui.py,278,Synthesizer,not
Real-Time-Voice-Cloning/toolbox/ui.py,287,Vocoder,not
Real-Time-Voice-Cloning/toolbox/ui.py,343,Initialize the application,not
Real-Time-Voice-Cloning/toolbox/ui.py,349,Main layouts,not
Real-Time-Voice-Cloning/toolbox/ui.py,350,Root,not
Real-Time-Voice-Cloning/toolbox/ui.py,354,Browser,not
Real-Time-Voice-Cloning/toolbox/ui.py,358,Visualizations,not
Real-Time-Voice-Cloning/toolbox/ui.py,362,Generation,not
Real-Time-Voice-Cloning/toolbox/ui.py,366,Projections,not
Real-Time-Voice-Cloning/toolbox/ui.py,371,Projections,not
Real-Time-Voice-Cloning/toolbox/ui.py,372,UMap,not
Real-Time-Voice-Cloning/toolbox/ui.py,381,Browser,not
Real-Time-Voice-Cloning/toolbox/ui.py,382,"Dataset, speaker and utterance selection",not
Real-Time-Voice-Cloning/toolbox/ui.py,399,Random buttons,not
Real-Time-Voice-Cloning/toolbox/ui.py,411,Utterance box,not
Real-Time-Voice-Cloning/toolbox/ui.py,415,Random & next utterance buttons,not
Real-Time-Voice-Cloning/toolbox/ui.py,420,Random & next utterance buttons,not
Real-Time-Voice-Cloning/toolbox/ui.py,431,Model selection,not
Real-Time-Voice-Cloning/toolbox/ui.py,444,Embed & spectrograms,not
Real-Time-Voice-Cloning/toolbox/ui.py,464,Generation,not
Real-Time-Voice-Cloning/toolbox/ui.py,488,Set the size of the window and of the elements,not
Real-Time-Voice-Cloning/toolbox/ui.py,492,Finalize the display,not
Real-Time-Voice-Cloning/toolbox/__init__.py,13,"Use this directory structure for your datasets, or modify it to fit your needs",not
Real-Time-Voice-Cloning/toolbox/__init__.py,43,"speaker_name, spec, breaks, wav",not
Real-Time-Voice-Cloning/toolbox/__init__.py,45,type: Synthesizer,not
Real-Time-Voice-Cloning/toolbox/__init__.py,47,Initialize the events and the interface,not
Real-Time-Voice-Cloning/toolbox/__init__.py,58,"Dataset, speaker and utterance selection",not
Real-Time-Voice-Cloning/toolbox/__init__.py,69,Model selection,not
Real-Time-Voice-Cloning/toolbox/__init__.py,76,Utterance selection,not
Real-Time-Voice-Cloning/toolbox/__init__.py,86,Generation,not
Real-Time-Voice-Cloning/toolbox/__init__.py,92,UMAP legend,not
Real-Time-Voice-Cloning/toolbox/__init__.py,108,Select the next utterance,not
Real-Time-Voice-Cloning/toolbox/__init__.py,117,Get the wav from the disk. We take the wav with the vocoder/synthesizer format for,not
Real-Time-Voice-Cloning/toolbox/__init__.py,118,"playback, so as to have a fair comparison with the generated audio",not
Real-Time-Voice-Cloning/toolbox/__init__.py,135,Compute the mel spectrogram,not
Real-Time-Voice-Cloning/toolbox/__init__.py,139,Compute the embedding,not
Real-Time-Voice-Cloning/toolbox/__init__.py,145,Add the utterance,not
Real-Time-Voice-Cloning/toolbox/__init__.py,150,Plot it,not
Real-Time-Voice-Cloning/toolbox/__init__.py,162,Synthesize the spectrogram,not
Real-Time-Voice-Cloning/toolbox/__init__.py,185,Synthesize the waveform,not
Real-Time-Voice-Cloning/toolbox/__init__.py,203,Add breaks,not
Real-Time-Voice-Cloning/toolbox/__init__.py,210,Play it,not
Real-Time-Voice-Cloning/toolbox/__init__.py,214,Compute the embedding,not
Real-Time-Voice-Cloning/toolbox/__init__.py,215,"TODO: this is problematic with different sampling rates, gotta fix it",SATD
Real-Time-Voice-Cloning/toolbox/__init__.py,221,Add the utterance,not
Real-Time-Voice-Cloning/toolbox/__init__.py,226,Plot it,not
Real-Time-Voice-Cloning/toolbox/__init__.py,242,Case of Griffin-lim,not
Real-Time-Voice-Cloning/synthesizer/audio.py,14,proposed by @dsmiller,not
Real-Time-Voice-Cloning/synthesizer/audio.py,30,From https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py,not
Real-Time-Voice-Cloning/synthesizer/audio.py,74,Convert back to linear,not
Real-Time-Voice-Cloning/synthesizer/audio.py,91,Convert back to linear,not
Real-Time-Voice-Cloning/synthesizer/audio.py,126,,not
Real-Time-Voice-Cloning/synthesizer/audio.py,127,Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!),not
Real-Time-Voice-Cloning/synthesizer/audio.py,147,,not
Real-Time-Voice-Cloning/synthesizer/audio.py,148,Librosa correct padding,not
Real-Time-Voice-Cloning/synthesizer/audio.py,152,Conversions,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,3,Default hyperparameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,5,Comma-separated list of cleaners to run on text prior to training and eval. For non-English,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,6,"text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"".",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,9,"If you only have 1 GPU or want to use only one GPU, please set num_gpus=0 and specify the",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,10,GPU idx on run. example:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,11,"expample 1 GPU of index 2 (train on ""/gpu2"" only): CUDA_VISIBLE_DEVICES=2 python train.py",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,12,"--model=""Tacotron"" --hparams=""tacotron_gpu_start_idx=2""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,13,"If you want to train on multiple GPUs, simply specify the number of GPUs available,",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,14,and the idx of the first GPU to use. example:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,15,"example 4 GPUs starting from index 0 (train on ""/gpu0""->""/gpu3""): python train.py",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,16,"--model=""Tacotron"" --hparams=""tacotron_num_gpus=4, tacotron_gpu_start_idx=0""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,17,The hparams arguments can be directly modified on this hparams.py file instead of being,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,18,specified on run if preferred!,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,20,If one wants to train both Tacotron and WaveNet in parallel (provided WaveNet will be,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,21,"trained on True mel spectrograms), one needs to specify different GPU idxes.",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,22,example Tacotron+WaveNet on a machine with 4 or plus GPUs. Two GPUs for each model:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,23,"CUDA_VISIBLE_DEVICES=0,1 python train.py --model=""Tacotron""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,24,"--hparams=""tacotron_gpu_start_idx=0, tacotron_num_gpus=2""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,25,"Cuda_VISIBLE_DEVICES=2,3 python train.py --model=""WaveNet""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,26,"--hparams=""wavenet_gpu_start_idx=2; wavenet_num_gpus=2""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,28,"IMPORTANT NOTE: If using N GPUs, please multiply the tacotron_batch_size by N below in the",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,29,hparams! (tacotron_batch_size = 32 * N),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,30,Never use lower batch size than 32 on a single GPU!,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,31,Same applies for Wavenet: wavenet_batch_size = 8 * N (wavenet_batch_size can be smaller than,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,32,"8 if GPU is having OOM, minimum 2)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,33,Please also apply the synthesis batch size modification likewise. (if N GPUs are used for,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,34,"synthesis, minimal batch size must be N, minimum of 1 sample per GPU)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,35,We did not add an automatic multi-GPU batch size computation to avoid confusion in the,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,36,"user""s mind and to provide more control to the user for",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,37,resources related decisions.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,39,Acknowledgement:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,40,Many thanks to @MlWoo for his awesome work on multi-GPU Tacotron which showed to work a,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,41,little faster than the original,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,42,pipeline for a single GPU as well. Great work!,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,44,"Hardware setup: Default supposes user has only one GPU: ""/gpu:0"" (Tacotron only for now!",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,45,"WaveNet does not support multi GPU yet, WIP)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,46,Synthesis also uses the following hardware parameters for multi-GPU parallel synthesis.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,47,idx of the first GPU to be used for Tacotron training.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,48,Determines the number of gpus in use for Tacotron training.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,50,Determines whether to split data on CPU or on first GPU. This is automatically True when,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,51,more than 1 GPU is used.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,52,,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,54,Audio,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,55,Audio parameters are the most important parameters to tune when using this work on your,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,56,personal data. Below are the beginner steps to adapt,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,57,this work to your personal data:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,58,1- Determine my data sample rate: First you need to determine your audio sample_rate (how,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,59,"many samples are in a second of audio). This can be done using sox: ""sox --i <filename>""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,60,"(For this small tuto, I will consider 24kHz (24000 Hz), and defaults are 22050Hz,",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,61,so there are plenty of examples to refer to),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,62,2- set sample_rate parameter to your data correct sample rate,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,63,3- Fix win_size and and hop_size accordingly: (Supposing you will follow our advice: 50ms,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,64,"window_size, and 12.5ms frame_shift(hop_size))",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,65,"a- win_size = 0.05 * sample_rate. In the tuto example, 0.05 * 24000 = 1200",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,66,b- hop_size = 0.25 * win_size. Also equal to 0.0125 * sample_rate. In the tuto,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,67,"example, 0.25 * 1200 = 0.0125 * 24000 = 300 (Can set frame_shift_ms=12.5 instead)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,68,"4- Fix n_fft, num_freq and upsample_scales parameters accordingly.",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,69,a- n_fft can be either equal to win_size or the first power of 2 that comes after,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,70,win_size. I usually recommend using the latter,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,71,to be more consistent with signal processing friends. No big difference to be seen,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,72,however. For the tuto example: n_fft = 2048 = 2**11,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,73,b- num_freq = (n_fft / 2) + 1. For the tuto example: num_freq = 2048 / 2 + 1 = 1024 +,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,74,1 = 1025.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,75,"c- For WaveNet, upsample_scales products must be equal to hop_size. For the tuto",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,76,"example: upsample_scales=[15, 20] where 15 * 20 = 300",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,77,"it is also possible to use upsample_scales=[3, 4, 5, 5] instead. One must only",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,78,keep in mind that upsample_kernel_size[0] = 2*upsample_scales[0],not
Real-Time-Voice-Cloning/synthesizer/hparams.py,79,so the training segments should be long enough (2.8~3x upsample_scales[0] *,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,80,hop_size or longer) so that the first kernel size can see the middle,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,81,of the samples efficiently. The length of WaveNet training segments is under the,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,82,"parameter ""max_time_steps"".",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,83,"5- Finally comes the silence trimming. This very much data dependent, so I suggest trying",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,84,"preprocessing (or part of it, ctrl-C to stop), then use the",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,85,.ipynb provided in the repo to listen to some inverted mel/linear spectrograms. That,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,86,"will first give you some idea about your above parameters, and",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,87,"it will also give you an idea about trimming. If silences persist, try reducing",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,88,"trim_top_db slowly. If samples are trimmed mid words, try increasing it.",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,89,6- If audio quality is too metallic or fragmented (or if linear spectrogram plots are,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,90,"showing black silent regions on top), then restart from step 2.",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,91,Number of mel-spectrogram channels and local conditioning dimensionality,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,92,network,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,93,Whether to rescale audio prior to preprocessing,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,94,Rescaling value,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,95,"Whether to clip silence in Audio (at beginning and end of audio only, not the middle)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,96,train samples of lengths between 3sec and 14sec are more than enough to make a model capable,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,97,of good parallelization.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,99,"For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors,",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,100,also consider clipping your samples to smaller chunks),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,102,"Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,103,and still getting OOM errors.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,105,Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,106,"It""s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,107,Does not work if n_ffit is not multiple of hop_size!!,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,109,"Only used to set as True if using WaveNet, no difference in performance is observed in",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,110,either cases.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,111,silence threshold used for sound trimming for wavenet preprocessing,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,113,Mel spectrogram,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,114,Extra window size is filled with 0 paddings to match this parameter,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,115,"For 16000Hz, 200 = 12.5 ms (0.0125 * sample_rate)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,116,"For 16000Hz, 800 = 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,117,16000Hz (corresponding to librispeech) (sox --i <filename>),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,119,Can replace hop_size parameter. (Recommended: 12.5),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,121,M-AILABS (and other datasets) trim params (these parameters are usually correct for any,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,122,"data, but definitely must be tuned for specific speakers)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,127,Mel and Linear spectrograms normalization/scaling and clipping,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,129,Whether to normalize mel spectrograms to some predefined range (following below parameters),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,130,Only relevant if mel_normalization = True,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,132,"Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2,",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,133,faster and cleaner convergence),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,135,"max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,136,"be too big to avoid gradient explosion,",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,137,not too small for fast convergence),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,139,"whether to rescale to [0, 1] for wavenet. (better audio quality)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,141,"whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,143,Contribution by @begeekmyfriend,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,144,Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,145,levels. Also allows for better G&L phase reconstruction),SATD
Real-Time-Voice-Cloning/synthesizer/hparams.py,146,whether to apply filter,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,147,filter coefficient.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,149,Limits,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,153,"Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,154,"test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,155,To be increased/reduced depending on data.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,157,Griffin Lim,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,159,"Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,161,"Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,162,,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,164,Tacotron,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,165,Was 1,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,166,number of frames to generate at each decoding step (increase to speed up computation and,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,167,"allows for higher batch size, decreases G&L audio quality)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,169,Determines whether the decoder should stop when predicting <stop> to any frame or to all of,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,170,them (True works pretty well),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,172,dimension of embedding space (these are NOT the speaker embeddings),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,174,Encoder parameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,175,number of encoder convolutional layers,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,176,size of encoder convolution filters for each layer,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,177,number of encoder convolutions filters for each layer,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,178,number of lstm units for each direction (forward and backward),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,180,Attention mechanism,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,181,Whether to smooth the attention normalization function,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,182,dimension of attention space,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,183,number of attention convolution filters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,184,kernel size of attention convolution,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,186,Whether to cumulate (sum) all previous attention weights or simply feed previous weights (,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,187,Recommended: True),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,189,Decoder,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,190,number of layers and number of units of prenet,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,191,number of decoder lstm layers,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,192,number of decoder lstm units on each layer,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,194,Max decoder steps during inference (Just for safety from infinite loop cases),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,196,Residual postnet,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,197,number of postnet convolutional layers,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,198,size of postnet convolution filters for each layer,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,199,number of postnet convolution filters for each layer,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,201,CBHG mel->linear postnet,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,203,All kernel sizes from 1 to cbhg_kernels will be used in the convolution bank of CBHG to act,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,204,"as ""K-grams""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,205,Channels of the convolution bank,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,206,pooling size of the CBHG,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,208,"projection channels of the CBHG (1st projection, 2nd is automatically set to num_mels)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,209,kernel_size of the CBHG projections,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,210,Number of HighwayNet layers,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,211,Number of units used in HighwayNet fully connected layers,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,213,Number of GRU units used in bidirectional RNN of CBHG block. CBHG output is 2x rnn_units in,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,214,shape,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,216,Loss params,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,218,whether to mask encoder padding while computing attention. Set to True for better prosody,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,219,but slower convergence.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,221,"Whether to use loss mask for padded sequences (if False, <stop_token> loss function will not",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,222,"be weighted, else recommended pos_weight = 20)",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,224,Use class weights to reduce the stop token classes imbalance (by adding more penalty on,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,225,False Negatives (FN)) (1 = disabled),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,227,Whether to add a post-processing network to the Tacotron to predict linear spectrograms (,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,228,True mode Not tested!!),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,229,,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,231,Tacotron Training,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,232,Reproduction seeds,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,234,Determines initial graph and operations (i.e: model) random state for reproducibility,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,235,random state for train test split repeatability,SATD
Real-Time-Voice-Cloning/synthesizer/hparams.py,237,performance parameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,239,Whether to use cpu as support to gpu for decoder computation (Not recommended: may cause,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,240,major slowdowns! Only use when critical!),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,242,"train/test split ratios, mini-batches sizes",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,243,number of training samples on each training steps (was 32),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,244,Tacotron Batch synthesis supports ~16x the training batch size (no gradients during,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,245,testing).,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,246,"Training Tacotron with unmasked paddings makes it aware of them, which makes synthesis times",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,247,different from training. We thus recommend masking the encoder.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,249,"DO NOT MAKE THIS BIGGER THAN 1 IF YOU DIDN""T TRAIN TACOTRON WITH ""mask_encoder=True""!!",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,251,"% of data to keep as test data, if None, tacotron_test_batches must be not None. (5% is",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,252,enough to have a good idea about overfit),SATD
Real-Time-Voice-Cloning/synthesizer/hparams.py,253,number of test batches.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,255,Learning rate schedule,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,257,"boolean, determines if the learning rate will follow an exponential decay",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,258,Step at which learning decay starts,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,259,Determines the learning rate decay slope (UNDER TEST),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,260,learning rate decay rate (UNDER TEST),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,261,starting learning rate,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,262,minimal learning rate,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,264,Optimization parameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,265,AdamOptimizer beta1 parameter,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,266,AdamOptimizer beta2 parameter,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,267,AdamOptimizer Epsilon parameter,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,269,Regularization parameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,270,regularization weight (for L2 regularization),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,272,Whether to rescale regularization weight to adapt for outputs range (used when reg_weight is,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,273,high and biasing the model),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,274,zoneout rate for all LSTM cells in the network,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,275,dropout rate for all convolutional layers + prenet,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,276,whether to clip gradients,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,278,Evaluation parameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,280,Whether to use 100% natural eval (to evaluate Curriculum Learning performance) or with same,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,281,teacher-forcing ratio as in training (just for overfit),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,283,Decoder RNN learning can take be done in one of two ways:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,284,"Teacher Forcing: vanilla teacher forcing (usually with ratio = 1). mode=""constant""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,285,Curriculum Learning Scheme: From Teacher-Forcing to sampling from previous outputs is,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,286,"function of global step. (teacher forcing ratio decay) mode=""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,287,The second approach is inspired by:,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,288,Bengio et al. 2015: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,289,Can be found under: https://arxiv.org/pdf/1506.03099.pdf,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,291,"Can be (""constant"" or ""scheduled""). ""scheduled"" mode applies a cosine teacher forcing ratio",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,292,decay. (Preference: scheduled),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,294,"Value from [0., 1.], 0.=0%, 1.=100%, determines the % of times we force next decoder",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,295,"inputs, Only relevant if mode=""constant""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,297,"initial teacher forcing ratio. Relevant if mode=""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,299,"final teacher forcing ratio. Relevant if mode=""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,301,"starting point of teacher forcing ratio decay. Relevant if mode=""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,303,"Determines the teacher forcing ratio decay slope. Relevant if mode=""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,305,"teacher forcing ratio decay rate. Relevant if mode=""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,306,,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,308,Tacotron-2 integration parameters,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,310,Whether to use GTA mels to train WaveNet instead of ground truth mels.,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,311,,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,313,"Eval sentences (if no eval text file was specified during synthesis, these sentences are",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,314,used for eval),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,316,"From July 8, 2017 New York Times:",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,322,"From Google""s Tacotron example page:",not
Real-Time-Voice-Cloning/synthesizer/hparams.py,334,From The web (random long utterance),not
Real-Time-Voice-Cloning/synthesizer/hparams.py,342,SV2TTS,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,344,Duration in seconds of a silence for an utterance to be split,not
Real-Time-Voice-Cloning/synthesizer/hparams.py,345,Duration in seconds below which utterances are discarded,not
Real-Time-Voice-Cloning/synthesizer/synthesize.py,14,"Create output path if it doesn""t exist",not
Real-Time-Voice-Cloning/synthesizer/synthesize.py,23,Set inputs batch wise,not
Real-Time-Voice-Cloning/synthesizer/synthesize.py,45,Load the model in memory,not
Real-Time-Voice-Cloning/synthesizer/synthesize.py,50,Load the metadata,not
Real-Time-Voice-Cloning/synthesizer/synthesize.py,57,Set inputs batch wise,not
Real-Time-Voice-Cloning/synthesizer/synthesize.py,60,"TODO: come on big boy, fix this",SATD
Real-Time-Voice-Cloning/synthesizer/synthesize.py,61,Quick and dirty fix to make sure that all batches have the same size,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,14,Force the batch size to be known in order to use attention masking in batch synthesis,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,38,pad input sequences with the <pad_token> 0 ( _ ),not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,40,"explicitely setting the padding to a value that doesn""t originally exist in the spectogram",not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,41,"to avoid any possible conflicts, without affecting the output range of the model too much",not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,54,Memory allocation on the GPUs as needed,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,70,Prepare the input,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,83,Forward it,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,89,Trim the output,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,95,"If no token is generated, we simply do not trim the output",not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,110,Pad inputs according to each GPU max length,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,128,pad targets according to each GPU max length,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,134,Not really used but setting it in case for future development maybe?,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,146,Linearize outputs (1D arrays),not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,152,Natural batch synthesis,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,153,Get Mel lengths for the entire batch from stop_tokens predictions,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,156,Take off the batch wise padding,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,165,Linearize outputs (1D arrays),not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,171,Natural batch synthesis,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,172,Get Mel/Linear lengths for the entire batch from stop_tokens predictions,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,173,target_lengths = self._get_output_lengths(stop_tokens),not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,176,Take off the batch wise padding,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,186,Write the spectrogram to disk,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,187,"Note: outputs mel-spectrogram files and target ones have same names, just different folders",not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,193,save wav (mel -> wav),not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,197,save alignments,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,201,save mel spectrogram plot,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,206,save wav (linear -> wav),not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,210,save linear spectrogram plot,not
Real-Time-Voice-Cloning/synthesizer/tacotron2.py,236,Determine each mel length by the stop token predictions. (len = first occurence of 1 in stop_tokens row wise),not
Real-Time-Voice-Cloning/synthesizer/train.py,20,Create tensorboard projector,not
Real-Time-Voice-Cloning/synthesizer/train.py,25,Initialize config,not
Real-Time-Voice-Cloning/synthesizer/train.py,27,Specifiy the embedding variable and the metadata,not
Real-Time-Voice-Cloning/synthesizer/train.py,31,Project the embeddings to space dimensions for visualization,not
Real-Time-Voice-Cloning/synthesizer/train.py,52,Control learning rate decay speed,not
Real-Time-Voice-Cloning/synthesizer/train.py,54,Control teacher forcing,not
Real-Time-Voice-Cloning/synthesizer/train.py,55,"ratio decay when mode = ""scheduled""",not
Real-Time-Voice-Cloning/synthesizer/train.py,58,visualize,not
Real-Time-Voice-Cloning/synthesizer/train.py,59,gradients (in case of explosion),not
Real-Time-Voice-Cloning/synthesizer/train.py,138,Start by setting a seed for repeatability,not
Real-Time-Voice-Cloning/synthesizer/train.py,141,Set up data feeder,not
Real-Time-Voice-Cloning/synthesizer/train.py,146,Set up model:,not
Real-Time-Voice-Cloning/synthesizer/train.py,151,Embeddings metadata,not
Real-Time-Voice-Cloning/synthesizer/train.py,157,"For visual purposes, swap space with \s",not
Real-Time-Voice-Cloning/synthesizer/train.py,163,Book keeping,not
Real-Time-Voice-Cloning/synthesizer/train.py,171,Memory allocation on the GPU as needed,not
Real-Time-Voice-Cloning/synthesizer/train.py,176,Train,not
Real-Time-Voice-Cloning/synthesizer/train.py,183,saved model restoring,not
Real-Time-Voice-Cloning/synthesizer/train.py,185,"Restore saved model if the user requested it, default = True",not
Real-Time-Voice-Cloning/synthesizer/train.py,204,initializing feeder,not
Real-Time-Voice-Cloning/synthesizer/train.py,207,Training loop,not
Real-Time-Voice-Cloning/synthesizer/train.py,227,Run eval and save eval stats,not
Real-Time-Voice-Cloning/synthesizer/train.py,289,Save some log to monitor model improvement on same unseen sequence,not
Real-Time-Voice-Cloning/synthesizer/train.py,329,Save model and current global step,not
Real-Time-Voice-Cloning/synthesizer/train.py,341,save predicted mel spectrogram to disk (debug),not
Real-Time-Voice-Cloning/synthesizer/train.py,346,save griffin lim inverted wav for debug (mel -> wav),not
Real-Time-Voice-Cloning/synthesizer/train.py,352,save alignment plot to disk (control purposes),not
Real-Time-Voice-Cloning/synthesizer/train.py,359,save real and predicted mel-spectrogram plot to disk (control purposes),not
Real-Time-Voice-Cloning/synthesizer/train.py,371,Get current checkpoint state,not
Real-Time-Voice-Cloning/synthesizer/train.py,374,Update Projector,not
Real-Time-Voice-Cloning/synthesizer/__init__.py,1,,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,25,Load metadata,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,34,Train test split,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,44,Make sure test_indices is a multiple of batch_size else round up,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,58,pad input sequences with the <pad_token> 0 ( _ ),not
Real-Time-Voice-Cloning/synthesizer/feeder.py,60,"explicitely setting the padding to a value that doesn""t originally exist in the spectogram",not
Real-Time-Voice-Cloning/synthesizer/feeder.py,61,"to avoid any possible conflicts, without affecting the output range of the model too much",not
Real-Time-Voice-Cloning/synthesizer/feeder.py,66,Mark finished sequences with 1s,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,70,"Create placeholders for inputs and targets. Don""t specify batch size because we want",not
Real-Time-Voice-Cloning/synthesizer/feeder.py,71,to be able to feed different batch sizes at eval time.,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,82,SV2TTS,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,87,Create queue for buffering data,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,102,Create eval queue for buffering eval data,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,122,Thread will close when parent quits,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,126,Thread will close when parent quits,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,137,Create parallel sequences containing zeros to represent a non finished sequence,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,145,Read a group of examples,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,149,Test on entire test set,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,152,Bucket examples based on similar output sequence length for efficiency,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,164,Read a group of examples,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,169,Bucket examples based on similar output sequence length for efficiency,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,180,Create test batches once and evaluate on them for all test steps,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,201,Create parallel sequences containing zeros to represent a non finished sequence,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,217,Used to mask loss,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,227,Pad sequences with 1 to infer that the sequence is done,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,234,SV2TTS,not
Real-Time-Voice-Cloning/synthesizer/feeder.py,238,,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,15,Gather the input directories,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,22,Create the output directories for each output file type,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,26,Create a metadata file,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,30,Preprocess the dataset,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,40,Verify the contents of the metadata file,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,57,Gather the utterance audios and texts,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,63,A few alignment files will be missing,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,66,Iterate over each entry in the alignments file,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,73,Process each sub-utterance,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,84,Load the audio waveform,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,95,Find pauses that are too long,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,100,Profile the noise from the silences and perform noise reduction on the waveform,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,108,Re-attach segments that are too short,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,114,See if the segment can be re-attached with the right or the left segment,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,119,Do not re-attach if it causes the joined utterance to be too long,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,124,Re-attach the segment with the neighbour of shortest duration,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,132,Split the utterance,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,138,# DEBUG: play the audio segments (run with -n=1),not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,139,import sounddevice as sd,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,140,if len(wavs) > 1:,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,141,"print(""This sentence was split in %d segments:"" % len(wavs))",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,142,else:,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,143,"print(""There are no silences long enough for this sentence to be split:"")",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,144,"for wav, text in zip(wavs, texts):",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,145,# Pad the waveform with 1 second of silence because sounddevice tends to cut them early,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,146,# when playing them. You shouldn't need to do that in your parsers.,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,147,"wav = np.concatenate((wav, [0] * 16000))",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,148,"print(""\t%s"" % text)",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,149,"sd.play(wav, 16000, blocking=True)",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,150,"print("""")",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,157,FOR REFERENCE:,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,158,For you not to lose your head if you ever wish to change things here or implement your own,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,159,synthesizer.,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,160,- Both the audios and the mel spectrograms are saved as numpy arrays,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,161,- There is no processing done to the audios that will be saved to disk beyond volume,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,162,normalization (in split_on_silences),not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,163,"- However, pre-emphasis is applied to the audios before computing the mel spectrogram. This",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,164,is why we re-apply it on the audio on the side of the vocoder.,SATD
Real-Time-Voice-Cloning/synthesizer/preprocess.py,165,"- Librosa pads the waveform before computing the mel spectrogram. Here, the waveform is saved",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,166,without extra padding. This means that you won't have an exact relation between the length,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,167,of the wav and of the mel spectrogram. See the vocoder data loader.,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,170,Skip existing utterances if needed,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,176,Skip utterances that are too short,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,180,Compute the mel spectrogram,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,184,Skip utterances that are too long,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,188,"Write the spectrogram, embed and audio to disk",not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,192,Return a tuple describing this training example,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,200,Compute the speaker embedding of the utterance,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,215,Gather the input wave filepath and the target output embed filepath,not
Real-Time-Voice-Cloning/synthesizer/preprocess.py,220,"TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.",SATD
Real-Time-Voice-Cloning/synthesizer/preprocess.py,221,Embed the utterances in separate threads,not
Real-Time-Voice-Cloning/synthesizer/inference.py,3,You're free to use either one,not
Real-Time-Voice-Cloning/synthesizer/inference.py,4,from multiprocessing import Pool   #,not
Real-Time-Voice-Cloning/synthesizer/inference.py,33,Prepare the model,not
Real-Time-Voice-Cloning/synthesizer/inference.py,34,type: Tacotron2,not
Real-Time-Voice-Cloning/synthesizer/inference.py,75,Usual inference mode: load the model on the first request and keep it loaded.,not
Real-Time-Voice-Cloning/synthesizer/inference.py,80,Low memory inference mode: load the model upon every request. The model has to be,not
Real-Time-Voice-Cloning/synthesizer/inference.py,81,loaded in a separate process to be able to release GPU memory (a simple workaround,not
Real-Time-Voice-Cloning/synthesizer/inference.py,82,to tensorflow's intricacies),not
Real-Time-Voice-Cloning/synthesizer/inference.py,90,Load the model and forward the inputs,not
Real-Time-Voice-Cloning/synthesizer/inference.py,95,Detach the outputs (not doing so will cause the process to hang),not
Real-Time-Voice-Cloning/synthesizer/inference.py,98,Close cuda for this process,not
Real-Time-Voice-Cloning/synthesizer/utils/plot.py,50,Set common labels,not
Real-Time-Voice-Cloning/synthesizer/utils/plot.py,53,target spectrogram subplot,not
Real-Time-Voice-Cloning/synthesizer/utils/cleaners.py,17,Regular expression matching whitespace:,not
Real-Time-Voice-Cloning/synthesizer/utils/cleaners.py,20,"List of (regular expression, replacement) pairs for abbreviations:",not
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,7,from . import cmudict,not
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,13,"Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):",not
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,14,"_arpabet = [""@' + s for s in cmudict.valid_symbols]",not
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,16,Export all symbols:,not
Real-Time-Voice-Cloning/synthesizer/utils/symbols.py,17,+ _arpabet,not
Real-Time-Voice-Cloning/synthesizer/utils/numbers.py,25,Unexpected format,not
Real-Time-Voice-Cloning/synthesizer/utils/text.py,5,Mappings from symbol to numeric ID and vice versa:,not
Real-Time-Voice-Cloning/synthesizer/utils/text.py,9,Regular expression matching text enclosed in curly braces:,not
Real-Time-Voice-Cloning/synthesizer/utils/text.py,28,Check for curly braces and treat their contents as ARPAbet:,not
Real-Time-Voice-Cloning/synthesizer/utils/text.py,38,Append EOS token,not
Real-Time-Voice-Cloning/synthesizer/utils/text.py,49,Enclose ARPAbet back in curly braces:,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,34,Return all 0; we ignore them,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,39,A sequence is finished when the output probability is > 0.5,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,42,"Since we are predicting r frames at each step, two modes are",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,43,then possible:,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,44,Stop when the model outputs a p > 0.5 for any frame between r frames (Recommended),not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,45,Stop when the model outputs a p > 0.5 for all r frames (Safer),not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,46,Note:,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,47,"With enough training steps, the model should be able to predict when to stop correctly",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,48,"and the use of stop_at_any = True would be recommended. If however the model didn""t",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,49,"learn to stop correctly yet, (stops too soon) one could choose to use the safer option",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,50,to get a correct synthesis,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,52,Recommended,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,54,Safer option,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,56,"Feed last output frame as next input. outputs is [N, output_dim * r]",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,64,"inputs is [N, T_in], targets is [N, T_out, D]",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,76,Feed every r-th target frame as input,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,79,Maximal sequence length,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,99,Compute teacher forcing ratio for this global step.,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,100,"In GTA mode, override teacher forcing scheme to work with full teacher forcing",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,102,Force GTA model to always feed ground-truth,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,104,Force eval model to always feed predictions,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,113,Return all 0; we ignore them,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,117,synthesis stop (we let the model see paddings as we mask them when computing loss functions),not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,120,Pick previous outputs randomly with respect to teacher forcing ratio,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,123,Teacher-forcing: return true frame,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,126,Pass on state,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,136,,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,137,Narrow Cosine Decay:,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,139,Phase 1: tfr = 1,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,140,We only start learning rate decay after 10k steps,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,142,"Phase 2: tfr in ]0, 1[",not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,143,decay reach minimal value at step ~280k,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,145,Phase 3: tfr = 0,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,146,clip by minimal teacher forcing ratio value (step >~ 280k),not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,147,,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,148,Compute natural cosine decay,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,150,tfr = 1 at step 10k,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,151,tfr = 0 at step ~280k,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,152,tfr = 0% of init_tfr as final value,not
Real-Time-Voice-Cloning/synthesizer/models/helpers.py,155,force teacher forcing ratio to take initial value when global step < start decay step.,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,30,Initialize encoder layers,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,35,Pass input sequence through a stack of convolutional layers,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,38,Extract hidden representation from encoder lstm cells,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,41,For shape visualization,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,99,Initialize decoder layers,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,166,Information bottleneck (essential for learning attention),not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,169,Concat context vector and prenet output to form LSTM cells input (input feeding),not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,172,Unidirectional LSTM layers,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,176,Compute the attention (context) vector and alignments using,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,177,the new decoder cell hidden state as query vector,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,178,and cumulative alignments to extract location features,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,179,The choice of the new cell hidden state (s_{i}) of the last,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,180,decoder RNN Cell is based on Luong et Al. (2015):,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,181,https://arxiv.org/pdf/1508.04025.pdf,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,189,Concat LSTM outputs and context vector to form projections inputs,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,192,Compute predicted frames and predicted <stop_token>,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,196,Save alignment history,not
Real-Time-Voice-Cloning/synthesizer/models/architecture_wrappers.py,199,Prepare next decoder state,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,9,From https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,16,"Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,18,Context is the inner product of alignments and values along the,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,19,memory time dimension.,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,20,alignments shape is,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,21,"[batch_size, 1, memory_time]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,22,attention_mechanism.values shape is,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,23,"[batch_size, memory_time, memory_size]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,24,"the batched matmul is over memory_time, so the output shape is",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,25,"[batch_size, 1, memory_size].",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,26,we then squeeze out the singleton dim.,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,59,Get the number of hidden units from the trailing dimension of keys,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,147,Create normalization function,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,148,Setting it to None defaults in using softmax,not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,181,"processed_query shape [batch_size, query_depth] -> [batch_size, attention_dim]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,183,"-> [batch_size, 1, attention_dim]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,186,"processed_location_features shape [batch_size, max_time, attention dimension]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,187,"[batch_size, max_time] -> [batch_size, max_time, 1]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,189,"location features [batch_size, max_time, filters]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,191,"Projected location features [batch_size, max_time, attention_dim]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,194,"energy shape [batch_size, max_time]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,198,"alignments shape = energy shape = [batch_size, max_time]",not
Real-Time-Voice-Cloning/synthesizer/models/attention.py,201,Cumulate alignments,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,43,Convolution bank: concatenate on the last axis to stack channels from all,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,44,convolutions,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,45,The convolution bank uses multiple different kernel sizes to have many insights,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,46,of the input sequence,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,47,This makes one of the strengths of the CBHG block on sequences.,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,54,"Maxpooling (dimension reduction, Using max instead of average helps finding ""Edges""",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,55,in mels),not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,62,Two projection layers,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,68,Residual connection,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,71,"Additional projection in case of dimension mismatch (for HighwayNet ""residual""",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,72,connection),not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,76,4-layer HighwayNet,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,81,Bidirectional RNN,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,88,Concat forward and backward outputs,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,129,Apply vanilla LSTM,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,143,Apply zoneout,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,145,nn.dropout takes keep_prob (probability to keep activations) not drop_prob (,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,146,probability to mask activations)!,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,216,Create forward LSTM Cell,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,222,Create backward LSTM Cell,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,238,Concat and return forward + backward outputs,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,270,The paper discussed introducing diversity in generation at inference time,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,271,by using a dropout of 0.5 only in prenet layers (in both training and inference).,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,298,Create a set of LSTM layers,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,335,"If activation==None, this returns a simple Linear projection",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,336,else the projection will be passed through an activation function,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,337,"output = tf.layers.dense(inputs, units=self.shape, activation=self.activation,",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,338,"name=""projection_{}"".format(self.scope))",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,369,"During training, don""t use activation as it is integrated inside the",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,370,sigmoid_cross_entropy loss function,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,429,Tf version of remainder = x % multiple,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,431,Tf version of return x if remainder == 0 else x + multiple - remainder,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,453,"[batch_size, time_dimension, 1]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,454,example:,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,455,"sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,456,"[[1., 1., 1., 0., 0.]],",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,457,"[[1., 1., 0., 0., 0.]]]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,458,Note the maxlen argument that ensures mask shape is compatible with r>1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,459,This will by default mask the extra paddings caused by r>1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,463,"[batch_size, time_dimension, channel_dimension(mels)]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,476,"[batch_size, time_dimension]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,477,example:,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,478,"sequence_mask([1, 3, 2], 5) = [[1., 0., 0., 0., 0.],",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,479,"[1., 1., 1., 0., 0.],",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,480,"[1., 1., 0., 0., 0.]]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,481,Note the maxlen argument that ensures mask shape is compatible with r>1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,482,This will by default mask the extra paddings caused by r>1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,487,Use a weighted sigmoid cross entropy to measure the <stop_token> loss. Set,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,488,hparams.cross_entropy_pos_weight to 1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,489,will have the same effect as  vanilla tf.nn.sigmoid_cross_entropy_with_logits.,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,503,"[batch_size, time_dimension, 1]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,504,example:,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,505,"sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,506,"[[1., 1., 1., 0., 0.]],",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,507,"[[1., 1., 0., 0., 0.]]]",not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,508,Note the maxlen argument that ensures mask shape is compatible with r>1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,509,This will by default mask the extra paddings caused by r>1,not
Real-Time-Voice-Cloning/synthesizer/models/modules.py,513,"[batch_size, time_dimension, channel_dimension(freq)]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,17,x will be a numpy array with the contents of the placeholder below,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,79,SV2TTS,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,84,,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,119,1. Declare GPU Devices,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,130,"GTA is only used for predicting mels to train Wavenet vocoder, so we ommit",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,131,post processing when doing GTA synthesis,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,134,"Embeddings ==> [batch_size, sequence_length, embedding_dim]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,139,"Encoder Cell ==> [batch_size, encoder_steps, encoder_lstm_units]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,147,For shape visualization purpose,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,151,SV2TT2,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,153,Append the speaker embedding to the encoder output at each timestep,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,160,,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,163,Decoder Parts,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,164,Attention Decoder Prenet,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,167,Attention Mechanism,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,177,Decoder LSTM Cells,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,182,Frames Projection layer,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,185,<stop_token> projection layer,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,190,"Decoder Cell ==> [batch_size, decoder_steps, num_mels * r] (after decoding)",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,198,Define the helper for our decoder,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,205,initial decoder state,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,209,Only use max iterations at synthesis time,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,212,Decode,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,220,Reshape outputs to be one output per entry,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,221,"==> [batch_size, non_reduced_decoder_steps (decoder_steps * r), num_mels]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,225,Postnet,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,228,"Compute residual using post-net ==> [batch_size, decoder_steps * r,",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,229,postnet_channels],not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,232,Project residual to same dimension as mel spectrogram,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,233,"==> [batch_size, decoder_steps * r, num_mels]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,237,Compute the mel spectrogram,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,241,Add post-processing CBHG. This does a great job at extracting features,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,242,from mels before projection to Linear specs.,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,249,"[batch_size, decoder_steps(mel_frames), cbhg_channels]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,252,Linear projection of extracted features to make linear spectrogram,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,256,"[batch_size, decoder_steps(linear_frames), num_freq]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,259,Grab alignments from the final decoder state,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,282,self.tower_linear_targets = tower_linear_targets,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,307,1_000_000 is causing syntax problems for some people?! Python please :),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,338,Compute loss of predictions before postnet,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,342,Compute loss after postnet,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,346,Compute <stop_token> loss (for learning dynamic generation stop),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,351,SV2TTS extra L1 loss (disabled for now),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,352,"linear_loss = MaskedLinearLoss(self.tower_mel_targets[i],",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,353,"self.tower_decoder_output[i],",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,354,"self.tower_targets_lengths[i],",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,355,hparams=self._hparams),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,358,Compute loss of predictions before postnet,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,361,Compute loss after postnet,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,364,Compute <stop_token> loss (for learning dynamic generation stop),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,369,SV2TTS extra L1 loss,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,373,if hp.predict_linear:,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,374,# Compute linear loss,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,375,# From https://github.com/keithito/tacotron/blob/tacotron2-work-in,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,376,# -progress/models/tacotron.py,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,377,# Prioritize loss for frequencies under 2000 Hz.,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,378,l1 = tf.abs(self.tower_linear_targets[i] - self.tower_linear_outputs[i]),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,379,n_priority_freq = int(2000 / (hp.sample_rate * 0.5) * hp.num_freq),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,380,linear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,381,"l1[:, :, 0:n_priority_freq])",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,382,else:,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,383,linear_loss = 0.,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,385,Compute the regularization weight,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,394,Regularize variables,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,395,"Exclude all types of bias, RNN (Bengio et al. On the difficulty of training recurrent neural networks), embeddings and prediction projection layers.",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,396,"Note that we consider attention mechanism v_a weights as a prediction projection layer and we don""t regularize it. (This gave better stability)",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,402,Compute final loss term,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,435,1. Declare GPU Devices,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,454,2. Compute Gradient,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,456,Device placement,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,459,agg_loss += self.tower_loss[i],not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,464,3. Average Gradient,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,469,"grads_vars = [(grad1, var), (grad2, var), ...]",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,473,"Append on a ""tower"" dimension which we will average over below.",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,475,"Average over the ""tower"" dimension.",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,484,Just for causion,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,485,https://github.com/Rayhane-mamah/Tacotron-2/issues/11,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,487,__mark 0.5 refer,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,491,"Add dependency on UPDATE_OPS; otherwise batchnorm won""t work correctly. See:",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,492,https://github.com/tensorflow/tensorflow/issues/1122,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,498,,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,499,Narrow Exponential Decay:,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,501,Phase 1: lr = 1e-3,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,502,We only start learning rate decay after 50k steps,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,504,"Phase 2: lr in ]1e-5, 1e-3[",not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,505,decay reach minimal value at step 310k,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,507,Phase 3: lr = 1e-5,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,508,clip by minimal learning rate value (step > 310k),not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,509,,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,512,Compute natural exponential decay,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,515,lr = 1e-3 at step 50k,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,517,lr = 1e-5 around step 310k,not
Real-Time-Voice-Cloning/synthesizer/models/tacotron.py,520,clip learning rate by max and min values (initial and final values),not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,62,"To use layer""s compute_output_shape, we need to convert the",not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,63,"RNNCell""s output_size entries into shapes with an unknown",not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,64,"batch size.  We then pass this through the layer""s",not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,65,compute_output_shape and read off all but the first (batch),not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,66,dimensions to get the output size of the rnn with the layer,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,67,applied to the top.,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,71,pylint: disable=protected-access,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,77,Return the cell output and the id,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,85,Assume the dtype of the cell is the output_size structure,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,86,"containing the input_state""s first component's dtype.",not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,87,Return that structure and the sample_ids_dtype from the helper.,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,115,Call outputprojection wrapper cell,not
Real-Time-Voice-Cloning/synthesizer/models/custom_decoder.py,118,apply output_layer (if existant),not
Real-Time-Voice-Cloning/encoder/model.py,17,Network defition,not
Real-Time-Voice-Cloning/encoder/model.py,26,Cosine similarity scaling (with fixed initial parameter values),not
Real-Time-Voice-Cloning/encoder/model.py,30,Loss,not
Real-Time-Voice-Cloning/encoder/model.py,34,Gradient scale,not
Real-Time-Voice-Cloning/encoder/model.py,38,Gradient clipping,not
Real-Time-Voice-Cloning/encoder/model.py,51,"Pass the input through the LSTM layers and retrieve all outputs, the final hidden state",not
Real-Time-Voice-Cloning/encoder/model.py,52,and the final cell state.,not
Real-Time-Voice-Cloning/encoder/model.py,55,We take only the hidden state of the last layer,not
Real-Time-Voice-Cloning/encoder/model.py,58,L2-normalize it,not
Real-Time-Voice-Cloning/encoder/model.py,74,Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation,not
Real-Time-Voice-Cloning/encoder/model.py,78,Exclusive centroids (1 per utterance),not
Real-Time-Voice-Cloning/encoder/model.py,83,Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot,not
Real-Time-Voice-Cloning/encoder/model.py,84,product of these vectors (which is just an element-wise multiplication reduced by a sum).,not
Real-Time-Voice-Cloning/encoder/model.py,85,We vectorize the computation for efficiency.,not
Real-Time-Voice-Cloning/encoder/model.py,94,Even more vectorized version (slower maybe because of transpose),not
Real-Time-Voice-Cloning/encoder/model.py,95,"sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker",not
Real-Time-Voice-Cloning/encoder/model.py,96,).to(self.loss_device),not
Real-Time-Voice-Cloning/encoder/model.py,97,"eye = np.eye(speakers_per_batch, dtype=np.int)",not
Real-Time-Voice-Cloning/encoder/model.py,98,mask = np.where(1 - eye),not
Real-Time-Voice-Cloning/encoder/model.py,99,sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2),not
Real-Time-Voice-Cloning/encoder/model.py,100,mask = np.where(eye),not
Real-Time-Voice-Cloning/encoder/model.py,101,sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2),not
Real-Time-Voice-Cloning/encoder/model.py,102,"sim_matrix2 = sim_matrix2.transpose(1, 2)",not
Real-Time-Voice-Cloning/encoder/model.py,117,Loss,not
Real-Time-Voice-Cloning/encoder/model.py,125,EER (not backpropagated),not
Real-Time-Voice-Cloning/encoder/model.py,131,Snippet from https://yangcha.github.io/EER-ROC/,not
Real-Time-Voice-Cloning/encoder/audio.py,26,Load the wav from disk if needed,not
Real-Time-Voice-Cloning/encoder/audio.py,32,Resample the wav if needed,not
Real-Time-Voice-Cloning/encoder/audio.py,36,Apply the preprocessing: normalize volume and shorten long silences,not
Real-Time-Voice-Cloning/encoder/audio.py,66,Compute the voice detection window size,not
Real-Time-Voice-Cloning/encoder/audio.py,69,Trim the end of the audio to have a multiple of the window size,not
Real-Time-Voice-Cloning/encoder/audio.py,72,Convert the float waveform to 16-bit mono PCM,not
Real-Time-Voice-Cloning/encoder/audio.py,75,Perform voice activation detection,not
Real-Time-Voice-Cloning/encoder/audio.py,84,Smooth the voice detection with a moving average,not
Real-Time-Voice-Cloning/encoder/audio.py,94,Dilate the voiced regions,not
Real-Time-Voice-Cloning/encoder/visualizations.py,6,import webbrowser,not
Real-Time-Voice-Cloning/encoder/visualizations.py,29,Tracking data,not
Real-Time-Voice-Cloning/encoder/visualizations.py,37,If visdom is disabled TODO: use a better paradigm for that,SATD
Real-Time-Voice-Cloning/encoder/visualizations.py,42,Set the environment name,not
Real-Time-Voice-Cloning/encoder/visualizations.py,49,Connect to visdom and open the corresponding window in the browser,not
Real-Time-Voice-Cloning/encoder/visualizations.py,55,"webbrowser.open(""http://localhost:8097/env/"" + self.env_name)",not
Real-Time-Voice-Cloning/encoder/visualizations.py,57,Create the windows,not
Real-Time-Voice-Cloning/encoder/visualizations.py,60,self.lr_win = None,not
Real-Time-Voice-Cloning/encoder/visualizations.py,103,Update the tracking data,not
Real-Time-Voice-Cloning/encoder/visualizations.py,111,Update the plots every <update_every> steps,not
Real-Time-Voice-Cloning/encoder/visualizations.py,150,Reset the tracking,not
Real-Time-Voice-Cloning/encoder/params_data.py,2,Mel-filterbank,not
Real-Time-Voice-Cloning/encoder/params_data.py,3,In milliseconds,not
Real-Time-Voice-Cloning/encoder/params_data.py,4,In milliseconds,not
Real-Time-Voice-Cloning/encoder/params_data.py,8,Audio,not
Real-Time-Voice-Cloning/encoder/params_data.py,10,Number of spectrogram frames in a partial utterance,not
Real-Time-Voice-Cloning/encoder/params_data.py,11,1600 ms,not
Real-Time-Voice-Cloning/encoder/params_data.py,12,Number of spectrogram frames at inference,not
Real-Time-Voice-Cloning/encoder/params_data.py,13,800 ms,not
Real-Time-Voice-Cloning/encoder/params_data.py,16,Voice Activation Detection,not
Real-Time-Voice-Cloning/encoder/params_data.py,17,"Window size of the VAD. Must be either 10, 20 or 30 milliseconds.",not
Real-Time-Voice-Cloning/encoder/params_data.py,18,This sets the granularity of the VAD. Should not need to be changed.,not
Real-Time-Voice-Cloning/encoder/params_data.py,19,In milliseconds,not
Real-Time-Voice-Cloning/encoder/params_data.py,20,Number of frames to average together when performing the moving average smoothing.,not
Real-Time-Voice-Cloning/encoder/params_data.py,21,"The larger this value, the larger the VAD variations must be to not get smoothed out.",not
Real-Time-Voice-Cloning/encoder/params_data.py,23,Maximum number of consecutive silent frames a segment can have.,not
Real-Time-Voice-Cloning/encoder/params_data.py,27,Audio volume normalization,not
Real-Time-Voice-Cloning/encoder/params_model.py,2,Model parameters,not
Real-Time-Voice-Cloning/encoder/params_model.py,8,Training parameters,not
Real-Time-Voice-Cloning/encoder/train.py,10,FIXME,SATD
Real-Time-Voice-Cloning/encoder/train.py,12,For correct profiling (cuda operations are async),not
Real-Time-Voice-Cloning/encoder/train.py,19,Create a dataset and a dataloader,not
Real-Time-Voice-Cloning/encoder/train.py,28,"Setup the device on which to run the forward pass and the loss. These can be different,",not
Real-Time-Voice-Cloning/encoder/train.py,29,because the forward pass is faster on the GPU whereas the loss is often (depending on your,not
Real-Time-Voice-Cloning/encoder/train.py,30,hyperparameters) faster on the CPU.,not
Real-Time-Voice-Cloning/encoder/train.py,32,"FIXME: currently, the gradient is None if loss_device is cuda",SATD
Real-Time-Voice-Cloning/encoder/train.py,35,Create the model and the optimizer,not
Real-Time-Voice-Cloning/encoder/train.py,40,Configure file path for the model,not
Real-Time-Voice-Cloning/encoder/train.py,44,Load any existing model,not
Real-Time-Voice-Cloning/encoder/train.py,59,Initialize the visualization environment,not
Real-Time-Voice-Cloning/encoder/train.py,66,Training loop,not
Real-Time-Voice-Cloning/encoder/train.py,71,Forward pass,not
Real-Time-Voice-Cloning/encoder/train.py,83,Backward pass,not
Real-Time-Voice-Cloning/encoder/train.py,91,Update visualizations,not
Real-Time-Voice-Cloning/encoder/train.py,92,"learning_rate = optimizer.param_groups[0][""lr""]",not
Real-Time-Voice-Cloning/encoder/train.py,95,Draw projections and save them to the backup folder,not
Real-Time-Voice-Cloning/encoder/train.py,104,Overwrite the latest version of the model,not
Real-Time-Voice-Cloning/encoder/train.py,113,Make a backup,not
Real-Time-Voice-Cloning/encoder/preprocess.py,65,Function to preprocess utterances for one speaker,not
Real-Time-Voice-Cloning/encoder/preprocess.py,67,Give a name to the speaker that includes its dataset,not
Real-Time-Voice-Cloning/encoder/preprocess.py,70,"Create an output directory with that name, as well as a txt file containing a",not
Real-Time-Voice-Cloning/encoder/preprocess.py,71,reference to each source file.,not
Real-Time-Voice-Cloning/encoder/preprocess.py,76,"There's a possibility that the preprocessing was interrupted earlier, check if",not
Real-Time-Voice-Cloning/encoder/preprocess.py,77,there already is a sources file.,not
Real-Time-Voice-Cloning/encoder/preprocess.py,87,Gather all audio files for that speaker recursively,not
Real-Time-Voice-Cloning/encoder/preprocess.py,90,Check if the target output file already exists,not
Real-Time-Voice-Cloning/encoder/preprocess.py,96,Load and preprocess the waveform,not
Real-Time-Voice-Cloning/encoder/preprocess.py,101,"Create the mel spectrogram, discard those that are too short",not
Real-Time-Voice-Cloning/encoder/preprocess.py,113,Process the utterances for each speaker,not
Real-Time-Voice-Cloning/encoder/preprocess.py,123,Initialize the preprocessing,not
Real-Time-Voice-Cloning/encoder/preprocess.py,128,Preprocess all speakers,not
Real-Time-Voice-Cloning/encoder/preprocess.py,135,Initialize the preprocessing,not
Real-Time-Voice-Cloning/encoder/preprocess.py,141,Get the contents of the meta file,not
Real-Time-Voice-Cloning/encoder/preprocess.py,145,"Select the ID and the nationality, filter out non-anglophone speakers",not
Real-Time-Voice-Cloning/encoder/preprocess.py,152,Get the speaker directories for anglophone speakers only,not
Real-Time-Voice-Cloning/encoder/preprocess.py,159,Preprocess all speakers,not
Real-Time-Voice-Cloning/encoder/preprocess.py,165,Initialize the preprocessing,not
Real-Time-Voice-Cloning/encoder/preprocess.py,171,Get the speaker directories,not
Real-Time-Voice-Cloning/encoder/preprocess.py,172,Preprocess all speakers,not
Real-Time-Voice-Cloning/encoder/inference.py,3,We want to expose this function from here,not
Real-Time-Voice-Cloning/encoder/inference.py,11,type: SpeakerEncoder,not
Real-Time-Voice-Cloning/encoder/inference.py,12,type: torch.device,not
Real-Time-Voice-Cloning/encoder/inference.py,25,TODO: I think the slow loading of the encoder might have something to do with the device it,SATD
Real-Time-Voice-Cloning/encoder/inference.py,26,was saved on. Worth investigating.,not
Real-Time-Voice-Cloning/encoder/inference.py,92,Compute the slices,not
Real-Time-Voice-Cloning/encoder/inference.py,101,Evaluate whether extra padding is warranted or not,not
Real-Time-Voice-Cloning/encoder/inference.py,130,Process the entire utterance if not using partials,not
Real-Time-Voice-Cloning/encoder/inference.py,138,Compute where to split the utterance into partials and pad if necessary,not
Real-Time-Voice-Cloning/encoder/inference.py,144,Split the utterance into partials,not
Real-Time-Voice-Cloning/encoder/inference.py,149,Compute the utterance embedding from the partial embeddings,not
Real-Time-Voice-Cloning/encoder/data_objects/speaker.py,5,Contains the set of utterances of a single speaker,not
Real-Time-Voice-Cloning/encoder/data_objects/speaker_verification_dataset.py,8,TODO: improve with a pool of speakers for data efficiency,SATD
Real-Time-Voice-Cloning/encoder/data_objects/speaker_batch.py,10,"Array of shape (n_speakers * n_utterances, n_frames, mel_n), e.g. for 3 speakers with",not
Real-Time-Voice-Cloning/encoder/data_objects/speaker_batch.py,11,"4 utterances each of 160 frames of 40 mel coefficients: (12, 160, 40)",not
Real-Time-Voice-Cloning/utils/argutils.py,5,In decreasing order,not
Real-Time-Voice-Cloning/utils/profiler.py,17,Log the time needed to execute that function,not
Real-Time-Voice-Cloning/utils/logmmse.py,1,The MIT License (MIT),not
Real-Time-Voice-Cloning/utils/logmmse.py,2,,not
Real-Time-Voice-Cloning/utils/logmmse.py,3,Copyright (c) 2015 braindead,not
Real-Time-Voice-Cloning/utils/logmmse.py,4,,not
Real-Time-Voice-Cloning/utils/logmmse.py,5,"Permission is hereby granted, free of charge, to any person obtaining a copy",not
Real-Time-Voice-Cloning/utils/logmmse.py,6,"of this software and associated documentation files (the ""Software""), to deal",not
Real-Time-Voice-Cloning/utils/logmmse.py,7,"in the Software without restriction, including without limitation the rights",not
Real-Time-Voice-Cloning/utils/logmmse.py,8,"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",not
Real-Time-Voice-Cloning/utils/logmmse.py,9,"copies of the Software, and to permit persons to whom the Software is",not
Real-Time-Voice-Cloning/utils/logmmse.py,10,"furnished to do so, subject to the following conditions:",not
Real-Time-Voice-Cloning/utils/logmmse.py,11,,not
Real-Time-Voice-Cloning/utils/logmmse.py,12,The above copyright notice and this permission notice shall be included in all,not
Real-Time-Voice-Cloning/utils/logmmse.py,13,copies or substantial portions of the Software.,not
Real-Time-Voice-Cloning/utils/logmmse.py,14,,not
Real-Time-Voice-Cloning/utils/logmmse.py,15,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR",not
Real-Time-Voice-Cloning/utils/logmmse.py,16,"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,",not
Real-Time-Voice-Cloning/utils/logmmse.py,17,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE,not
Real-Time-Voice-Cloning/utils/logmmse.py,18,"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER",not
Real-Time-Voice-Cloning/utils/logmmse.py,19,"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",not
Real-Time-Voice-Cloning/utils/logmmse.py,20,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE,not
Real-Time-Voice-Cloning/utils/logmmse.py,21,SOFTWARE.,not
Real-Time-Voice-Cloning/utils/logmmse.py,22,,not
Real-Time-Voice-Cloning/utils/logmmse.py,23,,not
Real-Time-Voice-Cloning/utils/logmmse.py,24,This code was extracted from the logmmse package (https://pypi.org/project/logmmse/) and I,not
Real-Time-Voice-Cloning/utils/logmmse.py,25,simply modified the interface to meet my needs.,not
Real-Time-Voice-Cloning/utils/logmmse.py,136,Alternative VAD algorithm to webrctvad. It has the advantage of not requiring to install that,not
Real-Time-Voice-Cloning/utils/logmmse.py,137,darn package and it also works for any sampling rate. Maybe I'll eventually use it instead of,not
Real-Time-Voice-Cloning/utils/logmmse.py,138,webrctvad,not
Real-Time-Voice-Cloning/utils/logmmse.py,139,"def vad(wav, sampling_rate, eta=0.15, window_size=0):",not
Real-Time-Voice-Cloning/utils/logmmse.py,140,"""""""",not
Real-Time-Voice-Cloning/utils/logmmse.py,141,TODO: fix doc,SATD
Real-Time-Voice-Cloning/utils/logmmse.py,142,Creates a profile of the noise in a given waveform.,not
Real-Time-Voice-Cloning/utils/logmmse.py,143,,not
Real-Time-Voice-Cloning/utils/logmmse.py,144,":param wav: a waveform containing noise ONLY, as a numpy array of floats or ints.",not
Real-Time-Voice-Cloning/utils/logmmse.py,145,:param sampling_rate: the sampling rate of the audio,not
Real-Time-Voice-Cloning/utils/logmmse.py,146,:param window_size: the size of the window the logmmse algorithm operates on. A default value,not
Real-Time-Voice-Cloning/utils/logmmse.py,147,will be picked if left as 0.,not
Real-Time-Voice-Cloning/utils/logmmse.py,148,:param eta: voice threshold for noise update. While the voice activation detection value is,not
Real-Time-Voice-Cloning/utils/logmmse.py,149,"below this threshold, the noise profile will be continuously updated throughout the audio.",not
Real-Time-Voice-Cloning/utils/logmmse.py,150,Set to 0 to disable updating the noise profile.,not
Real-Time-Voice-Cloning/utils/logmmse.py,151,"""""""",not
Real-Time-Voice-Cloning/utils/logmmse.py,152,"wav, dtype = to_float(wav)",not
Real-Time-Voice-Cloning/utils/logmmse.py,153,wav += np.finfo(np.float64).eps,not
Real-Time-Voice-Cloning/utils/logmmse.py,154,,not
Real-Time-Voice-Cloning/utils/logmmse.py,155,if window_size == 0:,not
Real-Time-Voice-Cloning/utils/logmmse.py,156,window_size = int(math.floor(0.02 * sampling_rate)),not
Real-Time-Voice-Cloning/utils/logmmse.py,157,,not
Real-Time-Voice-Cloning/utils/logmmse.py,158,if window_size % 2 == 1:,not
Real-Time-Voice-Cloning/utils/logmmse.py,159,window_size = window_size + 1,not
Real-Time-Voice-Cloning/utils/logmmse.py,160,,not
Real-Time-Voice-Cloning/utils/logmmse.py,161,perc = 50,not
Real-Time-Voice-Cloning/utils/logmmse.py,162,len1 = int(math.floor(window_size * perc / 100)),not
Real-Time-Voice-Cloning/utils/logmmse.py,163,len2 = int(window_size - len1),not
Real-Time-Voice-Cloning/utils/logmmse.py,164,,not
Real-Time-Voice-Cloning/utils/logmmse.py,165,win = np.hanning(window_size),not
Real-Time-Voice-Cloning/utils/logmmse.py,166,win = win * len2 / np.sum(win),not
Real-Time-Voice-Cloning/utils/logmmse.py,167,n_fft = 2 * window_size,not
Real-Time-Voice-Cloning/utils/logmmse.py,168,,not
Real-Time-Voice-Cloning/utils/logmmse.py,169,wav_mean = np.zeros(n_fft),not
Real-Time-Voice-Cloning/utils/logmmse.py,170,n_frames = len(wav) // window_size,not
Real-Time-Voice-Cloning/utils/logmmse.py,171,"for j in range(0, window_size * n_frames, window_size):",not
Real-Time-Voice-Cloning/utils/logmmse.py,172,"wav_mean += np.absolute(np.fft.fft(win * wav[j:j + window_size], n_fft, axis=0))",not
Real-Time-Voice-Cloning/utils/logmmse.py,173,noise_mu2 = (wav_mean / n_frames) ** 2,not
Real-Time-Voice-Cloning/utils/logmmse.py,174,,not
Real-Time-Voice-Cloning/utils/logmmse.py,175,"wav, dtype = to_float(wav)",not
Real-Time-Voice-Cloning/utils/logmmse.py,176,wav += np.finfo(np.float64).eps,not
Real-Time-Voice-Cloning/utils/logmmse.py,177,,not
Real-Time-Voice-Cloning/utils/logmmse.py,178,nframes = int(math.floor(len(wav) / len2) - math.floor(window_size / len2)),not
Real-Time-Voice-Cloning/utils/logmmse.py,179,"vad = np.zeros(nframes * len2, dtype=np.bool)",not
Real-Time-Voice-Cloning/utils/logmmse.py,180,,not
Real-Time-Voice-Cloning/utils/logmmse.py,181,aa = 0.98,not
Real-Time-Voice-Cloning/utils/logmmse.py,182,mu = 0.98,not
Real-Time-Voice-Cloning/utils/logmmse.py,183,ksi_min = 10 ** (-25 / 10),not
Real-Time-Voice-Cloning/utils/logmmse.py,184,,not
Real-Time-Voice-Cloning/utils/logmmse.py,185,xk_prev = np.zeros(len1),not
Real-Time-Voice-Cloning/utils/logmmse.py,186,noise_mu2 = noise_mu2,not
Real-Time-Voice-Cloning/utils/logmmse.py,187,"for k in range(0, nframes * len2, len2):",not
Real-Time-Voice-Cloning/utils/logmmse.py,188,insign = win * wav[k:k + window_size],not
Real-Time-Voice-Cloning/utils/logmmse.py,189,,not
Real-Time-Voice-Cloning/utils/logmmse.py,190,"spec = np.fft.fft(insign, n_fft, axis=0)",not
Real-Time-Voice-Cloning/utils/logmmse.py,191,sig = np.absolute(spec),not
Real-Time-Voice-Cloning/utils/logmmse.py,192,sig2 = sig ** 2,not
Real-Time-Voice-Cloning/utils/logmmse.py,193,,not
Real-Time-Voice-Cloning/utils/logmmse.py,194,"gammak = np.minimum(sig2 / noise_mu2, 40)",not
Real-Time-Voice-Cloning/utils/logmmse.py,195,,not
Real-Time-Voice-Cloning/utils/logmmse.py,196,if xk_prev.all() == 0:,not
Real-Time-Voice-Cloning/utils/logmmse.py,197,"ksi = aa + (1 - aa) * np.maximum(gammak - 1, 0)",not
Real-Time-Voice-Cloning/utils/logmmse.py,198,else:,not
Real-Time-Voice-Cloning/utils/logmmse.py,199,"ksi = aa * xk_prev / noise_mu2 + (1 - aa) * np.maximum(gammak - 1, 0)",not
Real-Time-Voice-Cloning/utils/logmmse.py,200,"ksi = np.maximum(ksi_min, ksi)",not
Real-Time-Voice-Cloning/utils/logmmse.py,201,,not
Real-Time-Voice-Cloning/utils/logmmse.py,202,log_sigma_k = gammak * ksi / (1 + ksi) - np.log(1 + ksi),not
Real-Time-Voice-Cloning/utils/logmmse.py,203,vad_decision = np.sum(log_sigma_k) / window_size,not
Real-Time-Voice-Cloning/utils/logmmse.py,204,if vad_decision < eta:,not
Real-Time-Voice-Cloning/utils/logmmse.py,205,noise_mu2 = mu * noise_mu2 + (1 - mu) * sig2,not
Real-Time-Voice-Cloning/utils/logmmse.py,206,print(vad_decision),not
Real-Time-Voice-Cloning/utils/logmmse.py,207,,not
Real-Time-Voice-Cloning/utils/logmmse.py,208,a = ksi / (1 + ksi),not
Real-Time-Voice-Cloning/utils/logmmse.py,209,vk = a * gammak,not
Real-Time-Voice-Cloning/utils/logmmse.py,210,"ei_vk = 0.5 * expn(1, np.maximum(vk, 1e-8))",not
Real-Time-Voice-Cloning/utils/logmmse.py,211,hw = a * np.exp(ei_vk),not
Real-Time-Voice-Cloning/utils/logmmse.py,212,sig = sig * hw,not
Real-Time-Voice-Cloning/utils/logmmse.py,213,xk_prev = sig ** 2,not
Real-Time-Voice-Cloning/utils/logmmse.py,214,,not
Real-Time-Voice-Cloning/utils/logmmse.py,215,vad[k:k + len2] = vad_decision >= eta,not
Real-Time-Voice-Cloning/utils/logmmse.py,216,,not
Real-Time-Voice-Cloning/utils/logmmse.py,217,"vad = np.pad(vad, (0, len(wav) - len(vad)), mode=""constant"")",not
Real-Time-Voice-Cloning/utils/logmmse.py,218,return vad,not
Real-Time-Voice-Cloning/vocoder/distribution.py,8,TF ordering,not
Real-Time-Voice-Cloning/vocoder/distribution.py,15,It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py,not
Real-Time-Voice-Cloning/vocoder/distribution.py,25,(B x T x C),not
Real-Time-Voice-Cloning/vocoder/distribution.py,28,"unpack parameters. (B, T, num_mixtures) x 3",not
Real-Time-Voice-Cloning/vocoder/distribution.py,33,B x T x 1 -> B x T x num_mixtures,not
Real-Time-Voice-Cloning/vocoder/distribution.py,43,log probability for edge case of 0 (before scaling),not
Real-Time-Voice-Cloning/vocoder/distribution.py,44,equivalent: torch.log(F.sigmoid(plus_in)),not
Real-Time-Voice-Cloning/vocoder/distribution.py,47,log probability for edge case of 255 (before scaling),not
Real-Time-Voice-Cloning/vocoder/distribution.py,48,equivalent: (1 - F.sigmoid(min_in)).log(),not
Real-Time-Voice-Cloning/vocoder/distribution.py,51,probability for all other cases,not
Real-Time-Voice-Cloning/vocoder/distribution.py,55,"log probability in the center of the bin, to be used in extreme cases",not
Real-Time-Voice-Cloning/vocoder/distribution.py,56,(not actually used in our code),not
Real-Time-Voice-Cloning/vocoder/distribution.py,59,tf equivalent,not
Real-Time-Voice-Cloning/vocoder/distribution.py,67,TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value,SATD
Real-Time-Voice-Cloning/vocoder/distribution.py,68,for num_classes=65536 case? 1e-7? not sure..,not
Real-Time-Voice-Cloning/vocoder/distribution.py,101,B x T x C,not
Real-Time-Voice-Cloning/vocoder/distribution.py,105,sample mixture indicator from softmax,not
Real-Time-Voice-Cloning/vocoder/distribution.py,110,"(B, T) -> (B, T, nr_mix)",not
Real-Time-Voice-Cloning/vocoder/distribution.py,112,select logistic parameters,not
Real-Time-Voice-Cloning/vocoder/distribution.py,116,sample from logistic & clip to interval,not
Real-Time-Voice-Cloning/vocoder/distribution.py,117,we don't actually round to the nearest 8bit value when sampling,not
Real-Time-Voice-Cloning/vocoder/distribution.py,127,we perform one hot encore with respect to the last axis,not
Real-Time-Voice-Cloning/vocoder/hparams.py,4,Audio settings------------------------------------------------------------------------,not
Real-Time-Voice-Cloning/vocoder/hparams.py,5,Match the values of the synthesizer,not
Real-Time-Voice-Cloning/vocoder/hparams.py,18,bit depth of signal,not
Real-Time-Voice-Cloning/vocoder/hparams.py,19,Recommended to suppress noise if using raw bits in hp.voc_mode,not
Real-Time-Voice-Cloning/vocoder/hparams.py,20,below,not
Real-Time-Voice-Cloning/vocoder/hparams.py,23,WAVERNN / VOCODER --------------------------------------------------------------------------------,not
Real-Time-Voice-Cloning/vocoder/hparams.py,24,either 'RAW' (softmax on raw bits) or 'MOL' (sample from,not
Real-Time-Voice-Cloning/vocoder/hparams.py,25,mixture of logistics),not
Real-Time-Voice-Cloning/vocoder/hparams.py,26,NB - this needs to correctly factorise hop_length,not
Real-Time-Voice-Cloning/vocoder/hparams.py,33,Training,not
Real-Time-Voice-Cloning/vocoder/hparams.py,36,number of samples to generate at each checkpoint,not
Real-Time-Voice-Cloning/vocoder/hparams.py,37,this will pad the input so that the resnet can 'see' wider,not
Real-Time-Voice-Cloning/vocoder/hparams.py,38,than input length,not
Real-Time-Voice-Cloning/vocoder/hparams.py,39,must be a multiple of hop_length,not
Real-Time-Voice-Cloning/vocoder/hparams.py,41,Generating / Synthesizing,not
Real-Time-Voice-Cloning/vocoder/hparams.py,42,very fast (realtime+) single utterance batched generation,not
Real-Time-Voice-Cloning/vocoder/hparams.py,43,target number of samples to be generated in each batch entry,not
Real-Time-Voice-Cloning/vocoder/hparams.py,44,number of samples for crossfading between batches,not
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,27,"Load the mel spectrogram and adjust its range to [-1, 1]",not
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,30,Load the wav,not
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,36,Fix for missing padding   # TODO: settle on whether this is any useful,SATD
Real-Time-Voice-Cloning/vocoder/vocoder_dataset.py,43,Quantize the wav,not
Real-Time-Voice-Cloning/vocoder/train.py,17,Check to make sure the hop length is correctly factorised,not
Real-Time-Voice-Cloning/vocoder/train.py,20,Instantiate the model,not
Real-Time-Voice-Cloning/vocoder/train.py,37,Initialize the optimizer,not
Real-Time-Voice-Cloning/vocoder/train.py,43,Load the weights,not
Real-Time-Voice-Cloning/vocoder/train.py,55,Initialize the dataset,not
Real-Time-Voice-Cloning/vocoder/train.py,66,Begin the training,not
Real-Time-Voice-Cloning/vocoder/train.py,84,Forward pass,not
Real-Time-Voice-Cloning/vocoder/train.py,92,Backward pass,not
Real-Time-Voice-Cloning/vocoder/inference.py,6,type: WaveRNN,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,15,The main matmul,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,18,Output fc layers,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,24,Input fc layers,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,28,biases for the gates,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,33,display num params,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,39,Main matmul - the projection is split 3 ways,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,43,Project the prev input,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,48,Project the prev input and current coarse sample,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,54,concatenate for the gates,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,59,Compute all gates for coarse and fine,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,65,Split the hidden state,SATD
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,68,Compute outputs,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,77,First split up the biases for the gates,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,82,Lists for the two output seqs,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,85,Some initial inputs,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,89,We'll meed a hidden state,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,92,Need a clock for display,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,95,Loop for generation,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,98,Split into two hidden states,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,102,Scale and concat previous predictions,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,107,Project input,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,112,Project hidden state and split 6 ways,SATD
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,118,Compute the coarse gates,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,124,Compute the coarse output,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,131,Project the [prev outputs and predicted coarse sample],not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,138,Compute the fine gates,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,144,Compute the fine output,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,151,Put the hidden state back together,not
Real-Time-Voice-Cloning/vocoder/models/deepmind_version.py,154,Display progress,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,204,x = torch.FloatTensor([[sample]]).cuda(),not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,235,Fade-out at the end to avoid signal cutting out suddenly,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,259,NB - this is just a quick method i need right now,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,260,"i.e., it won't generalise to other shapes/dims",not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,298,Calculate variables needed,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,303,Pad if some time steps poking out,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,311,Get the values for the folded tensor,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,355,Need some silence for the rnn warmup,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,360,Equal power crossfade,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,365,Concat the silence to the fades,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,369,Apply the gain to the overlap samples,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,375,Loop to add up all the samples,not
Real-Time-Voice-Cloning/vocoder/models/fatchord_version.py,400,Backwards compatibility,not
