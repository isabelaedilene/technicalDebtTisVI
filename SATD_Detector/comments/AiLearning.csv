file path,line #,comment,satd
AiLearning/tool/DecisionTree_getInfoGain.py,1,!/usr/bin/python,
AiLearning/tool/DecisionTree_getInfoGain.py,2,coding: utf8,
AiLearning/tool/DecisionTree_getInfoGain.py,17,求list的长度，表示计算参与训练的数据量,
AiLearning/tool/DecisionTree_getInfoGain.py,19,"print(type(dataSet), 'numEntries: ', numEntries)",
AiLearning/tool/DecisionTree_getInfoGain.py,21,计算分类标签label出现的次数,
AiLearning/tool/DecisionTree_getInfoGain.py,23,the the number of unique elements and their occurance,
AiLearning/tool/DecisionTree_getInfoGain.py,29,"print('-----', featVec, labelCounts)",
AiLearning/tool/DecisionTree_getInfoGain.py,31,对于label标签的占比，求出label标签的香农熵,
AiLearning/tool/DecisionTree_getInfoGain.py,35,log base 2,
AiLearning/tool/DecisionTree_getInfoGain.py,37,"print('---', prob, prob * log(prob, 2), shannonEnt)",
AiLearning/tool/DecisionTree_getInfoGain.py,55,axis列为value的数据集【该数据集需要排除axis列】,
AiLearning/tool/DecisionTree_getInfoGain.py,57,chop out axis used for splitting,
AiLearning/tool/DecisionTree_getInfoGain.py,63,收集结果值 axis列为value的行【该行需要排除axis列】,
AiLearning/tool/DecisionTree_getInfoGain.py,78,求第一行有多少列的 Feature,
AiLearning/tool/DecisionTree_getInfoGain.py,80,label的信息熵,
AiLearning/tool/DecisionTree_getInfoGain.py,82,"最优的信息增益值, 和最优的Featurn编号",
AiLearning/tool/DecisionTree_getInfoGain.py,84,iterate over all the features,
AiLearning/tool/DecisionTree_getInfoGain.py,86,create a list of all the examples of this feature,
AiLearning/tool/DecisionTree_getInfoGain.py,87,获取每一个feature的list集合,
AiLearning/tool/DecisionTree_getInfoGain.py,89,get a set of unique values,
AiLearning/tool/DecisionTree_getInfoGain.py,90,获取剔重后的集合,
AiLearning/tool/DecisionTree_getInfoGain.py,92,创建一个临时的信息熵,
AiLearning/tool/DecisionTree_getInfoGain.py,94,遍历某一列的value集合，计算该列的信息熵,
AiLearning/tool/DecisionTree_getInfoGain.py,99,gain[信息增益] 值越大，意味着该分类提供的信息量越大，该特征对分类的不确定程度越小,
AiLearning/tool/DecisionTree_getInfoGain.py,100,"gain[信息增益]=0, 表示与类别相同，无需其他的分类",
AiLearning/tool/DecisionTree_getInfoGain.py,101,"gain[信息增益]=baseEntropy, 表示分类和没分类没有区别",
AiLearning/tool/DecisionTree_getInfoGain.py,103,print(infoGain),
AiLearning/tool/python2libsvm.py,1,!/usr/bin/python,
AiLearning/tool/python2libsvm.py,2,coding:utf8,
AiLearning/tool/python2libsvm.py,24,print len(features),
AiLearning/tool/python2libsvm.py,29,print svm_format,
AiLearning/tool/python2libsvm.py,31,print svm_format,
AiLearning/tool/python2libsvm.py,48,获取数据集,
AiLearning/tool/python2libsvm.py,52,导出数据为 libsvm,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,1,*-* coding:utf-8 *-*,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,16,该目录下的 config.py文件， 数据文件是: poetry.txt,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,21,读取文本内容，合并到一个大字符中，用 ] 隔开,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,25,"每行的末尾加上""]""符号代表一首诗结束",
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,29,按照字存到字典中，字+频率,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,38,去掉低频的字,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,39,"[('。', 567), ('，', 565), ('风', 47), ('花', 42), ('云', 40)]",
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,41,print(wordPairs),
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,44,word到id的映射,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,58,文件预处理,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,61,如果模型文件存在则直接加载模型，否则开始训练,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,79,设置优化器,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,91,如果给的text不到四个字，则随机补全,
AiLearning/src/py3.x/tensorflow2.x/text_PoetryModel.py,122,如果越界了，就从0再开始,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,24,save initial config data,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,34,"in windows the new line is '\r\n\r\n' the space is '\r\n' . so if you use windows system,",
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,35,you have to use recorsponding instructions,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,54,set to <unk> (index 1) if not in vocab,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,58,left padding,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,73,left padding,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,84,Random embedding,
AiLearning/src/py3.x/tensorflow2.x/text_NER.py,99,train model,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,1,-*- coding: utf-8 -*-,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,17,Commented out IPython magic to ensure Python compatibility.,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,21,Colab only,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,22,%tensorflow_version 2.x,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,65,一个映射单词到整数索引的词典,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,68,保留第一个索引,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,72,unknown,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,125,"输入形状是用于电影评论的词汇数目（10,000 词）",
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,217,"“bo”代表 ""蓝点""",
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,219,b代表“蓝色实线”,
AiLearning/src/py3.x/tensorflow2.x/text_classification.py,228,清除数字,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,1,%%,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,2,-*- coding: utf-8 -*-,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,6,%%,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,7,载入并准备好 MNIST 数据集,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,8,存放地址: /home/xxx/.keras/datasets/mnist.npz,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,13,%%,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,14,将模型的各层堆叠起来，以搭建 tf.keras.Sequential 模型。为训练选择优化器和损失函数：,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,22,%%,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,27,%%,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,28,训练模型,
AiLearning/src/py3.x/tensorflow2.x/quickstart.py,30,验证模型,
AiLearning/src/py3.x/tensorflow2.x/config.py,1,*-* coding:utf-8 *-*,
AiLearning/src/py3.x/tensorflow2.x/config.py,15,根据前六个字预测第七个字,
AiLearning/src/py3.x/tensorflow2.x/config.py,20,每个文本或者句子的截断长度，只保留1000个单词,
AiLearning/src/py3.x/tensorflow2.x/config.py,21,词向量维度,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,1,*-* coding:utf-8 *-*,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,2,词向量:,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,3,https://www.cnblogs.com/Darwin2000/p/5786984.html,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,4,数据集:,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,5,https://blog.csdn.net/alip39/article/details/95891321,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,6,参考代码:,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,7,https://blog.csdn.net/u012052268/article/details/90238282,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,8,Attention:,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,9,https://github.com/philipperemy/keras-attention-mechanism,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,31,存储模型: 持久化,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,43,训练自己的词向量，并保存。,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,45,读取分词后的 文本,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,46,训练模型,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,51,导入 预训练的词向量,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,57,训练词向量(用空格隔开的文本),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,60,"trainWord2Vec(infile, outfile)",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,61,加载词向量,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,67,2 构造包含所有词语的 list，以及初始化 “词语-序号”字典 和 “词向量”矩阵,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,68,存储 所有的 词语,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,70,初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,71,初始化`[word : vector]`字典,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,73,初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,74,行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如60。,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,77,3 填充 上述 的字典 和 大矩阵,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,79,print(i),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,80,每个词语,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,81,词语：序号,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,82,词语：词向量,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,83,词向量矩阵,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,96,"“bo”代表 ""蓝点""",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,98,b代表“蓝色实线”,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,105,plt.show(),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,107,清除数字,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,116,plt.show(),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,130,如果模型文件存在则直接加载模型，否则开始训练,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,138,4 在 keras的Embedding层中使用 预训练词向量,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,140,字典长度,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,141,词向量 长度（60）,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,142,重点：预训练的词向量系数,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,143,每句话的 最大长度（必须padding）,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,144,是否在 训练的过程中 更新词向量,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,146,如果不加载外界的，可以自己训练,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,147,可以看出在使用 Keras的中Embedding层时候，不指定参数 weights=[embeddings_matrix] 即可自动生成词向量。,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,148,embedding_layer = Embedding(,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,149,"input_dim = len(word_index) + 1, # 由于 没有预训练，设置+1",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,150,"output_dim = EMBEDDING_DIM, # 设置词向量的维度",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,151,input_length=MAX_SEQUENCE_LENGTH,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,152,) #设置句子的最大长度,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,154,返回一个张量，长度为1000，也就是模型的输入为batch_size*1000,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,155,返回batch_size*1000*100,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,156,添加 注意力(本质上是通过加入  一个随机向量 作为 权重 来优化 输入的值 - 与全链接不同的是，这个还会作为输入项 和 输入做点乘 ),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,162,x = BatchNormalization()(x),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,165,设置优化器,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,194,"将文本 ['1, 2, 3', '1, 2, .., n'] 分解为: [[1, 2, 3], [1, 2, .., n]]",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,203,"按照大小和顺序，生成 label(0,1,2...自然数类型)",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,230,画相关的 loss 和 accuracy=(预测正确-正or负/总预测的),
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,234,"self.model.fit(x_train, y_train, batch_size=60, epochs=40)",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,240,测试加载外界word2vec词向量,
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,241,"vocab_list, word_index, embeddings_matrix = load_embeding()",
AiLearning/src/py3.x/tensorflow2.x/text_Emotion.py,248,首次启动加载jieba词库,
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,1,-*- coding: utf-8 -*-,
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,16,使用 seaborn 绘制矩阵图 (pairplot),
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,19,Commented out IPython magic to ensure Python compatibility.,
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,29,%tensorflow_version only exists in Colab.,
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,30,%tensorflow_version 2.x,
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,175,通过为每个完成的时期打印一个点来显示训练进度,
AiLearning/src/py3.x/tensorflow2.x/text_regression.py,230,patience 值用来检查改进 epochs 的数量,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,2,coding: utf-8,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,3,# 探索过拟合和欠拟合,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,5,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,6,与往常一样，此示例中的代码将使用 `tf.keras` API，您可以在TensorFlow [Keras 指南](https://www.tensorflow.org/guide/keras)中了解更多信息。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,7,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,8,在前面的两个示例（对电影评论进行分类和预测燃油效率）中，我们看到了在验证数据上的模型的准确性在经过多个时期的训练后将达到峰值，然后开始下降。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,9,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,10,换句话说，我们的模型将 *过拟合* 训练数据。学习如何应对过拟合很重要。尽管通常可以在*训练集*上达到高精度，但我们真正想要的是开发能够很好地推广到*测试集*（或之前未见的数据）的模型。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,11,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,12,过拟合的反面是*欠拟合*。当测试数据仍有改进空间时，就会发生欠拟合。发生这种情况的原因有很多：如果模型不够强大，模型过于规范化，或者仅仅是没有经过足够长时间的训练。这意味着网络尚未学习训练数据中的相关模式。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,13,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,14,但是，如果训练时间过长，则模型将开始过拟合并从训练数据中学习无法推广到测试数据的模式。我们需要保持平衡。如下所述，了解如何训练适当的时期是一项有用的技能。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,15,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,16,为了防止过拟合，最好的解决方案是使用更多的训练数据。经过更多数据训练的模型自然会更好地推广。当这不再可能时，下一个最佳解决方案是使用正则化之类的技术。这些因素限制了模型可以存储的信息的数量和类型。如果一个网络只能存储少量模式，那么优化过程将迫使它专注于最突出的模式，这些模式有更好的概括机会。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,17,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,18,在本笔记本中，我们将探讨两种常见的正则化技术（权重正则化和 dropout），并使用它们来改进我们的IMDB电影评论分类笔记本。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,21,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,27,%tensorflow_version only exists in Colab.,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,40,## 下载IMDB数据集,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,41,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,42,而不是像以前的笔记本中那样使用embedding，这里我们将对句子进行 multi-hot 编码。 该模型将很快适合训练集。 它将用于演示何时发生过拟合以及如何应对。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,43,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,44,"对列表进行 multi-hot 编码意味着将它们变成0和1的向量。 具体来说，这意味着例如将序列 `[3, 5]` 变成10,000维向量，该向量除了索引3和5将是1，其他将是全为零。",
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,46,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,54,"Create an all-zero matrix of shape (len(sequences), dimension)",
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,57,set specific indices of results[i] to 1s,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,65,让我们看一下产生的 multi-hot 向量之一。 单词索引按频率排序，因此可以预期在索引零附近有更多的1值，如我们在该图中所看到的：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,66,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,67,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,68,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,69,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,71,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,77,## 证明过拟合,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,78,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,79,防止过拟合的最简单方法是减小模型的大小，即减小模型中可学习的参数的数量（由层数和每层单元数确定）。在深度学习中，模型中可学习参数的数量通常称为模型的“容量”。直观地讲，具有更多参数的模型将具有更多的“记忆能力”，因此将能够轻松学习训练样本与其目标之间的完美的字典式映射，这种映射没有任何泛化能力，但是在进行预测时这将是无用的根据以前看不见的数据。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,80,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,81,始终牢记这一点：深度学习模型往往擅长拟合训练数据，但真正的挑战是泛化而不是拟合。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,82,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,83,另一方面，如果网络的存储资源有限，则将无法轻松地学习映射。为了最大程度地减少损失，它必须学习具有更强预测能力的压缩表示形式。同时，如果您使模型过小，将难以拟合训练数据。 “容量过多”和“容量不足”之间存在平衡。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,84,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,85,不幸的是，没有神奇的公式来确定模型的正确大小或体系结构（根据层数或每层的正确大小）。您将不得不尝试使用一系列不同的体系结构。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,86,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,87,为了找到合适的模型大小，最好从相对较少的图层和参数开始，然后开始增加图层的大小或添加新的图层，直到看到验证损失的收益递减为止。让我们在电影评论分类网络上尝试一下。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,88,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,89,我们将仅使用 `Dense` 层作为基准来创建一个简单的模型，然后创建较小和较大的版本并进行比较。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,90,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,91,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,93,### Create a baseline model,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,95,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,99,`input_shape` is only required here so that `.summary` works.,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,112,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,123,### 创建 smaller model,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,124,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,125,让我们创建一个隐藏单元更少的模型，以与我们刚刚创建的基线模型进行比较：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,127,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,143,并使用相同的数据训练模型：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,145,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,156,### 创建 bigger model,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,157,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,158,作为练习，您可以创建一个更大的模型，并查看它开始过拟合的速度。 接下来，让我们将具有更大容量的网络添加到此基准网络中，远远超出问题所能保证的范围：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,160,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,176,再次，使用相同的数据训练模型：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,178,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,188,### 绘制训练和验证损失,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,189,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,190,<!--TODO(markdaoust): This should be a one-liner with tensorboard -->,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,191,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,192,实线表示训练损失，而虚线表示验证损失（请记住：验证损失越小表示模型越好）。 在这里，较小的网络比基准模型开始过度拟合（在6个时期而不是4个周期之后），并且一旦开始过度拟合，其性能下降的速度就会慢得多。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,193,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,195,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,219,请注意，较大的网络仅在一个时期后就开始过拟合，而且过拟合严重。网络的容量越多，将能够更快地对训练数据进行建模（导致较低的训练损失），但网络越容易过拟合（导致训练和验证损失之间存在较大差异）。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,221,## 防止过度拟合的策略,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,223,### 添加权重正则化,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,225,您可能熟悉Occam的Razor原理：给某事两种解释，最可能正确的解释是“最简单”的解释，即假设最少的一种。这也适用于通过神经网络学习的模型：给定一些训练数据和网络体系结构，可以使用多组权重值（多个模型）来解释数据，并且较简单的模型比复杂的模型不太可能过拟合。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,226,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,227,在这种情况下，“简单模型”是参数值的分布具有较小熵的模型（或如上节所述，具有总共较少参数的模型）。因此，减轻过拟合的一种通用方法是通过仅将网络的权重强制取小的值来对网络的复杂性施加约束，这使得权重值的分布更加“规则”。这称为“权重调整”，它是通过向网络的损失函数中添加与权重较大相关的成本来完成的。以下有两种形式：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,228,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,229,* [L1正则化](https://developers.google.com/machine-learning/glossary/#L1_regularization)，其中增加的成本与权重系数的绝对值成正比（即所谓的“ L1规范” ”）。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,230,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,231,* [L2正则化](https://developers.google.com/machine-learning/glossary/#L2_regularization)，其中增加的成本与权重系数的值的平方成正比（即与平方的平方成正比）权重的“ L2规范”。 L2正则化在神经网络中也称为权重衰减。不要让其他名称使您感到困惑：权重衰减在数学上与L2正则化完全相同。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,232,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,233,L1正则化引入稀疏性，以使您的某些权重参数为零。 L2正则化将惩罚权重参数而不使其稀疏，这是L2更为常见的原因之一。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,234,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,235,在 `tf.keras` 中，通过将权重正则化器实例作为关键字参数传递给图层来添加权重正则化。让我们现在添加L2权重正则化。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,237,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,259,`l2(0.001)` 表示该层权重矩阵中的每个系数将为网络的总损耗增加 `0.001 * weight_coefficient_value**2`。 请注意，由于此惩罚仅在训练时增加，因此在训练时此网络的损失将比在测试时高得多。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,260,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,261,这是我们的L2正则化惩罚的影响：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,262,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,264,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,271,如您所见，即使两个模型具有相同数量的参数，L2正则化模型也比基线模型具有更高的抗过度拟合能力。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,273,### 添加 dropout,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,274,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,275,"dropout 是 Hinton 和他在多伦多大学的学生开发的最有效，最常用的神经网络正则化技术之一。应用于图层的辍学包括在训练过程中随机“dropping out”（即设置为零）该图层的许多输出特征。假设在训练过程中，给定的图层通常会为给定的输入样本返回向量  [0.2, 0.5, 1.3, 0.8, 1.1]；应用删除后，此向量将有一些零个条目随机分布，例如 [0, 0.5, 1.3, 0, 1.1]。 “dropout 率”是被清零的特征的一部分。通常设置在0.2到0.5之间。在测试时，不会丢失任何单元，而是将图层的输出值按等于丢失率的比例缩小，以平衡一个活跃的单元（而不是训练时）的事实。",
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,276,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,277,在tf.keras中，您可以通过Dropout层在网络中引入Dropout，该层将立即应用于该层的输出。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,278,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,279,让我们在IMDB网络中添加两个Dropout层，看看它们在减少过拟合方面的表现如何：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,281,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,303,In[ ]:,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,310,添加 dropout 是对基线模型的明显改进。,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,311,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,312,回顾一下：以下是防止神经网络过拟合的最常用方法：,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,313,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,314,* 获取更多训练数据,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,315,* 减少网络容量,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,316,* 添加权重调整,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,317,* 添加 dropout,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,318,,
AiLearning/src/py3.x/tensorflow2.x/overfit_and_underfit.py,319,本指南未涵盖的两个重要方法是数据增强和批处理规范化。,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/train.py,5,train model,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/process_data.py,16,save initial config data,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/process_data.py,26,"in windows the new line is '\r\n\r\n' the space is '\r\n' . so if you use windows system,",
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/process_data.py,27,you have to use recorsponding instructions,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/process_data.py,46,set to <unk> (index 1) if not in vocab,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/process_data.py,50,left padding,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/process_data.py,65,left padding,
AiLearning/src/py3.x/tensorflow2.x/zh-NER-keras-master/bilsm_crf_model.py,18,Random embedding,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,13,逻辑回归中的 L1 惩罚和稀缺性 L1 Penalty and Sparsity in Logistic Regression,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,77,具有 L1-逻辑回归的路径,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,121,绘制多项式和一对二的逻辑回归 Plot multinomial and One-vs-Rest Logistic Regression,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,183,Logistic Regression 3-class Classifier 逻辑回归 3-类 分类器,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,191,引入一些数据来玩,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,193,我们只采用样本数据的前两个feature,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,197,网格中的步长,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,201,我们创建了一个 Neighbours Classifier 的实例，并拟合数据。,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,204,"绘制决策边界。为此我们将为网格 [x_min, x_max]x[y_min, y_max] 中的每个点分配一个颜色。",
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,210,将结果放入彩色图中,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,215,将训练点也同样放入彩色图中,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,227,Logistic function 逻辑回归函数,
AiLearning/src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,228,这个类似于咱们之前讲解 logistic 回归的 Sigmoid 函数，模拟的阶跃函数,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,15,------使用 Logistic 回归在简单数据集上的分类-----------,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,29,为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,36,这里其实非常有必要解释一下，会出现的错误 RuntimeWarning: overflow encountered in exp,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,37,这个错误在学习阶段虽然可以忽略，但是我们至少应该知道为什么,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,38,这里是因为我们输入的有的 x 实在是太小了，比如 -6000之类的，那么计算一个数字 np.exp(6000)这个结果太大了，没法表示，所以就溢出了,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,39,如果是计算 np.exp（-6000），这样虽然也会溢出，但是这是下溢，就是表示成零,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,40,去网上搜了很多方法，比如 使用bigfloat这个库（我竟然没有安装成功，就不尝试了，反正应该是有用的,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,52,"注意一下，我把原来 data_mat_in 改成data_arr,因为传进来的是一个数组，用这个比较不容易搞混",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,53,turn the data_arr to numpy matrix,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,55,变成矩阵之后进行转置,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,57,m->数据量，样本数 n->特征数,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,59,学习率，learning rate,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,61,最大迭代次数，假装迭代这么多次就能收敛2333,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,63,"生成一个长度和特征数相同的矩阵，此处n为3 -> [[1],[1],[1]]",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,64,"weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,67,这里是点乘  m x 3 dot 3 x 1,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,70,这里比较建议看一下推导，为什么这么做可以，这里已经是求导之后的,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,127,"sum(data_mat[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,128,此处求出的 h 是一个具体的数值，而不是一个矩阵,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,131,还是和上面一样，这个先去看推导，再写程序,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,147,这里必须要用list，不然后面的del没法使用,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,150,i和j的不断增大，导致alpha的值不断减少，但是不为0,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,152,随机产生一个 0～len()之间的一个值,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,153,"random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,168,"注意，这里的grad_ascent返回的是一个 matrix, 所以要使用getA方法变成ndarray类型",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,169,"weights = grad_ascent(data_arr, class_labels).getA()",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,170,"weights = stoc_grad_ascent0(np.array(data_arr), class_labels)",
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,175,-------从疝气病症预测病马的死亡率------,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,185,print(np.sum(in_x * weights)),
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,201,解析训练数据集中的数据特征和Labels,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,202,trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,206,这里如果就一个空的元素，则跳过本次循环,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,210,使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,214,读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,219,这里如果就一个空的元素，则跳过本次循环,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,241,请依次运行下面三个函数做代码测试,
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,243,colic_test(),
AiLearning/src/py3.x/ml/5.Logistic/logistic.py,244,multi_test(),
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于物品.py,6,calculate co-rated users between items,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于物品.py,17,calculate finial similarity matrix W,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于物品.py,26,calculate co-rated users between items,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于物品.py,37,calculate finial similarity matrix W,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-item.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-item.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,18,加载数据集,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,33,创建用户产品矩阵，针对测试数据和训练数据，创建两个矩阵：,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,41,使用sklearn的pairwise_distances函数来计算余弦相似性。,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,42,行：人，列：电影,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,43,行：电影，列：人,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,50,统计在所有的用户中，不同电影的总出现次数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,54,"print ""pop="", i_index, self.item_popular[i_index]",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,56,save the total number of items,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,68,求出每一个用户，所有电影的综合评分（axis=0 表示对列操作， 1表示对行操作）,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,69,"print ""rating="", np.shape(rating)",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,71,np.newaxis参考地址: http://blog.csdn.net/xtingjie/article/details/72510834,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,72,"print ""mean_user_rating="", np.shape(mean_user_rating)",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,73,"print ""mean_user_rating.newaxis="", np.shape(mean_user_rating[:, np.newaxis])",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,75,"print ""rating="", rating[:3, :3]",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,76,"print ""mean_user_rating[:, np.newaxis]="", mean_user_rating[:, np.newaxis][:3, :3]",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,77,"print ""rating_diff="", rating_diff[:3, :3]",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,79,"均分  +  人-人-距离(943, 943)*人-电影-评分diff(943, 1682)=结果-人-电影（每个人对同一电影的综合得分）(943, 1682)  再除以  个人与其他人总的距离 = 人-电影综合得分",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,83,"综合打分： 人-电影-评分(943, 1682)*电影-电影-距离(1682, 1682)=结果-人-电影(各个电影对同一电影的综合得分)(943, 1682)  ／  再除以  电影与其他电影总的距离 = 人-电影综合得分",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,109,"对比测试集和推荐集的差异 item, w",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,115,计算用户对应的电影出现次数log值的sum加和,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,144,基于内存的协同过滤,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,145,...,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,146,拆分数据集,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,147,http://files.grouplens.org/datasets/movielens/ml-100k.zip,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,152,计算相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,159,评估：均方根误差,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,165,基于模型的协同过滤,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,166,...,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,167,计算MovieLens数据集的稀疏度 （n_users，n_items 是常量，所以，用户行为数据越少，意味着信息量少；越稀疏，优化的空间也越大）,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,171,计算稀疏矩阵的最大k个奇异值/向量,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-sklearn-rating.py,189,推荐结果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-user.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-user.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_graph-based.py,8,"j, wij",
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_evaluation_model.py,16,准确率,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_evaluation_model.py,30,召回率,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_evaluation_model.py,44,覆盖率,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_evaluation_model.py,57,新颖度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo.py,11,设有2个隐主题,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,22,作用：使得随机数据可预测,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,30,拆分数据集,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,34,总用户数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,38,n_sim_user: top 20个用户， n_rec_item: top 10个推荐结果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,42,item_mat_similarity: 电影之间的相似度， item_popular: 电影的出现次数， item_count: 总电影数量,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,51,加载数据集,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,61,拆分数据集： 用户+电影,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,69,创建用户产品矩阵，针对测试数据和训练数据，创建两个矩阵：,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,76,"print ""line"", line.user_id-1, line.item_id-1, line.rating",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,80,使用sklearn的pairwise_distances函数来计算余弦相似性。,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,81,行：电影，列：人,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,82,"电影-电影-距离(1682, 1682)",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,90,统计在所有的用户中，不同电影的总出现次数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,95,"print ""pop="", i_index, self.item_popular[i_index]",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,97,save the total number of items,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,101,@profile,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,115,"print ""i_items="", i_items",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,118,计算top K 电影的相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,119,"rating=电影评分, w=不同电影出现的次数",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,120,耗时分析：98.2%的时间在 line-154行,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,135,return the N best items,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,142,varables for precision and recall,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,143,hit表示命中(测试集和推荐集相同+1)，rec_count 每个用户的推荐数， test_count 每个用户对应的测试数据集的电影数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,147,varables for coverage,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,149,varables for popularity,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,152,enumerate 将其组成一个索引序列，利用它可以同时获得索引和值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,153,参考地址：http://blog.csdn.net/churximi/article/details/51648388,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,159,对比测试集和推荐集的差异,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,162,"item, w",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,164,"print 'test_mat[u_index, item]=', item, self.test_mat[u_index, item]",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,170,计算用户对应的电影出现次数log值的sum加和,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,176,"print ""test_count="", np.sum(self.test_mat[u_index, :] != 0), np.sum(self.train_mat[u_index, :] != 0)",
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,191,创建ItemCF对象,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,193,将数据按照 7:3的比例，拆分成：训练集和测试集，存储在usercf的trainset和testset中,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,195,计算用户之间的相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,197,评估推荐效果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,198,itemcf.evaluate(),
AiLearning/src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-cf-item-test.py,199,查看推荐结果用户,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_lfm.py,4,负样本采样过程,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,17,作用：使得随机数据可预测,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,28,n_sim_user: top 20个用户， n_rec_movie: top 10个推荐结果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,32,user_sim_mat: 用户之间的相似度， movie_popular: 电影的出现次数， movie_count: 总电影数量,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,68,用户ID，电影名称，评分，时间戳timestamp,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,69,"user, movie, rating, timestamp = line.split('::')",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,71,通过pivot和随机函数比较，然后初始化用户和对应的值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,74,"dict.setdefault(key, default=None)",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,75,key -- 查找的键值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,76,default -- 键不存在时，设置的默认键值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,92,build inverse table for item-users,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,93,"key=movieID, value=list of userIDs who have seen this movie",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,97,同一个电影中，收集用户的集合,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,98,统计在所有的用户中，不同电影的总出现次数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,101,inverse table for item-users,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,105,count item popularity at the same time,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,112,"save the total movie number, which will be used in evaluation",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,117,统计在相同电影时，不同用户同时出现的次数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,130,calculate similarity matrix,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,136,余弦相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,140,打印进度条,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,147,@profile,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,162,计算top K 用户的相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,163,"v=similar user, wuv=不同用户同时出现的次数，根据wuv倒序从大到小选出K个用户进行排列",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,164,耗时分析：50.4%的时间在 line-160行,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,171,"predict the user's ""interest"" for each movie",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,174,return the N best movies,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,188,返回top N的推荐结果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,190,varables for precision and recall,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,191,hit表示命中(测试集和推荐集相同+1)，rec_count 每个用户的推荐数， test_count 每个用户对应的测试数据集的电影数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,195,varables for coverage,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,197,varables for popularity,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,200,enumerate将其组成一个索引序列，利用它可以同时获得索引和值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,201,参考地址：http://blog.csdn.net/churximi/article/details/51648388,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,208,"对比测试集和推荐集的差异 movie, w",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,213,计算用户对应的电影出现次数log值的sum加和,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,228,ratingfile = 'data/16.RecommenderSystems/ml-1m/ratings.dat',
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,231,创建UserCF对象,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,233,将数据按照 7:3的比例，拆分成：训练集和测试集，存储在usercf的trainset和testset中,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,235,计算用户之间的相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-usercf.py,237,评估推荐效果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,2,coding:utf8,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,17,作用：使得随机数据可预测,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,28,n_sim_user: top 20个用户， n_rec_movie: top 10个推荐结果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,32,user_sim_mat: 电影之间的相似度， movie_popular: 电影的出现次数， movie_count: 总电影数量,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,68,用户ID，电影名称，评分，时间戳,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,69,"user, movie, rating, _ = line.split('::')",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,71,通过pivot和随机函数比较，然后初始化用户和对应的值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,74,"dict.setdefault(key, default=None)",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,75,key -- 查找的键值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,76,default -- 键不存在时，设置的默认键值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,94,"统计在所有的用户中，不同电影的总出现次数， user, movies",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,97,count item popularity,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,104,save the total number of movies,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,108,统计在相同用户时，不同电影同时出现的次数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,111,"user, movies",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,122,calculate similarity matrix,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,128,余弦相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,132,打印进度条,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,139,@profile,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,154,计算top K 电影的相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,155,"rating=电影评分, w=不同电影出现的次数",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,156,耗时分析：98.2%的时间在 line-154行,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,166,return the N best movies,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,173,返回top N的推荐结果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,175,varables for precision and recall,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,176,hit表示命中(测试集和推荐集相同+1)，rec_count 每个用户的推荐数， test_count 每个用户对应的测试数据集的电影数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,180,varables for coverage,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,182,varables for popularity,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,185,enumerate将其组成一个索引序列，利用它可以同时获得索引和值,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,186,参考地址：http://blog.csdn.net/churximi/article/details/51648388,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,193,"对比测试集和推荐集的差异 movie, w",
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,198,计算用户对应的电影出现次数log值的sum加和,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,213,ratingfile = 'data/16.RecommenderSystems/ml-1m/ratings.dat',
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,216,创建ItemCF对象,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,218,将数据按照 7:3的比例，拆分成：训练集和测试集，存储在usercf的trainset和testset中,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,220,计算用户之间的相似度,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,222,评估推荐效果,
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,223,itemcf.evaluate(),
AiLearning/src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py,224,查看推荐结果用户,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,16,build inverse table for item_users,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,24,calculate co-rated items between users,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,35,calculate finial similarity matrix W,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,44,build inverse table for item_users,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,52,calculate co-rated items between users,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,63,calculate finial similarity matrix W,
AiLearning/src/py3.x/ml/16.RecommenderSystems/test_基于用户.py,77,we should filter items user interacted before,
AiLearning/src/py3.x/ml/16.RecommenderSystems/python/Recommender.py,4,自定义杰卡德相似系数函数，仅对0-1矩阵有效,
AiLearning/src/py3.x/ml/16.RecommenderSystems/python/Recommender.py,11,相似度矩阵,
AiLearning/src/py3.x/ml/16.RecommenderSystems/python/Recommender.py,14,计算相似度矩阵的函数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/python/Recommender.py,22,训练函数,
AiLearning/src/py3.x/ml/16.RecommenderSystems/python/Recommender.py,26,推荐函数,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,2,coding: utf-8,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,15,利用SVD提高推荐效果，菜肴矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,30,书上代码给的示例矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,55,# 原矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,56,"return[[1, 1, 1, 0, 0],",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,57,"[2, 2, 2, 0, 0],",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,58,"[1, 1, 1, 0, 0],",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,59,"[5, 5, 5, 0, 0],",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,60,"[1, 1, 0, 2, 2],",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,61,"[0, 0, 0, 3, 3],",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,62,"[0, 0, 0, 1, 1]]",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,64,原矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,71,相似度计算，假定inA和inB 都是列向量,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,72,基于欧氏距离,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,77,pearsSim()函数会检查是否存在3个或更多的点。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,78,"corrcoef直接计算皮尔逊相关系数，范围[-1, 1]，归一化后[0, 1]",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,80,如果不存在，该函数返回1.0，此时两个向量完全相关。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,86,计算余弦相似度，如果夹角为90度，相似度为0；如果两个向量的方向相同，相似度为1.0,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,93,基于物品相似度的推荐引擎,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,104,得到数据集中的物品数目,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,106,初始化两个评分值,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,109,遍历行中的每个物品（对用户评过分的物品进行遍历，并将它与其他物品进行比较）,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,112,如果某个物品的评分值为0，则跳过这个物品,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,115,寻找两个用户都评级的物品,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,116,变量 overLap 给出的是两个物品当中已经被评分的那个元素的索引ID,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,117,logical_and 计算x1和x2元素的真值。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,119,如果相似度为0，则两着没有任何重合元素，终止本次循环,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,122,如果存在重合的物品，则基于这些重合物重新计算相似度。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,125,"print('the %d and %d similarity is : %f'(iten,j,similarity))",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,126,相似度会不断累加，每次计算时还考虑相似度和当前用户评分的乘积,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,127,similarity  用户相似度，   userRating 用户评分,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,132,通过除以所有的评分总和，对上述相似度评分的乘积进行归一化，使得最后评分在0~5之间，这些评分用来对预测值进行排序,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,137,基于SVD的评分估计,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,138,在recommend() 中，这个函数用于替换对standEst()的调用，该函数对给定用户给定物品构建了一个评分估计值,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,149,物品数目,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,151,对数据集进行SVD分解,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,154,奇异值分解,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,155,在SVD分解之后，我们只利用包含了90%能量值的奇异值，这些奇异值会以NumPy数组的形式得以保存,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,158,# 分析 Sigma 的长度取值,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,159,"analyse_data(Sigma, 20)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,161,如果要进行矩阵运算，就必须要用这些奇异值构建出一个对角矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,164,利用U矩阵将物品转换到低维空间中，构建转换后的物品(物品+4个主要的特征),
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,172,对于给定的用户，for循环在用户对应行的元素上进行遍历,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,173,这和standEst()函数中的for循环的目的一样，只不过这里的相似度计算时在低维空间下进行的。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,178,相似度的计算方法也会作为一个参数传递给该函数,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,180,for 循环中加入了一条print语句，以便了解相似度计算的进展情况。如果觉得累赘，可以去掉,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,182,对相似度不断累加求和,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,184,对相似度及对应评分值的乘积求和,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,189,计算估计评分,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,193,recommend()函数，就是推荐引擎，它默认调用standEst()函数，产生了最高的N个推荐结果。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,194,如果不指定N的大小，则默认值为3。该函数另外的参数还包括相似度计算方法和估计方法,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,205,寻找未评级的物品,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,206,对给定的用户建立一个未评分的物品列表,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,208,如果不存在未评分物品，那么就退出函数,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,211,物品的编号和评分值,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,213,在未评分物品上进行循环,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,215,获取 item 该物品的评分,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,218,按照评分得分 进行逆排序，获取前N个未评级物品进行推荐,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,228,总方差的集合（总能量值）,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,240,图像压缩函数,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,241,加载并转换数据,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,244,打开文本文件，并从文件以数组方式读入字符,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,250,矩阵调入后，就可以在屏幕上输出该矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,255,打印矩阵,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,257,由于矩阵保护了浮点数，因此定义浅色和深色，遍历所有矩阵元素，当元素大于阀值时打印1，否则打印0,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,267,实现图像压缩，允许基于任意给定的奇异值数目来重构图像,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,274,构建一个列表,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,278,对原始图像进行SVD分解并重构图像e,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,281,通过Sigma 重新构成SigRecom来实现,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,282,Sigma是一个对角矩阵，因此需要建立一个全0矩阵，然后将前面的那些奇异值填充到对角线上。,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,284,"SigRecon = mat(zeros((numSV, numSV)))",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,285,for k in range(numSV):,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,286,"SigRecon[k, k] = Sigma[k]",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,288,分析插入的 Sigma 长度,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,299,# 对矩阵进行SVD分解(用python实现SVD),
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,300,Data = loadExData(),
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,301,"print('Data:', Data)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,302,"U, Sigma, VT = linalg.svd(Data)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,303,# 打印Sigma的结果，因为前3个数值比其他的值大了很多，为9.72140007e+00，5.29397912e+00，6.84226362e-01,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,304,# 后两个值比较小，每台机器输出结果可能有不同可以将这两个值去掉,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,305,"print('U:', U)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,306,"print('Sigma', Sigma)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,307,"print('VT:', VT)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,308,"print('VT:', VT.T)",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,310,# 重构一个3x3的矩阵Sig3,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,311,"Sig3 = mat([[Sigma[0], 0, 0], [0, Sigma[1], 0], [0, 0, Sigma[2]]])",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,312,"print(U[:, :3] * Sig3 * VT[:3, :])",
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,328,计算相似度的方法,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,330,print(myMat),
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,331,计算相似度的第一种方式,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,333,计算相似度的第二种方式,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,336,默认推荐（菜馆菜肴推荐示例）,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,351,压缩图片,
AiLearning/src/py3.x/ml/14.SVD/svdRecommend.py,352,imgCompress(2),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,2,coding:utf8,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,58,"data_mat[:, dimen] 表示数据集中第dimen列的所有值",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,59,thresh_ineq == 'lt'表示修改左边的值，gt表示修改右边的值,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,60,（这里其实我建议理解为转换左右边，就是一棵树的左右孩子，可能有点问题。。。待考证）,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,85,无穷大,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,97,这里是矩阵乘法,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,106,"print('split: dim {}, thresh {}, thresh inequal: {}, the weighted err is {}'.format(",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,107,"i, thresh_val, inequal, weighted_err",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,108,)),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,115,best_stump 表示分类器的结果，在第几个列上，用大于／小于比较，阈值是多少 (单个弱分类器),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,130,初始化 D，设置每个特征的权重值，平均分为m份,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,134,得到决策树的模型,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,136,print('D: {}'.format(D.T)),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,137,alpha 目的主要是计算每一个分类器实例的权重(加和就是分类结果),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,138,计算每个分类器的 alpha 权重值,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,141,store Stump Params in Array,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,143,print('class_est: {}'.format(class_est.T)),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,144,分类正确：乘积为1，不会影响结果，-1主要是下面求e的-alpha次方,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,145,分类错误：乘积为 -1，结果会受影响，所以也乘以 -1,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,147,判断正确的，就乘以-1，否则就乘以1， 为什么？ 书上的公式。,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,148,"print('(-1取反)预测值 expon=', expon.T)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,149,计算e的expon次方，然后计算得到一个综合的概率的值,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,150,结果发现： 判断错误的样本，D对于的样本权重值会变大。,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,151,multiply是对应项相乘,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,154,预测的分类结果值，在上一轮结果的基础上，进行加和操作,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,155,print('叠加前的分类结果class_est: {}'.format(class_est.T)),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,157,print('叠加后的分类结果agg_class_est: {}'.format(agg_class_est.T)),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,158,sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,159,"结果为：错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,163,print('total error: {}\n'.format(error_rate)),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,198,variable to calculate AUC,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,200,对正样本的进行求和,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,202,正样本的概率,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,204,负样本的概率,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,206,np.argsort函数返回的是数组值从小到大的索引值,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,207,"get sorted index, it's reverse",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,209,测试结果是否是从小到大排列,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,210,可以选择打印看一下,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,211,开始创建模版对象,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,215,cursor光标值,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,217,"loop through all the values, drawing a line segment at each point",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,226,"draw line from cur to (cur[0]-delX, cur[1]-delY)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,227,"画点连线 (x1, x2, y1, y2)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,228,"print cur[0], cur[0]-delX, cur[1], cur[1]-delY",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,231,画对角的虚线线,
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,236,"设置画图的范围区间 (x1, x2, y1, y2)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,249,"D = np.mat(np.ones((5, 1)) / 5)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,250,"data_mat, class_labels = load_sim_data()",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,251,print(data_mat.shape),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,252,"result = build_stump(data_mat, class_labels, D)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,253,print(result),
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,254,"classifier_array, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 9)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,255,"print(classifier_array, agg_class_est)",
AiLearning/src/py3.x/ml/7.AdaBoost/adaboost.py,265,测试：计算总样本数，错误样本数，错误率,
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,7,"D = np.mat(np.ones((5, 1)) / 5)",
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,8,"data_mat, class_labels = load_sim_data()",
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,9,print(data_mat.shape),
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,10,"result = build_stump(data_mat, class_labels, D)",
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,11,print(result),
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,12,"classifier_array, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 9)",
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,13,"print(classifier_array, agg_class_est)",
AiLearning/src/py3.x/ml/7.AdaBoost/roc_test.py,26,测试：计算总样本数，错误样本数，错误率,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,2,coding:utf8,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,12,importing necessary libraries,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,21,Create the dataset,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,25,"dataArr, labelArr = loadDataSet(""data/7.AdaBoost/horseColicTraining2.txt"")",
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,28,Fit regression model,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,35,Predict,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,39,Plot the results,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,54,适合2分类,
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,60,"print(""-"" * 100)",
AiLearning/src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,61,"print(metrics.roc_auc_score(y[:1], y_2[:1]))",
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,19,参数,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,24,加载数据,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,28,我们只用两个相应的features,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,32,训练,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,35,绘制决策边界,
AiLearning/src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,51,绘制训练点,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,3,原始链接： http://blog.csdn.net/lsldd/article/details/41223147,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,4,GitHub: https://github.com/apachecn/AiLearning,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,18,特征： 身高 体重   label： 胖瘦,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,22,特征数据,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,24,label分类的标签数据,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,26,预估结果的标签数据,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,41,print(clf),
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,48,print(x_train),
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,61,计算全量的预估结果,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,72,target_names 以 y的label分类为准,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,88,"with open(""testResult/tree.dot"", 'w') as f:",
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,89,from sklearn.externals.six import StringIO,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,90,"tree.export_graphviz(clf, out_file=f)",
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,99,from IPython.display import Image,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,100,Image(graph.create_png()),
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,110,得到训练的预测结果集,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,113,展现 准确率与召回率,
AiLearning/src/py3.x/ml/3.DecisionTree/DTSklearn.py,116,可视化输出,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,12,"定义文本框 和 箭头格式 【 sawtooth 波浪方框, round4 矩形方框 , fc表示字体颜色的深浅 0.1~0.9 依次变浅，没错是变浅】",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,22,根节点开始遍历,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,24,"判断子节点是否为dict, 不是+1",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,36,根节点开始遍历,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,38,"判断子节点是不是dict, 求分枝的深度",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,39,----------写法1 start ---------------,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,44,----------写法1 end ---------------,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,46,----------写法2 start --------------,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,47,thisDepth = 1 + getTreeDepth(secondDict[key]) if type(secondDict[key]) is dict else 1,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,48,----------写法2 end --------------,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,49,记录最大的分支深度,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,65,获取叶子节点的数量,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,67,获取树的深度,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,68,depth = getTreeDepth(myTree),
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,70,找出第1个中心点的位置，然后与 parentPt定点进行划线,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,72,print(cntrPt),
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,73,并打印输入对应的文字,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,77,可视化Node分支点,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,79,根节点的值,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,81,y值 = 最高点-层数的高度[第二个节点位置],
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,84,判断该节点是否是Node节点,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,86,如果是就递归调用[recursion],
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,89,如果不是，就在原来节点一半的地方找到节点的坐标,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,91,可视化该节点位置,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,93,并打印输入对应的文字,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,99,创建一个figure的模版,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,104,表示创建一个1行，1列的图，createPlot.ax1 为第 1 个子图，,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,109,半个节点的长度,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,116,# 测试画图,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,117,def createPlot():,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,118,"fig = plt.figure(1, facecolor='white')",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,119,fig.clf(),
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,120,# ticks for demo puropses,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,121,"createPlot.ax1 = plt.subplot(111, frameon=False)",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,122,"plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,123,"plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)",
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,124,plt.show(),
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,127,测试数据集,
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,136,myTree = retrieveTree(1),
AiLearning/src/py3.x/ml/3.DecisionTree/decisionTreePlot.py,137,createPlot(myTree),
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,14,引入必要的模型和库,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,19,创建一个随机的数据集,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,20,参考 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,22,"print('lalalalala===', rng)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,23,"rand() 是给定形状的随机值，rng.rand(80, 1)即矩阵的形状是 80行，1列",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,24,sort(),
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,26,"print('X=', X)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,28,"print('y=', y)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,30,"print('yyy=', y)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,32,拟合回归模型,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,33,regr_1 = DecisionTreeRegressor(max_depth=2),
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,34,保持 max_depth=5 不变，增加 min_samples_leaf=6 的参数，效果进一步提升了,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,37,regr_3 = DecisionTreeRegressor(max_depth=4),
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,38,"regr_1.fit(X, y)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,40,"regr_3.fit(X, y)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,42,预测,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,44,y_1 = regr_1.predict(X_test),
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,46,y_3 = regr_3.predict(X_test),
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,48,绘制结果,
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,51,"plt.plot(X_test, y_1, color=""cornflowerblue"", label=""max_depth=2"", linewidth=2)",
AiLearning/src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,53,"plt.plot(X_test, y_3, color=""red"", label=""max_depth=3"", linewidth=2)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,27,dataSet 前两列是特征，最后一列对应的是每条数据对应的分类标签,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,33,"dataSet = [['yes'],",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,34,"['yes'],",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,35,"['no'],",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,36,"['no'],",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,37,['no']],
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,38,labels  露出水面   脚蹼，注意：这里的labels是写的 dataSet 中特征的含义，并不是对应的分类标签或者说目标变量,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,40,返回,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,53,-----------计算香农熵的第一种实现方式start--------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,54,求list的长度，表示计算参与训练的数据量,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,56,下面输出我们测试的数据集的一些信息,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,57,例如：<type 'list'> numEntries:  5 是下面的代码的输出,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,58,"print(type(dataSet), 'numEntries: ', numEntries)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,60,计算分类标签label出现的次数,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,62,the the number of unique elements and their occurance,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,64,将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,66,为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,70,"print('-----', featVec, labelCounts)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,72,对于label标签的占比，求出label标签的香农熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,75,使用所有类标签的发生频率计算类别出现的概率。,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,77,log base 2,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,78,计算香农熵，以 2 为底求对数,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,80,"print('---', prob, prob * log(prob, 2), shannonEnt)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,81,-----------计算香农熵的第一种实现方式end--------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,83,# -----------计算香农熵的第二种实现方式start--------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,84,# 统计标签出现的次数,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,85,label_count = Counter(data[-1] for data in dataSet),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,86,# 计算概率,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,87,probs = [p[1] / len(dataSet) for p in label_count.items()],
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,88,# 计算香农熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,89,"shannonEnt = sum([-p * log(p, 2) for p in probs])",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,90,# -----------计算香农熵的第二种实现方式end--------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,107,-----------切分数据集的第一种方式 start------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,110,index列为value的数据集【该数据集需要排除index列】,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,111,判断index列的值是否为value,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,113,chop out index used for splitting,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,114,[:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,135,[index+1:]表示从跳过 index 的 index+1行，取接下来的数据,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,136,收集结果值 index列为value的行【该行需要排除index列】,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,138,-----------切分数据集的第一种方式 end------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,140,# -----------切分数据集的第二种方式 start------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,141,"retDataSet = [data[:index] + data[index + 1:] for data in dataSet for i, v in enumerate(data) if i == index and v == value]",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,142,# -----------切分数据集的第二种方式 end------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,156,-----------选择最优特征的第一种方式 start------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,157,"求第一行有多少列的 Feature, 最后一列是label列嘛",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,159,label的信息熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,161,"最优的信息增益值, 和最优的Featurn编号",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,163,iterate over all the features,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,165,create a list of all the examples of this feature,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,166,获取每一个实例的第i+1个feature，组成list集合,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,168,get a set of unique values,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,169,获取剔重后的集合，使用set对list数据进行去重,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,171,创建一个临时的信息熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,173,遍历某一列的value集合，计算该列的信息熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,174,遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,179,gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,180,信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,187,-----------选择最优特征的第一种方式 end------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,189,# -----------选择最优特征的第二种方式 start------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,190,# 计算初始香农熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,191,base_entropy = calcShannonEnt(dataSet),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,192,best_info_gain = 0,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,193,best_feature = -1,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,194,# 遍历每一个特征,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,195,for i in range(len(dataSet[0]) - 1):,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,196,# 对当前特征进行统计,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,197,feature_count = Counter([data[i] for data in dataSet]),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,198,# 计算分割后的香农熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,199,"new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) \",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,200,for feature in feature_count.items()),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,201,# 更新值,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,202,info_gain = base_entropy - new_entropy,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,203,"print('No. {0} feature info gain is {1:.3f}'.format(i, info_gain))",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,204,if info_gain > best_info_gain:,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,205,best_info_gain = info_gain,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,206,best_feature = i,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,207,return best_feature,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,208,# -----------选择最优特征的第二种方式 end------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,220,-----------majorityCnt的第一种方式 start------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,226,倒叙排列classCount得到一个字典集合，然后取出第一个就是结果（yes/no），即出现次数最多的结果,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,228,"print('sortedClassCount:', sortedClassCount)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,230,-----------majorityCnt的第一种方式 end------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,232,# -----------majorityCnt的第二种方式 start------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,233,major_label = Counter(classList).most_common(1)[0],
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,234,return major_label,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,235,# -----------majorityCnt的第二种方式 end------------------------------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,249,如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,250,第一个停止条件：所有的类标签完全相同，则直接返回该类标签。,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,251,count() 函数是统计括号中的值在list中出现的次数,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,254,如果数据集只有1列，那么最初出现label次数最多的一类，作为结果,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,255,第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,259,选择最优的列，得到最优列对应的label含义,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,261,获取label的名称,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,263,初始化myTree,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,265,注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,266,所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,268,取出最优列，然后它的branch做分类,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,272,求出剩余的标签label,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,274,遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree(),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,276,"print('myTree', value, myTree)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,291,获取tree的根节点对于的key值,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,293,通过key得到根节点对应的value,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,295,判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,297,测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,301,判断分枝是否结束: 判断valueOfFeat是否是dict类型,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,320,-------------- 第一种方法 start --------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,324,-------------- 第一种方法 end --------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,326,-------------- 第二种方法 start --------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,329,-------------- 第二种方法 start --------------,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,355,1.创建数据和结果标签,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,357,"print(myDat, labels)",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,359,计算label分类标签的香农熵,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,360,calcShannonEnt(myDat),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,362,# 求第0列 为 1/0的列的数据集【排除第0列】,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,363,"print('1---', splitDataSet(myDat, 0, 1))",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,364,"print('0---', splitDataSet(myDat, 0, 0))",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,366,# 计算最好的信息增益的列,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,367,print(chooseBestFeatureToSplit(myDat)),
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,372,"[1, 1]表示要取的分支上的节点位置，对应的结果值",
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,375,画图可视化展现,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,389,加载隐形眼镜相关的 文本文件 数据,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,391,解析数据，获得 features 数据,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,393,得到数据的对应的 Labels,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,395,使用上面的创建决策树的代码，构造预测隐形眼镜的决策树,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,398,画图可视化展现,
AiLearning/src/py3.x/ml/3.DecisionTree/DecisionTree.py,403,fishTest(),
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,17,初始化一个空列表,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,19,读取文件,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,21,循环遍历文件所有行,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,23,切割每一行的数据,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,25,"将数据转换为浮点类型,便于后面的计算",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,26,fltLine = [float(x) for x in curLine],
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,27,将数据追加到dataMat,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,28,映射所有的元素为 float（浮点数）类型,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,30,返回dataMat,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,53,获取样本数与特征值,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,55,"初始化质心,创建(k,n)个以零填充的矩阵",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,57,循环遍历特征值,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,59,计算每一列的最小值,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,61,计算每一列的范围值,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,63,"计算每一列的质心,并将值赋给centroids",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,65,返回质心,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,79,获取样本数和特征数,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,81,初始化一个矩阵来存储每个点的簇分配结果,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,82,"clusterAssment包含两个列:一列记录簇索引值,第二列存储误差(误差是指当前点到簇质心的距离,后面会使用该误差来评价聚类的效果)",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,84,"创建质心,随机K个质心",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,86,"初始化标志变量,用于判断迭代是否继续,如果True,则继续迭代",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,90,"遍历所有数据找到距离每个点最近的质心,",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,91,可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,96,计算数据点到质心的距离,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,97,"计算距离是使用distMeas参数给出的距离公式,默认距离函数是distEclud",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,99,"如果距离比minDist(最小距离)还小,更新minDist(最小距离)和最小质心的index(索引)",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,103,"如果任一点的簇分配结果发生改变,则更新clusterChanged标志",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,105,"更新簇分配结果为最小质心的index(索引),minDist(最小距离)的平方",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,107,print(centroids),
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,108,遍历所有质心并更新它们的取值,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,110,通过数据过滤来获得给定簇的所有点,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,112,"计算所有点的均值,axis=0表示沿矩阵的列方向进行均值计算",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,114,返回所有的类质心与点分配结果,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,127,创建一个矩阵来存储数据集中每个点的簇分配结果及平方误差,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,129,"计算整个数据集的质心,并使用一个列表来保留所有的质心",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,132,遍历数据集中所有点来计算每个点到质心的误差值,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,135,"对簇不停的进行划分,直到得到想要的簇数目为止",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,137,"初始化最小SSE为无穷大,用于比较划分前后的SSE",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,139,"通过考察簇列表中的值来获得当前簇的数目,遍历所有的簇来决定最佳的簇进行划分",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,141,"对每一个簇,将该簇中的所有点堪称一个小的数据集",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,143,"将ptsInCurrCluster输入到函数kMeans中进行处理,k=2,",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,144,"kMeans会生成两个质心(簇),同时给出每个簇的误差值",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,146,将误差值与剩余数据集的误差之和作为本次划分的误差,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,150,"如果本次划分的SSE值最小,则本次划分被保存",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,156,找出最好的簇分配结果,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,157,"调用kmeans函数并且指定簇数为2时,会得到两个编号分别为0和1的结果簇",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,159,更新为最佳质心,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,163,更新质心列表,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,164,更新原质心list中的第i个质心为使用二分kMeans后bestNewCents的第一个质心,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,166,添加bestNewCents的第二个质心,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,168,重新分配最好簇下的数据(质心)以及SSE,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,181,"经度和维度用角度作为单位,但是sin()和cos()以弧度为输入.",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,182,可以将江都除以180度然后再诚意圆周率pi转换为弧度,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,197,创建一个空列表,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,199,"打开文本文件获取第4列和第5列,这两列分别对应维度和经度,然后将这些值封装到datList",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,204,调用biKmeans并使用distSLC函数作为聚类中使用的距离计算方式,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,206,"创建一幅图和一个举行,使用该矩形来决定绘制图的哪一部分",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,209,构建一个标记形状的列表用于绘制散点图,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,213,使用imread函数基于一幅图像来创建矩阵,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,215,使用imshow绘制该矩阵,
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,217,"再同一幅图上绘制一张新图,允许使用两套坐标系统并不做任何缩放或偏移",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,219,"遍历每一个簇并将它们一一画出来,标记类型从前面创建的scatterMarkers列表中得到",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,222,"使用索引i % len(scatterMarkers)来选择标记形状,这意味这当有更多簇时,可以循环使用这标记",
AiLearning/src/py3.x/ml/10.kmeans/kMeans.py,224,使用十字标记来表示簇中心并在图中显示,
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,9,dataMat = mat(kMeans.loadDataSet('../../../../data/k-means/testSet.txt')),
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,10,"print('min(dataMat[:, 0])', min(dataMat[:, 0]), '\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,11,"print('min(dataMat[:, 1])', min(dataMat[:, 1]), '\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,12,"print('max(dataMat[:, 0])', max(dataMat[:, 0]), '\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,13,"print('max(dataMat[:, 1])', max(dataMat[:, 1]), '\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,14,"print(kMeans.randCent(dataMat, 2),'\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,15,"print(kMeans.distEclud(dataMat[0],dataMat[1]))",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,16,"centroids, clusterAssment = kMeans.kMeans(dataMat, 4)",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,17,"print('centroids:\n', centroids, '\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,18,"print('clusterAssment:\n',clusterAssment, '\n')",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,19,dataMat3 = mat(kMeans.loadDataSet('../../../../data/k-means/testSet2.txt')),
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,20,"centList, myNewAssments = kMeans.biKmeans(dataMat3, 3)",
AiLearning/src/py3.x/ml/10.kmeans/__init__.py,21,"print('centList: \n', centList, '\n')",
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,1,-*- coding:UTF-8 -*-,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,7,加载数据集,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,9,注意，这个是相对路径，请保证是在 MachineLearning 这个目录下执行。,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,12,映射所有的元素为 float（浮点数）类型,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,15,训练模型,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,16,初始化,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,17,拟合,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,18,预测,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,19,质心,
AiLearning/src/py3.x/ml/10.kmeans/kMeansSklearn.py,21,可视化结果,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,2,coding:utf8,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,18,导入csv文件,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,27,strip()返回移除字符串头尾指定的字符生成的新字符串,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,29,判断是否是数字,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,30,将数据集的第column列转换成float形式,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,33,添加分类标签,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,49,"复制一份 dataset,防止 dataset 的内容改变",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,52,每次循环 fold 清零，防止重复导入 dataset_split,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,53,这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,54,有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,56,将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,57,pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,58,fold.append(dataset_copy.pop(index))  # 无放回的方式,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,59,有放回的方式,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,61,由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,65,Split a dataset based on an attribute and an attribute value # 根据特征和特征值分割数据集,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,76,Calculate the Gini index for a split dataset,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,77,个人理解：计算代价，分类越准确，则 gini 越小,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,79,"class_values = [0, 1]",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,80,"groups = (left, right)",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,85,个人理解：计算代价，分类越准确，则 gini 越小,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,89,"找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,91,"class_values =[0, 1]",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,95,往 features 添加 n_features 个特征（ n_feature 等于特征数的根号），特征索引从 dataset 中随机取,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,98,在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,100,"groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,102,左右两边的数量越一样，说明数据区分度不高，gini系数越大,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,104,"最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,105,print(b_score),
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,109,Create a terminal node value # 输出group中出现次数较多的标签,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,111,max() 函数中，当 key 参数不为空时，就以 key 的函数对象为判断的标准,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,112,输出 group 中出现次数较多的标签,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,115,Create child splits for a node or make terminal  # 创建子分割器，递归分类，直到分类结束,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,116,"max_depth = 10, min_size = 1, n_features = int(sqrt((dataset[0])-1))",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,119,check for a no split,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,123,check for max depth,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,124,max_depth=10 表示递归十次，若分类还未结束，则选取数据中分类标签较多的作为结果，使分类提前结束，防止过拟合,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,127,process left child,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,131,"node['left']是一个字典，形式为{'index':b_index, 'value':b_value, 'groups':b_groups}，所以node是一个多层字典",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,132,递归，depth+1计算递归层数,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,133,process right child,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,141,Build a decision tree,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,154,返回最优列和相关的信息,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,157,对左右2边的数据 进行递归的调用，由于最优特征使用过，所以在后面进行使用的时候，就没有意义了,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,158,例如： 性别-男女，对男使用这一特征就没任何意义了,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,163,Make a prediction with a decision tree,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,164,预测模型分类结果,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,166,isinstance 是 Python 中的一个内建函数。是用来判断一个对象是否是一个已知的类型。,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,177,Make a prediction with a list of bagged trees,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,188,使用多个决策树trees对测试集test的第row行进行预测，再使用简单投票法判断出该行所属分类,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,193,Create a random subsample from the dataset with replacement,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,194,创建数据集的随机子样本,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,205,训练样本的按比例抽样。,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,206,round() 方法返回浮点数x的四舍五入值。,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,209,有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,215,Random Forest Algorithm,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,232,n_trees 表示决策树的数量,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,234,随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,236,创建一个决策树,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,240,每一行的预测结果，bagging 预测最后的分类结果,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,245,Calculate accuracy percentage,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,246,导入实际值和预测值，计算精确度,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,254,评估算法性能，返回模型得分,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,267,将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次 list 的元素是无重复的,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,270,每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,274,"将多个 fold 列表组合成一个 train_set 列表, 类似 union all",
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,288,fold 表示从原始数据集 dataset 提取出来的测试集,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,296,计算随机森林的预测结果的正确率,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,304,加载数据,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,306,print(dataset),
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,308,分成5份数据，进行交叉验证,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,309,调参（自己修改） #决策树深度不能太深，不然容易导致过拟合,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,310,决策树的叶子节点最少的元素数量,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,311,做决策树时候的样本的比例,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,312,n_features = int((len(dataset[0])-1)),
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,313,调参（自己修改） #准确性与多样性之间的权衡,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,314,理论上树是越多越好,
AiLearning/src/py3.x/ml/7.RandomForest/randomForest.py,316,每一次执行本文件时都能产生同一个随机数,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,2,coding: utf8,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,13,加载数据集,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,17,创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,31,遍历所有的元素，如果不在 C1 出现过，那么就 append,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,33,对数组进行 `从小到大` 的排序,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,34,"print 'sort 前=', C1",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,36,frozenset 表示冻结的 set 集合，元素无改变；可以把它当字典的 key 来使用,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,37,"print 'sort 后=', C1",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,38,"print 'frozenset=', map(frozenset, C1)",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,41,计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,54,"ssCnt 临时存放选数据集 Ck 的频率. 例如: a->10, b->5, c->8",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,58,s.issubset(t)  测试是否 s 中的每一个元素都在 t 中,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,64,数据集 D 的数量,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,68,支持度 = 候选项（key）出现的次数 / 所有数据集的数量,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,71,在 retList 的首位插入元素，只存储支持度满足频繁项集的值,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,73,存储所有的候选项（key）和对应的支持度（support）,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,77,输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,97,"print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,98,"print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,101,"第一次 L1,L2 为空，元素直接进行合并，返回元素两两合并的数据集",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,102,if first k-2 elements are equal,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,104,set union,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,105,"print 'union=', Lk[i] | Lk[j], Lk[i], Lk[j]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,109,找出数据集 dataSet 中支持度 >= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,120,C1 即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,122,"print 'C1: ', C1",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,123,对每一行进行 set 转换，然后存放到集合中,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,125,"print 'D=', D",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,126,计算候选数据集 C1 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,128,"print ""L1="", L1, ""\n"", ""outcome: "", supportData",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,130,"L 加了一层 list, L 一共 2 层 list",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,133,"判断 L 的第 k-2 项的数据长度是否 > 0。第一次执行时 L 为 [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]。L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]，最后面 k += 1",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,135,"print 'k=', k, L, L[k-2]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,136,"例如: 以 {0},{1},{2} 为输入且 k = 2 则输出 {0,1}, {0,2}, {1,2}. 以 {0,1},{0,2},{1,2} 为输入且 k = 3 则输出 {0,1,2}",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,137,"print 'Ck', Ck",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,139,计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,140,保存所有候选项集的支持度，如果字典没有，就追加元素，如果有，就更新元素,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,144,Lk 表示满足频繁子项的集合，L 元素在增加，例如:,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,145,"l=[[set(1), set(2), set(3)]]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,146,"l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,149,"print 'k=', k, len(L[k-2])",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,152,计算可信度（confidence）,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,165,记录可信度大于最小可信度（minConf）的集合,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,167,"假设 freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]，那么现在需要求出 frozenset([1]) -> frozenset([3]) 的可信度和 frozenset([3]) -> frozenset([1]) 的可信度",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,169,"print 'confData=', freqSet, H, conseq, freqSet-conseq",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,170,"支持度定义: a -> b = support(a | b) / support(a). 假设  freqSet = frozenset([1, 3]), conseq = [frozenset([1])]，那么 frozenset([1]) 至 frozenset([3]) 的可信度为 = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,172,只要买了 freqSet-conseq 集合，一定会买 conseq 集合（freqSet-conseq 集合和 conseq集合 是全集）,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,178,递归计算频繁项集的规则,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,189,"H[0] 是 freqSet 的元素组合的第一个元素，并且 H 中所有元素的长度都一样，长度由 aprioriGen(H, m+1) 这里的 m + 1 来控制",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,190,该函数递归时，H[0] 的长度从 1 开始增长 1 2 3 ...,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,191,"假设 freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,192,那么 m = len(H[0]) 的递归的值依次为 1 2,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,193,"在 m = 2 时, 跳出该递归。假设再递归一次，那么 H[0] = frozenset([2, 3, 5])，freqSet = frozenset([2, 3, 5]) ，没必要再计算 freqSet 与 H[0] 的关联规则了。",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,196,"print 'freqSet******************', len(freqSet), m + 1, freqSet, H, H[0]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,197,"生成 m+1 个长度的所有可能的 H 中的组合，假设 H = [frozenset([2]), frozenset([3]), frozenset([5])]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,198,"第一次递归调用时生成 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,199,第二次 。。。没有第二次，递归条件判断时已经退出了,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,201,返回可信度大于最小可信度的集合,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,205,计算可信度后，还有数据大于最小可信度的话，那么继续递归调用，否则跳出递归,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,207,"print '----------------------', Hmp1",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,208,"print len(freqSet),  len(Hmp1[0]) + 1",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,211,生成关联规则,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,223,"假设 L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,225,获取频繁项集中每个组合的所有元素,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,227,"假设：freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,228,组合总的元素并遍历子元素，并转化为 frozenset 集合，再存放到 list 列表中,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,230,"2 个的组合，走 else, 2 个以上的组合，走 if",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,241,votesmart.apikey = 'get your api key first',
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,249,api call,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,258,delay to be polite,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,262,this will return a list of lists containing ints,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,263,list of what each item stands for,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,264,fill up itemMeaning list,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,267,list of items in each transaction (politician),
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,291,暂时没用上,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,292,"def pntRules(ruleList, itemMeaning):",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,293,for ruleTup in ruleList:,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,294,for item in ruleTup[0]:,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,295,print itemMeaning[item],
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,296,"print ""           -------->""",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,297,for item in ruleTup[1]:,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,298,print itemMeaning[item],
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,299,"print ""confidence: %f"" % ruleTup[2]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,300,print       #print a blank line,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,303,加载测试数据集,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,307,Apriori 算法生成频繁项集以及它们的支持度,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,314,Apriori 算法生成频繁项集以及它们的支持度,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,320,加载测试数据集,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,324,Apriori 算法生成频繁项集以及它们的支持度,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,329,生成关联规则,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,334,测试 Apriori 算法,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,335,testApriori(),
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,337,生成关联规则,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,338,testGenerateRules(),
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,340,项目案例,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,341,# 构建美国国会投票记录的事务数据集,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,342,"actionIdList, billTitleList = getActionIds()",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,343,# 测试前2个,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,344,"transDict, itemMeaning = getTransList(actionIdList[: 2], billTitleList[: 2])",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,345,"transDict 表示 action_id的集合，transDict[key]这个就是action_id对应的选项，例如 [1, 2, 3]",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,346,"transDict, itemMeaning = getTransList(actionIdList, billTitleList)",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,347,# 得到全集的数据,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,348,dataSet = [transDict[key] for key in transDict.keys()],
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,349,"L, supportData = apriori(dataSet, minSupport=0.3)",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,350,"rules = generateRules(L, supportData, minConf=0.95)",
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,351,print (rules),
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,353,# 项目案例,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,354,# 发现毒蘑菇的相似特性,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,355,# 得到全集的数据,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,358,# 2表示毒蘑菇，1表示可食用的蘑菇,
AiLearning/src/py3.x/ml/11.Apriori/apriori.py,359,# 找出关于2的频繁子项出来，就知道如果是毒蘑菇，那么出现频繁的也可能是毒蘑菇,
AiLearning/src/py3.x/ml/8.Regression/regression.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/8.Regression/regression.py,2,coding:utf8,
AiLearning/src/py3.x/ml/8.Regression/regression.py,16,"在Python3中将urllib2和urllib3合并为一个标准库urllib,其中的urllib2.urlopen更改为urllib.request.urlopen",
AiLearning/src/py3.x/ml/8.Regression/regression.py,26,获取样本特征的总数，不算最后的目标变量,
AiLearning/src/py3.x/ml/8.Regression/regression.py,32,读取每一行,
AiLearning/src/py3.x/ml/8.Regression/regression.py,34,删除一行中以tab分隔的数据前后的空白符号,
AiLearning/src/py3.x/ml/8.Regression/regression.py,36,i 从0到2，不包括2,
AiLearning/src/py3.x/ml/8.Regression/regression.py,38,将数据添加到lineArr List中，每一行数据测试数据组成一个行向量,
AiLearning/src/py3.x/ml/8.Regression/regression.py,40,将测试数据的输入数据部分存储到dataMat 的List中,
AiLearning/src/py3.x/ml/8.Regression/regression.py,42,将每一行的最后一个数据，即类别，或者叫目标变量存储到labelMat List中,
AiLearning/src/py3.x/ml/8.Regression/regression.py,58,mat()函数将xArr，yArr转换为矩阵 mat().T 代表的是对矩阵进行转置操作,
AiLearning/src/py3.x/ml/8.Regression/regression.py,61,矩阵乘法的条件是左矩阵的列数等于右矩阵的行数,
AiLearning/src/py3.x/ml/8.Regression/regression.py,63,因为要用到xTx的逆矩阵，所以事先需要确定计算得到的xTx是否可逆，条件是矩阵的行列式不为0,
AiLearning/src/py3.x/ml/8.Regression/regression.py,64,linalg.det() 函数是用来求得矩阵的行列式的，如果矩阵的行列式为0，则这个矩阵是不可逆的，就无法进行接下来的运算,
AiLearning/src/py3.x/ml/8.Regression/regression.py,68,最小二乘法,
AiLearning/src/py3.x/ml/8.Regression/regression.py,69,http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133,
AiLearning/src/py3.x/ml/8.Regression/regression.py,70,书中的公式，求得w的最优解,
AiLearning/src/py3.x/ml/8.Regression/regression.py,93,mat() 函数是将array转换为矩阵的函数， mat().T 是转换为矩阵之后，再进行转置操作,
AiLearning/src/py3.x/ml/8.Regression/regression.py,96,获得xMat矩阵的行数,
AiLearning/src/py3.x/ml/8.Regression/regression.py,98,eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重,
AiLearning/src/py3.x/ml/8.Regression/regression.py,101,testPoint 的形式是 一个行向量的形式,
AiLearning/src/py3.x/ml/8.Regression/regression.py,102,计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值,
AiLearning/src/py3.x/ml/8.Regression/regression.py,104,k控制衰减的速度,
AiLearning/src/py3.x/ml/8.Regression/regression.py,106,根据矩阵乘法计算 xTx ，其中的 weights 矩阵是样本点对应的权重矩阵,
AiLearning/src/py3.x/ml/8.Regression/regression.py,111,计算出回归系数的一个估计,
AiLearning/src/py3.x/ml/8.Regression/regression.py,128,得到样本点的总数,
AiLearning/src/py3.x/ml/8.Regression/regression.py,130,构建一个全部都是 0 的 1 * m 的矩阵,
AiLearning/src/py3.x/ml/8.Regression/regression.py,132,循环所有的数据点，并将lwlr运用于所有的数据点,
AiLearning/src/py3.x/ml/8.Regression/regression.py,135,返回估计值,
AiLearning/src/py3.x/ml/8.Regression/regression.py,151,生成一个与目标变量数目相同的 0 向量,
AiLearning/src/py3.x/ml/8.Regression/regression.py,153,将 xArr 转换为 矩阵形式,
AiLearning/src/py3.x/ml/8.Regression/regression.py,155,排序,
AiLearning/src/py3.x/ml/8.Regression/regression.py,157,开始循环，为每个样本点进行局部加权线性回归，得到最终的目标变量估计值,
AiLearning/src/py3.x/ml/8.Regression/regression.py,192,岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆,
AiLearning/src/py3.x/ml/8.Regression/regression.py,194,检查行列式是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆。,
AiLearning/src/py3.x/ml/8.Regression/regression.py,215,计算Y的均值,
AiLearning/src/py3.x/ml/8.Regression/regression.py,217,Y的所有的特征减去均值,
AiLearning/src/py3.x/ml/8.Regression/regression.py,219,标准化 x，计算 xMat 平均值,
AiLearning/src/py3.x/ml/8.Regression/regression.py,221,然后计算 X的方差,
AiLearning/src/py3.x/ml/8.Regression/regression.py,223,所有特征都减去各自的均值并除以方差,
AiLearning/src/py3.x/ml/8.Regression/regression.py,225,可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数。,
AiLearning/src/py3.x/ml/8.Regression/regression.py,227,创建30 * m 的全部数据为0 的矩阵,
AiLearning/src/py3.x/ml/8.Regression/regression.py,230,exp() 返回 e^x,
AiLearning/src/py3.x/ml/8.Regression/regression.py,236,按列进行规范化,
AiLearning/src/py3.x/ml/8.Regression/regression.py,238,计算平均值然后减去它,
AiLearning/src/py3.x/ml/8.Regression/regression.py,239,计算除以Xi的方差,
AiLearning/src/py3.x/ml/8.Regression/regression.py,248,也可以规则化ys但会得到更小的coef,
AiLearning/src/py3.x/ml/8.Regression/regression.py,251,测试代码删除,
AiLearning/src/py3.x/ml/8.Regression/regression.py,272,"def scrapePage(inFile, outFile, yr, numPce, origPrc):",
AiLearning/src/py3.x/ml/8.Regression/regression.py,273,fr = open(inFile),
AiLearning/src/py3.x/ml/8.Regression/regression.py,274,"fw = open(outFile, 'a')  # a is append mode writing",
AiLearning/src/py3.x/ml/8.Regression/regression.py,275,soup = BeautifulSoup(fr.read()),
AiLearning/src/py3.x/ml/8.Regression/regression.py,276,i = 1,
AiLearning/src/py3.x/ml/8.Regression/regression.py,277,"currentRow = soup.findAll('table', r=""%d"" % i)",
AiLearning/src/py3.x/ml/8.Regression/regression.py,278,while (len(currentRow) != 0):,
AiLearning/src/py3.x/ml/8.Regression/regression.py,279,title = currentRow[0].findAll('a')[1].text,
AiLearning/src/py3.x/ml/8.Regression/regression.py,280,lwrTitle = title.lower(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,281,if (lwrTitle.find('new') > -1) or (lwrTitle.find('nisb') > -1):,
AiLearning/src/py3.x/ml/8.Regression/regression.py,282,newFlag = 1.0,
AiLearning/src/py3.x/ml/8.Regression/regression.py,283,else:,
AiLearning/src/py3.x/ml/8.Regression/regression.py,284,newFlag = 0.0,
AiLearning/src/py3.x/ml/8.Regression/regression.py,285,soldUnicde = currentRow[0].findAll('td')[3].findAll('span'),
AiLearning/src/py3.x/ml/8.Regression/regression.py,286,if len(soldUnicde) == 0:,
AiLearning/src/py3.x/ml/8.Regression/regression.py,287,"print(""item #%d did not sell"" % i)",
AiLearning/src/py3.x/ml/8.Regression/regression.py,288,else:,
AiLearning/src/py3.x/ml/8.Regression/regression.py,289,soldPrice = currentRow[0].findAll('td')[4],
AiLearning/src/py3.x/ml/8.Regression/regression.py,290,priceStr = soldPrice.text,
AiLearning/src/py3.x/ml/8.Regression/regression.py,291,"priceStr = priceStr.replace('$', '')  # strips out $",
AiLearning/src/py3.x/ml/8.Regression/regression.py,292,"priceStr = priceStr.replace(',', '')  # strips out ,",
AiLearning/src/py3.x/ml/8.Regression/regression.py,293,if len(soldPrice) > 1:,
AiLearning/src/py3.x/ml/8.Regression/regression.py,294,"priceStr = priceStr.replace('Free shipping', '')  # strips out Free Shipping",
AiLearning/src/py3.x/ml/8.Regression/regression.py,295,"print(""%s\t%d\t%s"" % (priceStr, newFlag, title))",
AiLearning/src/py3.x/ml/8.Regression/regression.py,296,"fw.write(""%d\t%d\t%d\t%f\t%s\n"" % (yr, numPce, newFlag, origPrc, priceStr))",
AiLearning/src/py3.x/ml/8.Regression/regression.py,297,i += 1,
AiLearning/src/py3.x/ml/8.Regression/regression.py,298,"currentRow = soup.findAll('table', r=""%d"" % i)",
AiLearning/src/py3.x/ml/8.Regression/regression.py,299,fw.close(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,302,--------------------------------------------------------------,
AiLearning/src/py3.x/ml/8.Regression/regression.py,303,预测乐高玩具套装的价格 ------ 最初的版本，因为现在 google 的 api 变化，无法获取数据,
AiLearning/src/py3.x/ml/8.Regression/regression.py,304,故改为了下边的样子，但是需要安装一个 beautifulSoup 这个第三方网页文本解析器，安装很简单，见下边,
AiLearning/src/py3.x/ml/8.Regression/regression.py,305,from time import sleep,
AiLearning/src/py3.x/ml/8.Regression/regression.py,306,import json,
AiLearning/src/py3.x/ml/8.Regression/regression.py,307,"这里特别指出 正确的使用方法为下面的语句使用,from urllib import request 将会报错,具体细节查看官方文档",
AiLearning/src/py3.x/ml/8.Regression/regression.py,308,"import urllib.request   # 在Python3中将urllib2和urllib等五个模块合并为一个标准库urllib,其中的urllib2.urlopen更改为urllib.request.urlopen",
AiLearning/src/py3.x/ml/8.Regression/regression.py,315,转换为json格式,
AiLearning/src/py3.x/ml/8.Regression/regression.py,342,create error mat 30columns numVal rows创建error mat 30columns numVal 行,
AiLearning/src/py3.x/ml/8.Regression/regression.py,347,create training set based on first 90% of values in indexList,
AiLearning/src/py3.x/ml/8.Regression/regression.py,348,基于indexList中的前90%的值创建训练集,
AiLearning/src/py3.x/ml/8.Regression/regression.py,355,get 30 weight vectors from ridge,
AiLearning/src/py3.x/ml/8.Regression/regression.py,356,loop over all of the ridge estimates,
AiLearning/src/py3.x/ml/8.Regression/regression.py,360,regularize test with training params,
AiLearning/src/py3.x/ml/8.Regression/regression.py,361,test ridge results and store,
AiLearning/src/py3.x/ml/8.Regression/regression.py,363,"print (errorMat[i,k])",
AiLearning/src/py3.x/ml/8.Regression/regression.py,364,calc avg performance of the different ridge weight vectors,
AiLearning/src/py3.x/ml/8.Regression/regression.py,367,can unregularize to get model,
AiLearning/src/py3.x/ml/8.Regression/regression.py,368,when we regularized we wrote Xreg = (x-meanX)/var(x),
AiLearning/src/py3.x/ml/8.Regression/regression.py,369,we can now write in terms of x not Xreg:  x*w/var(x) - meanX/var(x) +meanY,
AiLearning/src/py3.x/ml/8.Regression/regression.py,376,----------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/8.Regression/regression.py,377,"预测乐高玩具套装的价格 可运行版本，我们把乐高数据存储到了我们的 input 文件夹下，使用 urllib爬取,bs4解析内容",
AiLearning/src/py3.x/ml/8.Regression/regression.py,378,前提：安装 BeautifulSoup，步骤如下,
AiLearning/src/py3.x/ml/8.Regression/regression.py,379,在这个页面 https://www.crummy.com/software/BeautifulSoup/bs4/download/4.4/ 下载，beautifulsoup4-4.4.1.tar.gz,
AiLearning/src/py3.x/ml/8.Regression/regression.py,380,将下载文件解压，使用 windows 版本的 cmd 命令行，进入解压的包，输入以下两行命令即可完成安装,
AiLearning/src/py3.x/ml/8.Regression/regression.py,381,python setup.py build,
AiLearning/src/py3.x/ml/8.Regression/regression.py,382,python setup.py install,
AiLearning/src/py3.x/ml/8.Regression/regression.py,383,如果为linux或者mac系统可以直接使用pip进行安装 pip3 install bs4,
AiLearning/src/py3.x/ml/8.Regression/regression.py,384,----------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/8.Regression/regression.py,387,从页面读取数据，生成retX和retY列表,
AiLearning/src/py3.x/ml/8.Regression/regression.py,389,打开并读取HTML文件,
AiLearning/src/py3.x/ml/8.Regression/regression.py,390,"这里推荐使用with open() 生成器,这样节省内存也可以避免最后忘记关闭文件的问题",
AiLearning/src/py3.x/ml/8.Regression/regression.py,393,根据HTML页面结构进行解析,
AiLearning/src/py3.x/ml/8.Regression/regression.py,399,查找是否有全新标签,
AiLearning/src/py3.x/ml/8.Regression/regression.py,404,查找是否已经标志出售，我们只收集已出售的数据,
AiLearning/src/py3.x/ml/8.Regression/regression.py,409,解析页面获取当前价格,
AiLearning/src/py3.x/ml/8.Regression/regression.py,412,strips out $,
AiLearning/src/py3.x/ml/8.Regression/regression.py,413,"strips out ,",
AiLearning/src/py3.x/ml/8.Regression/regression.py,417,去掉不完整的套装价格,
AiLearning/src/py3.x/ml/8.Regression/regression.py,483,test for standRegression,
AiLearning/src/py3.x/ml/8.Regression/regression.py,490,add_subplot(349)函数的参数的意思是，将画布分成3行4列图像画在从左到右从上到下第9块,
AiLearning/src/py3.x/ml/8.Regression/regression.py,491,scatter 的x是xMat中的第二列，y是yMat的第一列,
AiLearning/src/py3.x/ml/8.Regression/regression.py,503,argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出,
AiLearning/src/py3.x/ml/8.Regression/regression.py,512,test for abloneDataSet,
AiLearning/src/py3.x/ml/8.Regression/regression.py,522,加载数据,
AiLearning/src/py3.x/ml/8.Regression/regression.py,524,使用不同的核进行预测,
AiLearning/src/py3.x/ml/8.Regression/regression.py,528,打印出不同的核预测值与训练数据集上的真实值之间的误差大小,
AiLearning/src/py3.x/ml/8.Regression/regression.py,533,打印出 不同的核预测值 与 新数据集（测试数据集）上的真实值之间的误差大小,
AiLearning/src/py3.x/ml/8.Regression/regression.py,541,使用简单的 线性回归 进行预测，与上面的计算进行比较,
AiLearning/src/py3.x/ml/8.Regression/regression.py,547,test for ridgeRegression,
AiLearning/src/py3.x/ml/8.Regression/regression.py,557,test for stageWise,
AiLearning/src/py3.x/ml/8.Regression/regression.py,570,predict for lego's price,
AiLearning/src/py3.x/ml/8.Regression/regression.py,579,regression1(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,580,regression2(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,581,abaloneTest(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,582,regression3(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,583,regression4(),
AiLearning/src/py3.x/ml/8.Regression/regression.py,584,regression5(),
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,2,coding:utf8,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,12,Isotonic Regression 等式回归,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,15,Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,16,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,17,License: BSD,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,37,线性回归的 x 需要为 2d,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,53,Kernel ridge regression ( 内核岭回归 ),
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,55,2.1 Comparison of kernel ridge regression and SVR ( 内核岭回归与 SVR 的比较 ),
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,57,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
AiLearning/src/py3.x/ml/8.Regression/sklearn-regression-demo.py,58,License: BSD 3 clause,
AiLearning/src/py3.x/ml/13.PCA/pca.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/13.PCA/pca.py,2,coding:utf8,
AiLearning/src/py3.x/ml/13.PCA/pca.py,19,注意这里和python2的区别，需要在map函数外加一个list（），否则显示结果为 map at 0x3fed1d0,
AiLearning/src/py3.x/ml/13.PCA/pca.py,34,计算每一列的均值,
AiLearning/src/py3.x/ml/13.PCA/pca.py,36,"print('meanVals', meanVals)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,38,每个向量同时都减去 均值,
AiLearning/src/py3.x/ml/13.PCA/pca.py,40,"print('meanRemoved=', meanRemoved)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,42,cov协方差=[(x1-x均值)*(y1-y均值)+(x2-x均值)*(y2-y均值)+...+(xn-x均值)*(yn-y均值)+]/(n-1),
AiLearning/src/py3.x/ml/13.PCA/pca.py,54,eigVals为特征值， eigVects为特征向量,
AiLearning/src/py3.x/ml/13.PCA/pca.py,56,"print('eigVals=', eigVals)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,57,"print('eigVects=', eigVects)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,58,对特征值，进行从小到大的排序，返回从小到大的index序号,
AiLearning/src/py3.x/ml/13.PCA/pca.py,59,特征值的逆序就可以得到topNfeat个最大的特征向量,
AiLearning/src/py3.x/ml/13.PCA/pca.py,73,"print('eigValInd1=', eigValInd)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,75,-1表示倒序，返回topN的特征值[-1 到 -(topNfeat+1) 但是不包括-(topNfeat+1)本身的倒叙],
AiLearning/src/py3.x/ml/13.PCA/pca.py,77,"print('eigValInd2=', eigValInd)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,78,重组 eigVects 最大到最小,
AiLearning/src/py3.x/ml/13.PCA/pca.py,80,"print('redEigVects=', redEigVects.T)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,81,将数据转换到新空间,
AiLearning/src/py3.x/ml/13.PCA/pca.py,82,"print( ""---"", shape(meanRemoved), shape(redEigVects))",
AiLearning/src/py3.x/ml/13.PCA/pca.py,85,"print('lowDDataMat=', lowDDataMat)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,86,"print('reconMat=', reconMat)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,94,对value不为NaN的求均值,
AiLearning/src/py3.x/ml/13.PCA/pca.py,95,.A 返回矩阵基于的数组,
AiLearning/src/py3.x/ml/13.PCA/pca.py,97,将value为NaN的值赋值为均值,
AiLearning/src/py3.x/ml/13.PCA/pca.py,137,# 加载数据，并转化数据类型为float,
AiLearning/src/py3.x/ml/13.PCA/pca.py,138,dataMat = loadDataSet('data/13.PCA/testSet.txt'),
AiLearning/src/py3.x/ml/13.PCA/pca.py,139,# 只需要1个特征向量,
AiLearning/src/py3.x/ml/13.PCA/pca.py,140,"lowDmat, reconMat = pca(dataMat, 1)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,141,# 只需要2个特征向量，和原始数据一致，没任何变化,
AiLearning/src/py3.x/ml/13.PCA/pca.py,142,"# lowDmat, reconMat = pca(dataMat, 2)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,143,# print(shape(lowDmat)),
AiLearning/src/py3.x/ml/13.PCA/pca.py,144,"show_picture(dataMat, reconMat)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,146,利用PCA对半导体制造数据降维,
AiLearning/src/py3.x/ml/13.PCA/pca.py,149,分析数据,
AiLearning/src/py3.x/ml/13.PCA/pca.py,151,"lowDmat, reconMat = pca(dataMat, 20)",
AiLearning/src/py3.x/ml/13.PCA/pca.py,152,print(shape(lowDmat)),
AiLearning/src/py3.x/ml/13.PCA/pca.py,153,"show_picture(dataMat, reconMat)",
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,19,创建40个分离点,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,21,"X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]",
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,22,Y = [0] * 20 + [1] * 20,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,50,拟合一个SVM模型,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,54,获取分割超平面,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,56,斜率,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,58,从-5到5，顺序间隔采样50个样本，默认是num=50,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,59,"xx = np.linspace(-5, 5)  # , num=50)",
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,60,", num=50)",
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,61,二维的直线方程,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,65,plot the parallels to the separating hyperplane that pass through the support vectors,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,66,通过支持向量绘制分割超平面,
AiLearning/src/py3.x/ml/6.SVM/sklearn-svm-demo.py,73,"plot the line, the points, and the nearest vectors to the plane",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,36,数据的行数,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,41,误差缓存，第一列给出的是eCache是否有效的标志位，第二列给出的是实际的E值。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,44,m行m列的矩阵,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,50,calc the kernel or transform data to a higher dimensional space,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,64,linear kernel:   m*n * n*1 = m*1,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,70,径向基函数的高斯版本,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,71,divide in NumPy is element-wise not matrix like Matlab,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,127,"this is the second choice -heurstic, and calcs Ej",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,146,首先将输入值Ei在缓存中设置成为有效的。这里的有效意味着它已经计算好了。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,149,"print('oS.eCache[%s]=%s' % (i, oS.eCache[i]))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,150,"print('oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,151,"""""""",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,152,# 返回非0的：行列值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,153,"nonzero(oS.eCache[:, 0].A)= (",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,154,"行： array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]),",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,155,"列： array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,156,),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,157,"""""""",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,158,"print('nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,159,# 取行的list,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,160,"print('nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,161,非零E值的行的list列表，所对应的alpha值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,164,在所有的值上进行循环，并选择其中使得改变最大的那个值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,166,"don't calc for i, waste of time",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,168,求 Ek误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,172,选择具有最大步长的j,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,177,如果是第一次循环，则随机选择一个alpha值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,180,求 Ek误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,194,求 误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,225,求 Ek误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,228,约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,229,0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,230,表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,238,选择最大的误差对应的j进行优化。效果更明显,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,243,L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接return 0,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,251,"print(""L==H"")",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,254,eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,255,参考《统计学习方法》李航-P125~P128<序列最小最优化算法>,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,256,changed for kernel,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,261,计算出一个新的alphas[j]值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,263,并使用辅助函数，以及L和H对其进行调整,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,265,更新误差缓存,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,268,检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,270,"print(""j not moving enough"")",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,273,然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,275,更新误差缓存,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,278,"在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,279,w= Σ[1~n] ai*yi*xi => b = yi- Σ[1~n] ai*yi(xi*xj),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,280,所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,281,为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,312,创建一个 optStruct 对象,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,318,循环遍历：循环maxIter次 并且 （alphaPairsChanged存在可以改变 or 所有行遍历一遍）,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,321,----------- 第一种写法 start -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,322,当entireSet=true or 非边界alpha对没有了；就开始寻找 alpha对，然后决定是否要进行else。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,324,在数据集上遍历所有可能的alpha,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,326,是否存在alpha对，存在就+1,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,328,"print(""fullSet, iter: %d i:%d, pairs changed %d"" % (iter, i, alphaPairsChanged))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,331,对已存在 alpha对，选出非边界的alpha值，进行优化。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,333,遍历所有的非边界alpha值，也就是不在边界0或C上的值。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,337,"print(""non-bound, iter: %d i:%d, pairs changed %d"" % (iter, i, alphaPairsChanged))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,339,----------- 第一种写法 end -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,341,----------- 第二种方法 start -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,342,if entireSet:																				#遍历整个数据集,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,343,"alphaPairsChanged += sum(innerL(i, oS) for i in range(oS.m))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,344,else: 																						#遍历非边界值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,345,nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]						#遍历不在边界0和C的alpha,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,346,"alphaPairsChanged += sum(innerL(i, oS) for i in nonBoundIs)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,347,iter += 1,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,348,----------- 第二种方法 end -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,349,如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,351,toggle entire set loop,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,380,C=200 important,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,384,get matrix of only support vectors,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,392,"和这个svm-simple类似： fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,425,load the training set,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,430,take off .txt,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,442,1. 导入训练数据,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,450,"print(""there are %d Support Vectors"" % shape(sVs)[0])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,455,1*m * m*1 = 1*1 单个预测结果,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,459,2. 导入测试数据,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,483,"b原来是矩阵，先转为数组类型后其数组大小为（1,1），所以后面加[0]，变为(1,)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,488,注意flatten的用法,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,491,"x最大值，最小值根据原数据集dataArr[:, 0]的大小而定",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,494,"根据x.w + b = 0 得到，其式子展开为w0.x1 + w1.x2 + b = 0, x2就是y值",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,504,找到支持向量，并在图中标红,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,513,# 无核函数的测试,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,514,# 获取特征和目标变量,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,515,"dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,516,# print(labelArr),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,518,# b是常量值， alphas是拉格朗日乘子,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,519,"b, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,520,print('/n/n/n'),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,521,"print('b=', b)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,522,"print('alphas[alphas>0]=', alphas[alphas > 0])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,523,"print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,524,for i in range(100):,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,525,if alphas[i] > 0:,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,526,"print(dataArr[i], labelArr[i])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,527,# 画图,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,528,"ws = calcWs(alphas, dataArr, labelArr)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,529,"plotfig_SVM(dataArr, labelArr, ws, b, alphas)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,531,有核函数的测试,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,532,testRbf(0.8),
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,534,项目实战,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,535,示例：手写识别问题回顾,
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,536,"testDigits(('rbf', 0.1))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,537,"testDigits(('rbf', 5))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,539,"testDigits(('rbf', 50))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,540,"testDigits(('rbf', 100))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete.py,541,"testDigits(('lin', 10))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,16,Initialize the structure with the parameters,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,24,first column is valid flag,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,91,"this is the second choice -heurstic, and calcs Ej",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,110,首先将输入值Ei在缓存中设置成为有效的。这里的有效意味着它已经计算好了。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,113,"print('oS.eCache[%s]=%s' % (i, oS.eCache[i]))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,114,"print('oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,115,"""""""",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,116,# 返回非0的：行列值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,117,"nonzero(oS.eCache[:, 0].A)= (",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,118,"行： array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]),",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,119,"列： array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,120,),
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,121,"""""""",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,122,"print('nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,123,# 取行的list,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,124,"print('nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0])",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,125,非零E值的行的list列表，所对应的alpha值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,128,在所有的值上进行循环，并选择其中使得改变最大的那个值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,130,"don't calc for i, waste of time",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,132,求 Ek误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,140,如果是第一次循环，则随机选择一个alpha值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,143,求 Ek误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,148,after any alpha has changed update the new value in the cache,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,157,求 误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,174,求 Ek误差：预测值-真实值的差,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,177,约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值),
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,178,0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,179,表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,187,选择最大的误差对应的j进行优化。效果更明显,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,192,L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接return 0,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,203,eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,204,参考《统计学习方法》李航-P125~P128<序列最小最优化算法>,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,211,计算出一个新的alphas[j]值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,213,并使用辅助函数，以及L和H对其进行调整,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,215,更新误差缓存,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,218,检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,223,然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,225,更新误差缓存,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,228,"在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,229,w= Σ[1~n] ai*yi*xi => b = yj Σ[1~n] ai*yi(xi*xj),
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,230,所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1),
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,231,为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,261,创建一个 optStruct 对象,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,267,循环遍历：循环maxIter次 并且 （alphaPairsChanged存在可以改变 or 所有行遍历一遍）,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,268,循环迭代结束 或者 循环遍历所有alpha后，alphaPairs还是没变化,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,271,----------- 第一种写法 start -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,272,当entireSet=true or 非边界alpha对没有了；就开始寻找 alpha对，然后决定是否要进行else。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,274,在数据集上遍历所有可能的alpha,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,276,是否存在alpha对，存在就+1,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,280,对已存在 alpha对，选出非边界的alpha值，进行优化。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,282,遍历所有的非边界alpha值，也就是不在边界0或C上的值。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,288,----------- 第一种写法 end -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,290,----------- 第二种方法 start -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,291,if entireSet:																				#遍历整个数据集,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,292,"alphaPairsChanged += sum(innerL(i, oS) for i in range(oS.m))",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,293,else: 																						#遍历非边界值,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,294,nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]						#遍历不在边界0和C的alpha,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,295,"alphaPairsChanged += sum(innerL(i, oS) for i in nonBoundIs)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,296,iter += 1,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,297,----------- 第二种方法 end -------------------------,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,298,如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,300,toggle entire set loop,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,338,"b原来是矩阵，先转为数组类型后其数组大小为（1,1），所以后面加[0]，变为(1,)",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,343,注意flatten的用法,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,346,"x最大值，最小值根据原数据集dataArr[:, 0]的大小而定",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,349,"根据x.w + b = 0 得到，其式子展开为w0.x1 + w1.x2 + b = 0, x2就是y值",
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,359,找到支持向量，并在图中标红,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,367,获取特征和目标变量,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,369,print(labelArr),
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,371,b是常量值， alphas是拉格朗日乘子,
AiLearning/src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py,380,画图,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,81,矩阵转置 和 .T 一样的功能,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,85,初始化 b和alphas(alpha有点类似权重值。),
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,89,没有任何alpha改变的情况下遍历数据的次数,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,92,"w = calcWs(alphas, dataMatIn, classLabels)",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,93,"print(""w:"", w)",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,95,记录alpha是否已经进行优化，每次循环时设为0，然后再对整个集合顺序遍历,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,98,"print('alphas=', alphas)",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,99,"print('labelMat=', labelMat)",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,100,"print('multiply(alphas, labelMat)=', multiply(alphas, labelMat))",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,101,我们预测的类别 y = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*lable[n]*x[n],
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,103,预测结果与真实结果比对，计算误差Ei,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,106,约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值),
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,107,0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,108,表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,117,如果满足优化的条件，我们就随机选取非i的一个点，进行优化比较,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,119,预测j的结果,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,125,L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,126,labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,133,如果相同，就没发优化了,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,138,eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,139,参考《统计学习方法》李航-P125~P128<序列最小最优化算法>,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,145,计算出一个新的alphas[j]值,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,147,并使用辅助函数，以及L和H对其进行调整,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,149,检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,153,然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,155,"在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,156,w= Σ[1~n] ai*yi*xi => b = yj- Σ[1~n] ai*yi(xi*xj),
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,157,所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1),
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,158,为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,169,在for循环外，检查alpha值是否做了更新，如果在更新则将iter设为0后继续运行程序,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,170,知道更新完毕后，iter次循环无变化，才推出循环。,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,210,"b原来是矩阵，先转为数组类型后其数组大小为（1,1），所以后面加[0]，变为(1,)",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,215,注意flatten的用法,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,218,"x最大值，最小值根据原数据集dataArr[:, 0]的大小而定",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,221,"根据x.w + b = 0 得到，其式子展开为w0.x1 + w1.x2 + b = 0, x2就是y值",
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,231,找到支持向量，并在图中标红,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,239,获取特征和目标变量,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,241,print(labelArr),
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,243,b是常量值， alphas是拉格朗日乘子,
AiLearning/src/py3.x/ml/6.SVM/svm-simple.py,252,画图,
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,2,coding:utf-8,
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,25,生成一个 4*4 的随机数组,
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,28,转化关系， 数组转化为矩阵,
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,45,输出结果,
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,51,矩阵和逆矩阵 进行求积 (单位矩阵，对角线都为1嘛，理论上4*4的矩阵其他的都为0),
AiLearning/src/py3.x/ml/1.MLFoundation/NumPy.py,53,误差,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,2,coding:utf8,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,14,默认解析的数据是用tab分隔，并且是数值类型,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,15,general function to parse tab -delimited floats,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,25,假定最后一列是结果值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,26,assume last column is target value,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,31,将每行转换成浮点数,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,49,# 测试案例,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,50,"print 'dataSet[:, feature]=', dataSet[:, feature]",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,51,"print 'nonzero(dataSet[:, feature] > value)[0]=', nonzero(dataSet[:, feature] > value)[0]",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,52,"print 'nonzero(dataSet[:, feature] <= value)[0]=', nonzero(dataSet[:, feature] <= value)[0]",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,54,"dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,55,"nonzero(dataSet[:, feature] > value)  返回结果为true行的index下标",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,61,返回每一个叶子结点的均值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,62,returns the value used for each leaf,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,63,我的理解是：regLeaf 是产生叶节点的函数，就是求均值，即用聚类中心点来代表这类数据,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,68,计算总方差=方差*样本数,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,69,我的理解是：求这组数据的方差，即通过决策树划分，可以让靠近的数据分到同一类中去,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,71,shape(dataSet)[0] 表示行数,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,75,1.用最佳方式切分数据集,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,76,2.生成相应的叶节点,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,91,"ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,92,之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,93,最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,95,划分最小 size 小于，就不继续划分了,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,97,如果数据集的最后一列所有值相等就退出,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,98,"dataSet[:, -1].T.tolist()[0] 取数据集的最后一列，转置为行向量，然后转换为list,取该list中的第一个元素。",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,99,如果集合size为1，也就是说全部的数据都是同一个类别，不用继续划分。,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,100,exit cond 1,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,102,计算行列值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,104,无分类误差的总方差和,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,105,the choice of the best feature is driven by Reduction in RSS error from mean,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,107,inf 正无穷大,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,109,循环处理每一列对应的feature值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,110,对于每个特征,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,111,下面的一行表示的是将某一列全部的数据转换为行，然后设置为list形式,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,113,对该列进行分组，然后组内的成员的val值进行 二元切分,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,115,判断二元切分的方式的元素数量是否符合预期,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,119,如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,120,如果划分后误差小于 bestS，则说明找到了新的bestS,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,125,判断二元切分的方式的元素误差是否符合预期,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,126,if the decrease (S-bestS) is less than a threshold don't do the split,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,130,对整体的成员进行判断，是否符合预期,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,131,如果集合的 size 小于 tolN,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,132,当最佳划分后，集合过小，也不划分，产生叶节点,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,137,assume dataSet is NumPy Mat so we can array filtering,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,138,假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,150,选择最好的切分方式： feature索引值，最优切分值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,151,choose the best split,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,153,if the splitting hit a stop condition return val,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,154,如果 splitting 达到一个停止条件，那么返回 val,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,160,大于在右边，小于在左边，分为2个数据集,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,162,递归的进行调用，在左右子树中继续递归生成树,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,168,判断节点是否是一个字典,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,181,计算左右枝丫的均值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,199,检查是否适合合并分枝,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,210,判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,214,判断分枝是否是dict字典，如果是就将测试数据集进行切分,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,217,如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,220,如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,224,上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,226,如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,227,1. 如果正确,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,228,* 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,229,* 如果 合并的总方差 < 不合并的总方差，那么就进行合并,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,230,注意返回的结果： 如果可以合并，原来的dict就变为了 数值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,233,"power(x, y)表示x的y次方",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,237,如果 合并的总方差 < 不合并的总方差，那么就进行合并,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,247,得到模型的ws系数：f(x) = x0 + x1*featrue1+ x3*featrue2 ...,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,248,create linear model and return coeficients,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,262,计算线性模型的误差值,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,274,"print corrcoef(yHat, Y, rowvar=0)",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,278,helper function used in two places,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,291,产生一个关于1的矩阵,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,294,X的0列为1，常数项，用于计算平衡误差,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,298,转置矩阵*矩阵,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,300,如果矩阵的逆不存在，会造成程序异常,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,303,最小二乘法求最优解:  w0*1+w1*x1=y,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,308,回归树测试案例,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,309,为了和 modelTreeEval() 保持一致，保留两个输入参数,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,323,模型树测试案例,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,324,对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,325,也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,339,"print X, model",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,343,计算预测的结果,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,344,在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,345,modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,346,此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,347,调用modelEval()函数，该函数的默认值为regTreeEval(),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,373,预测结果,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,387,print yHat,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,390,"print ""yHat==>"", yHat[i, 0]",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,395,# 测试数据集,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,396,testMat = mat(eye(4)),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,397,print testMat,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,398,print type(testMat),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,399,"mat0, mat1 = binSplitDataSet(testMat, 1, 0.5)",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,400,"print mat0, '\n-----------\n', mat1",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,402,# 回归树,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,403,myDat = loadDataSet('data/9.RegTrees/data1.txt'),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,404,# myDat = loadDataSet('data/9.RegTrees/data2.txt'),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,405,"# print 'myDat=', myDat",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,406,myMat = mat(myDat),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,407,"# print 'myMat=',  myMat",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,408,myTree = createTree(myMat),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,409,print myTree,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,411,# 1. 预剪枝就是：提起设置最大误差数和最少元素数,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,412,myDat = loadDataSet('data/9.RegTrees/data3.txt'),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,413,myMat = mat(myDat),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,414,"myTree = createTree(myMat, ops=(0, 1))",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,415,print myTree,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,417,# 2. 后剪枝就是：通过测试数据，对预测模型进行合并判断,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,418,myDatTest = loadDataSet('data/9.RegTrees/data3test.txt'),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,419,myMat2Test = mat(myDatTest),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,420,"myFinalTree = prune(myTree, myMat2Test)",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,421,print '\n\n\n-------------------',
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,422,print myFinalTree,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,424,# --------,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,425,# 模型树求解,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,426,myDat = loadDataSet('data/9.RegTrees/data4.txt'),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,427,myMat = mat(myDat),
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,428,"myTree = createTree(myMat, modelLeaf, modelErr)",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,429,print myTree,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,431,# 回归树 VS 模型树 VS 线性回归,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,434,# 回归树,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,439,print yHat1,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,440,"print ""ssss==>"", testMat[:, 1]",
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,443,模型树,
AiLearning/src/py3.x/ml/9.RegTrees/regTrees.py,449,线性回归,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,2,coding:utf8,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,23,"相当于告诉 布局管理器(Geometry Manager),如果不设定位置，默认在 0行0列的位置",
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,27,最大为误差， 最大子叶节点的数量,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,29,clear the figure,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,33,检查复选框是否选中,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,43,use scatter for data set,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,45,use plot for yHat,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,68,画新的tree,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,70,#get values from Entry boxes,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,76,标题,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,78,"输入栏1, 叶子的数量",
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,84,"输入栏2, 误差量",
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,89,设置输出值,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,92,设置提交的按钮,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,95,设置复选按钮,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,101,退出按钮,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,104,创建一个画板 canvas,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,117,创建一个事件,
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,119,test_widget_text(root),
AiLearning/src/py3.x/ml/9.RegTrees/treeExplore.py,122,启动事件循环,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,2,coding:utf8,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,14,引入必要的模型和库,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,19,创建一个随机的数据集,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,20,参考 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,22,"print 'lalalalala===', rng",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,23,"rand() 是给定形状的随机值，rng.rand(80, 1)即矩阵的形状是 80行，1列",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,24,sort(),
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,26,"print 'X=', X",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,28,"print 'y=', y",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,30,"print 'yyy=', y",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,32,拟合回归模型,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,33,regr_1 = DecisionTreeRegressor(max_depth=2),
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,34,保持 max_depth=5 不变，增加 min_samples_leaf=6 的参数，效果进一步提升了,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,37,regr_3 = DecisionTreeRegressor(max_depth=4),
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,38,"regr_1.fit(X, y)",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,40,"regr_3.fit(X, y)",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,42,预测,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,44,y_1 = regr_1.predict(X_test),
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,46,y_3 = regr_3.predict(X_test),
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,48,绘制结果,
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,51,"plt.plot(X_test, y_1, color=""cornflowerblue"", label=""max_depth=2"", linewidth=2)",
AiLearning/src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py,53,"plt.plot(X_test, y_3, color=""red"", label=""max_depth=3"", linewidth=2)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,2,coding:utf8,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,4,''',
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,5,Created on 2017-03-10,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,6,Update on 2017-03-10,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,7,author: jiangzhonglian,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,8,content: 回归树,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,9,''',
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,11,print(__doc__),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,14,# Import the necessary modules and libraries,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,15,import numpy as np,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,16,from sklearn.tree import DecisionTreeRegressor,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,17,import matplotlib.pyplot as plt,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,20,# Create a random dataset,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,21,rng = np.random.RandomState(1),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,22,"X = np.sort(5 * rng.rand(80, 1), axis=0)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,23,y = np.sin(X).ravel(),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,24,"print X, '\n\n\n-----------\n\n\n', y",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,25,y[::5] += 3 * (0.5 - rng.rand(16)),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,28,# Fit regression model,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,29,"regr_1 = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,30,"regr_2 = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,31,"regr_1.fit(X, y)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,32,"regr_2.fit(X, y)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,35,# Predict,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,36,"X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,37,y_1 = regr_1.predict(X_test),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,38,y_2 = regr_2.predict(X_test),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,41,# Plot the results,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,42,plt.figure(),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,43,"plt.scatter(X, y, c=""darkorange"", label=""data"")",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,44,"plt.plot(X_test, y_1, color=""cornflowerblue"", label=""max_depth=2"", linewidth=2)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,45,"plt.plot(X_test, y_2, color=""yellowgreen"", label=""max_depth=5"", linewidth=2)",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,46,"plt.xlabel(""data"")",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,47,"plt.ylabel(""target"")",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,48,"plt.title(""Decision Tree Regression"")",
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,49,plt.legend(),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,50,plt.show(),
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,68,Author: Noel Dawe <noel.dawe@gmail.com>,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,69,,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,70,License: BSD 3 clause,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,72,importing necessary libraries,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,78,Create the dataset,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,83,Fit regression model,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,92,Predict,
AiLearning/src/py3.x/ml/9.RegTrees/RTSklearn.py,96,Plot the results,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,2,coding:utf8,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,23,needs to be updated,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,46,"['r', 'x', 'n', 'o', 's'],",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,62,this version does not use recursion,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,73,建立相同元素之间的关系，例如： 左边的r指向右边的r值,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,90,取出 元素 出现次数最高的,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,91,如果该元素在 inTree.children 这个字典中，就进行累加,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,92,如果该元素不存在 就 inTree.children 字典中新增key，value为初始化的 treeNode 对象,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,94,更新 最大元素，对应的 treeNode 对象的count进行叠加,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,97,如果不存在子节点，我们为该inTree添加子节点,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,99,如果满足minSup的dist字典的value值第二位为null， 我们就设置该元素为 本节点对应的tree节点,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,100,如果元素第二位不为null，我们就更新header节点,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,102,headerTable只记录第一次节点出现的位置,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,105,本质上是修改headerTable的key对应的Tree，的nodeLink值,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,108,递归的调用，在items[0]的基础上，添加item0[1]做子节点， count只要循环的进行累计加和而已，统计出节点的最后的统计值。,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,122,支持度>=minSup的dist{所有元素：出现的次数},
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,124,循环 dist{行：出现次数}的样本数据,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,126,对所有的行进行循环，得到行里面的所有元素,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,127,统计每一行中，每个元素出现的总次数,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,129,例如： {'ababa': 3}  count(a)=3+3+3=9   count(b)=3+3=6,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,131,删除 headerTable中，元素次数<最小支持度的元素,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,132,"python3中.keys()返回的是迭代器不是list,不能在遍历时对其改变。",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,136,满足minSup: set(各元素集合),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,138,如果不存在，直接返回None,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,142,"格式化： dist{元素key: [元素次数, None]}",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,145,create tree,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,147,循环 dist{行：出现次数}的样本数据,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,149,"print('tranSet, count=', tranSet, count)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,150,localD = dist{元素key: 元素总出现次数},
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,153,判断是否在满足minSup的集合中,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,155,"print('headerTable[item][0]=', headerTable[item][0], headerTable[item])",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,157,"print('localD=', localD)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,158,对每一行的key 进行排序，然后开始往树添加枝丫，直到丰满,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,159,第二次，如果在同一个排名下出现，那么就对该枝丫的值进行追加，继续递归调用！,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,161,"p=key,value; 所以是通过value值的大小，进行从大到小进行排序",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,162,orderedItems 表示取出元组的key值，也就是字母本身，但是字母本身是大到小的顺序,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,164,"print 'orderedItems=', orderedItems, 'headerTable', headerTable, '\n\n\n'",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,165,填充树，通过有序的orderedItems的第一位，进行顺序填充 第一层的子节点。,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,193,对 treeNode的link进行循环,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,196,寻找改节点的父节点，相当于找到了该节点的频繁项集,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,198,"排除自身这个元素，判断是否存在父元素（所以要>1, 说明存在父元素）",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,200,"对非basePat的倒叙值作为key,赋值为count数",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,201,prefixPath[1:] 变frozenset后，字母就变无序了,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,202,condPats[frozenset(prefixPath)] = treeNode.count,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,204,递归，寻找改节点的下一个 相同值的链接节点,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,206,print(treeNode),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,220,通过value进行从小到大的排序， 得到频繁项集的key,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,221,最小支持项集的key的list集合,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,225,循环遍历 最频繁项集的key，从小到大的递归寻找对应的频繁项集,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,227,preFix为newFreqSet上一次的存储记录，一旦没有myHead，就不会更新,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,237,构建FP-tree,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,240,"挖掘条件 FP-tree, 如果myHead不为空，表示满足minSup {所有的元素+(value, treeNode)}",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,244,递归 myHead 找出频繁项集,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,249,import twitter,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,250,from time import sleep,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,251,import re,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,254,def getLotsOfTweets(searchStr):,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,255,"""""""",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,256,获取 100个搜索结果页面,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,257,"""""""",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,258,CONSUMER_KEY = '',
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,259,CONSUMER_SECRET = '',
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,260,ACCESS_TOKEN_KEY = '',
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,261,ACCESS_TOKEN_SECRET = '',
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,262,"api = twitter.Api(consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET, access_token_key=ACCESS_TOKEN_KEY, access_token_secret=ACCESS_TOKEN_SECRET)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,264,# you can get 1500 results 15 pages * 100 per page,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,265,resultsPages = [],
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,266,"for i in range(1, 15):",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,267,"print(""fetching page %d"" % i)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,268,"searchResults = api.GetSearch(searchStr, per_page=100, page=i)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,269,resultsPages.append(searchResults),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,270,sleep(6),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,271,return resultsPages,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,274,def textParse(bigString):,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,275,"""""""",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,276,解析页面内容,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,277,"""""""",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,278,"urlsRemoved = re.sub('(http:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[~])*', '', bigString)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,279,"listOfTokens = re.split(r'\W*', urlsRemoved)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,280,return [tok.lower() for tok in listOfTokens if len(tok) > 2],
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,283,"def mineTweets(tweetArr, minSup=5):",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,284,"""""""",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,285,获取频繁项集,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,286,"""""""",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,287,parsedList = [],
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,288,for i in range(14):,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,289,for j in range(100):,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,290,parsedList.append(textParse(tweetArr[i][j].text)),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,291,initSet = createInitSet(parsedList),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,292,"myFPtree, myHeaderTab = createTree(initSet, minSup)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,293,myFreqList = [],
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,294,"mineTree(myFPtree, myHeaderTab, minSup, set([]), myFreqList)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,295,return myFreqList,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,299,"rootNode = treeNode('pyramid', 9, None)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,300,"rootNode.children['eye'] = treeNode('eye', 13, None)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,301,"rootNode.children['phoenix'] = treeNode('phoenix', 3, None)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,302,# 将树以文本形式显示,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,303,# print(rootNode.disp()),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,305,load样本数据,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,307,"print(simpDat, '\n')",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,308,frozen set 格式化 并 重新装载 样本数据，对所有的行进行统计求和，格式: {行：出现次数},
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,312,创建FP树,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,313,输入：dist{行：出现次数}的样本数据  和  最小的支持度,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,314,输出：最终的PF-tree，通过循环获取第一层的节点，然后每一层的节点进行递归的获取每一行的字节点，也就是分支。然后所谓的指针，就是后来的指向已存在的,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,318,抽取条件模式基,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,319,查询树节点的，频繁子项,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,324,创建条件模式基,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,329,# 项目实战,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,330,# 1.twitter项目案例,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,331,# 无法运行，因为没发链接twitter,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,332,lotsOtweets = getLotsOfTweets('RIMM'),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,333,"listOfTerms = mineTweets(lotsOtweets, 20)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,334,print(len(listOfTerms)),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,335,for t in listOfTerms:,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,336,print(t),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,338,# 2.新闻网站点击流中挖掘，例如：文章1阅读过的人，还阅读过什么？,
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,339,parsedDat = [line.split() for line in open('data/12.FPGrowth/kosarak.dat').readlines()],
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,340,initSet = createInitSet(parsedDat),
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,341,"myFPtree, myHeaderTab = createTree(initSet, 100000)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,343,myFreList = [],
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,344,"mineTree(myFPtree, myHeaderTab, 100000, set([]), myFreList)",
AiLearning/src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py,345,print myFreList,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,2,coding:utf-8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,20,"dataMat.append([float(lineArr[0]), float(lineArr[1]), float(lineArr[2])])",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,42,就是预测 y 的值,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,58,回归系数,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,61,重置 wDelta,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,63,它是学习率，代表了权重调整幅度的大小。（也可以理解为随机梯度的步长，使它不断减小，便于拟合）,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,64,输入T和K分别设定了迭代次数和待处理列表的大小。在T次迭代过程中，每次需要重新计算eta,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,67,全部的训练集  内循环中执行批处理，将分类错误的值全部做累加后更新权重向量,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,69,mapper 代码,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,71,"如果预测正确，并且预测结果的绝对值>=1，因为最大间隔为1, 认为没问题。",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,72,"否则算是预测错误, 通过预测错误的结果，来累计更新w.",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,73,mapper 代码,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,74,累积变化,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,75,w通过不断的随机梯度的方式来优化,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,76,在每个 T上应用更改,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,77,"print '-----', w",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,78,"print '++++++', w",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,84,"finalWs = seqPegasos(datMat, labelList, 2, 5000)",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,107,y2 = (0.43799*x)/0.12316,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/pegasos.py,108,2 iterations,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,2,coding:utf8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,16,"input key= class for one training example, e.g. ""-1.0""",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,17,e.g. [-1.0],
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,20,"input value = feature vector for one training example, e.g. ""3.0, 7.0, 2.0""",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,24,create matrix E and vector e,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,28,create a tuple with the values to be used by reducer,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,29,and encode it with base64 to avoid potential trouble with '\t' and '\n' used,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,30,as default separators in Hadoop Streaming,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,33,"note: a single constant key ""producedkey"" sends to only one reducer",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,34,"somewhat ""atypical"" due to low degree of parallism on reducer side",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,41,"key isn't used, so ignoring it with _ (underscore).",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,43,unpickle values,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,46,create the I/mu with correct dimensions,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,51,create sumETDe with correct dimensions,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,56,note: omega = result[:-1] and gamma = result[-1],
AiLearning/src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py,57,but printing entire vector as output,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,2,coding:utf-8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,16,对数据初始化,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,22,接受输入数据流,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,23,需要 2 个参数，求数据的和与平方和,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,31,所有输入到达后开始处理,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,32,计算数据的平均值，平方的均值，并返回,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,41,从输入流中获取值,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMean.py,48,发出平均值和方差,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,2,coding:utf-8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,24,返回值中包含输入文件的每一行的数据的一个大的List,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,27,创建一个输入的数据行的列表list,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,29,将输入行分割成单独的项目并存储在列表的列表中,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,31,输入 数据的个数，n个数据的均值，n个数据平方之后的均值,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,34,累计样本总和，总和 和 平分和的总和,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,42,计算均值( varSum是计算方差的展开形式 ),
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py,45,输出 数据总量，均值，平方的均值（方差）,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,2,coding:utf-8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,28,返回一个 yield 迭代器，每次获取下一个值，节约内存。,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,31,创建一个输入的数据行的列表list,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,32,将得到的数据转化为 float 类型,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,33,获取数据的个数，即输入文件的数据的行数,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,34,将 List 转换为矩阵,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,35,将矩阵的数据分别求 平方，即 2次方,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,37,输出 数据的个数，n个数据的均值，n个数据平方之后的均值,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,38,第一行是标准输出，也就是reducer的输出,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,39,第二行识标准错误输出，即对主节点作出的响应报告，表明本节点工作正常。,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,40,【这不就是面试的装逼重点吗？如何设计监听架构细节】注意：一个好的习惯是想标准错误输出发送报告。如果某任务10分钟内没有报告输出，则将被Hadoop中止。,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py,41,计算均值,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,2,coding:utf-8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,28,iteration number,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,39,需要 2 个参数,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,40,"input: nodeId, ('w', w-vector) OR nodeId, ('x', int)",
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,43,积累 w向量,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,46,累积数据点计算,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,47,迭代次数,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,50,这用于 debug， eta未在map中使用,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,54,将数据重新形成 X 和 Y,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,56,在第一次迭代时，初始化 w,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,58,calc p=w*dataSet[key].T,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,60,确保一切数据包含相同的key,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,61,它们将在同一个 reducer,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,65,从流输入获取值,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,79,wDelta += label*dataSet,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,80,calc new: eta,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,81,calc new: w = (1.0 - 1/t)*w + (eta/k)*wDelta,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,84,发出 w,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,86,增量 T,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/mrSVM.py,87,emit random ints for mappers iid,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/wc.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/wc.py,2,coding:utf8,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/wc.py,16,I'm a generator!,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/wc.py,18,+1 for newline,
AiLearning/src/py3.x/ml/15.BigData_MapReduce/py27dbg.py,14,needs exactly 2 arguments,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,10,导入科学计算包numpy和运算符模块operator,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,53,-----------实现 classify0() 方法的第一种方式----------------------------------------------------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,54,1. 距离计算,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,56,tile生成和训练样本对应的矩阵，并与训练样本求差,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,83,取平方,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,85,将矩阵的每一行相加,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,87,开方,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,89,根据距离排序从小到大的排序，返回对应的索引位置,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,90,argsort() 是将x中的元素从小到大排列，提取其对应的index（索引），然后输出到y。,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,91,"例如：y=array([3,0,2,1,4,5]) 则，x[3]=1最小，所以y[0]=3;x[5]=5最大，所以y[5]=5。",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,92,"print 'distances=', distances",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,94,"print 'distances.argsort()=', sortedDistIndicies",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,96,2. 选择距离最小的k个点,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,99,找到该样本的类型,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,101,在字典中将该类型加一,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,102,字典的get方法,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,103,"如：list.get(k,d) 其中 get相当于一条if...else...语句,参数k在字典中，字典将返回list[k];如果参数k不在字典中则返回参数d,如果K在字典中则返回k对应的value值",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,104,"l = {5:2,3:4}",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,105,"print l.get(3,0)返回的值是4；",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,106,"Print l.get（1,0）返回值是0；",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,108,3. 排序并返回出现最多的那个类型,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,109,字典的 items() 方法，以列表返回可遍历的(键，值)元组数组。,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,110,"例如：dict = {'Name': 'Zara', 'Age': 7}   print ""Value : %s"" %  dict.items()   Value : [('Age', 7), ('Name', 'Zara')]",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,111,sorted 中的第2个参数 key=operator.itemgetter(1) 这个参数的意思是先比较第几个元素,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,112,"例如：a=[('b',2),('a',1),('c',0)]  b=sorted(a,key=operator.itemgetter(1)) >>>b=[('c',0),('a',1),('b',2)] 可以看到排序是按照后边的0,1,2进行排序的，而不是a,b,c",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,113,"b=sorted(a,key=operator.itemgetter(0)) >>>b=[('a',1),('b',2),('c',0)] 这次比较的是前边的a,b,c而不是0,1,2",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,114,"b=sorted(a,key=opertator.itemgetter(1,0)) >>>b=[('c',0),('a',1),('b',2)] 这个是先比较第2个元素，然后对第一个元素进行排序，形成多级排序。",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,118,------------------------------------------------------------------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,119,实现 classify0() 方法的第二种方式,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,121,"""""""",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,122,1. 计算距离,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,124,欧氏距离： 点到点之间的距离,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,125,第一行： 同一个点 到 dataSet的第一个点的距离。,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,126,第二行： 同一个点 到 dataSet的第二个点的距离。,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,127,...,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,128,第N行： 同一个点 到 dataSet的第N个点的距离。,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,130,"[[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,131,(A1-A2)^2+(B1-B2)^2+(c1-c2)^2,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,133,inx - dataset 使用了numpy broadcasting，见 https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,134,np.sum() 函数的使用见 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,135,"""""""",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,136,"dist = np.sum((inx - dataset)**2, axis=1)**0.5",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,138,"""""""",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,139,2. k个最近的标签,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,141,对距离排序使用numpy中的argsort函数， 见 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sort.html#numpy.sort,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,142,函数返回的是索引，因此取前k个索引使用[0 : k],
AiLearning/src/py3.x/ml/2.KNN/kNN.py,143,将这k个标签存在列表k_labels中,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,144,"""""""",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,145,k_labels = [labels[index] for index in dist.argsort()[0 : k]],
AiLearning/src/py3.x/ml/2.KNN/kNN.py,146,"""""""",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,147,3. 出现次数最多的标签即为最终类别,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,149,"使用collections.Counter可以统计各个标签的出现次数，most_common返回出现次数最多的标签tuple，例如[('lable1', 2)]，因此[0][0]可以取出标签值",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,150,"""""""",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,151,label = Counter(k_labels).most_common(1)[0][0],
AiLearning/src/py3.x/ml/2.KNN/kNN.py,152,return label,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,154,------------------------------------------------------------------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,167,----------------------------------------------------------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,175,获得文件中的数据行的行数,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,177,生成对应的空矩阵,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,178,例如：zeros(2，3)就是生成一个 2*3 的矩阵，各个位置上全是 0,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,179,prepare matrix to return,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,180,prepare labels return,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,184,str.strip([chars]) --返回移除字符串头尾指定的字符生成的新字符串,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,186,以 '\t' 切割字符串,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,188,每列的属性数据，即 features,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,190,每列的类别数据，就是 label 标签数据,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,193,返回数据矩阵returnMat和对应的类别classLabelVector,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,212,计算每种属性的最大值、最小值、范围,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,215,极差,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,217,-------第一种实现方式---start-------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,220,生成与最小值之差组成的矩阵,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,222,将最小值之差除以范围组成矩阵,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,223,element wise divide,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,224,-------第一种实现方式---end---------------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,226,# -------第二种实现方式---start---------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,227,norm_dataset = (dataset - minvalue) / ranges,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,228,# -------第二种实现方式---end---------------------------------------------,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,241,设置测试数据的的一个比例（训练数据集比例=1-hoRatio）,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,242,"测试范围,一部分测试一部分作为样本",
AiLearning/src/py3.x/ml/2.KNN/kNN.py,243,从文件中加载数据,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,244,load data setfrom file,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,245,归一化数据,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,247,m 表示数据的行数，即矩阵的第一维,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,249,设置测试的样本数量， numTestVecs:m表示训练样本的数量,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,254,对数据测试,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,292,1. 导入数据,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,294,load the training set,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,297,hwLabels存储0～9对应的index位置， trainingMat存放的每个位置对应的图片向量,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,300,take off .txt,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,303,将 32*32的矩阵->1*1024的矩阵,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,306,2. 导入测试数据,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,307,iterate through the test set,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,312,take off .txt,
AiLearning/src/py3.x/ml/2.KNN/kNN.py,323,test1(),
AiLearning/src/py3.x/ml/2.KNN/kNN.py,324,datingClassTest(),
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,21,导入一些要玩的数据,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,23,我们只采用前两个feature. 我们可以使用二维数据集避免这个丑陋的切片,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,26,"print 'X=', type(X), X",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,27,"print 'y=', type(y), y",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,29,"X = array([[-1.0, -1.1], [-1.0, -1.0], [0, 0], [1.0, 1.1], [2.0, 2.0], [2.0, 2.1]])",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,30,"y = array([0, 0, 0, 1, 1, 1])",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,32,"print 'X=', type(X), X",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,33,"print 'y=', type(y), y",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,35,网格中的步长,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,37,创建彩色的图,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,41,"cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,42,"cmap_bold = ListedColormap(['#FF0000', '#00FF00'])",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,45,我们创建了一个knn分类器的实例，并拟合数据。,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,49,绘制决策边界。为此，我们将为每个分配一个颜色,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,50,"来绘制网格中的点 [x_min, x_max]x[y_min, y_max].",
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,57,将结果放入一个彩色图中,
AiLearning/src/py3.x/ml/2.KNN/sklearn-knn-demo.py,62,绘制训练点,
AiLearning/src/py3.x/ml/4.NaiveBayes/sklearn-nb-demo.py,1,!/usr/bin/python,
AiLearning/src/py3.x/ml/4.NaiveBayes/sklearn-nb-demo.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/4.NaiveBayes/sklearn-nb-demo.py,13,GaussianNB_高斯朴素贝叶斯,
AiLearning/src/py3.x/ml/4.NaiveBayes/sklearn-nb-demo.py,25,MultinomialNB_多项朴素贝叶斯,
AiLearning/src/py3.x/ml/4.NaiveBayes/sklearn-nb-demo.py,36,BernoulliNB_伯努利朴素贝叶斯,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,10,我个人非常不喜欢 from numpy import *,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,11,"因为这样会和一些系统函数冲突，比如log, sum之类的",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,20,------项目案例1: 屏蔽社区留言板的侮辱性言论------,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,35,"1 is 侮辱性的文字, 0 is not",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,45,create empty set,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,47,| 求两个集合的并集,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,59,创建一个和词汇表等长的向量，并将其元素都设置为0,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,61,遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,66,这个后面应该注释掉，因为对你没什么用，这只是为了辅助调试的,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,67,print('the word: {} is not in my vocabulary'.format(word)),
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,83,因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,84,侮辱性文件的出现概率，即train_category中所有的1的个数，,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,85,代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,87,单词出现的次数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,88,原版,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,92,整个数据集单词出现的次数（原来是0，后面改成2了）,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,97,遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,104,后面需要改成改成取 log 函数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,121,因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,122,侮辱性文件的出现概率，即train_category中所有的1的个数，,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,123,代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,125,单词出现的次数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,126,原版，变成ones是修改版，这是为了防止数字过小溢出,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,127,p0num = np.zeros(words_num),
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,128,p1num = np.zeros(words_num),
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,131,整个数据集单词出现的次数（原来是0，后面改成2了）,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,136,遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,143,后面改成取 log 函数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,161,计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C)),
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,162,使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,163,我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,164,可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,174,注意和原来的做对比,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,189,1. 加载数据集,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,191,2. 创建单词集合,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,194,3. 计算单词是否出现并创建数据矩阵,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,198,返回m*len(vocab_list)的矩阵， 记录的都是0，1信息,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,199,"其实就是那个东西的句子向量（就是data_set里面每一行,也不算句子吧)",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,202,4. 训练数据,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,204,5. 测试数据,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,213,--------项目案例2: 使用朴素贝叶斯过滤垃圾邮件--------------,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,223,其实这里比较推荐用　\W+ 代替 \W*，,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,224,因为 \W*会match empty patten，在py3.5+之后就会出现什么问题，推荐自己修改尝试一下，可能就会re.split理解更深了,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,240,添加垃圾邮件信息,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,241,这里需要做一个说明，为什么我会使用try except 来做,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,242,"因为我们其中有几个文件的编码格式是 windows 1252　（spam: 17.txt, ham: 6.txt...)",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,243,这里其实还可以 :,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,244,import os,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,245,然后检查 os.system(' file {}.txt'.format(i))，看一下返回的是什么,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,246,如果正常能读返回的都是：　ASCII text,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,247,"对于except需要处理的都是返回： Non-ISO extended-ASCII text, with very long lines",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,256,添加非垃圾邮件,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,263,创建词汇表,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,267,"生成随机取10个数, 为了避免警告将每个数都转换为整型",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,269,并在原来的training_set中去掉这10个数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,282,开始测试,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,297,----- 项目案例3: 使用朴素贝叶斯从个人广告中获取区域倾向 ------,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,298,其中有几个函数上面都写过了，没必要再写一遍了，所以删了,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,302,RSS源分类器及高频词去除函数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,312,import feedparser # 其实呢，这一行没用到，最好删了,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,313,下面操作和上面那个 spam_test函数基本一样，理解了一个，两个都ok,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,317,找出两个中最小的一个,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,320,类别　１,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,325,类别　０,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,331,去掉高频词,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,336,获取训练数据和测试数据,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,339,"生成随机取10个数, 为了避免警告将每个数都转换为整型",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,341,并在原来的training_set中去掉这10个数,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,344,把这些训练集和测试集变成向量的形式,
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,373,"返回值都没用上，可以用_, _, _代替",
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,399,testing_naive_bayes(),
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,400,spam_test(),
AiLearning/src/py3.x/ml/4.NaiveBayes/bayes.py,401,test_rss(),
AiLearning/src/py3.x/dl/mnist.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/mnist.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/mnist.py,8,忽略警告一把梭，忽略了sigmoid函数位数溢出的警告,
AiLearning/src/py3.x/dl/mnist.py,12,数据加载器基类,
AiLearning/src/py3.x/dl/mnist.py,36,"return struct.unpack('B', byte)[0]",
AiLearning/src/py3.x/dl/mnist.py,39,图像数据加载器,
AiLearning/src/py3.x/dl/mnist.py,77,标签数据加载器,
AiLearning/src/py3.x/dl/activators.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/activators.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/activators.py,10,return weighted_input,
AiLearning/src/py3.x/dl/cnn.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/cnn.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/cnn.py,9,获取卷积区域,
AiLearning/src/py3.x/dl/cnn.py,28,获取一个2D区域的最大值所在的索引,
AiLearning/src/py3.x/dl/cnn.py,41,计算卷积,
AiLearning/src/py3.x/dl/cnn.py,62,为数组增加Zero padding,
AiLearning/src/py3.x/dl/cnn.py,93,对numpy数组进行element wise操作,
AiLearning/src/py3.x/dl/cnn.py,197,处理卷积步长，对原始sensitivity map进行扩展,
AiLearning/src/py3.x/dl/cnn.py,200,full卷积，对sensitivitiy map进行zero padding,
AiLearning/src/py3.x/dl/cnn.py,201,虽然原始输入的zero padding单元也会获得残差,
AiLearning/src/py3.x/dl/cnn.py,202,但这个残差不需要继续向上传递，因此就不计算了,
AiLearning/src/py3.x/dl/cnn.py,207,初始化delta_array，用于保存传递到上一层的,
AiLearning/src/py3.x/dl/cnn.py,208,sensitivity map,
AiLearning/src/py3.x/dl/cnn.py,210,对于具有多个filter的卷积层来说，最终传递到上一层的,
AiLearning/src/py3.x/dl/cnn.py,211,sensitivity map相当于所有的filter的,
AiLearning/src/py3.x/dl/cnn.py,212,sensitivity map之和,
AiLearning/src/py3.x/dl/cnn.py,215,将filter权重翻转180度,
AiLearning/src/py3.x/dl/cnn.py,217,计算与一个filter对应的delta_array,
AiLearning/src/py3.x/dl/cnn.py,223,将计算结果与激活函数的偏导数做element-wise乘法操作,
AiLearning/src/py3.x/dl/cnn.py,230,处理卷积步长，对原始sensitivity map进行扩展,
AiLearning/src/py3.x/dl/cnn.py,234,计算每个权重的梯度,
AiLearning/src/py3.x/dl/cnn.py,240,计算偏置项的梯度,
AiLearning/src/py3.x/dl/cnn.py,245,确定扩展后sensitivity map的大小,
AiLearning/src/py3.x/dl/cnn.py,246,计算stride为1时sensitivity map的大小,
AiLearning/src/py3.x/dl/cnn.py,251,构建新的sensitivity_map,
AiLearning/src/py3.x/dl/cnn.py,254,从原始sensitivity map拷贝误差值,
AiLearning/src/py3.x/dl/cnn.py,387,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py3.x/dl/cnn.py,390,计算forward值,
AiLearning/src/py3.x/dl/cnn.py,394,求取sensitivity map,
AiLearning/src/py3.x/dl/cnn.py,397,计算梯度,
AiLearning/src/py3.x/dl/cnn.py,400,检查梯度,
AiLearning/src/py3.x/dl/recursive.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/recursive.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/recursive.py,18,递归神经网络实现,
AiLearning/src/py3.x/dl/recursive.py,33,权重数组W,
AiLearning/src/py3.x/dl/recursive.py,36,偏置项b,
AiLearning/src/py3.x/dl/recursive.py,38,递归神经网络生成的树的根节点,
AiLearning/src/py3.x/dl/recursive.py,84,根据式2计算每个子节点的delta,
AiLearning/src/py3.x/dl/recursive.py,88,slices = [(子节点编号，子节点delta起始位置，子节点delta结束位置)],
AiLearning/src/py3.x/dl/recursive.py,92,针对每个子节点，递归调用calc_delta函数,
AiLearning/src/py3.x/dl/recursive.py,138,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py3.x/dl/recursive.py,143,计算forward值,
AiLearning/src/py3.x/dl/recursive.py,148,求取sensitivity map,
AiLearning/src/py3.x/dl/recursive.py,151,计算梯度,
AiLearning/src/py3.x/dl/recursive.py,154,检查梯度,
AiLearning/src/py3.x/dl/bp.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/bp.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/bp.py,8,sigmoid 函数,
AiLearning/src/py3.x/dl/bp.py,21,定义神经网络的节点类,
AiLearning/src/py3.x/dl/bp.py,37,设置节点所在的层的位置,
AiLearning/src/py3.x/dl/bp.py,39,设置层中的节点的索引,
AiLearning/src/py3.x/dl/bp.py,41,设置此节点的下游节点，也就是这个节点与下一层的哪个节点相连,
AiLearning/src/py3.x/dl/bp.py,43,设置此节点的上游节点，也就是哪几个节点的下游节点与此节点相连,
AiLearning/src/py3.x/dl/bp.py,45,此节点的输出,
AiLearning/src/py3.x/dl/bp.py,47,此节点真实值与计算值之间的差值,
AiLearning/src/py3.x/dl/bp.py,70,使用 list 的 append 方法来将 conn 中的节点添加到 downstream 中,
AiLearning/src/py3.x/dl/bp.py,82,使用 list 的 append 方法来将 conn 中的节点添加到 upstream 中,
AiLearning/src/py3.x/dl/bp.py,94,使用 reduce() 函数对其中的因素求和,
AiLearning/src/py3.x/dl/bp.py,96,对上游节点的 output 乘 weights 之后求和得到的结果应用 sigmoid 函数，得到当前节点的 output,
AiLearning/src/py3.x/dl/bp.py,108,根据 https://www.zybuluo.com/hanbingtao/note/476663 的 式4 计算隐藏层的delta,
AiLearning/src/py3.x/dl/bp.py,110,计算此节点的 delta,
AiLearning/src/py3.x/dl/bp.py,122,就是那输出层的 delta,
AiLearning/src/py3.x/dl/bp.py,134,打印格式：第几层 - 第几个节点，output 是多少，delta 是多少,
AiLearning/src/py3.x/dl/bp.py,136,下游节点,
AiLearning/src/py3.x/dl/bp.py,138,上游节点,
AiLearning/src/py3.x/dl/bp.py,140,将本节点 + 下游节点 + 上游节点 的信息打印出来,
AiLearning/src/py3.x/dl/bp.py,144,ConstNode 对象，为了实现一个输出恒为 1 的节点（计算偏置项 wb 时需要）,
AiLearning/src/py3.x/dl/bp.py,175,使用 list 的 append 方法将包含下游节点的 conn 添加到 downstream 中,
AiLearning/src/py3.x/dl/bp.py,188,使用我们的 公式 4 来计算下游节点的 delta，求和,
AiLearning/src/py3.x/dl/bp.py,190,计算隐藏层的本节点的 delta,
AiLearning/src/py3.x/dl/bp.py,203,将节点的信息打印出来,
AiLearning/src/py3.x/dl/bp.py,204,格式 第几层-第几个节点的 output,
AiLearning/src/py3.x/dl/bp.py,206,此节点的下游节点的信息,
AiLearning/src/py3.x/dl/bp.py,208,将此节点与下游节点的信息组合，一起打印出来,
AiLearning/src/py3.x/dl/bp.py,212,神经网络的层对象，负责初始化一层。此外，作为 Node 的集合对象，提供对 Node 集合的操作,
AiLearning/src/py3.x/dl/bp.py,229,设置 层的索引,
AiLearning/src/py3.x/dl/bp.py,231,设置层中的节点的 list,
AiLearning/src/py3.x/dl/bp.py,233,将 Node 节点添加到 nodes 中,
AiLearning/src/py3.x/dl/bp.py,236,将 ConstNode 节点也添加到 nodes 中,
AiLearning/src/py3.x/dl/bp.py,248,设置输入层中各个节点的 output,
AiLearning/src/py3.x/dl/bp.py,261,遍历本层的所有节点（除去最后一个节点，因为它是恒为常数的偏置项b）,
AiLearning/src/py3.x/dl/bp.py,262,调用节点的 calc_output 方法来计算输出向量,
AiLearning/src/py3.x/dl/bp.py,275,遍历层的所有的节点 nodes，将节点信息打印出来,
AiLearning/src/py3.x/dl/bp.py,280,Connection 对象类，主要负责记录连接的权重，以及这个连接所关联的上下游的节点,
AiLearning/src/py3.x/dl/bp.py,296,设置上游节点,
AiLearning/src/py3.x/dl/bp.py,298,设置下游节点,
AiLearning/src/py3.x/dl/bp.py,300,设置权重，这里设置的权重是 -0.1 到 0.1 之间的任何数,
AiLearning/src/py3.x/dl/bp.py,302,设置梯度 为 0.0,
AiLearning/src/py3.x/dl/bp.py,314,下游节点的 delta * 上游节点的 output 计算得到梯度,
AiLearning/src/py3.x/dl/bp.py,326,调用计算梯度的函数来将梯度计算出来,
AiLearning/src/py3.x/dl/bp.py,328,使用梯度下降算法来更新权重,
AiLearning/src/py3.x/dl/bp.py,351,格式为：上游节点的层的索引+上游节点的节点索引 ---> 下游节点的层的索引+下游节点的节点索引，最后一个数是权重,
AiLearning/src/py3.x/dl/bp.py,361,Connections 对象，提供 Connection 集合操作。,
AiLearning/src/py3.x/dl/bp.py,376,初始化一个列表 list,
AiLearning/src/py3.x/dl/bp.py,403,Network 对象，提供相应 API,
AiLearning/src/py3.x/dl/bp.py,418,初始化 connections，使用的是 Connections 对象,
AiLearning/src/py3.x/dl/bp.py,420,初始化 layers,
AiLearning/src/py3.x/dl/bp.py,422,我们的神经网络的层数,
AiLearning/src/py3.x/dl/bp.py,424,节点数,
AiLearning/src/py3.x/dl/bp.py,426,遍历所有的层，将每层信息添加到 layers 中去,
AiLearning/src/py3.x/dl/bp.py,429,遍历除去输出层之外的所有层，将连接信息添加到 connections 对象中,
AiLearning/src/py3.x/dl/bp.py,432,遍历 connections，将 conn 添加到 connections 中,
AiLearning/src/py3.x/dl/bp.py,435,为下游节点添加上游节点为 conn,
AiLearning/src/py3.x/dl/bp.py,437,为上游节点添加下游节点为 conn,
AiLearning/src/py3.x/dl/bp.py,453,循环迭代 epoch 次,
AiLearning/src/py3.x/dl/bp.py,455,遍历每个训练样本,
AiLearning/src/py3.x/dl/bp.py,457,使用此样本进行训练（一条样本进行训练）,
AiLearning/src/py3.x/dl/bp.py,459,print 'sample %d training finished' % d,
AiLearning/src/py3.x/dl/bp.py,472,调用 Network 的 predict 方法，对这个样本进行预测,
AiLearning/src/py3.x/dl/bp.py,474,计算根据此样本得到的结果的 delta,
AiLearning/src/py3.x/dl/bp.py,476,更新权重,
AiLearning/src/py3.x/dl/bp.py,488,获取输出层的所有节点,
AiLearning/src/py3.x/dl/bp.py,490,遍历所有的 label,
AiLearning/src/py3.x/dl/bp.py,492,计算输出层节点的 delta,
AiLearning/src/py3.x/dl/bp.py,494,"这个用法就是切片的用法， [-2::-1] 就是将 layers 这个数组倒过来，从没倒过来的时候的倒数第二个元素开始，到翻转过来的倒数第一个数，比如这样：aaa = [1,2,3,4,5,6,7,8,9],bbb = aaa[-2::-1] ==> bbb = [8, 7, 6, 5, 4, 3, 2, 1]",
AiLearning/src/py3.x/dl/bp.py,495,实际上就是除掉输出层之外的所有层按照相反的顺序进行遍历,
AiLearning/src/py3.x/dl/bp.py,497,遍历每层的所有节点,
AiLearning/src/py3.x/dl/bp.py,499,计算隐藏层的 delta,
AiLearning/src/py3.x/dl/bp.py,511,按照正常顺序遍历除了输出层的层,
AiLearning/src/py3.x/dl/bp.py,513,遍历每层的所有节点,
AiLearning/src/py3.x/dl/bp.py,515,遍历节点的下游节点,
AiLearning/src/py3.x/dl/bp.py,517,根据下游节点来更新连接的权重,
AiLearning/src/py3.x/dl/bp.py,529,按照正常顺序遍历除了输出层之外的层,
AiLearning/src/py3.x/dl/bp.py,531,遍历层中的所有节点,
AiLearning/src/py3.x/dl/bp.py,533,遍历节点的下游节点,
AiLearning/src/py3.x/dl/bp.py,535,计算梯度,
AiLearning/src/py3.x/dl/bp.py,548,调用 predict() 方法，利用样本的特征数据对样本进行预测,
AiLearning/src/py3.x/dl/bp.py,550,计算 delta,
AiLearning/src/py3.x/dl/bp.py,552,计算梯度,
AiLearning/src/py3.x/dl/bp.py,564,首先为输入层设置输出值output为样本的输入向量，即不发生任何变化,
AiLearning/src/py3.x/dl/bp.py,566,遍历除去输入层开始到最后一层,
AiLearning/src/py3.x/dl/bp.py,568,计算 output,
AiLearning/src/py3.x/dl/bp.py,570,将计算得到的输出，也就是我们的预测值返回,
AiLearning/src/py3.x/dl/bp.py,582,遍历所有的 layers,
AiLearning/src/py3.x/dl/bp.py,584,将所有的层的信息打印出来,
AiLearning/src/py3.x/dl/bp.py,588,# ------------------------- 至此，基本上我们把 我们的神经网络实现完成，下面还会介绍一下对应的梯度检查相关的算法，现在我们首先回顾一下我们上面写道的类及他们的作用 ------------------------,
AiLearning/src/py3.x/dl/bp.py,637,#--------------------------------------回顾完成了，有些问题可能还是没有弄懂，没事，我们接着看下面---------------------------------------------,
AiLearning/src/py3.x/dl/bp.py,657,初始化 16 进制的数，用来判断位的，分别是,
AiLearning/src/py3.x/dl/bp.py,658,0x1 ---- 00000001,
AiLearning/src/py3.x/dl/bp.py,659,0x2 ---- 00000010,
AiLearning/src/py3.x/dl/bp.py,660,0x4 ---- 00000100,
AiLearning/src/py3.x/dl/bp.py,661,0x8 ---- 00001000,
AiLearning/src/py3.x/dl/bp.py,662,0x10 --- 00010000,
AiLearning/src/py3.x/dl/bp.py,663,0x20 --- 00100000,
AiLearning/src/py3.x/dl/bp.py,664,0x40 --- 01000000,
AiLearning/src/py3.x/dl/bp.py,665,0x80 --- 10000000,
AiLearning/src/py3.x/dl/bp.py,677,此方法就相当于判断一个 8 位的向量，哪一位上有数字，如果有就将这个数设置为  0.9 ，否则，设置为 0.1，通俗比较来说，就是我们这里用 0.9 表示 1，用 0.1 表示 0,
AiLearning/src/py3.x/dl/bp.py,689,进行二分类，大于 0.5 就设置为 1，小于 0.5 就设置为 0,
AiLearning/src/py3.x/dl/bp.py,691,遍历 mask,
AiLearning/src/py3.x/dl/bp.py,694,将结果相加得到最终的预测结果,
AiLearning/src/py3.x/dl/bp.py,723,计算网络误差,
AiLearning/src/py3.x/dl/bp.py,726,获取网络在当前样本下每个连接的梯度,
AiLearning/src/py3.x/dl/bp.py,729,对每个权重做梯度检查,
AiLearning/src/py3.x/dl/bp.py,731,获取指定连接的梯度,
AiLearning/src/py3.x/dl/bp.py,734,增加一个很小的值，计算网络的误差,
AiLearning/src/py3.x/dl/bp.py,739,减去一个很小的值，计算网络的误差,
AiLearning/src/py3.x/dl/bp.py,740,刚才加过了一次，因此这里需要减去2倍,
AiLearning/src/py3.x/dl/bp.py,743,根据式6计算期望的梯度值,
AiLearning/src/py3.x/dl/bp.py,746,打印,
AiLearning/src/py3.x/dl/bp.py,759,调用 Normalizer() 类,
AiLearning/src/py3.x/dl/bp.py,761,初始化一个 list，用来存储后面的数据,
AiLearning/src/py3.x/dl/bp.py,764,0 到 256 ，其中以 8 为步长,
AiLearning/src/py3.x/dl/bp.py,766,调用 normalizer 对象的 norm 方法,
AiLearning/src/py3.x/dl/bp.py,768,在 data_set 中 append n,
AiLearning/src/py3.x/dl/bp.py,770,在 labels 中 append n,
AiLearning/src/py3.x/dl/bp.py,772,将它们返回,
AiLearning/src/py3.x/dl/bp.py,785,获取训练数据集,
AiLearning/src/py3.x/dl/bp.py,789,调用 network 中的 train方法来训练我们的神经网络,
AiLearning/src/py3.x/dl/bp.py,794,此函数不明觉厉，但是传参就有问题，如果跑不通就把这段代码注释掉吧。。。,
AiLearning/src/py3.x/dl/bp.py,805,调用 Normalizer() 类,
AiLearning/src/py3.x/dl/bp.py,808,调用 norm 方法，对数据进行规范化,
AiLearning/src/py3.x/dl/bp.py,811,对测试数据进行预测,
AiLearning/src/py3.x/dl/bp.py,813,将结果打印出来,
AiLearning/src/py3.x/dl/bp.py,843,创建一个有 3 层的网络，每层有 2 个节点,
AiLearning/src/py3.x/dl/bp.py,845,样本的特征,
AiLearning/src/py3.x/dl/bp.py,847,样本对应的标签,
AiLearning/src/py3.x/dl/bp.py,849,使用梯度检查来查看是否正确,
AiLearning/src/py3.x/dl/bp.py,862,初始化一个神经网络，输入层 8 个节点，隐藏层 3 个节点，输出层 8 个节点,
AiLearning/src/py3.x/dl/bp.py,864,训练我们的神经网络,
AiLearning/src/py3.x/dl/bp.py,866,将我们的神经网络的信息打印出来,
AiLearning/src/py3.x/dl/bp.py,868,打印出神经网络的正确率,
AiLearning/src/py3.x/dl/lstm.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/lstm.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/lstm.py,17,门的激活函数,
AiLearning/src/py3.x/dl/lstm.py,19,输出的激活函数,
AiLearning/src/py3.x/dl/lstm.py,21,当前时刻初始化为t0,
AiLearning/src/py3.x/dl/lstm.py,23,各个时刻的单元状态向量c,
AiLearning/src/py3.x/dl/lstm.py,25,各个时刻的输出向量h,
AiLearning/src/py3.x/dl/lstm.py,27,各个时刻的遗忘门f,
AiLearning/src/py3.x/dl/lstm.py,29,各个时刻的输入门i,
AiLearning/src/py3.x/dl/lstm.py,31,各个时刻的输出门o,
AiLearning/src/py3.x/dl/lstm.py,33,各个时刻的即时状态c~,
AiLearning/src/py3.x/dl/lstm.py,35,"遗忘门权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py3.x/dl/lstm.py,38,"输入门权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py3.x/dl/lstm.py,41,"输出门权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py3.x/dl/lstm.py,44,"单元状态权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py3.x/dl/lstm.py,73,遗忘门,
AiLearning/src/py3.x/dl/lstm.py,77,输入门,
AiLearning/src/py3.x/dl/lstm.py,81,输出门,
AiLearning/src/py3.x/dl/lstm.py,85,即时状态,
AiLearning/src/py3.x/dl/lstm.py,89,单元状态,
AiLearning/src/py3.x/dl/lstm.py,92,输出,
AiLearning/src/py3.x/dl/lstm.py,100,上次的LSTM输出,
AiLearning/src/py3.x/dl/lstm.py,131,初始化各个时刻的误差项,
AiLearning/src/py3.x/dl/lstm.py,132,输出误差项,
AiLearning/src/py3.x/dl/lstm.py,133,输出门误差项,
AiLearning/src/py3.x/dl/lstm.py,134,输入门误差项,
AiLearning/src/py3.x/dl/lstm.py,135,遗忘门误差项,
AiLearning/src/py3.x/dl/lstm.py,136,即时输出误差项,
AiLearning/src/py3.x/dl/lstm.py,138,保存从上一层传递下来的当前时刻的误差项,
AiLearning/src/py3.x/dl/lstm.py,141,迭代计算每个时刻的误差项,
AiLearning/src/py3.x/dl/lstm.py,160,获得k时刻前向计算的值,
AiLearning/src/py3.x/dl/lstm.py,170,根据式9计算delta_o,
AiLearning/src/py3.x/dl/lstm.py,189,保存全部delta值,
AiLearning/src/py3.x/dl/lstm.py,197,初始化遗忘门权重梯度矩阵和偏置项,
AiLearning/src/py3.x/dl/lstm.py,200,初始化输入门权重梯度矩阵和偏置项,
AiLearning/src/py3.x/dl/lstm.py,203,初始化输出门权重梯度矩阵和偏置项,
AiLearning/src/py3.x/dl/lstm.py,206,初始化单元状态权重梯度矩阵和偏置项,
AiLearning/src/py3.x/dl/lstm.py,210,计算对上一次输出h的权重梯度,
AiLearning/src/py3.x/dl/lstm.py,212,计算各个时刻的梯度,
AiLearning/src/py3.x/dl/lstm.py,218,实际梯度是各时刻梯度之和,
AiLearning/src/py3.x/dl/lstm.py,228,计算对本次输入x的权重梯度,
AiLearning/src/py3.x/dl/lstm.py,263,当前时刻初始化为t0,
AiLearning/src/py3.x/dl/lstm.py,265,各个时刻的单元状态向量c,
AiLearning/src/py3.x/dl/lstm.py,267,各个时刻的输出向量h,
AiLearning/src/py3.x/dl/lstm.py,269,各个时刻的遗忘门f,
AiLearning/src/py3.x/dl/lstm.py,271,各个时刻的输入门i,
AiLearning/src/py3.x/dl/lstm.py,273,各个时刻的输出门o,
AiLearning/src/py3.x/dl/lstm.py,275,各个时刻的即时状态c~,
AiLearning/src/py3.x/dl/lstm.py,290,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py3.x/dl/lstm.py,295,计算forward值,
AiLearning/src/py3.x/dl/lstm.py,300,求取sensitivity map,
AiLearning/src/py3.x/dl/lstm.py,303,计算梯度,
AiLearning/src/py3.x/dl/lstm.py,306,检查梯度,
AiLearning/src/py3.x/dl/perceptron.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/perceptron.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/perceptron.py,29,设置的激活函数,
AiLearning/src/py3.x/dl/perceptron.py,31,权重向量初始化为 0,
AiLearning/src/py3.x/dl/perceptron.py,33,偏置项初始化为 0,
AiLearning/src/py3.x/dl/perceptron.py,56,将输入向量的计算结果返回,
AiLearning/src/py3.x/dl/perceptron.py,57,调用 激活函数 activator ，将输入向量输入，计算感知器的结果,
AiLearning/src/py3.x/dl/perceptron.py,58,reduce() 函数是 python 2 的内置函数，从 python 3 开始移到了 functools 模块,
AiLearning/src/py3.x/dl/perceptron.py,59,"reduce() 从左到右对一个序列的项累计地应用有两个参数的函数，以此合并序列到一个单一值，例如 reduce(lambda x,y: x+y, [1,2,3,4,5]) 计算的就是 ((((1+2)+3)+4)+5)",
AiLearning/src/py3.x/dl/perceptron.py,60,"map() 接收一个函数 f 和一个 list ，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 返回。比如我们的 f 函数是计算平方， map(f, [1,2,3,4,5]) ===> 返回 [1,4,9,16,25]",
AiLearning/src/py3.x/dl/perceptron.py,61,"zip() 接收任意多个（包括 0 个和 1个）序列作为参数，返回一个 tuple 列表。例：x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]",
AiLearning/src/py3.x/dl/perceptron.py,68,"此处python3 lambda无法传入一个tuple的两个变量，因此将tuple当作一个整体，tp[0]为input_vec,tp[1]为self.weights",
AiLearning/src/py3.x/dl/perceptron.py,70,还有一种更加简洁明了的写法，很清楚明白,
AiLearning/src/py3.x/dl/perceptron.py,71,"return self.activator(sum([x*w for (x,w) in zip(input_vec,self.weights)])+self.bias)",
AiLearning/src/py3.x/dl/perceptron.py,99,"zip() 接收任意多个（包括 0 个和 1个）序列作为参数，返回一个 tuple 列表。例：x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]",
AiLearning/src/py3.x/dl/perceptron.py,101,对每个样本，按照感知器规则更新权重,
AiLearning/src/py3.x/dl/perceptron.py,103,计算感知器在当前权重下的输出,
AiLearning/src/py3.x/dl/perceptron.py,105,更新权重,
AiLearning/src/py3.x/dl/perceptron.py,120,利用感知器规则更新权重,
AiLearning/src/py3.x/dl/perceptron.py,123,"map() 接收一个函数 f 和一个 list ，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 返回。比如我们的 f 函数是计算平方， map(f, [1,2,3,4,5]) ===> 返回 [1,4,9,16,25]",
AiLearning/src/py3.x/dl/perceptron.py,124,"zip() 接收任意多个（包括 0 个和 1个）序列作为参数，返回一个 tuple 列表。例：x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]",
AiLearning/src/py3.x/dl/perceptron.py,125,此处python3必须对map函数进行list操作，不然 self.weights为map类型，最后无法打印出具体数值,
AiLearning/src/py3.x/dl/perceptron.py,131,更新 bias,
AiLearning/src/py3.x/dl/perceptron.py,158,构建训练数据，输入向量的列表,
AiLearning/src/py3.x/dl/perceptron.py,160,期望的输出列表，也就是上面的输入向量的列表中数据对应的标签，是一一对应的,
AiLearning/src/py3.x/dl/perceptron.py,174,创建感知器，输入参数的个数是 2 个（因为 and 是个二元函数），激活函数为 f,
AiLearning/src/py3.x/dl/perceptron.py,176,进行训练，迭代 10 轮，学习速率是我们设定的 rate ，为 0.1,
AiLearning/src/py3.x/dl/perceptron.py,179,返回训练好的感知器,
AiLearning/src/py3.x/dl/perceptron.py,191,训练 and 感知器,
AiLearning/src/py3.x/dl/perceptron.py,193,打印训练获得的权重,
AiLearning/src/py3.x/dl/perceptron.py,195,测试,
AiLearning/src/py3.x/dl/linear_unit.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/linear_unit.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/linear_unit.py,4,引入 Perceptron 类,
AiLearning/src/py3.x/dl/linear_unit.py,7,定义激活函数 f,
AiLearning/src/py3.x/dl/linear_unit.py,28,初始化我们的感知器类，设置输入参数的个数 input_num 和 激活函数 f,
AiLearning/src/py3.x/dl/linear_unit.py,31,构造简单的数据集,
AiLearning/src/py3.x/dl/linear_unit.py,42,构建数据集，输入向量列表，每一项是工作年限,
AiLearning/src/py3.x/dl/linear_unit.py,44,期望的输出列表，也就是输入向量的对应的标签，与工作年限对应的收入年薪,
AiLearning/src/py3.x/dl/linear_unit.py,49,使用我们的训练数据集对线性单元进行训练,
AiLearning/src/py3.x/dl/linear_unit.py,59,创建感知器对象，输入参数的个数也就是特征数为 1（工作年限）,
AiLearning/src/py3.x/dl/linear_unit.py,61,获取构建的数据集,
AiLearning/src/py3.x/dl/linear_unit.py,63,训练感知器，迭代 10 轮，学习率为 0.01,
AiLearning/src/py3.x/dl/linear_unit.py,65,返回训练好的线性单元,
AiLearning/src/py3.x/dl/linear_unit.py,69,将图像画出来,
AiLearning/src/py3.x/dl/linear_unit.py,79,引入绘图的库,
AiLearning/src/py3.x/dl/linear_unit.py,81,获取训练数据：特征 input_vecs 与 对应的标签 labels,
AiLearning/src/py3.x/dl/linear_unit.py,83,figure() 创建一个 Figure 对象，与用户交互的整个窗口，这个 figure 中容纳着 subplots,
AiLearning/src/py3.x/dl/linear_unit.py,85,在 figure 对象中创建 1行1列中的第一个图,
AiLearning/src/py3.x/dl/linear_unit.py,87,"scatter(x, y) 绘制散点图，其中的 x,y 是相同长度的数组序列",
AiLearning/src/py3.x/dl/linear_unit.py,91,设置权重,
AiLearning/src/py3.x/dl/linear_unit.py,93,设置偏置项,
AiLearning/src/py3.x/dl/linear_unit.py,98,将图画出来,
AiLearning/src/py3.x/dl/linear_unit.py,101,将最终的图展示出来,
AiLearning/src/py3.x/dl/linear_unit.py,114,首先训练我们的线性单元,
AiLearning/src/py3.x/dl/linear_unit.py,116,打印训练获得的权重 和 偏置,
AiLearning/src/py3.x/dl/linear_unit.py,118,测试,
AiLearning/src/py3.x/dl/linear_unit.py,127,定义激活函数f,
AiLearning/src/py3.x/dl/rnn.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/rnn.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/rnn.py,18,当前时刻初始化为t0,
AiLearning/src/py3.x/dl/rnn.py,19,保存各个时刻的state,
AiLearning/src/py3.x/dl/rnn.py,21,初始化s0,
AiLearning/src/py3.x/dl/rnn.py,23,初始化U,
AiLearning/src/py3.x/dl/rnn.py,25,初始化W,
AiLearning/src/py3.x/dl/rnn.py,52,用来保存各个时刻的误差项,
AiLearning/src/py3.x/dl/rnn.py,57,迭代计算每个时刻的误差项,
AiLearning/src/py3.x/dl/rnn.py,73,保存各个时刻的权重梯度,
AiLearning/src/py3.x/dl/rnn.py,79,实际的梯度是各个时刻梯度之和,
AiLearning/src/py3.x/dl/rnn.py,82,[0]被初始化为0且没有被修改过,
AiLearning/src/py3.x/dl/rnn.py,93,当前时刻初始化为t0,
AiLearning/src/py3.x/dl/rnn.py,94,保存各个时刻的state,
AiLearning/src/py3.x/dl/rnn.py,96,初始化s0,
AiLearning/src/py3.x/dl/rnn.py,110,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py3.x/dl/rnn.py,115,计算forward值,
AiLearning/src/py3.x/dl/rnn.py,120,求取sensitivity map,
AiLearning/src/py3.x/dl/rnn.py,123,计算梯度,
AiLearning/src/py3.x/dl/rnn.py,126,检查梯度,
AiLearning/src/py3.x/dl/fc.py,1,!/usr/bin/env python,
AiLearning/src/py3.x/dl/fc.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py3.x/dl/fc.py,11,全连接层实现类,
AiLearning/src/py3.x/dl/fc.py,24,权重数组W,
AiLearning/src/py3.x/dl/fc.py,27,偏置项b,
AiLearning/src/py3.x/dl/fc.py,29,输出向量,
AiLearning/src/py3.x/dl/fc.py,37,式2,
AiLearning/src/py3.x/dl/fc.py,47,式8,
AiLearning/src/py3.x/dl/fc.py,64,神经网络类,
AiLearning/src/py3.x/dl/fc.py,136,获取网络在当前样本下每个连接的梯度,
AiLearning/src/py3.x/dl/fc.py,140,检查梯度,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,2,coding: utf8,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,13,逻辑回归中的 L1 惩罚和稀缺性 L1 Penalty and Sparsity in Logistic Regression,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,77,具有 L1-逻辑回归的路径,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,121,绘制多项式和一对二的逻辑回归 Plot multinomial and One-vs-Rest Logistic Regression,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,184,Logistic Regression 3-class Classifier 逻辑回归 3-类 分类器,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,192,引入一些数据来玩,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,194,我们只采用样本数据的前两个feature,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,198,网格中的步长,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,202,我们创建了一个 Neighbours Classifier 的实例，并拟合数据。,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,205,"绘制决策边界。为此我们将为网格 [x_min, x_max]x[y_min, y_max] 中的每个点分配一个颜色。",
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,211,将结果放入彩色图中,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,216,将训练点也同样放入彩色图中,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,228,Logistic function 逻辑回归函数,
AiLearning/src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py,229,这个类似于咱们之前讲解 logistic 回归的 Sigmoid 函数，模拟的阶跃函数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,2,coding: utf8,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,14,---------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,15,使用 Logistic 回归在简单数据集上的分类,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,18,解析数据,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,29,dataMat为原始数据， labelMat为原始数据的标签,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,36,这里如果就一个空的元素，则跳过本次循环,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,37,为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,43,sigmoid跳跃函数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,45,return 1.0 / (1 + exp(-inX)),
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,47,Tanh是Sigmoid的变形，与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好。,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,51,正常的处理方案,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,52,两个参数：第一个参数==> dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,53,第二个参数==> classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,65,"转化为矩阵[[1,1,2],[1,1,2]....]",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,66,转换为 NumPy 矩阵,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,67,"转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,68,transpose() 行列转置函数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,69,将行向量转化为列向量   =>  矩阵的转置,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,70,首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,71,m->数据量，样本数 n->特征数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,73,"print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,74,alpha代表向目标移动的步长,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,76,迭代次数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,78,"生成一个长度和特征数相同的矩阵，此处n为3 -> [[1],[1],[1]]",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,79,"weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,81,heavy on matrix operations,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,82,m*3 的矩阵 * 3*1 的单位矩阵 ＝ m*1的矩阵,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,83,那么乘上单位矩阵的意义，就代表：通过公式得到的理论值,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,84,参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,85,"print 'dataMatrix====', dataMatrix",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,86,"print 'weights====', weights",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,87,n*3   *  3*1  = n*1,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,88,矩阵乘法,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,89,"print 'hhhhhhh====', h",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,90,labelMat是实际值,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,91,向量相减,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,92,"0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,93,矩阵乘法，最后得到回归系数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,97,随机梯度下降,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,98,梯度下降优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,99,随机梯度下降一次只用一个样本点来更新回归系数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,112,n*1的矩阵,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,113,函数ones创建一个全1的数组,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,114,初始化长度为n的数组，元素全部为 1,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,116,"sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,118,"print 'dataMatrix[i]===', dataMatrix[i]",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,119,计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,121,0.01*(1*1)*(1*n),
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,122,"print weights, ""*"" * 10, dataMatrix[i], ""*"" * 10, error",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,127,随机梯度下降算法（随机化）,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,140,创建与列数相同的矩阵的系数矩阵，所有的元素都是1,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,141,"随机梯度, 循环150,观察是否收敛",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,143,"[0, 1, 2 .. m-1]",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,146,i和j的不断增大，导致alpha的值不断减少，但是不为0,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,149,alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,150,随机产生一个 0～len()之间的一个值,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,151,"random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,153,sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,156,"print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,162,可视化展示,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,208,1.收集并准备数据,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,211,"print dataMat, '---\n', labelMat",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,212,"2.训练模型，  f(x)=a1*x1+b2*x2+..+nn*xn中 (a1,b2, .., nn).T的矩阵值",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,213,因为数组没有是复制n份， array的乘法就是乘法,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,215,print dataArr,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,216,"weights = gradAscent(dataArr, labelMat)",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,217,"weights = stocGradAscent0(dataArr, labelMat)",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,219,"print '*'*30, weights",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,221,数据可视化,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,225,--------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,226,从疝气病症预测病马的死亡率,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,227,分类函数，根据回归系数和特征向量来计算 Sigmoid的值,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,244,"打开测试集和训练集,并对数据进行格式化处理",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,258,解析训练数据集中的数据特征和Labels,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,259,trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,267,使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,269,"trainWeights = stocGradAscent0(array(trainingSet), trainingLabels)",
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,272,读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,287,调用 colicTest() 10次并求结果的平均值,
AiLearning/src/py2.x/ml/5.Logistic/logistic.py,298,multiTest(),
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,2,coding: utf-8,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,16,利用SVD提高推荐效果，菜肴矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,31,书上代码给的示例矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,56,# 原矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,57,"return[[1, 1, 1, 0, 0],",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,58,"[2, 2, 2, 0, 0],",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,59,"[1, 1, 1, 0, 0],",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,60,"[5, 5, 5, 0, 0],",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,61,"[1, 1, 0, 2, 2],",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,62,"[0, 0, 0, 3, 3],",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,63,"[0, 0, 0, 1, 1]]",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,65,原矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,72,相似度计算，假定inA和inB 都是列向量,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,73,基于欧氏距离,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,78,pearsSim()函数会检查是否存在3个或更多的点。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,79,"corrcoef直接计算皮尔逊相关系数，范围[-1, 1]，归一化后[0, 1]",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,81,如果不存在，该函数返回1.0，此时两个向量完全相关。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,87,计算余弦相似度，如果夹角为90度，相似度为0；如果两个向量的方向相同，相似度为1.0,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,94,基于物品相似度的推荐引擎,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,106,得到数据集中的物品数目,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,108,初始化两个评分值,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,111,遍历行中的每个物品（对用户评过分的物品进行遍历，并将它与其他物品进行比较）,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,114,如果某个物品的评分值为0，则跳过这个物品,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,117,寻找两个用户都评级的物品,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,118,变量 overLap 给出的是两个物品当中已经被评分的那个元素的索引ID,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,119,logical_and 计算x1和x2元素的真值。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,121,如果相似度为0，则两着没有任何重合元素，终止本次循环,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,124,如果存在重合的物品，则基于这些重合物重新计算相似度。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,127,"print 'the %d and %d similarity is : %f'(iten,j,similarity)",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,128,相似度会不断累加，每次计算时还考虑相似度和当前用户评分的乘积,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,129,similarity  用户相似度，   userRating 用户评分,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,134,通过除以所有的评分总和，对上述相似度评分的乘积进行归一化，使得最后评分在0~5之间，这些评分用来对预测值进行排序,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,139,基于SVD的评分估计,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,140,在recommend() 中，这个函数用于替换对standEst()的调用，该函数对给定用户给定物品构建了一个评分估计值,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,152,物品数目,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,154,对数据集进行SVD分解,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,157,奇异值分解,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,158,在SVD分解之后，我们只利用包含了90%能量值的奇异值，这些奇异值会以NumPy数组的形式得以保存,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,161,# 分析 Sigma 的长度取值,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,162,"analyse_data(Sigma, 20)",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,164,如果要进行矩阵运算，就必须要用这些奇异值构建出一个对角矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,167,利用U矩阵将物品转换到低维空间中，构建转换后的物品(物品+4个主要的特征),
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,175,对于给定的用户，for循环在用户对应行的元素上进行遍历,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,176,这和standEst()函数中的for循环的目的一样，只不过这里的相似度计算时在低维空间下进行的。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,181,相似度的计算方法也会作为一个参数传递给该函数,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,183,for 循环中加入了一条print语句，以便了解相似度计算的进展情况。如果觉得累赘，可以去掉,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,185,对相似度不断累加求和,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,187,对相似度及对应评分值的乘积求和,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,192,计算估计评分,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,196,recommend()函数，就是推荐引擎，它默认调用standEst()函数，产生了最高的N个推荐结果。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,197,如果不指定N的大小，则默认值为3。该函数另外的参数还包括相似度计算方法和估计方法,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,209,寻找未评级的物品,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,210,对给定的用户建立一个未评分的物品列表,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,212,如果不存在未评分物品，那么就退出函数,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,215,物品的编号和评分值,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,217,在未评分物品上进行循环,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,219,获取 item 该物品的评分,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,222,按照评分得分 进行逆排序，获取前N个未评级物品进行推荐,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,233,总方差的集合（总能量值）,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,246,图像压缩函数,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,247,加载并转换数据,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,250,打开文本文件，并从文件以数组方式读入字符,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,256,矩阵调入后，就可以在屏幕上输出该矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,261,打印矩阵,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,263,由于矩阵保护了浮点数，因此定义浅色和深色，遍历所有矩阵元素，当元素大于阀值时打印1，否则打印0,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,273,实现图像压缩，允许基于任意给定的奇异值数目来重构图像,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,281,构建一个列表,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,285,对原始图像进行SVD分解并重构图像e,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,288,通过Sigma 重新构成SigRecom来实现,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,289,Sigma是一个对角矩阵，因此需要建立一个全0矩阵，然后将前面的那些奇异值填充到对角线上。,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,291,"SigRecon = mat(zeros((numSV, numSV)))",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,292,for k in range(numSV):,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,293,"SigRecon[k, k] = Sigma[k]",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,295,分析插入的 Sigma 长度,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,306,# 对矩阵进行SVD分解(用python实现SVD),
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,307,Data = loadExData(),
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,308,"print 'Data:', Data",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,309,"U, Sigma, VT = linalg.svd(Data)",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,310,# 打印Sigma的结果，因为前3个数值比其他的值大了很多，为9.72140007e+00，5.29397912e+00，6.84226362e-01,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,311,# 后两个值比较小，每台机器输出结果可能有不同可以将这两个值去掉,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,312,"print 'U:', U",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,313,"print 'Sigma', Sigma",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,314,"print 'VT:', VT",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,315,"print 'VT:', VT.T",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,317,# 重构一个3x3的矩阵Sig3,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,318,"Sig3 = mat([[Sigma[0], 0, 0], [0, Sigma[1], 0], [0, 0, Sigma[2]]])",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,319,"print U[:, :3] * Sig3 * VT[:3, :]",
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,338,计算相似度的方法,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,340,print myMat,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,341,计算相似度的第一种方式,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,343,计算相似度的第二种方式,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,346,默认推荐（菜馆菜肴推荐示例）,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,362,压缩图片,
AiLearning/src/py2.x/ml/14.SVD/svdRecommend.py,363,imgCompress(2),
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,2,coding:utf8,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,26,general function to parse tab -delimited floats,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,28,get number of fields,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,53,默认都是1,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,55,"dataMat[:, dimen] 表示数据集中第dimen列的所有值",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,56,threshIneq == 'lt'表示修改左边的值，gt表示修改右边的值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,57,"print '-----', threshIneq, dataMat[:, dimen], threshVal",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,77,转换数据,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,80,m行 n列,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,83,初始化数据,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,87,初始化的最小误差为无穷大,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,90,循环所有的feature列，将列切分成 若干份，每一段以最左边的点作为分类节点,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,94,"print 'rangeMin=%s, rangeMax=%s' % (rangeMin, rangeMax)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,95,计算每一份的元素个数,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,97,例如： 4=(10-1)/2   那么  1-4(-1次)   1(0次)  1+1*4(1次)   1+2*4(2次),
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,98,所以： 循环 -1/0/1/2,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,100,go over less than and greater than,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,102,如果是-1，那么得到rangeMin-stepSize; 如果是numSteps，那么得到rangeMax,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,104,对单层决策树进行简单分类，得到预测的分类值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,106,print predictedVals,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,108,正确为0，错误为1,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,110,计算 平均每个特征的概率0.2*错误概率的总和为多少，就知道错误率多高,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,111,例如： 一个都没错，那么错误率= 0.2*0=0 ， 5个都错，那么错误率= 0.2*5=1， 只错3个，那么错误率= 0.2*3=0.6,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,120,"print ""split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f"" % (i, threshVal, inequal, weightedError)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,128,bestStump 表示分类器的结果，在第几个列上，用大于／小于比较，阈值是多少,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,145,初始化 D，设置每行数据的样本的所有特征权重集合，平均分为m份,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,149,得到决策树的模型,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,152,alpha 目的主要是计算每一个分类器实例的权重(加和就是分类结果),
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,153,计算每个分类器的 alpha 权重值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,156,store Stump Params in Array,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,159,"print ""alpha=%s, classEst=%s, bestStump=%s, error=%s "" % (alpha, classEst.T, bestStump, error)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,160,分类正确：乘积为1，不会影响结果，-1主要是下面求e的-alpha次方,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,161,分类错误：乘积为 -1，结果会受影响，所以也乘以 -1,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,163,print '\n',
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,164,"print 'labelArr=', labelArr",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,165,"print 'classEst=', classEst.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,166,print '\n',
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,167,"print '乘积: ', multiply(mat(labelArr).T, classEst).T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,168,判断正确的，就乘以-1，否则就乘以1， 为什么？ 书上的公式。,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,169,"print '(-1取反)预测值expon=', expon.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,170,计算e的expon次方，然后计算得到一个综合的概率的值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,171,结果发现： 判断错误的样本，D对于的样本权重值会变大。,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,174,"print ""D: "", D.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,175,print '\n',
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,177,预测的分类结果值，在上一轮结果的基础上，进行加和操作,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,178,"print '当前的分类结果：', alpha*classEst.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,180,"print ""叠加后的分类结果aggClassEst: "", aggClassEst.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,181,sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,182,"结果为：错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,185,"print ""total error=%s "" % (errorRate)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,192,do stuff similar to last aggClassEst in adaBoostTrainDS,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,197,循环 多个分类器,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,199,前提： 我们已经知道了最佳的分类器的实例,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,200,通过分类器来核算每一次的分类结果，然后通过alpha*每一次的结果 得到最后的权重加和的值。,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,203,print aggClassEst,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,218,variable to calculate AUC,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,220,对正样本的进行求和,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,222,正样本的概率,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,224,负样本的概率,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,226,argsort函数返回的是数组值从小到大的索引值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,227,"get sorted index, it's reverse",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,229,测试结果是否是从小到大排列,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,232,开始创建模版对象,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,236,cursor光标值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,238,"loop through all the values, drawing a line segment at each point",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,247,"draw line from cur to (cur[0]-delX, cur[1]-delY)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,248,"画点连线 (x1, x2, y1, y2)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,252,画对角的虚线线,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,257,"设置画图的范围区间 (x1, x2, y1, y2)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,270,# 我们要将5个点进行分类,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,271,"dataArr, labelArr = loadSimpData()",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,272,"print 'dataArr', dataArr, 'labelArr', labelArr",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,274,# D表示最初值，对1进行均分为5份，平均每一个初始的概率都为0.2,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,275,# D的目的是为了计算错误概率： weightedError = D.T*errArr,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,276,"D = mat(ones((5, 1))/5)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,277,"print 'D=', D.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,279,"# bestStump, minError, bestClasEst = buildStump(dataArr, labelArr, D)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,280,"# print 'bestStump=', bestStump",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,281,"# print 'minError=', minError",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,282,"# print 'bestClasEst=', bestClasEst.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,284,# 分类器：weakClassArr,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,285,# 历史累计的分类结果集,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,286,"weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 9)",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,287,"print '\nweakClassArr=', weakClassArr, '\naggClassEst=', aggClassEst.T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,289,"""""""",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,290,发现:,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,291,分类的权重值：最大的值，为alpha的加和，最小值为-最大值,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,292,特征的权重值：如果一个值误判的几率越小，那么D的特征权重越少,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,293,"""""""",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,295,"# 测试数据的分类结果, 观测：aggClassEst分类的最终权重",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,296,"print adaClassify([0, 0], weakClassArr).T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,297,"print adaClassify([[5, 5], [0, 0]], weakClassArr).T",
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,299,马疝病数据集,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,300,训练集合,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,304,计算ROC下面的AUC的面积大小,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,306,测试集合,
AiLearning/src/py2.x/ml/7.AdaBoost/adaboost.py,311,测试：计算总样本数，错误样本数，错误率,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,13,importing necessary libraries,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,22,Create the dataset,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,26,"dataArr, labelArr = loadDataSet(""data/7.AdaBoost/horseColicTraining2.txt"")",
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,29,Fit regression model,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,36,Predict,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,40,Plot the results,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,55,适合2分类,
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,61,"print ""-"" * 100",
AiLearning/src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py,62,"print metrics.roc_auc_score(y[:1], y_2[:1])",
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,2,coding:utf-8,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,20,参数,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,25,加载数据,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,29,我们只用两个相应的features,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,33,训练,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,36,绘制决策边界,
AiLearning/src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py,52,绘制训练点,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,2,coding: utf8,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,3,原始链接： http://blog.csdn.net/lsldd/article/details/41223147,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,4,GitHub: https://github.com/apachecn/AiLearning,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,19,特征： 身高 体重   label： 胖瘦,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,23,特征数据,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,25,label分类的标签数据,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,27,预估结果的标签数据,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,42,print(clf),
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,49,print(x_train),
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,62,计算全量的预估结果,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,73,target_names 以 y的label分类为准,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,89,"with open(""testResult/tree.dot"", 'w') as f:",
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,90,from sklearn.externals.six import StringIO,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,91,"tree.export_graphviz(clf, out_file=f)",
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,100,from IPython.display import Image,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,101,Image(graph.create_png()),
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,111,得到训练的预测结果集,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,114,展现 准确率与召回率,
AiLearning/src/py2.x/ml/3.DecisionTree/DTSklearn.py,117,可视化输出,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,2,coding:utf8,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,12,"定义文本框 和 箭头格式 【 sawtooth 波浪方框, round4 矩形方框 , fc表示字体颜色的深浅 0.1~0.9 依次变浅，没错是变浅】",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,22,根节点开始遍历,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,24,"判断子节点是否为dict, 不是+1",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,36,根节点开始遍历,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,38,"判断子节点是不是dict, 求分枝的深度",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,43,记录最大的分支深度,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,61,获取叶子节点的数量,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,63,获取树的深度,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,64,depth = getTreeDepth(myTree),
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,66,找出第1个中心点的位置，然后与 parentPt定点进行划线,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,67,x坐标为 (numLeafs-1.)/plotTree.totalW/2+1./plotTree.totalW，化简如下,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,69,print cntrPt,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,70,并打印输入对应的文字,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,74,可视化Node分支点；第一次调用plotTree时，cntrPt与parentPt相同,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,76,根节点的值,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,78,y值 = 最高点-层数的高度[第二个节点位置]；1.0相当于树的高度,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,81,判断该节点是否是Node节点,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,83,如果是就递归调用[recursion],
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,86,如果不是，就在原来节点一半的地方找到节点的坐标,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,88,可视化该节点位置,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,90,并打印输入对应的文字,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,96,创建一个figure的模版,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,101,表示创建一个1行，1列的图，createPlot.ax1 为第 1 个子图，,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,106,半个节点的长度；xOff表示当前plotTree未遍历到的最左的叶节点的左边一个叶节点的x坐标,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,107,"所有叶节点中，最左的叶节点的x坐标是0.5/plotTree.totalW（因为totalW个叶节点在x轴方向是平均分布在[0, 1]区间上的）",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,108,因此，xOff的初始值应该是 0.5/plotTree.totalW-相邻两个叶节点的x轴方向距离,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,110,根节点的y坐标为1.0，树的最低点y坐标为0,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,112,第二个参数是根节点的坐标,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,117,# 测试画图,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,118,def createPlot():,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,119,"fig = plt.figure(1, facecolor='white')",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,120,fig.clf(),
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,121,# ticks for demo puropses,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,122,"createPlot.ax1 = plt.subplot(111, frameon=False)",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,123,"plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,124,"plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)",
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,125,plt.show(),
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,128,测试数据集,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,136,用测试数据绘制树,
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,137,myTree = retrieveTree(1),
AiLearning/src/py2.x/ml/3.DecisionTree/decisionTreePlot.py,138,createPlot(myTree),
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,15,引入必要的模型和库,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,20,创建一个随机的数据集,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,21,参考 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,23,"print 'lalalalala===', rng",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,24,"rand() 是给定形状的随机值，rng.rand(80, 1)即矩阵的形状是 80行，1列",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,25,sort(),
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,27,"print 'X=', X",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,29,"print 'y=', y",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,31,"print 'yyy=', y",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,33,拟合回归模型,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,34,regr_1 = DecisionTreeRegressor(max_depth=2),
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,35,保持 max_depth=5 不变，增加 min_samples_leaf=6 的参数，效果进一步提升了,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,38,regr_3 = DecisionTreeRegressor(max_depth=4),
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,39,"regr_1.fit(X, y)",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,41,"regr_3.fit(X, y)",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,43,预测,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,45,y_1 = regr_1.predict(X_test),
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,47,y_3 = regr_3.predict(X_test),
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,49,绘制结果,
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,52,"plt.plot(X_test, y_1, color=""cornflowerblue"", label=""max_depth=2"", linewidth=2)",
AiLearning/src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py,54,"plt.plot(X_test, y_3, color=""red"", label=""max_depth=3"", linewidth=2)",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,2,coding:utf-8,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,31,"dataSet = [['yes'],",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,32,"['yes'],",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,33,"['no'],",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,34,"['no'],",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,35,['no']],
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,36,labels  露出水面   脚蹼,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,38,change to discrete values,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,50,-----------计算香农熵的第一种实现方式start--------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,51,求list的长度，表示计算参与训练的数据量,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,53,下面输出我们测试的数据集的一些信息,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,54,例如：<type 'list'> numEntries:  5 是下面的代码的输出,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,55,"print type(dataSet), 'numEntries: ', numEntries",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,57,计算分类标签label出现的次数,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,59,the the number of unique elements and their occurance,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,61,将当前实例的标签存储，即每一行数据的最后一个数据代表的是标签,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,63,为所有可能的分类创建字典，如果当前的键值不存在，则扩展字典并将当前键值加入字典。每个键值都记录了当前类别出现的次数。,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,67,"print '-----', featVec, labelCounts",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,69,对于label标签的占比，求出label标签的香农熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,72,使用所有类标签的发生频率计算类别出现的概率。,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,74,log base 2,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,75,计算香农熵，以 2 为底求对数,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,77,"print '---', prob, prob * log(prob, 2), shannonEnt",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,78,-----------计算香农熵的第一种实现方式end--------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,80,# -----------计算香农熵的第二种实现方式start--------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,81,# 统计标签出现的次数,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,82,label_count = Counter(data[-1] for data in dataSet),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,83,# 计算概率,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,84,probs = [p[1] / len(dataSet) for p in label_count.items()],
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,85,# 计算香农熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,86,"shannonEnt = sum([-p * log(p, 2) for p in probs])",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,87,# -----------计算香农熵的第二种实现方式end--------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,101,-----------切分数据集的第一种方式 start------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,104,index列为value的数据集【该数据集需要排除index列】,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,105,判断index列的值是否为value,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,107,chop out index used for splitting,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,108,[:index]表示前index行，即若 index 为2，就是取 featVec 的前 index 行,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,129,[index+1:]表示从跳过 index 的 index+1行，取接下来的数据,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,130,收集结果值 index列为value的行【该行需要排除index列】,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,132,-----------切分数据集的第一种方式 end------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,134,# -----------切分数据集的第二种方式 start------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,135,"retDataSet = [data for data in dataSet for i, v in enumerate(data) if i == axis and v == value]",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,136,# -----------切分数据集的第二种方式 end------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,149,-----------选择最优特征的第一种方式 start------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,150,"求第一行有多少列的 Feature, 最后一列是label列嘛",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,152,label的信息熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,154,"最优的信息增益值, 和最优的Featurn编号",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,156,iterate over all the features,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,158,create a list of all the examples of this feature,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,159,获取每一个实例的第i+1个feature，组成list集合,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,161,get a set of unique values,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,162,获取剔重后的集合，使用set对list数据进行去重,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,164,创建一个临时的信息熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,166,遍历某一列的value集合，计算该列的信息熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,167,遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,172,gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,173,信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,180,-----------选择最优特征的第一种方式 end------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,182,# -----------选择最优特征的第二种方式 start------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,183,# 计算初始香农熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,184,base_entropy = calcShannonEnt(dataSet),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,185,best_info_gain = 0,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,186,best_feature = -1,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,187,# 遍历每一个特征,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,188,for i in range(len(dataSet[0]) - 1):,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,189,# 对当前特征进行统计,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,190,feature_count = Counter([data[i] for data in dataSet]),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,191,# 计算分割后的香农熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,192,"new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) \",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,193,for feature in feature_count.items()),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,194,# 更新值,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,195,info_gain = base_entropy - new_entropy,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,196,"print('No. {0} feature info gain is {1:.3f}'.format(i, info_gain))",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,197,if info_gain > best_info_gain:,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,198,best_info_gain = info_gain,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,199,best_feature = i,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,200,return best_feature,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,201,# -----------选择最优特征的第二种方式 end------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,212,-----------majorityCnt的第一种方式 start------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,218,倒叙排列classCount得到一个字典集合，然后取出第一个就是结果（yes/no），即出现次数最多的结果,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,220,"print 'sortedClassCount:', sortedClassCount",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,222,-----------majorityCnt的第一种方式 end------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,224,# -----------majorityCnt的第二种方式 start------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,225,major_label = Counter(classList).most_common(1)[0],
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,226,return major_label,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,227,# -----------majorityCnt的第二种方式 end------------------------------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,232,如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,233,第一个停止条件：所有的类标签完全相同，则直接返回该类标签。,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,234,count() 函数是统计括号中的值在list中出现的次数,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,237,如果数据集只有1列，那么最初出现label次数最多的一类，作为结果,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,238,第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,242,选择最优的列，得到最优列对应的label含义,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,244,获取label的名称,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,246,初始化myTree,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,248,注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,249,所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,251,取出最优列，然后它的branch做分类,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,255,求出剩余的标签label,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,257,遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree(),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,259,"print 'myTree', value, myTree",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,273,获取tree的根节点对于的key值,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,275,通过key得到根节点对应的value,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,277,判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,279,测试数据，找到根节点对应的label位置，也就知道从输入的数据的第几位来开始分类,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,283,判断分枝是否结束: 判断valueOfFeat是否是dict类型,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,293,-------------- 第一种方法 start --------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,297,-------------- 第一种方法 end --------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,299,-------------- 第二种方法 start --------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,302,-------------- 第二种方法 start --------------,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,312,1.创建数据和结果标签,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,314,"print myDat, labels",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,316,计算label分类标签的香农熵,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,317,calcShannonEnt(myDat),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,319,# 求第0列 为 1/0的列的数据集【排除第0列】,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,320,"print '1---', splitDataSet(myDat, 0, 1)",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,321,"print '0---', splitDataSet(myDat, 0, 0)",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,323,# 计算最好的信息增益的列,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,324,print chooseBestFeatureToSplit(myDat),
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,329,"[1, 1]表示要取的分支上的节点位置，对应的结果值",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,332,获得树的高度,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,335,画图可视化展现,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,349,加载隐形眼镜相关的 文本文件 数据,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,351,解析数据，获得 features 数据,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,353,得到数据的对应的 Labels,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,355,使用上面的创建决策树的代码，构造预测隐形眼镜的决策树,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,358,画图可视化展现,
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,377,"遍历子树, 获得子树的最大高度",
AiLearning/src/py2.x/ml/3.DecisionTree/DecisionTree.py,390,ContactLensesTest(),
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,2,coding:utf8,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,14,从文本中构建矩阵，加载文本文件，然后处理,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,15,通用函数，用来解析以 tab 键分隔的 floats（浮点数）,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,20,映射所有的元素为 float（浮点数）类型,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,25,计算两个向量的欧式距离（可根据场景选择）,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,27,la.norm(vecA-vecB),
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,30,为给定数据集构建一个包含 k 个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以通过找到数据集每一维的最小和最大值来完成。然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,32,列的数量,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,33,创建k个质心矩阵,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,34,创建随机簇质心，并且在每一维的边界内,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,35,最小值,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,36,范围 = 最大值 - 最小值,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,37,随机生成,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,41,k-means 聚类算法,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,42,该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,43,这个过程重复数次，知道数据点的簇分配结果不再改变位置。,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,44,运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,46,行数,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,48,创建一个与 dataMat 行数一样，但是有两列的矩阵，用来保存簇分配结果,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,49,创建质心，随机k个质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,53,循环每一个数据点并分配到最近的质心中去,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,58,计算数据点到质心的距离,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,59,如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,62,簇分配结果改变,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,63,簇改变,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,65,更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,67,更新质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,69,获取该簇中的所有点,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,71,将质心修改为簇中所有点的平均值，mean 就是求平均值的,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,75,"二分 KMeans 聚类算法, 基于 kMeans 基础之上的优化，以避免陷入局部最小值",
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,78,保存每个数据点的簇分配结果和平方误差,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,79,质心初始化为所有数据点的均值,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,80,初始化只有 1 个质心的 list,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,81,计算所有数据点到初始质心的距离平方误差,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,83,当质心数量小于 k 时,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,85,对每一个质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,87,获取当前簇 i 下的所有数据点,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,89,将当前簇 i 进行二分 kMeans 处理,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,90,将二分 kMeans 结果中的平方和的距离进行求和,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,93,将未参与二分 kMeans 分配结果中的平方和的距离进行求和,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,100,找出最好的簇分配结果,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,102,"调用二分 kMeans 的结果，默认簇是 0,1. 当然也可以改成其它的数字",
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,104,更新为最佳质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,107,更新质心列表,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,109,更新原质心 list 中的第 i 个质心为使用二分 kMeans 后 bestNewCents 的第一个质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,111,添加 bestNewCents 的第二个质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,113,重新分配最好簇下的数据（质心）以及SSE,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,118,加载测试数据集,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,121,测试 randCent() 函数是否正常运行。,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,122,首先，先看一下矩阵中的最大值与最小值,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,128,然后看看 randCent() 函数能否生成 min 到 max 之间的值,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,131,最后测试一下距离计算方法,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,136,加载测试数据集,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,139,该算法会创建k个质心，然后将每个点分配到最近的质心，再重新计算质心。,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,140,这个过程重复数次，知道数据点的簇分配结果不再改变位置。,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,141,运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似）,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,148,加载测试数据集,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,158,测试基础的函数,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,159,testBasicFunc(),
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,161,测试 kMeans 函数,
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,162,testKMeans(),
AiLearning/src/py2.x/ml/10.kmeans/kMeans.py,164,测试二分 biKMeans 函数,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,1,-*- coding:UTF-8 -*-,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,7,加载数据集,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,9,注意，这个是相对路径，请保证是在 MachineLearning 这个目录下执行。,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,12,映射所有的元素为 float（浮点数）类型,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,15,训练模型,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,16,初始化,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,17,拟合,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,18,预测,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,19,质心,
AiLearning/src/py2.x/ml/10.kmeans/kMeansSklearn.py,21,可视化结果,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,2,coding:utf8,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,18,导入csv文件,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,27,strip()返回移除字符串头尾指定的字符生成的新字符串,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,30,isdigit 如果是浮点型数值，就是 false，所以换成 isalpha() 函数,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,31,if str_f.isdigit():   # 判断是否是数字,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,32,如果是字母，说明是标签,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,33,添加分类标签,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,36,将数据集的第column列转换成float形式,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,52,"复制一份 dataset,防止 dataset 的内容改变",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,55,每次循环 fold 清零，防止重复导入 dataset_split,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,56,这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,57,有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,59,将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,60,pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,61,fold.append(dataset_copy.pop(index))  # 无放回的方式,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,62,有放回的方式,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,64,由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,68,Split a dataset based on an attribute and an attribute value # 根据特征和特征值分割数据集,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,102,个人理解：计算代价，分类越准确，则 gini 越小,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,105,"class_values = [0, 1]",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,106,"groups = (left, right)",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,111,个人理解：计算代价，分类越准确，则 gini 越小,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,115,"找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,117,"class_values =[0, 1]",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,121,往 features 添加 n_features 个特征（ n_feature 等于特征数的根号），特征索引从 dataset 中随机取,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,124,在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,126,"groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,128,左右两边的数量越一样，说明数据区分度不高，gini系数越大,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,130,"最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,131,print b_score,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,135,Create a terminal node value # 输出group中出现次数较多的标签,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,137,max() 函数中，当 key 参数不为空时，就以 key 的函数对象为判断的标准,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,138,输出 group 中出现次数较多的标签,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,141,Create child splits for a node or make terminal  # 创建子分割器，递归分类，直到分类结束,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,142,"max_depth = 10, min_size = 1, n_features=int(sqrt((len(dataset[0])-1)",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,145,check for a no split,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,149,check for max depth,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,150,max_depth=10 表示递归十次，若分类还未结束，则选取数据中分类标签较多的作为结果，使分类提前结束，防止过拟合,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,153,process left child,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,157,"node['left']是一个字典，形式为{'index':b_index, 'value':b_value, 'groups':b_groups}，所以node是一个多层字典",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,158,递归，depth+1计算递归层数,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,159,process right child,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,167,Build a decision tree,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,180,返回最优列和相关的信息,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,183,对左右2边的数据 进行递归的调用，由于最优特征使用过，所以在后面进行使用的时候，就没有意义了,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,184,例如： 性别-男女，对男使用这一特征就没任何意义了,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,189,Make a prediction with a decision tree,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,190,预测模型分类结果,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,192,isinstance 是 Python 中的一个内建函数。是用来判断一个对象是否是一个已知的类型。,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,203,Make a prediction with a list of bagged trees,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,214,使用多个决策树trees对测试集test的第row行进行预测，再使用简单投票法判断出该行所属分类,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,219,Create a random subsample from the dataset with replacement,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,220,创建数据集的随机子样本,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,231,训练样本的按比例抽样。,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,232,round() 方法返回浮点数x的四舍五入值。,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,235,有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,241,Random Forest Algorithm,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,258,n_trees 表示决策树的数量,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,260,随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,262,创建一个决策树,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,266,每一行的预测结果，bagging 预测最后的分类结果,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,271,Calculate accuracy percentage,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,272,导入实际值和预测值，计算精确度,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,280,评估算法性能，返回模型得分,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,293,将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次 list 的元素是无重复的,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,296,每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,300,"将多个 fold 列表组合成一个 train_set 列表, 类似 union all",
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,314,fold 表示从原始数据集 dataset 提取出来的测试集,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,322,计算随机森林的预测结果的正确率,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,330,加载数据,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,332,print dataset,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,334,分成5份数据，进行交叉验证,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,335,调参（自己修改） #决策树深度不能太深，不然容易导致过拟合,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,336,决策树的叶子节点最少的元素数量,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,337,做决策树时候的样本的比例,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,338,n_features = int((len(dataset[0])-1)),
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,339,调参（自己修改） #准确性与多样性之间的权衡,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,340,理论上树是越多越好,
AiLearning/src/py2.x/ml/7.RandomForest/randomForest.py,342,每一次执行本文件时都能产生同一个随机数,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,2,coding: utf8,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,15,加载数据集,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,19,创建集合 C1。即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,33,遍历所有的元素，如果不在 C1 出现过，那么就 append,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,35,对数组进行 `从小到大` 的排序,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,36,"print 'sort 前=', C1",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,38,frozenset 表示冻结的 set 集合，元素无改变；可以把它当字典的 key 来使用,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,39,"print 'sort 后=', C1",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,40,"print 'frozenset=', map(frozenset, C1)",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,43,计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度（minSupport）的数据,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,56,"ssCnt 临时存放选数据集 Ck 的频率. 例如: a->10, b->5, c->8",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,60,s.issubset(t)  测试是否 s 中的每一个元素都在 t 中,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,66,数据集 D 的数量,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,70,支持度 = 候选项（key）出现的次数 / 所有数据集的数量,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,73,在 retList 的首位插入元素，只存储支持度满足频繁项集的值,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,75,存储所有的候选项（key）和对应的支持度（support）,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,79,输入频繁项集列表 Lk 与返回的元素个数 k，然后输出所有可能的候选项集 Ck,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,99,"print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,100,"print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,103,"第一次 L1,L2 为空，元素直接进行合并，返回元素两两合并的数据集",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,104,if first k-2 elements are equal,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,106,set union,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,107,"print 'union=', Lk[i] | Lk[j], Lk[i], Lk[j]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,111,找出数据集 dataSet 中支持度 >= 最小支持度的候选项集以及它们的支持度。即我们的频繁项集。,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,122,C1 即对 dataSet 进行去重，排序，放入 list 中，然后转换所有的元素为 frozenset,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,124,"print 'C1: ', C1",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,125,对每一行进行 set 转换，然后存放到集合中,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,127,"print 'D=', D",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,128,计算候选数据集 C1 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,130,"print ""L1="", L1, ""\n"", ""outcome: "", supportData",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,132,"L 加了一层 list, L 一共 2 层 list",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,135,"判断 L 的第 k-2 项的数据长度是否 > 0。第一次执行时 L 为 [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]。L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]，最后面 k += 1",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,137,"print 'k=', k, L, L[k-2]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,138,"例如: 以 {0},{1},{2} 为输入且 k = 2 则输出 {0,1}, {0,2}, {1,2}. 以 {0,1},{0,2},{1,2} 为输入且 k = 3 则输出 {0,1,2}",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,139,"print 'Ck', Ck",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,141,计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于 minSupport 的数据,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,142,保存所有候选项集的支持度，如果字典没有，就追加元素，如果有，就更新元素,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,146,Lk 表示满足频繁子项的集合，L 元素在增加，例如:,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,147,"l=[[set(1), set(2), set(3)]]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,148,"l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,151,"print 'k=', k, len(L[k-2])",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,154,计算可信度（confidence）,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,167,记录可信度大于最小可信度（minConf）的集合,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,169,"假设 freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]，那么现在需要求出 frozenset([1]) -> frozenset([3]) 的可信度和 frozenset([3]) -> frozenset([1]) 的可信度",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,171,"print 'confData=', freqSet, H, conseq, freqSet-conseq",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,172,"支持度定义: a -> b = support(a | b) / support(a). 假设  freqSet = frozenset([1, 3]), conseq = [frozenset([1])]，那么 frozenset([1]) 至 frozenset([3]) 的可信度为 = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,174,只要买了 freqSet-conseq 集合，一定会买 conseq 集合（freqSet-conseq 集合和 conseq集合 是全集）,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,180,递归计算频繁项集的规则,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,191,"H[0] 是 freqSet 的元素组合的第一个元素，并且 H 中所有元素的长度都一样，长度由 aprioriGen(H, m+1) 这里的 m + 1 来控制",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,192,该函数递归时，H[0] 的长度从 1 开始增长 1 2 3 ...,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,193,"假设 freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,194,那么 m = len(H[0]) 的递归的值依次为 1 2,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,195,"在 m = 2 时, 跳出该递归。假设再递归一次，那么 H[0] = frozenset([2, 3, 5])，freqSet = frozenset([2, 3, 5]) ，没必要再计算 freqSet 与 H[0] 的关联规则了。",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,198,"print 'freqSet******************', len(freqSet), m + 1, freqSet, H, H[0]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,199,"生成 m+1 个长度的所有可能的 H 中的组合，假设 H = [frozenset([2]), frozenset([3]), frozenset([5])]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,200,"第一次递归调用时生成 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,201,第二次 。。。没有第二次，递归条件判断时已经退出了,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,203,返回可信度大于最小可信度的集合,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,207,计算可信度后，还有数据大于最小可信度的话，那么继续递归调用，否则跳出递归,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,209,"print '----------------------', Hmp1",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,210,"print len(freqSet),  len(Hmp1[0]) + 1",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,213,生成关联规则,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,225,"假设 L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,227,获取频繁项集中每个组合的所有元素,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,229,"假设：freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,230,组合总的元素并遍历子元素，并转化为 frozenset 集合，再存放到 list 列表中,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,232,"2 个的组合，走 else, 2 个以上的组合，走 if",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,243,votesmart.apikey = 'get your api key first',
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,251,api call,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,260,delay to be polite,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,264,this will return a list of lists containing ints,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,265,list of what each item stands for,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,266,fill up itemMeaning list,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,269,list of items in each transaction (politician),
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,293,暂时没用上,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,294,"def pntRules(ruleList, itemMeaning):",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,295,for ruleTup in ruleList:,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,296,for item in ruleTup[0]:,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,297,print itemMeaning[item],
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,298,"print ""           -------->""",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,299,for item in ruleTup[1]:,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,300,print itemMeaning[item],
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,301,"print ""confidence: %f"" % ruleTup[2]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,302,print       #print a blank line,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,305,加载测试数据集,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,309,Apriori 算法生成频繁项集以及它们的支持度,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,316,Apriori 算法生成频繁项集以及它们的支持度,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,322,加载测试数据集,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,326,Apriori 算法生成频繁项集以及它们的支持度,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,331,生成关联规则,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,336,测试 Apriori 算法,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,339,生成关联规则,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,340,testGenerateRules(),
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,342,# 项目案例,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,343,# 构建美国国会投票记录的事务数据集,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,344,"actionIdList, billTitleList = getActionIds()",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,345,# 测试前2个,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,346,"# transDict, itemMeaning = getTransList(actionIdList[: 2], billTitleList[: 2])",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,347,"# transDict 表示 action_id的集合，transDict[key]这个就是action_id对应的选项，例如 [1, 2, 3]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,348,"transDict, itemMeaning = getTransList(actionIdList, billTitleList)",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,349,# 得到全集的数据,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,350,dataSet = [transDict[key] for key in transDict.keys()],
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,351,"L, supportData = apriori(dataSet, minSupport=0.3)",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,352,"rules = generateRules(L, supportData, minConf=0.95)",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,353,print rules,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,355,# 项目案例,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,356,# 发现毒蘑菇的相似特性,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,357,# 得到全集的数据,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,358,"dataSet = [line.split() for line in open(""data/11.Apriori/mushroom.dat"").readlines()]",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,359,"L, supportData = apriori(dataSet, minSupport=0.3)",
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,360,# 2表示毒蘑菇，1表示可食用的蘑菇,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,361,# 找出关于2的频繁子项出来，就知道如果是毒蘑菇，那么出现频繁的也可能是毒蘑菇,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,362,for item in L[1]:,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,363,if item.intersection('2'):,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,364,print item,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,366,for item in L[2]:,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,367,if item.intersection('2'):,
AiLearning/src/py2.x/ml/11.Apriori/apriori.py,368,print item,
AiLearning/src/py2.x/ml/8.Regression/regression.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/8.Regression/regression.py,2,coding:utf8,
AiLearning/src/py2.x/ml/8.Regression/regression.py,22,获取样本特征的总数，不算最后的目标变量,
AiLearning/src/py2.x/ml/8.Regression/regression.py,28,读取每一行,
AiLearning/src/py2.x/ml/8.Regression/regression.py,30,删除一行中以tab分隔的数据前后的空白符号,
AiLearning/src/py2.x/ml/8.Regression/regression.py,32,i 从0到2，不包括2,
AiLearning/src/py2.x/ml/8.Regression/regression.py,34,将数据添加到lineArr List中，每一行数据测试数据组成一个行向量,
AiLearning/src/py2.x/ml/8.Regression/regression.py,36,将测试数据的输入数据部分存储到dataMat 的List中,
AiLearning/src/py2.x/ml/8.Regression/regression.py,38,将每一行的最后一个数据，即类别，或者叫目标变量存储到labelMat List中,
AiLearning/src/py2.x/ml/8.Regression/regression.py,54,mat()函数将xArr，yArr转换为矩阵 mat().T 代表的是对矩阵进行转置操作,
AiLearning/src/py2.x/ml/8.Regression/regression.py,57,矩阵乘法的条件是左矩阵的列数等于右矩阵的行数,
AiLearning/src/py2.x/ml/8.Regression/regression.py,59,因为要用到xTx的逆矩阵，所以事先需要确定计算得到的xTx是否可逆，条件是矩阵的行列式不为0,
AiLearning/src/py2.x/ml/8.Regression/regression.py,60,linalg.det() 函数是用来求得矩阵的行列式的，如果矩阵的行列式为0，则这个矩阵是不可逆的，就无法进行接下来的运算,
AiLearning/src/py2.x/ml/8.Regression/regression.py,64,最小二乘法,
AiLearning/src/py2.x/ml/8.Regression/regression.py,65,http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133,
AiLearning/src/py2.x/ml/8.Regression/regression.py,66,书中的公式，求得w的最优解,
AiLearning/src/py2.x/ml/8.Regression/regression.py,71,局部加权线性回归,
AiLearning/src/py2.x/ml/8.Regression/regression.py,90,mat() 函数是将array转换为矩阵的函数， mat().T 是转换为矩阵之后，再进行转置操作,
AiLearning/src/py2.x/ml/8.Regression/regression.py,93,获得xMat矩阵的行数,
AiLearning/src/py2.x/ml/8.Regression/regression.py,95,eye()返回一个对角线元素为1，其他元素为0的二维数组，创建权重矩阵weights，该矩阵为每个样本点初始化了一个权重,
AiLearning/src/py2.x/ml/8.Regression/regression.py,98,testPoint 的形式是 一个行向量的形式,
AiLearning/src/py2.x/ml/8.Regression/regression.py,99,计算 testPoint 与输入样本点之间的距离，然后下面计算出每个样本贡献误差的权值,
AiLearning/src/py2.x/ml/8.Regression/regression.py,101,k控制衰减的速度,
AiLearning/src/py2.x/ml/8.Regression/regression.py,103,根据矩阵乘法计算 xTx ，其中的 weights 矩阵是样本点对应的权重矩阵,
AiLearning/src/py2.x/ml/8.Regression/regression.py,108,计算出回归系数的一个估计,
AiLearning/src/py2.x/ml/8.Regression/regression.py,125,得到样本点的总数,
AiLearning/src/py2.x/ml/8.Regression/regression.py,127,构建一个全部都是 0 的 1 * m 的矩阵,
AiLearning/src/py2.x/ml/8.Regression/regression.py,129,循环所有的数据点，并将lwlr运用于所有的数据点,
AiLearning/src/py2.x/ml/8.Regression/regression.py,132,返回估计值,
AiLearning/src/py2.x/ml/8.Regression/regression.py,148,生成一个与目标变量数目相同的 0 向量,
AiLearning/src/py2.x/ml/8.Regression/regression.py,150,将 xArr 转换为 矩阵形式,
AiLearning/src/py2.x/ml/8.Regression/regression.py,152,排序,
AiLearning/src/py2.x/ml/8.Regression/regression.py,154,开始循环，为每个样本点进行局部加权线性回归，得到最终的目标变量估计值,
AiLearning/src/py2.x/ml/8.Regression/regression.py,189,岭回归就是在矩阵 xTx 上加一个 λI 从而使得矩阵非奇异，进而能对 xTx + λI 求逆,
AiLearning/src/py2.x/ml/8.Regression/regression.py,191,检查行列式是否为零，即矩阵是否可逆，行列式为0的话就不可逆，不为0的话就是可逆。,
AiLearning/src/py2.x/ml/8.Regression/regression.py,212,计算Y的均值,
AiLearning/src/py2.x/ml/8.Regression/regression.py,214,Y的所有的特征减去均值,
AiLearning/src/py2.x/ml/8.Regression/regression.py,216,标准化 x，计算 xMat 平均值,
AiLearning/src/py2.x/ml/8.Regression/regression.py,218,然后计算 X的方差,
AiLearning/src/py2.x/ml/8.Regression/regression.py,220,所有特征都减去各自的均值并除以方差,
AiLearning/src/py2.x/ml/8.Regression/regression.py,222,可以在 30 个不同的 lambda 下调用 ridgeRegres() 函数。,
AiLearning/src/py2.x/ml/8.Regression/regression.py,224,创建30 * m 的全部数据为0 的矩阵,
AiLearning/src/py2.x/ml/8.Regression/regression.py,227,exp() 返回 e^x,
AiLearning/src/py2.x/ml/8.Regression/regression.py,233,按列进行规范化,
AiLearning/src/py2.x/ml/8.Regression/regression.py,235,计算平均值然后减去它,
AiLearning/src/py2.x/ml/8.Regression/regression.py,236,计算除以Xi的方差,
AiLearning/src/py2.x/ml/8.Regression/regression.py,245,也可以规则化ys但会得到更小的coef,
AiLearning/src/py2.x/ml/8.Regression/regression.py,248,"returnMat = zeros((numIt,n)) # 测试代码删除",
AiLearning/src/py2.x/ml/8.Regression/regression.py,265,"returnMat[i,:]=ws.T",
AiLearning/src/py2.x/ml/8.Regression/regression.py,266,return returnMat,
AiLearning/src/py2.x/ml/8.Regression/regression.py,268,"def scrapePage(inFile,outFile,yr,numPce,origPrc):",
AiLearning/src/py2.x/ml/8.Regression/regression.py,269,from BeautifulSoup import BeautifulSoup,
AiLearning/src/py2.x/ml/8.Regression/regression.py,270,"fr = open(inFile); fw=open(outFile,'a') #a is append mode writing",
AiLearning/src/py2.x/ml/8.Regression/regression.py,271,soup = BeautifulSoup(fr.read()),
AiLearning/src/py2.x/ml/8.Regression/regression.py,272,i=1,
AiLearning/src/py2.x/ml/8.Regression/regression.py,273,"currentRow = soup.findAll('table', r=""%d"" % i)",
AiLearning/src/py2.x/ml/8.Regression/regression.py,274,while(len(currentRow)!=0):,
AiLearning/src/py2.x/ml/8.Regression/regression.py,275,title = currentRow[0].findAll('a')[1].text,
AiLearning/src/py2.x/ml/8.Regression/regression.py,276,lwrTitle = title.lower(),
AiLearning/src/py2.x/ml/8.Regression/regression.py,277,if (lwrTitle.find('new') > -1) or (lwrTitle.find('nisb') > -1):,
AiLearning/src/py2.x/ml/8.Regression/regression.py,278,newFlag = 1.0,
AiLearning/src/py2.x/ml/8.Regression/regression.py,279,else:,
AiLearning/src/py2.x/ml/8.Regression/regression.py,280,newFlag = 0.0,
AiLearning/src/py2.x/ml/8.Regression/regression.py,281,soldUnicde = currentRow[0].findAll('td')[3].findAll('span'),
AiLearning/src/py2.x/ml/8.Regression/regression.py,282,if len(soldUnicde)==0:,
AiLearning/src/py2.x/ml/8.Regression/regression.py,283,"print ""item #%d did not sell"" % i",
AiLearning/src/py2.x/ml/8.Regression/regression.py,284,else:,
AiLearning/src/py2.x/ml/8.Regression/regression.py,285,soldPrice = currentRow[0].findAll('td')[4],
AiLearning/src/py2.x/ml/8.Regression/regression.py,286,priceStr = soldPrice.text,
AiLearning/src/py2.x/ml/8.Regression/regression.py,287,"priceStr = priceStr.replace('$','') #strips out $",
AiLearning/src/py2.x/ml/8.Regression/regression.py,288,"priceStr = priceStr.replace(',','') #strips out ,",
AiLearning/src/py2.x/ml/8.Regression/regression.py,289,if len(soldPrice)>1:,
AiLearning/src/py2.x/ml/8.Regression/regression.py,290,"priceStr = priceStr.replace('Free shipping', '') #strips out Free Shipping",
AiLearning/src/py2.x/ml/8.Regression/regression.py,291,"print ""%s\t%d\t%s"" % (priceStr,newFlag,title)",
AiLearning/src/py2.x/ml/8.Regression/regression.py,292,"fw.write(""%d\t%d\t%d\t%f\t%s\n"" % (yr,numPce,newFlag,origPrc,priceStr))",
AiLearning/src/py2.x/ml/8.Regression/regression.py,293,i += 1,
AiLearning/src/py2.x/ml/8.Regression/regression.py,294,"currentRow = soup.findAll('table', r=""%d"" % i)",
AiLearning/src/py2.x/ml/8.Regression/regression.py,295,fw.close(),
AiLearning/src/py2.x/ml/8.Regression/regression.py,297,--------------------------------------------------------------,
AiLearning/src/py2.x/ml/8.Regression/regression.py,298,预测乐高玩具套装的价格 ------ 最初的版本，因为现在 google 的 api 变化，无法获取数据,
AiLearning/src/py2.x/ml/8.Regression/regression.py,299,故改为了下边的样子，但是需要安装一个 beautifulSoup 这个第三方爬虫库，安装很简单，见下边,
AiLearning/src/py2.x/ml/8.Regression/regression.py,373,----------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/8.Regression/regression.py,374,预测乐高玩具套装的价格 可运行版本，我们把乐高数据存储到了我们的 input 文件夹下，使用 beautifulSoup 爬去一下内容,
AiLearning/src/py2.x/ml/8.Regression/regression.py,375,前提：安装 BeautifulSoup 第三方爬虫库，步骤如下,
AiLearning/src/py2.x/ml/8.Regression/regression.py,376,在这个页面 https://www.crummy.com/software/BeautifulSoup/bs4/download/4.4/ 下载，beautifulsoup4-4.4.1.tar.gz,
AiLearning/src/py2.x/ml/8.Regression/regression.py,377,将下载文件解压，使用 windows 版本的 cmd 命令行，进入解压的包，输入以下两行命令即可完成安装,
AiLearning/src/py2.x/ml/8.Regression/regression.py,378,python setup.py build,
AiLearning/src/py2.x/ml/8.Regression/regression.py,379,python setup.py install,
AiLearning/src/py2.x/ml/8.Regression/regression.py,496,test for standRegression,
AiLearning/src/py2.x/ml/8.Regression/regression.py,504,add_subplot(349)函数的参数的意思是，将画布分成3行4列图像画在从左到右从上到下第9块,
AiLearning/src/py2.x/ml/8.Regression/regression.py,507,scatter 的x是xMat中的第二列，y是yMat的第一列,
AiLearning/src/py2.x/ml/8.Regression/regression.py,515,test for LWLR,
AiLearning/src/py2.x/ml/8.Regression/regression.py,521,argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出,
AiLearning/src/py2.x/ml/8.Regression/regression.py,533,test for abloneDataSet,
AiLearning/src/py2.x/ml/8.Regression/regression.py,543,加载数据,
AiLearning/src/py2.x/ml/8.Regression/regression.py,545,使用不同的核进行预测,
AiLearning/src/py2.x/ml/8.Regression/regression.py,549,打印出不同的核预测值与训练数据集上的真实值之间的误差大小,
AiLearning/src/py2.x/ml/8.Regression/regression.py,554,打印出 不同的核预测值 与 新数据集（测试数据集）上的真实值之间的误差大小,
AiLearning/src/py2.x/ml/8.Regression/regression.py,562,使用简单的 线性回归 进行预测，与上面的计算进行比较,
AiLearning/src/py2.x/ml/8.Regression/regression.py,568,test for ridgeRegression,
AiLearning/src/py2.x/ml/8.Regression/regression.py,578,test for stageWise,
AiLearning/src/py2.x/ml/8.Regression/regression.py,591,predict for lego's price,
AiLearning/src/py2.x/ml/8.Regression/regression.py,602,regression2(),
AiLearning/src/py2.x/ml/8.Regression/regression.py,603,abaloneTest(),
AiLearning/src/py2.x/ml/8.Regression/regression.py,604,regression3(),
AiLearning/src/py2.x/ml/8.Regression/regression.py,605,regression4(),
AiLearning/src/py2.x/ml/8.Regression/regression.py,606,regression5(),
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,13,Isotonic Regression 等式回归,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,16,Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,17,Alexandre Gramfort <alexandre.gramfort@inria.fr>,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,18,License: BSD,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,38,线性回归的 x 需要为 2d,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,54,Kernel ridge regression ( 内核岭回归 ),
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,56,2.1 Comparison of kernel ridge regression and SVR ( 内核岭回归与 SVR 的比较 ),
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,58,Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>,
AiLearning/src/py2.x/ml/8.Regression/sklearn-regression-demo.py,59,License: BSD 3 clause,
AiLearning/src/py2.x/ml/13.PCA/pca.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/13.PCA/pca.py,2,coding: utf-8,
AiLearning/src/py2.x/ml/13.PCA/pca.py,34,计算每一列的均值,
AiLearning/src/py2.x/ml/13.PCA/pca.py,36,"print 'meanVals', meanVals",
AiLearning/src/py2.x/ml/13.PCA/pca.py,38,每个向量同时都减去 均值,
AiLearning/src/py2.x/ml/13.PCA/pca.py,40,"print 'meanRemoved=', meanRemoved",
AiLearning/src/py2.x/ml/13.PCA/pca.py,42,cov协方差=[(x1-x均值)*(y1-y均值)+(x2-x均值)*(y2-y均值)+...+(xn-x均值)*(yn-y均值)+]/(n-1),
AiLearning/src/py2.x/ml/13.PCA/pca.py,54,eigVals为特征值， eigVects为特征向量,
AiLearning/src/py2.x/ml/13.PCA/pca.py,56,"print 'eigVals=', eigVals",
AiLearning/src/py2.x/ml/13.PCA/pca.py,57,"print 'eigVects=', eigVects",
AiLearning/src/py2.x/ml/13.PCA/pca.py,58,对特征值，进行从小到大的排序，返回从小到大的index序号,
AiLearning/src/py2.x/ml/13.PCA/pca.py,59,特征值的逆序就可以得到topNfeat个最大的特征向量,
AiLearning/src/py2.x/ml/13.PCA/pca.py,73,"print 'eigValInd1=', eigValInd",
AiLearning/src/py2.x/ml/13.PCA/pca.py,75,-1表示倒序，返回topN的特征值[-1 到 -(topNfeat+1) 但是不包括-(topNfeat+1)本身的倒叙],
AiLearning/src/py2.x/ml/13.PCA/pca.py,77,"print 'eigValInd2=', eigValInd",
AiLearning/src/py2.x/ml/13.PCA/pca.py,78,重组 eigVects 最大到最小,
AiLearning/src/py2.x/ml/13.PCA/pca.py,80,"print 'redEigVects=', redEigVects.T",
AiLearning/src/py2.x/ml/13.PCA/pca.py,81,将数据转换到新空间,
AiLearning/src/py2.x/ml/13.PCA/pca.py,82,"print ""---"", shape(meanRemoved), shape(redEigVects)",
AiLearning/src/py2.x/ml/13.PCA/pca.py,85,"print 'lowDDataMat=', lowDDataMat",
AiLearning/src/py2.x/ml/13.PCA/pca.py,86,"print 'reconMat=', reconMat",
AiLearning/src/py2.x/ml/13.PCA/pca.py,94,对value不为NaN的求均值,
AiLearning/src/py2.x/ml/13.PCA/pca.py,95,.A 返回矩阵基于的数组,
AiLearning/src/py2.x/ml/13.PCA/pca.py,97,将value为NaN的值赋值为均值,
AiLearning/src/py2.x/ml/13.PCA/pca.py,137,# 加载数据，并转化数据类型为float,
AiLearning/src/py2.x/ml/13.PCA/pca.py,138,dataMat = loadDataSet('data/13.PCA/testSet.txt'),
AiLearning/src/py2.x/ml/13.PCA/pca.py,139,# 只需要1个特征向量,
AiLearning/src/py2.x/ml/13.PCA/pca.py,140,"lowDmat, reconMat = pca(dataMat, 1)",
AiLearning/src/py2.x/ml/13.PCA/pca.py,141,# 只需要2个特征向量，和原始数据一致，没任何变化,
AiLearning/src/py2.x/ml/13.PCA/pca.py,142,"# lowDmat, reconMat = pca(dataMat, 2)",
AiLearning/src/py2.x/ml/13.PCA/pca.py,143,# print shape(lowDmat),
AiLearning/src/py2.x/ml/13.PCA/pca.py,144,"show_picture(dataMat, reconMat)",
AiLearning/src/py2.x/ml/13.PCA/pca.py,146,利用PCA对半导体制造数据降维,
AiLearning/src/py2.x/ml/13.PCA/pca.py,149,分析数据,
AiLearning/src/py2.x/ml/13.PCA/pca.py,151,"lowDmat, reconMat = pca(dataMat, 20)",
AiLearning/src/py2.x/ml/13.PCA/pca.py,152,print shape(lowDmat),
AiLearning/src/py2.x/ml/13.PCA/pca.py,153,"show_picture(dataMat, reconMat)",
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,20,创建40个分离点,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,22,"X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]",
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,23,Y = [0] * 20 + [1] * 20,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,51,拟合一个SVM模型,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,55,获取分割超平面,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,57,斜率,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,59,从-5到5，顺序间隔采样50个样本，默认是num=50,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,60,"xx = np.linspace(-5, 5)  # , num=50)",
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,61,", num=50)",
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,62,二维的直线方程,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,66,plot the parallels to the separating hyperplane that pass through the support vectors,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,67,通过支持向量绘制分割超平面,
AiLearning/src/py2.x/ml/6.SVM/sklearn-svm-demo.py,74,"plot the line, the points, and the nearest vectors to the plane",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,2,coding:utf8,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,37,数据的行数,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,42,误差缓存，第一列给出的是eCache是否有效的标志位，第二列给出的是实际的E值。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,45,m行m列的矩阵,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,51,calc the kernel or transform data to a higher dimensional space,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,65,linear kernel:   m*n * n*1 = m*1,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,71,径向基函数的高斯版本,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,72,divide in NumPy is element-wise not matrix like Matlab,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,128,"this is the second choice -heurstic, and calcs Ej",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,147,首先将输入值Ei在缓存中设置成为有效的。这里的有效意味着它已经计算好了。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,150,"print 'oS.eCache[%s]=%s' % (i, oS.eCache[i])",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,151,"print 'oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,152,"""""""",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,153,# 返回非0的：行列值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,154,"nonzero(oS.eCache[:, 0].A)= (",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,155,"行： array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]),",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,156,"列： array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,157,),
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,158,"""""""",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,159,"print 'nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A)",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,160,# 取行的list,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,161,"print 'nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0]",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,162,非零E值的行的list列表，所对应的alpha值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,165,在所有的值上进行循环，并选择其中使得改变最大的那个值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,167,"don't calc for i, waste of time",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,169,求 Ek误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,173,选择具有最大步长的j,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,178,如果是第一次循环，则随机选择一个alpha值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,181,求 Ek误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,195,求 误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,228,求 Ek误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,231,约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值),
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,232,0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,233,表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,241,选择最大的误差对应的j进行优化。效果更明显,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,246,L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接return 0,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,254,"print(""L==H"")",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,257,eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,258,参考《统计学习方法》李航-P125~P128<序列最小最优化算法>,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,259,changed for kernel,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,264,计算出一个新的alphas[j]值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,266,并使用辅助函数，以及L和H对其进行调整,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,268,更新误差缓存,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,271,检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,273,"print(""j not moving enough"")",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,276,然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,278,更新误差缓存,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,281,"在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,282,w= Σ[1~n] ai*yi*xi => b = yi- Σ[1~n] ai*yi(xi*xj),
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,283,所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1),
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,284,为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,315,创建一个 optStruct 对象,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,321,循环遍历：循环maxIter次 并且 （alphaPairsChanged存在可以改变 or 所有行遍历一遍）,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,325,当entireSet=true or 非边界alpha对没有了；就开始寻找 alpha对，然后决定是否要进行else。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,327,在数据集上遍历所有可能的alpha,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,329,是否存在alpha对，存在就+1,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,331,"print(""fullSet, iter: %d i:%d, pairs changed %d"" % (iter, i, alphaPairsChanged))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,334,对已存在 alpha对，选出非边界的alpha值，进行优化。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,336,遍历所有的非边界alpha值，也就是不在边界0或C上的值。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,340,"print(""non-bound, iter: %d i:%d, pairs changed %d"" % (iter, i, alphaPairsChanged))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,343,如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,345,toggle entire set loop,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,374,C=200 important,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,378,get matrix of only support vectors,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,386,"和这个svm-simple类似： fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,419,load the training set,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,424,take off .txt,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,436,1. 导入训练数据,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,444,"print(""there are %d Support Vectors"" % shape(sVs)[0])",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,449,1*m * m*1 = 1*1 单个预测结果,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,454,2. 导入测试数据,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,478,"b原来是矩阵，先转为数组类型后其数组大小为（1,1），所以后面加[0]，变为(1,)",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,483,注意flatten的用法,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,486,"x最大值，最小值根据原数据集dataArr[:, 0]的大小而定",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,489,"根据x.w + b = 0 得到，其式子展开为w0.x1 + w1.x2 + b = 0, x2就是y值",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,499,找到支持向量，并在图中标红,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,508,无核函数的测试,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,509,获取特征和目标变量,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,511,print labelArr,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,513,b是常量值， alphas是拉格朗日乘子,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,522,画图,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,526,有核函数的测试,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,529,# 项目实战,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,530,# 示例：手写识别问题回顾,
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,531,"testDigits(('rbf', 0.1))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,532,"testDigits(('rbf', 5))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,533,"testDigits(('rbf', 10))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,534,"testDigits(('rbf', 50))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,535,"testDigits(('rbf', 100))",
AiLearning/src/py2.x/ml/6.SVM/svm-complete.py,536,testDigits(('lin')),
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,2,coding:utf8,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,17,Initialize the structure with the parameters,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,25,first column is valid flag,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,94,"this is the second choice -heurstic, and calcs Ej",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,113,首先将输入值Ei在缓存中设置成为有效的。这里的有效意味着它已经计算好了。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,116,"print 'oS.eCache[%s]=%s' % (i, oS.eCache[i])",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,117,"print 'oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,118,"""""""",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,119,# 返回非0的：行列值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,120,"nonzero(oS.eCache[:, 0].A)= (",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,121,"行： array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]),",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,122,"列： array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,123,),
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,124,"""""""",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,125,"print 'nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A)",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,126,# 取行的list,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,127,"print 'nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0]",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,128,非零E值的行的list列表，所对应的alpha值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,131,在所有的值上进行循环，并选择其中使得改变最大的那个值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,133,"don't calc for i, waste of time",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,135,求 Ek误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,143,如果是第一次循环，则随机选择一个alpha值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,146,求 Ek误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,151,after any alpha has changed update the new value in the cache,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,160,求 误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,177,求 Ek误差：预测值-真实值的差,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,180,约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值),
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,181,0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,182,表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,190,选择最大的误差对应的j进行优化。效果更明显,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,195,L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接return 0,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,206,eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,207,参考《统计学习方法》李航-P125~P128<序列最小最优化算法>,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,213,计算出一个新的alphas[j]值,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,215,并使用辅助函数，以及L和H对其进行调整,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,217,更新误差缓存,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,220,检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,225,然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,227,更新误差缓存,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,230,"在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,231,w= Σ[1~n] ai*yi*xi => b = yj Σ[1~n] ai*yi(xi*xj),
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,232,所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1),
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,233,为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,263,创建一个 optStruct 对象,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,269,循环遍历：循环maxIter次 并且 （alphaPairsChanged存在可以改变 or 所有行遍历一遍）,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,270,循环迭代结束 或者 循环遍历所有alpha后，alphaPairs还是没变化,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,274,当entireSet=true or 非边界alpha对没有了；就开始寻找 alpha对，然后决定是否要进行else。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,276,在数据集上遍历所有可能的alpha,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,278,是否存在alpha对，存在就+1,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,282,对已存在 alpha对，选出非边界的alpha值，进行优化。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,284,遍历所有的非边界alpha值，也就是不在边界0或C上的值。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,291,如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,293,toggle entire set loop,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,331,"b原来是矩阵，先转为数组类型后其数组大小为（1,1），所以后面加[0]，变为(1,)",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,336,注意flatten的用法,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,339,"x最大值，最小值根据原数据集dataArr[:, 0]的大小而定",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,342,"根据x.w + b = 0 得到，其式子展开为w0.x1 + w1.x2 + b = 0, x2就是y值",
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,352,找到支持向量，并在图中标红,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,360,获取特征和目标变量,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,362,print labelArr,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,364,b是常量值， alphas是拉格朗日乘子,
AiLearning/src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py,373,画图,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,2,coding:utf8,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,82,矩阵转置 和 .T 一样的功能,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,86,初始化 b和alphas(alpha有点类似权重值。),
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,90,没有任何alpha改变的情况下遍历数据的次数,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,93,"w = calcWs(alphas, dataMatIn, classLabels)",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,94,"print(""w:"", w)",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,96,记录alpha是否已经进行优化，每次循环时设为0，然后再对整个集合顺序遍历,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,99,"print 'alphas=', alphas",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,100,"print 'labelMat=', labelMat",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,101,"print 'multiply(alphas, labelMat)=', multiply(alphas, labelMat)",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,102,我们预测的类别 y = w^Tx[i]+b; 其中因为 w = Σ(1~n) a[n]*lable[n]*x[n],
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,104,预测结果与真实结果比对，计算误差Ei,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,107,约束条件 (KKT条件是解决最优化问题的时用到的一种方法。我们这里提到的最优化问题通常是指对于给定的某一函数，求其在指定作用域上的全局最小值),
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,108,0<=alphas[i]<=C，但由于0和C是边界值，我们无法进行优化，因为需要增加一个alphas和降低一个alphas。,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,109,表示发生错误的概率：labelMat[i]*Ei 如果超出了 toler， 才需要优化。至于正负号，我们考虑绝对值就对了。,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,118,如果满足优化的条件，我们就随机选取非i的一个点，进行优化比较,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,120,预测j的结果,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,126,L和H用于将alphas[j]调整到0-C之间。如果L==H，就不做任何改变，直接执行continue语句,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,127,labelMat[i] != labelMat[j] 表示异侧，就相减，否则是同侧，就相加。,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,134,如果相同，就没发优化了,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,139,eta是alphas[j]的最优修改量，如果eta==0，需要退出for循环的当前迭代过程,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,140,参考《统计学习方法》李航-P125~P128<序列最小最优化算法>,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,146,计算出一个新的alphas[j]值,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,148,并使用辅助函数，以及L和H对其进行调整,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,150,检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环。,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,154,然后alphas[i]和alphas[j]同样进行改变，虽然改变的大小一样，但是改变的方向正好相反,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,156,"在对alpha[i], alpha[j] 进行优化之后，给这两个alpha值设置一个常数b。",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,157,w= Σ[1~n] ai*yi*xi => b = yj- Σ[1~n] ai*yi(xi*xj),
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,158,所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1),
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,159,为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,170,在for循环外，检查alpha值是否做了更新，如果在更新则将iter设为0后继续运行程序,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,171,知道更新完毕后，iter次循环无变化，才推出循环。,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,211,"b原来是矩阵，先转为数组类型后其数组大小为（1,1），所以后面加[0]，变为(1,)",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,216,注意flatten的用法,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,219,"x最大值，最小值根据原数据集dataArr[:, 0]的大小而定",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,222,"根据x.w + b = 0 得到，其式子展开为w0.x1 + w1.x2 + b = 0, x2就是y值",
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,232,找到支持向量，并在图中标红,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,240,获取特征和目标变量,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,242,print labelArr,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,244,b是常量值， alphas是拉格朗日乘子,
AiLearning/src/py2.x/ml/6.SVM/svm-simple.py,253,画图,
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,2,coding:utf-8,
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,26,生成一个 4*4 的随机数组,
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,29,转化关系， 数组转化为矩阵,
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,46,输出结果,
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,52,矩阵和逆矩阵 进行求积 (单位矩阵，对角线都为1嘛，理论上4*4的矩阵其他的都为0),
AiLearning/src/py2.x/ml/1.MLFoundation/NumPy.py,54,误差,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,2,coding:utf8,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,15,默认解析的数据是用tab分隔，并且是数值类型,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,16,general function to parse tab -delimited floats,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,26,假定最后一列是结果值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,27,assume last column is target value,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,32,将所有的元素转化为float类型,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,33,map all elements to float(),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,34,map() 函数具体的含义，可见 https://my.oschina.net/zyzzy/blog/115096,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,52,# 测试案例,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,53,"print 'dataSet[:, feature]=', dataSet[:, feature]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,54,"print 'nonzero(dataSet[:, feature] > value)[0]=', nonzero(dataSet[:, feature] > value)[0]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,55,"print 'nonzero(dataSet[:, feature] <= value)[0]=', nonzero(dataSet[:, feature] <= value)[0]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,57,"dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,58,"nonzero(dataSet[:, feature] > value)  返回结果为true行的index下标",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,64,返回每一个叶子结点的均值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,65,returns the value used for each leaf,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,66,我的理解是：regLeaf 是产生叶节点的函数，就是求均值，即用聚类中心点来代表这类数据,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,71,计算总方差=方差*样本数,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,72,我的理解是：求这组数据的方差，即通过决策树划分，可以让靠近的数据分到同一类中去,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,74,shape(dataSet)[0] 表示行数,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,78,1.用最佳方式切分数据集,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,79,2.生成相应的叶节点,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,94,"ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,95,之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,96,最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,98,划分最小 size 小于，就不继续划分了,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,100,如果结果集(最后一列为1个变量)，就返回退出,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,101,.T 对数据集进行转置,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,102,.tolist()[0] 转化为数组并取第0列,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,103,如果集合size为1，也就是说全部的数据都是同一个类别，不用继续划分。,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,104,exit cond 1,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,106,计算行列值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,108,无分类误差的总方差和,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,109,the choice of the best feature is driven by Reduction in RSS error from mean,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,111,inf 正无穷大,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,113,循环处理每一列对应的feature值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,114,对于每个特征,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,115,[0]表示这一列的[所有行]，不要[0]就是一个array[[所有行]]，下面的一行表示的是将某一列全部的数据转换为行，然后设置为list形式,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,117,对该列进行分组，然后组内的成员的val值进行 二元切分,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,119,判断二元切分的方式的元素数量是否符合预期,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,123,如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,124,如果划分后误差小于 bestS，则说明找到了新的bestS,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,129,判断二元切分的方式的元素误差是否符合预期,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,130,if the decrease (S-bestS) is less than a threshold don't do the split,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,134,对整体的成员进行判断，是否符合预期,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,135,如果集合的 size 小于 tolN,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,136,当最佳划分后，集合过小，也不划分，产生叶节点,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,141,assume dataSet is NumPy Mat so we can array filtering,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,142,假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,154,选择最好的切分方式： feature索引值，最优切分值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,155,choose the best split,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,157,if the splitting hit a stop condition return val,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,158,如果 splitting 达到一个停止条件，那么返回 val,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,167,大于在右边，小于在左边，分为2个数据集,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,169,递归的进行调用，在左右子树中继续递归生成树,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,175,判断节点是否是一个字典,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,188,计算左右枝丫的均值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,206,检查是否适合合并分枝,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,217,判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,221,判断分枝是否是dict字典，如果是就将测试数据集进行切分,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,224,如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,227,如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,231,上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,233,如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,234,1. 如果正确,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,235,* 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,236,* 如果 合并的总方差 < 不合并的总方差，那么就进行合并,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,237,注意返回的结果： 如果可以合并，原来的dict就变为了 数值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,240,"power(x, y)表示x的y次方；这时tree['left']和tree['right']都是具体数值",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,244,如果 合并的总方差 < 不合并的总方差，那么就进行合并,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,248,两个return可以简化成一个,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,255,得到模型的ws系数：f(x) = x0 + x1*featrue1+ x2*featrue2 ...,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,256,create linear model and return coeficients,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,270,计算线性模型的误差值,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,282,"print corrcoef(yHat, Y, rowvar=0)",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,286,helper function used in two places,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,299,产生一个关于1的矩阵,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,302,X的0列为1，常数项，用于计算平衡误差,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,306,转置矩阵*矩阵,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,308,如果矩阵的逆不存在，会造成程序异常,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,311,最小二乘法求最优解:  w0*1+w1*x1=y,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,316,回归树测试案例,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,317,为了和 modelTreeEval() 保持一致，保留两个输入参数,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,331,模型树测试案例,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,332,对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,333,也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,347,"print X, model",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,351,计算预测的结果,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,352,在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,353,modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,354,此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,355,调用modelEval()函数，该函数的默认值为regTreeEval(),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,369,书中写的是inData[tree['spInd']]，只适合inData只有一列的情况，否则会产生异常,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,371,可以把if-else去掉，只留if里面的分支,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,377,同上，可以把if-else去掉，只留if里面的分支,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,384,预测结果,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,398,print yHat,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,401,"print ""yHat==>"", yHat[i, 0]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,406,测试数据集,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,413,# 回归树,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,414,myDat = loadDataSet('data/9.RegTrees/data1.txt'),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,415,# myDat = loadDataSet('data/9.RegTrees/data2.txt'),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,416,"# print 'myDat=', myDat",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,417,myMat = mat(myDat),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,418,"# print 'myMat=',  myMat",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,419,myTree = createTree(myMat),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,420,print myTree,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,422,# 1. 预剪枝就是：提起设置最大误差数和最少元素数,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,423,myDat = loadDataSet('data/9.RegTrees/data3.txt'),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,424,myMat = mat(myDat),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,425,"myTree = createTree(myMat, ops=(0, 1))",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,426,print myTree,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,428,# 2. 后剪枝就是：通过测试数据，对预测模型进行合并判断,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,429,myDatTest = loadDataSet('data/9.RegTrees/data3test.txt'),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,430,myMat2Test = mat(myDatTest),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,431,"myFinalTree = prune(myTree, myMat2Test)",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,432,print '\n\n\n-------------------',
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,433,print myFinalTree,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,435,# --------,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,436,# 模型树求解,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,437,myDat = loadDataSet('data/9.RegTrees/data4.txt'),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,438,myMat = mat(myDat),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,439,"myTree = createTree(myMat, modelLeaf, modelErr)",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,440,print myTree,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,442,# # 回归树 VS 模型树 VS 线性回归,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,443,trainMat = mat(loadDataSet('data/9.RegTrees/bikeSpeedVsIq_train.txt')),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,444,testMat = mat(loadDataSet('data/9.RegTrees/bikeSpeedVsIq_test.txt')),
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,445,# # 回归树,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,446,"myTree1 = createTree(trainMat, ops=(1, 20))",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,447,print myTree1,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,448,"yHat1 = createForeCast(myTree1, testMat[:, 0])",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,449,"print ""--------------\n""",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,450,# print yHat1,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,451,"# print ""ssss==>"", testMat[:, 1]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,452,# corrcoef 返回皮尔森乘积矩相关系数,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,453,"print ""regTree:"", corrcoef(yHat1, testMat[:, 1],rowvar=0)[0, 1]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,455,# 模型树,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,456,"myTree2 = createTree(trainMat, modelLeaf, modelErr, ops=(1, 20))",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,457,"yHat2 = createForeCast(myTree2, testMat[:, 0], modelTreeEval)",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,458,print myTree2,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,459,"print ""modelTree:"", corrcoef(yHat2, testMat[:, 1],rowvar=0)[0, 1]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,461,# 线性回归,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,462,"ws, X, Y = linearSolve(trainMat)",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,463,print ws,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,464,"m = len(testMat[:, 0])",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,465,"yHat3 = mat(zeros((m, 1)))",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,466,for i in range(shape(testMat)[0]):,
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,467,"yHat3[i] = testMat[i, 0]*ws[1, 0] + ws[0, 0]",
AiLearning/src/py2.x/ml/9.RegTrees/regTrees.py,468,"print ""lr:"", corrcoef(yHat3, testMat[:, 1],rowvar=0)[0, 1]",
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,2,coding:utf8,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,24,"相当于告诉 布局管理器(Geometry Manager),如果不设定位置，默认在 0行0列的位置",
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,28,最大为误差， 最大子叶节点的数量,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,30,clear the figure,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,34,检查复选框是否选中,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,44,use scatter for data set,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,46,use plot for yHat,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,69,画新的tree,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,71,#get values from Entry boxes,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,77,标题,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,79,"输入栏1, 叶子的数量",
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,85,"输入栏2, 误差量",
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,90,设置输出值,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,93,设置提交的按钮,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,96,设置复选按钮,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,102,退出按钮,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,105,创建一个画板 canvas,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,118,创建一个事件,
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,120,test_widget_text(root),
AiLearning/src/py2.x/ml/9.RegTrees/treeExplore.py,123,启动事件循环,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,15,引入必要的模型和库,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,20,创建一个随机的数据集,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,21,参考 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,23,"print 'lalalalala===', rng",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,24,"rand() 是给定形状的随机值，rng.rand(80, 1)即矩阵的形状是 80行，1列",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,25,sort(),
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,27,"print 'X=', X",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,29,"print 'y=', y",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,31,"print 'yyy=', y",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,33,拟合回归模型,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,34,regr_1 = DecisionTreeRegressor(max_depth=2),
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,35,保持 max_depth=5 不变，增加 min_samples_leaf=6 的参数，效果进一步提升了,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,38,regr_3 = DecisionTreeRegressor(max_depth=4),
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,39,"regr_1.fit(X, y)",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,41,"regr_3.fit(X, y)",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,43,预测,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,45,y_1 = regr_1.predict(X_test),
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,47,y_3 = regr_3.predict(X_test),
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,49,绘制结果,
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,52,"plt.plot(X_test, y_1, color=""cornflowerblue"", label=""max_depth=2"", linewidth=2)",
AiLearning/src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py,54,"plt.plot(X_test, y_3, color=""red"", label=""max_depth=3"", linewidth=2)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,2,coding:utf8,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,4,''',
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,5,Created on 2017-03-10,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,6,Update on 2017-03-10,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,7,author: jiangzhonglian,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,8,content: 回归树,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,9,''',
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,11,print(__doc__),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,14,# Import the necessary modules and libraries,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,15,import numpy as np,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,16,from sklearn.tree import DecisionTreeRegressor,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,17,import matplotlib.pyplot as plt,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,20,# Create a random dataset,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,21,rng = np.random.RandomState(1),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,22,"X = np.sort(5 * rng.rand(80, 1), axis=0)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,23,y = np.sin(X).ravel(),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,24,"print X, '\n\n\n-----------\n\n\n', y",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,25,y[::5] += 3 * (0.5 - rng.rand(16)),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,28,# Fit regression model,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,29,"regr_1 = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,30,"regr_2 = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,31,"regr_1.fit(X, y)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,32,"regr_2.fit(X, y)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,35,# Predict,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,36,"X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,37,y_1 = regr_1.predict(X_test),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,38,y_2 = regr_2.predict(X_test),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,41,# Plot the results,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,42,plt.figure(),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,43,"plt.scatter(X, y, c=""darkorange"", label=""data"")",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,44,"plt.plot(X_test, y_1, color=""cornflowerblue"", label=""max_depth=2"", linewidth=2)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,45,"plt.plot(X_test, y_2, color=""yellowgreen"", label=""max_depth=5"", linewidth=2)",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,46,"plt.xlabel(""data"")",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,47,"plt.ylabel(""target"")",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,48,"plt.title(""Decision Tree Regression"")",
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,49,plt.legend(),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,50,plt.show(),
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,69,Author: Noel Dawe <noel.dawe@gmail.com>,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,70,,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,71,License: BSD 3 clause,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,73,importing necessary libraries,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,79,Create the dataset,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,84,Fit regression model,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,93,Predict,
AiLearning/src/py2.x/ml/9.RegTrees/RTSklearn.py,97,Plot the results,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,2,coding:utf8,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,24,needs to be updated,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,47,"['r', 'x', 'n', 'o', 's'],",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,63,this version does not use recursion,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,74,建立相同元素之间的关系，例如： 左边的r指向右边的r值,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,91,取出 元素 出现次数最高的,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,92,如果该元素在 inTree.children 这个字典中，就进行累加,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,93,如果该元素不存在 就 inTree.children 字典中新增key，value为初始化的 treeNode 对象,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,95,更新 最大元素，对应的 treeNode 对象的count进行叠加,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,98,如果不存在子节点，我们为该inTree添加子节点,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,100,如果满足minSup的dist字典的value值第二位为null， 我们就设置该元素为 本节点对应的tree节点,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,101,如果元素第二位不为null，我们就更新header节点,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,103,headerTable只记录第一次节点出现的位置,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,106,本质上是修改headerTable的key对应的Tree，的nodeLink值,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,109,递归的调用，在items[0]的基础上，添加item0[1]做子节点， count只要循环的进行累计加和而已，统计出节点的最后的统计值。,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,123,支持度>=minSup的dist{所有元素：出现的次数},
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,125,循环 dist{行：出现次数}的样本数据,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,127,对所有的行进行循环，得到行里面的所有元素,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,128,统计每一行中，每个元素出现的总次数,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,130,例如： {'ababa': 3}  count(a)=3+3+3=9   count(b)=3+3=6,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,132,删除 headerTable中，元素次数<最小支持度的元素,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,137,满足minSup: set(各元素集合),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,139,如果不存在，直接返回None,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,143,"格式化： dist{元素key: [元素次数, None]}",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,146,create tree,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,148,循环 dist{行：出现次数}的样本数据,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,150,"print 'tranSet, count=', tranSet, count",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,151,localD = dist{元素key: 元素总出现次数},
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,154,判断是否在满足minSup的集合中,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,156,"print 'headerTable[item][0]=', headerTable[item][0], headerTable[item]",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,158,"print 'localD=', localD",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,160,"p=key,value; 所以是通过value值的大小，进行从大到小进行排序",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,161,orderedItems 表示取出元组的key值，也就是字母本身，但是字母本身是大到小的顺序,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,163,"print 'orderedItems=', orderedItems, 'headerTable', headerTable, '\n\n\n'",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,164,填充树，通过有序的orderedItems的第一位，进行顺序填充 第一层的子节点。,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,192,对 treeNode的link进行循环,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,195,寻找改节点的父节点，相当于找到了该节点的频繁项集,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,197,避免 单独`Z`一个元素，添加了空节点,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,199,"对非basePat的倒叙值作为key,赋值为count数",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,200,prefixPath[1:] 变frozenset后，字母就变无序了,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,201,condPats[frozenset(prefixPath)] = treeNode.count,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,203,递归，寻找改节点的下一个 相同值的链接节点,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,205,print treeNode,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,219,通过value进行从小到大的排序， 得到频繁项集的key,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,220,最小支持项集的key的list集合,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,224,循环遍历 最频繁项集的key，从小到大的递归寻找对应的频繁项集,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,226,preFix为newFreqSet上一次的存储记录，一旦没有myHead，就不会更新,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,236,构建FP-tree,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,239,"挖掘条件 FP-tree, 如果myHead不为空，表示满足minSup {所有的元素+(value, treeNode)}",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,243,递归 myHead 找出频繁项集,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,248,import twitter,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,249,from time import sleep,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,250,import re,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,253,def getLotsOfTweets(searchStr):,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,254,"""""""",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,255,获取 100个搜索结果页面,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,256,"""""""",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,257,CONSUMER_KEY = '',
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,258,CONSUMER_SECRET = '',
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,259,ACCESS_TOKEN_KEY = '',
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,260,ACCESS_TOKEN_SECRET = '',
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,261,"api = twitter.Api(consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET, access_token_key=ACCESS_TOKEN_KEY, access_token_secret=ACCESS_TOKEN_SECRET)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,263,# you can get 1500 results 15 pages * 100 per page,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,264,resultsPages = [],
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,265,"for i in range(1, 15):",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,266,"print ""fetching page %d"" % i",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,267,"searchResults = api.GetSearch(searchStr, per_page=100, page=i)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,268,resultsPages.append(searchResults),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,269,sleep(6),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,270,return resultsPages,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,273,def textParse(bigString):,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,274,"""""""",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,275,解析页面内容,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,276,"""""""",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,277,"urlsRemoved = re.sub('(http:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[~])*', '', bigString)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,278,"listOfTokens = re.split(r'\W*', urlsRemoved)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,279,return [tok.lower() for tok in listOfTokens if len(tok) > 2],
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,282,"def mineTweets(tweetArr, minSup=5):",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,283,"""""""",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,284,获取频繁项集,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,285,"""""""",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,286,parsedList = [],
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,287,for i in range(14):,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,288,for j in range(100):,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,289,parsedList.append(textParse(tweetArr[i][j].text)),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,290,initSet = createInitSet(parsedList),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,291,"myFPtree, myHeaderTab = createTree(initSet, minSup)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,292,myFreqList = [],
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,293,"mineTree(myFPtree, myHeaderTab, minSup, set([]), myFreqList)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,294,return myFreqList,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,298,"rootNode = treeNode('pyramid', 9, None)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,299,"rootNode.children['eye'] = treeNode('eye', 13, None)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,300,"rootNode.children['phoenix'] = treeNode('phoenix', 3, None)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,301,# 将树以文本形式显示,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,302,# print rootNode.disp(),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,304,load样本数据,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,306,"print simpDat, '\n'",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,307,frozen set 格式化 并 重新装载 样本数据，对所有的行进行统计求和，格式: {行：出现次数},
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,311,创建FP树,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,312,输入：dist{行：出现次数}的样本数据  和  最小的支持度,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,313,输出：最终的PF-tree，通过循环获取第一层的节点，然后每一层的节点进行递归的获取每一行的字节点，也就是分支。然后所谓的指针，就是后来的指向已存在的,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,317,抽取条件模式基,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,318,查询树节点的，频繁子项,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,323,创建条件模式基,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,328,# 项目实战,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,329,# 1.twitter项目案例,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,330,# 无法运行，因为没发链接twitter,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,331,lotsOtweets = getLotsOfTweets('RIMM'),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,332,"listOfTerms = mineTweets(lotsOtweets, 20)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,333,print len(listOfTerms),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,334,for t in listOfTerms:,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,335,print t,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,337,# 2.新闻网站点击流中挖掘，例如：文章1阅读过的人，还阅读过什么？,
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,338,parsedDat = [line.split() for line in open('data/12.FPGrowth/kosarak.dat').readlines()],
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,339,initSet = createInitSet(parsedDat),
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,340,"myFPtree, myHeaderTab = createTree(initSet, 100000)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,342,myFreList = [],
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,343,"mineTree(myFPtree, myHeaderTab, 100000, set([]), myFreList)",
AiLearning/src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py,344,print myFreList,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,19,"dataMat.append([float(lineArr[0]), float(lineArr[1]), float(lineArr[2])])",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,41,就是预测 y 的值,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,57,回归系数,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,60,重置 wDelta,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,62,它是学习率，代表了权重调整幅度的大小。（也可以理解为随机梯度的步长，使它不断减小，便于拟合）,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,63,输入T和K分别设定了迭代次数和待处理列表的大小。在T次迭代过程中，每次需要重新计算eta,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,66,全部的训练集  内循环中执行批处理，将分类错误的值全部做累加后更新权重向量,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,68,mapper 代码,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,70,"如果预测正确，并且预测结果的绝对值>=1，因为最大间隔为1, 认为没问题。",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,71,"否则算是预测错误, 通过预测错误的结果，来累计更新w.",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,72,mapper 代码,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,73,累积变化,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,74,w通过不断的随机梯度的方式来优化,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,75,在每个 T上应用更改,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,76,"print '-----', w",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,77,"print '++++++', w",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,83,"finalWs = seqPegasos(datMat, labelList, 2, 5000)",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,106,y2 = (0.43799*x)/0.12316,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/pegasos.py,107,2 iterations,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,17,"input key= class for one training example, e.g. ""-1.0""",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,18,e.g. [-1.0],
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,21,"input value = feature vector for one training example, e.g. ""3.0, 7.0, 2.0""",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,25,create matrix E and vector e,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,29,create a tuple with the values to be used by reducer,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,30,and encode it with base64 to avoid potential trouble with '\t' and '\n' used,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,31,as default separators in Hadoop Streaming,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,34,"note: a single constant key ""producedkey"" sends to only one reducer",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,35,"somewhat ""atypical"" due to low degree of parallism on reducer side",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,42,"key isn't used, so ignoring it with _ (underscore).",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,44,unpickle values,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,47,create the I/mu with correct dimensions,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,52,create sumETDe with correct dimensions,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,57,note: omega = result[:-1] and gamma = result[-1],
AiLearning/src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py,58,but printing entire vector as output,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,13,对数据初始化,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,19,接受输入数据流,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,20,需要 2 个参数，求数据的和与平方和,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,28,所有输入到达后开始处理,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,29,计算数据的平均值，平方的均值，并返回,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,36,从输入流中获取值,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMean.py,43,发出平均值和方差,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,24,返回值中包含输入文件的每一行的数据的一个大的List,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,27,创建一个输入的数据行的列表list,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,29,将输入行分割成单独的项目并存储在列表的列表中,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,31,输入 数据的个数，n个数据的均值，n个数据平方之后的均值,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,34,累计样本总和，总和 和 平分和的总和,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,42,计算均值( varSum是计算方差的展开形式 ),
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py,45,输出 数据总量，均值，平方的均值（方差）,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,27,返回一个 yield 迭代器，每次获取下一个值，节约内存。,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,30,创建一个输入的数据行的列表list,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,31,将得到的数据转化为 float 类型,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,32,获取数据的个数，即输入文件的数据的行数,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,33,将 List 转换为矩阵,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,34,将矩阵的数据分别求 平方，即 2次方,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,36,输出 数据的个数，n个数据的均值，n个数据平方之后的均值,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,37,第一行是标准输出，也就是reducer的输出,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,38,第二行识标准错误输出，即对主节点作出的响应报告，表明本节点工作正常。,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,39,【这不就是面试的装逼重点吗？如何设计监听架构细节】注意：一个好的习惯是想标准错误输出发送报告。如果某任务10分钟内没有报告输出，则将被Hadoop中止。,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py,40,计算均值,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,28,iteration number,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,39,需要 2 个参数,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,40,"input: nodeId, ('w', w-vector) OR nodeId, ('x', int)",
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,43,积累 w向量,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,46,累积数据点计算,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,47,迭代次数,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,50,这用于 debug， eta未在map中使用,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,54,将数据重新形成 X 和 Y,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,56,在第一次迭代时，初始化 w,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,58,calc p=w*dataSet[key].T,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,60,确保一切数据包含相同的key,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,61,它们将在同一个 reducer,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,65,从流输入获取值,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,79,wDelta += label*dataSet,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,80,calc new: eta,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,81,calc new: w = (1.0 - 1/t)*w + (eta/k)*wDelta,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,84,发出 w,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,86,增量 T,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/mrSVM.py,87,emit random ints for mappers iid,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/wc.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/wc.py,2,coding:utf8,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/wc.py,16,I'm a generator!,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/wc.py,18,+1 for newline,
AiLearning/src/py2.x/ml/15.BigData_MapReduce/py27dbg.py,14,needs exactly 2 arguments,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,2,coding: utf-8,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,11,导入科学计算包numpy和运算符模块operator,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,44,-----------实现 classify0() 方法的第一种方式----------------------------------------------------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,45,1. 距离计算,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,47,tile生成和训练样本对应的矩阵，并与训练样本求差,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,74,取平方,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,76,将矩阵的每一行相加,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,78,开方,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,80,根据距离排序从小到大的排序，返回对应的索引位置,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,81,argsort() 是将x中的元素从小到大排列，提取其对应的index（索引），然后输出到y。,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,82,"例如：y=array([3,0,2,1,4,5]) 则，x[3]=-1最小，所以y[0]=3;x[5]=9最大，所以y[5]=5。",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,83,"print 'distances=', distances",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,85,"print 'distances.argsort()=', sortedDistIndicies",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,87,2. 选择距离最小的k个点,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,90,找到该样本的类型,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,92,在字典中将该类型加一,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,93,字典的get方法,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,94,"如：list.get(k,d) 其中 get相当于一条if...else...语句,参数k在字典中，字典将返回list[k];如果参数k不在字典中则返回参数d,如果K在字典中则返回k对应的value值",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,95,"l = {5:2,3:4}",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,96,"print l.get(3,0)返回的值是4；",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,97,"Print l.get（1,0）返回值是0；",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,99,3. 排序并返回出现最多的那个类型,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,100,字典的 items() 方法，以列表返回可遍历的(键，值)元组数组。,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,101,"例如：dict = {'Name': 'Zara', 'Age': 7}   print ""Value : %s"" %  dict.items()   Value : [('Age', 7), ('Name', 'Zara')]",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,102,sorted 中的第2个参数 key=operator.itemgetter(1) 这个参数的意思是先比较第几个元素,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,103,"例如：a=[('b',2),('a',1),('c',0)]  b=sorted(a,key=operator.itemgetter(1)) >>>b=[('c',0),('a',1),('b',2)] 可以看到排序是按照后边的0,1,2进行排序的，而不是a,b,c",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,104,"b=sorted(a,key=operator.itemgetter(0)) >>>b=[('a',1),('b',2),('c',0)] 这次比较的是前边的a,b,c而不是0,1,2",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,105,"b=sorted(a,key=opertator.itemgetter(1,0)) >>>b=[('c',0),('a',1),('b',2)] 这个是先比较第2个元素，然后对第一个元素进行排序，形成多级排序。",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,106,"sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,107,return sortedClassCount[0][0],
AiLearning/src/py2.x/ml/2.KNN/kNN.py,108,3.利用max函数直接返回字典中value最大的key,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,112,------------------------------------------------------------------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,113,实现 classify0() 方法的第二种方式,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,115,"""""""",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,116,1. 计算距离,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,118,欧氏距离： 点到点之间的距离,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,119,第一行： 同一个点 到 dataSet的第一个点的距离。,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,120,第二行： 同一个点 到 dataSet的第二个点的距离。,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,121,...,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,122,第N行： 同一个点 到 dataSet的第N个点的距离。,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,124,"[[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,125,(A1-A2)^2+(B1-B2)^2+(c1-c2)^2,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,127,inx - dataset 使用了numpy broadcasting，见 https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,128,np.sum() 函数的使用见 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,129,"""""""",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,130,"dist = np.sum((inx - dataset)**2, axis=1)**0.5",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,132,"""""""",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,133,2. k个最近的标签,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,135,对距离排序使用numpy中的argsort函数， 见 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sort.html#numpy.sort,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,136,函数返回的是索引，因此取前k个索引使用[0 : k],
AiLearning/src/py2.x/ml/2.KNN/kNN.py,137,将这k个标签存在列表k_labels中,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,138,"""""""",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,139,k_labels = [labels[index] for index in dist.argsort()[0 : k]],
AiLearning/src/py2.x/ml/2.KNN/kNN.py,140,"""""""",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,141,3. 出现次数最多的标签即为最终类别,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,143,"使用collections.Counter可以统计各个标签的出现次数，most_common返回出现次数最多的标签tuple，例如[('lable1', 2)]，因此[0][0]可以取出标签值",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,144,"""""""",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,145,label = Counter(k_labels).most_common(1)[0][0],
AiLearning/src/py2.x/ml/2.KNN/kNN.py,146,return label,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,148,------------------------------------------------------------------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,161,----------------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,169,获得文件中的数据行的行数,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,171,生成对应的空矩阵,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,172,例如：zeros(2，3)就是生成一个 2*3的矩阵，各个位置上全是 0,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,173,prepare matrix to return,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,174,prepare labels return,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,178,str.strip([chars]) --返回移除字符串头尾指定的字符生成的新字符串,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,180,以 '\t' 切割字符串,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,182,每列的属性数据,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,184,每列的类别数据，就是 label 标签数据,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,187,返回数据矩阵returnMat和对应的类别classLabelVector,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,201,计算每种属性的最大值、最小值、范围,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,204,极差,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,206,-------第一种实现方式---start-------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,209,生成与最小值之差组成的矩阵,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,211,将最小值之差除以范围组成矩阵,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,212,element wise divide,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,213,-------第一种实现方式---end---------------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,215,# -------第二种实现方式---start---------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,216,norm_dataset = (dataset - minvalue) / ranges,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,217,# -------第二种实现方式---end---------------------------------------------,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,226,设置测试数据的的一个比例（训练数据集比例=1-hoRatio）,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,227,"测试范围,一部分测试一部分作为样本",
AiLearning/src/py2.x/ml/2.KNN/kNN.py,228,从文件中加载数据,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,229,load data setfrom file,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,230,归一化数据,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,232,m 表示数据的行数，即矩阵的第一维,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,234,设置测试的样本数量， numTestVecs:m表示训练样本的数量,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,239,对数据测试,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,265,1. 导入数据,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,267,load the training set,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,270,hwLabels存储0～9对应的index位置， trainingMat存放的每个位置对应的图片向量,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,273,take off .txt,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,276,将 32*32的矩阵->1*1024的矩阵,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,279,2. 导入测试数据,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,280,iterate through the test set,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,285,take off .txt,
AiLearning/src/py2.x/ml/2.KNN/kNN.py,296,test1(),
AiLearning/src/py2.x/ml/2.KNN/kNN.py,297,datingClassTest(),
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,22,导入一些要玩的数据,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,23,iris = datasets.load_iris(),
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,24,"X = iris.data[:, :2]  # 我们只采用前两个feature. 我们可以使用二维数据集避免这个丑陋的切片",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,25,y = iris.target,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,27,"print 'X=', type(X), X",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,28,"print 'y=', type(y), y",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,33,"print 'X=', type(X), X",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,34,"print 'y=', type(y), y",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,36,网格中的步长,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,38,创建彩色的地图,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,39,"cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,40,"cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,46,我们创建了一个knn分类器的实例，并适合数据。,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,50,绘制决策边界。为此，我们将为每个分配一个颜色,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,51,"来绘制网格中的点 [x_min, x_max]x[y_min, y_max].",
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,58,将结果放入一个彩色图中,
AiLearning/src/py2.x/ml/2.KNN/sklearn-knn-demo.py,63,绘制训练点,
AiLearning/src/py2.x/ml/4.NaiveBayes/sklearn-nb-demo.py,1,!/usr/bin/python,
AiLearning/src/py2.x/ml/4.NaiveBayes/sklearn-nb-demo.py,2,coding:utf8,
AiLearning/src/py2.x/ml/4.NaiveBayes/sklearn-nb-demo.py,14,GaussianNB_高斯朴素贝叶斯,
AiLearning/src/py2.x/ml/4.NaiveBayes/sklearn-nb-demo.py,26,MultinomialNB_多项朴素贝叶斯,
AiLearning/src/py2.x/ml/4.NaiveBayes/sklearn-nb-demo.py,37,BernoulliNB_伯努利朴素贝叶斯,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,2,-*- coding:utf-8 -*-,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,17,项目案例1: 屏蔽社区留言板的侮辱性言论,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,24,"[0,0,1,1,1......]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,30,"1 is abusive, 0 not",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,40,create empty set,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,42,操作符 | 用于求两个集合的并集,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,43,union of the two sets,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,54,创建一个和词汇表等长的向量，并将其元素都设置为0,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,55,"[0,0......]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,56,遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,72,文件数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,74,单词数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,76,侮辱性文件的出现概率，即trainCategory中所有的1的个数，,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,77,代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,79,构造单词出现次数列表,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,80,"[0,0,0,.....]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,81,"[0,0,0,.....]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,83,整个数据集单词出现总数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,87,遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,89,"[0,1,1,....]->[0,1,1,...]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,92,如果不是侮辱性文件，则计算非侮辱性文件中出现的侮辱性单词的个数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,95,"类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,96,即 在1类别下，每个单词出现次数的占比,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,97,"[1,2,3,5]/90->[1/90,...]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,98,"类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,99,即 在0类别下，每个单词出现次数的占比,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,111,总文件数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,113,总单词数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,115,侮辱性文件的出现概率,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,117,构造单词出现次数列表,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,118,p0Num 正常的统计,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,119,p1Num 侮辱的统计,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,120,避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,121,"[0,0......]->[1,1,1,1,1.....]",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,124,整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,125,p0Denom 正常的统计,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,126,p1Denom 侮辱的统计,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,131,累加辱骂词的频次,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,133,对每篇文章的辱骂的频次 进行统计汇总,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,138,"类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,140,"类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,157,计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C)),
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,158,使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,159,我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,160,可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,181,1. 加载数据集,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,183,2. 创建单词集合,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,185,3. 计算单词是否出现并创建数据矩阵,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,188,返回m*len(myVocabList)的矩阵， 记录的都是0，1信息,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,190,4. 训练数据,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,192,5. 测试数据,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,201,------------------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,202,项目案例2: 使用朴素贝叶斯过滤垃圾邮件,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,204,切分文本,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,215,使用正则表达式来切分句子，其中分隔符是除单词、数字外的任意字符串,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,233,切分，解析数据，并归类为 1 类别,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,237,切分，解析数据，并归类为 0 类别,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,242,创建词汇表,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,246,随机取 10 个邮件用来测试,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,248,"random.uniform(x, y) 随机生成一个范围为 x - y 的实数",
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,272,-----------------------------------------------------------------------------------,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,273,项目案例3: 使用朴素贝叶斯从个人广告中获取区域倾向,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,275,将文本文件解析成 词条向量,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,277,创建一个其中所含元素都为0的向量,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,284,文件解析,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,291,RSS源分类器及高频词去除函数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,295,遍历词汇表中的每个词,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,296,统计每个词在文本中出现的次数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,297,根据每个词出现的次数从高到底对字典进行排序,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,298,返回出现次数最高的30个单词,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,304,每次访问一条RSS源,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,315,去掉出现次数最高的那些词,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,335,最具表征性的词汇显示函数,
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,354,testingNB(),
AiLearning/src/py2.x/ml/4.NaiveBayes/bayes.py,356,laTest(),
AiLearning/src/py2.x/dl/mnist.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/mnist.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/mnist.py,10,数据加载器基类,
AiLearning/src/py2.x/dl/mnist.py,37,图像数据加载器,
AiLearning/src/py2.x/dl/mnist.py,75,标签数据加载器,
AiLearning/src/py2.x/dl/activators.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/activators.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/activators.py,10,return weighted_input,
AiLearning/src/py2.x/dl/cnn.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/cnn.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/cnn.py,10,获取卷积区域,
AiLearning/src/py2.x/dl/cnn.py,29,获取一个2D区域的最大值所在的索引,
AiLearning/src/py2.x/dl/cnn.py,42,计算卷积,
AiLearning/src/py2.x/dl/cnn.py,64,为数组增加Zero padding,
AiLearning/src/py2.x/dl/cnn.py,95,对numpy数组进行element wise操作,
AiLearning/src/py2.x/dl/cnn.py,213,处理卷积步长，对原始sensitivity map进行扩展,
AiLearning/src/py2.x/dl/cnn.py,216,full卷积，对sensitivitiy map进行zero padding,
AiLearning/src/py2.x/dl/cnn.py,217,虽然原始输入的zero padding单元也会获得残差,
AiLearning/src/py2.x/dl/cnn.py,218,但这个残差不需要继续向上传递，因此就不计算了,
AiLearning/src/py2.x/dl/cnn.py,223,初始化delta_array，用于保存传递到上一层的,
AiLearning/src/py2.x/dl/cnn.py,224,sensitivity map,
AiLearning/src/py2.x/dl/cnn.py,226,对于具有多个filter的卷积层来说，最终传递到上一层的,
AiLearning/src/py2.x/dl/cnn.py,227,sensitivity map相当于所有的filter的,
AiLearning/src/py2.x/dl/cnn.py,228,sensitivity map之和,
AiLearning/src/py2.x/dl/cnn.py,231,将filter权重翻转180度,
AiLearning/src/py2.x/dl/cnn.py,235,计算与一个filter对应的delta_array,
AiLearning/src/py2.x/dl/cnn.py,241,将计算结果与激活函数的偏导数做element-wise乘法操作,
AiLearning/src/py2.x/dl/cnn.py,248,处理卷积步长，对原始sensitivity map进行扩展,
AiLearning/src/py2.x/dl/cnn.py,252,计算每个权重的梯度,
AiLearning/src/py2.x/dl/cnn.py,258,计算偏置项的梯度,
AiLearning/src/py2.x/dl/cnn.py,263,确定扩展后sensitivity map的大小,
AiLearning/src/py2.x/dl/cnn.py,264,计算stride为1时sensitivity map的大小,
AiLearning/src/py2.x/dl/cnn.py,269,构建新的sensitivity_map,
AiLearning/src/py2.x/dl/cnn.py,272,从原始sensitivity map拷贝误差值,
AiLearning/src/py2.x/dl/cnn.py,405,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py2.x/dl/cnn.py,408,计算forward值,
AiLearning/src/py2.x/dl/cnn.py,412,求取sensitivity map,
AiLearning/src/py2.x/dl/cnn.py,415,计算梯度,
AiLearning/src/py2.x/dl/cnn.py,418,检查梯度,
AiLearning/src/py2.x/dl/recursive.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/recursive.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/recursive.py,19,递归神经网络实现,
AiLearning/src/py2.x/dl/recursive.py,34,权重数组W,
AiLearning/src/py2.x/dl/recursive.py,37,偏置项b,
AiLearning/src/py2.x/dl/recursive.py,39,递归神经网络生成的树的根节点,
AiLearning/src/py2.x/dl/recursive.py,85,根据式2计算每个子节点的delta,
AiLearning/src/py2.x/dl/recursive.py,89,slices = [(子节点编号，子节点delta起始位置，子节点delta结束位置)],
AiLearning/src/py2.x/dl/recursive.py,93,针对每个子节点，递归调用calc_delta函数,
AiLearning/src/py2.x/dl/recursive.py,139,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py2.x/dl/recursive.py,144,计算forward值,
AiLearning/src/py2.x/dl/recursive.py,149,求取sensitivity map,
AiLearning/src/py2.x/dl/recursive.py,152,计算梯度,
AiLearning/src/py2.x/dl/recursive.py,155,检查梯度,
AiLearning/src/py2.x/dl/bp.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/bp.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/bp.py,8,sigmoid 函数,
AiLearning/src/py2.x/dl/bp.py,21,定义神经网络的节点类,
AiLearning/src/py2.x/dl/bp.py,37,设置节点所在的层的位置,
AiLearning/src/py2.x/dl/bp.py,39,设置层中的节点的索引,
AiLearning/src/py2.x/dl/bp.py,41,设置此节点的下游节点，也就是这个节点与下一层的哪个节点相连,
AiLearning/src/py2.x/dl/bp.py,43,设置此节点的上游节点，也就是哪几个节点的下游节点与此节点相连,
AiLearning/src/py2.x/dl/bp.py,45,此节点的输出,
AiLearning/src/py2.x/dl/bp.py,47,此节点真实值与计算值之间的差值,
AiLearning/src/py2.x/dl/bp.py,70,使用 list 的 append 方法来将 conn 中的节点添加到 downstream 中,
AiLearning/src/py2.x/dl/bp.py,82,使用 list 的 append 方法来将 conn 中的节点添加到 upstream 中,
AiLearning/src/py2.x/dl/bp.py,94,使用 reduce() 函数对其中的因素求和,
AiLearning/src/py2.x/dl/bp.py,96,对上游节点的 output 乘 weights 之后求和得到的结果应用 sigmoid 函数，得到当前节点的 output,
AiLearning/src/py2.x/dl/bp.py,108,根据 https://www.zybuluo.com/hanbingtao/note/476663 的 式4 计算隐藏层的delta,
AiLearning/src/py2.x/dl/bp.py,110,计算此节点的 delta,
AiLearning/src/py2.x/dl/bp.py,122,就是那输出层的 delta,
AiLearning/src/py2.x/dl/bp.py,134,打印格式：第几层 - 第几个节点，output 是多少，delta 是多少,
AiLearning/src/py2.x/dl/bp.py,136,下游节点,
AiLearning/src/py2.x/dl/bp.py,138,上游节点,
AiLearning/src/py2.x/dl/bp.py,140,将本节点 + 下游节点 + 上游节点 的信息打印出来,
AiLearning/src/py2.x/dl/bp.py,144,ConstNode 对象，为了实现一个输出恒为 1 的节点（计算偏置项 wb 时需要）,
AiLearning/src/py2.x/dl/bp.py,175,使用 list 的 append 方法将包含下游节点的 conn 添加到 downstream 中,
AiLearning/src/py2.x/dl/bp.py,188,使用我们的 公式 4 来计算下游节点的 delta，求和,
AiLearning/src/py2.x/dl/bp.py,190,计算隐藏层的本节点的 delta,
AiLearning/src/py2.x/dl/bp.py,203,将节点的信息打印出来,
AiLearning/src/py2.x/dl/bp.py,204,格式 第几层-第几个节点的 output,
AiLearning/src/py2.x/dl/bp.py,206,此节点的下游节点的信息,
AiLearning/src/py2.x/dl/bp.py,208,将此节点与下游节点的信息组合，一起打印出来,
AiLearning/src/py2.x/dl/bp.py,212,神经网络的层对象，负责初始化一层。此外，作为 Node 的集合对象，提供对 Node 集合的操作,
AiLearning/src/py2.x/dl/bp.py,229,设置 层的索引,
AiLearning/src/py2.x/dl/bp.py,231,设置层中的节点的 list,
AiLearning/src/py2.x/dl/bp.py,233,将 Node 节点添加到 nodes 中,
AiLearning/src/py2.x/dl/bp.py,236,将 ConstNode 节点也添加到 nodes 中,
AiLearning/src/py2.x/dl/bp.py,248,设置输入层中各个节点的 output,
AiLearning/src/py2.x/dl/bp.py,261,遍历本层的所有节点（除去最后一个节点，因为它是恒为常数的偏置项b）,
AiLearning/src/py2.x/dl/bp.py,262,调用节点的 calc_output 方法来计算输出向量,
AiLearning/src/py2.x/dl/bp.py,275,遍历层的所有的节点 nodes，将节点信息打印出来,
AiLearning/src/py2.x/dl/bp.py,280,Connection 对象类，主要负责记录连接的权重，以及这个连接所关联的上下游的节点,
AiLearning/src/py2.x/dl/bp.py,296,设置上游节点,
AiLearning/src/py2.x/dl/bp.py,298,设置下游节点,
AiLearning/src/py2.x/dl/bp.py,300,设置权重，这里设置的权重是 -0.1 到 0.1 之间的任何数,
AiLearning/src/py2.x/dl/bp.py,302,设置梯度 为 0.0,
AiLearning/src/py2.x/dl/bp.py,314,下游节点的 delta * 上游节点的 output 计算得到梯度,
AiLearning/src/py2.x/dl/bp.py,326,调用计算梯度的函数来将梯度计算出来,
AiLearning/src/py2.x/dl/bp.py,328,使用梯度下降算法来更新权重,
AiLearning/src/py2.x/dl/bp.py,351,格式为：上游节点的层的索引+上游节点的节点索引 ---> 下游节点的层的索引+下游节点的节点索引，最后一个数是权重,
AiLearning/src/py2.x/dl/bp.py,361,Connections 对象，提供 Connection 集合操作。,
AiLearning/src/py2.x/dl/bp.py,376,初始化一个列表 list,
AiLearning/src/py2.x/dl/bp.py,403,Network 对象，提供相应 API,
AiLearning/src/py2.x/dl/bp.py,418,初始化 connections，使用的是 Connections 对象,
AiLearning/src/py2.x/dl/bp.py,420,初始化 layers,
AiLearning/src/py2.x/dl/bp.py,422,我们的神经网络的层数,
AiLearning/src/py2.x/dl/bp.py,424,节点数,
AiLearning/src/py2.x/dl/bp.py,426,遍历所有的层，将每层信息添加到 layers 中去,
AiLearning/src/py2.x/dl/bp.py,429,遍历除去输出层之外的所有层，将连接信息添加到 connections 对象中,
AiLearning/src/py2.x/dl/bp.py,432,遍历 connections，将 conn 添加到 connections 中,
AiLearning/src/py2.x/dl/bp.py,435,为下游节点添加上游节点为 conn,
AiLearning/src/py2.x/dl/bp.py,437,为上游节点添加下游节点为 conn,
AiLearning/src/py2.x/dl/bp.py,453,循环迭代 epoch 次,
AiLearning/src/py2.x/dl/bp.py,455,遍历每个训练样本,
AiLearning/src/py2.x/dl/bp.py,457,使用此样本进行训练（一条样本进行训练）,
AiLearning/src/py2.x/dl/bp.py,459,print 'sample %d training finished' % d,
AiLearning/src/py2.x/dl/bp.py,472,调用 Network 的 predict 方法，对这个样本进行预测,
AiLearning/src/py2.x/dl/bp.py,474,计算根据此样本得到的结果的 delta,
AiLearning/src/py2.x/dl/bp.py,476,更新权重,
AiLearning/src/py2.x/dl/bp.py,488,获取输出层的所有节点,
AiLearning/src/py2.x/dl/bp.py,490,遍历所有的 label,
AiLearning/src/py2.x/dl/bp.py,492,计算输出层节点的 delta,
AiLearning/src/py2.x/dl/bp.py,494,"这个用法就是切片的用法， [-2::-1] 就是将 layers 这个数组倒过来，从没倒过来的时候的倒数第二个元素开始，到翻转过来的倒数第一个数，比如这样：aaa = [1,2,3,4,5,6,7,8,9],bbb = aaa[-2::-1] ==> bbb = [8, 7, 6, 5, 4, 3, 2, 1]",
AiLearning/src/py2.x/dl/bp.py,495,实际上就是除掉输出层之外的所有层按照相反的顺序进行遍历,
AiLearning/src/py2.x/dl/bp.py,497,遍历每层的所有节点,
AiLearning/src/py2.x/dl/bp.py,499,计算隐藏层的 delta,
AiLearning/src/py2.x/dl/bp.py,511,按照正常顺序遍历除了输出层的层,
AiLearning/src/py2.x/dl/bp.py,513,遍历每层的所有节点,
AiLearning/src/py2.x/dl/bp.py,515,遍历节点的下游节点,
AiLearning/src/py2.x/dl/bp.py,517,根据下游节点来更新连接的权重,
AiLearning/src/py2.x/dl/bp.py,529,按照正常顺序遍历除了输出层之外的层,
AiLearning/src/py2.x/dl/bp.py,531,遍历层中的所有节点,
AiLearning/src/py2.x/dl/bp.py,533,遍历节点的下游节点,
AiLearning/src/py2.x/dl/bp.py,535,计算梯度,
AiLearning/src/py2.x/dl/bp.py,548,调用 predict() 方法，利用样本的特征数据对样本进行预测,
AiLearning/src/py2.x/dl/bp.py,550,计算 delta,
AiLearning/src/py2.x/dl/bp.py,552,计算梯度,
AiLearning/src/py2.x/dl/bp.py,564,首先为输入层设置输出值output为样本的输入向量，即不发生任何变化,
AiLearning/src/py2.x/dl/bp.py,566,遍历除去输入层开始到最后一层,
AiLearning/src/py2.x/dl/bp.py,568,计算 output,
AiLearning/src/py2.x/dl/bp.py,570,将计算得到的输出，也就是我们的预测值返回,
AiLearning/src/py2.x/dl/bp.py,582,遍历所有的 layers,
AiLearning/src/py2.x/dl/bp.py,584,将所有的层的信息打印出来,
AiLearning/src/py2.x/dl/bp.py,588,# ------------------------- 至此，基本上我们把 我们的神经网络实现完成，下面还会介绍一下对应的梯度检查相关的算法，现在我们首先回顾一下我们上面写道的类及他们的作用 ------------------------,
AiLearning/src/py2.x/dl/bp.py,637,#--------------------------------------回顾完成了，有些问题可能还是没有弄懂，没事，我们接着看下面---------------------------------------------,
AiLearning/src/py2.x/dl/bp.py,657,初始化 16 进制的数，用来判断位的，分别是,
AiLearning/src/py2.x/dl/bp.py,658,0x1 ---- 00000001,
AiLearning/src/py2.x/dl/bp.py,659,0x2 ---- 00000010,
AiLearning/src/py2.x/dl/bp.py,660,0x4 ---- 00000100,
AiLearning/src/py2.x/dl/bp.py,661,0x8 ---- 00001000,
AiLearning/src/py2.x/dl/bp.py,662,0x10 --- 00010000,
AiLearning/src/py2.x/dl/bp.py,663,0x20 --- 00100000,
AiLearning/src/py2.x/dl/bp.py,664,0x40 --- 01000000,
AiLearning/src/py2.x/dl/bp.py,665,0x80 --- 10000000,
AiLearning/src/py2.x/dl/bp.py,677,此方法就相当于判断一个 8 位的向量，哪一位上有数字，如果有就将这个数设置为  0.9 ，否则，设置为 0.1，通俗比较来说，就是我们这里用 0.9 表示 1，用 0.1 表示 0,
AiLearning/src/py2.x/dl/bp.py,689,进行二分类，大于 0.5 就设置为 1，小于 0.5 就设置为 0,
AiLearning/src/py2.x/dl/bp.py,691,遍历 mask,
AiLearning/src/py2.x/dl/bp.py,694,将结果相加得到最终的预测结果,
AiLearning/src/py2.x/dl/bp.py,723,计算网络误差,
AiLearning/src/py2.x/dl/bp.py,726,获取网络在当前样本下每个连接的梯度,
AiLearning/src/py2.x/dl/bp.py,729,对每个权重做梯度检查,
AiLearning/src/py2.x/dl/bp.py,731,获取指定连接的梯度,
AiLearning/src/py2.x/dl/bp.py,734,增加一个很小的值，计算网络的误差,
AiLearning/src/py2.x/dl/bp.py,739,减去一个很小的值，计算网络的误差,
AiLearning/src/py2.x/dl/bp.py,740,刚才加过了一次，因此这里需要减去2倍,
AiLearning/src/py2.x/dl/bp.py,743,根据式6计算期望的梯度值,
AiLearning/src/py2.x/dl/bp.py,746,打印,
AiLearning/src/py2.x/dl/bp.py,759,调用 Normalizer() 类,
AiLearning/src/py2.x/dl/bp.py,761,初始化一个 list，用来存储后面的数据,
AiLearning/src/py2.x/dl/bp.py,764,0 到 256 ，其中以 8 为步长,
AiLearning/src/py2.x/dl/bp.py,766,调用 normalizer 对象的 norm 方法,
AiLearning/src/py2.x/dl/bp.py,768,在 data_set 中 append n,
AiLearning/src/py2.x/dl/bp.py,770,在 labels 中 append n,
AiLearning/src/py2.x/dl/bp.py,772,将它们返回,
AiLearning/src/py2.x/dl/bp.py,785,获取训练数据集,
AiLearning/src/py2.x/dl/bp.py,787,调用 network 中的 train方法来训练我们的神经网络,
AiLearning/src/py2.x/dl/bp.py,801,调用 Normalizer() 类,
AiLearning/src/py2.x/dl/bp.py,803,调用 norm 方法，对数据进行规范化,
AiLearning/src/py2.x/dl/bp.py,805,对测试数据进行预测,
AiLearning/src/py2.x/dl/bp.py,807,将结果打印出来,
AiLearning/src/py2.x/dl/bp.py,837,创建一个有 3 层的网络，每层有 2 个节点,
AiLearning/src/py2.x/dl/bp.py,839,样本的特征,
AiLearning/src/py2.x/dl/bp.py,841,样本对应的标签,
AiLearning/src/py2.x/dl/bp.py,843,使用梯度检查来查看是否正确,
AiLearning/src/py2.x/dl/bp.py,856,初始化一个神经网络，输入层 8 个节点，隐藏层 3 个节点，输出层 8 个节点,
AiLearning/src/py2.x/dl/bp.py,858,训练我们的神经网络,
AiLearning/src/py2.x/dl/bp.py,860,将我们的神经网络的信息打印出来,
AiLearning/src/py2.x/dl/bp.py,862,打印出神经网络的正确率,
AiLearning/src/py2.x/dl/lstm.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/lstm.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/lstm.py,18,门的激活函数,
AiLearning/src/py2.x/dl/lstm.py,20,输出的激活函数,
AiLearning/src/py2.x/dl/lstm.py,22,当前时刻初始化为t0,
AiLearning/src/py2.x/dl/lstm.py,24,各个时刻的单元状态向量c,
AiLearning/src/py2.x/dl/lstm.py,26,各个时刻的输出向量h,
AiLearning/src/py2.x/dl/lstm.py,28,各个时刻的遗忘门f,
AiLearning/src/py2.x/dl/lstm.py,30,各个时刻的输入门i,
AiLearning/src/py2.x/dl/lstm.py,32,各个时刻的输出门o,
AiLearning/src/py2.x/dl/lstm.py,34,各个时刻的即时状态c~,
AiLearning/src/py2.x/dl/lstm.py,36,"遗忘门权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py2.x/dl/lstm.py,39,"输入门权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py2.x/dl/lstm.py,42,"输出门权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py2.x/dl/lstm.py,45,"单元状态权重矩阵Wfh, Wfx, 偏置项bf",
AiLearning/src/py2.x/dl/lstm.py,74,遗忘门,
AiLearning/src/py2.x/dl/lstm.py,78,输入门,
AiLearning/src/py2.x/dl/lstm.py,82,输出门,
AiLearning/src/py2.x/dl/lstm.py,86,即时状态,
AiLearning/src/py2.x/dl/lstm.py,90,单元状态,
AiLearning/src/py2.x/dl/lstm.py,93,输出,
AiLearning/src/py2.x/dl/lstm.py,101,上次的LSTM输出,
AiLearning/src/py2.x/dl/lstm.py,132,初始化各个时刻的误差项,
AiLearning/src/py2.x/dl/lstm.py,133,输出误差项,
AiLearning/src/py2.x/dl/lstm.py,134,输出门误差项,
AiLearning/src/py2.x/dl/lstm.py,135,输入门误差项,
AiLearning/src/py2.x/dl/lstm.py,136,遗忘门误差项,
AiLearning/src/py2.x/dl/lstm.py,137,即时输出误差项,
AiLearning/src/py2.x/dl/lstm.py,139,保存从上一层传递下来的当前时刻的误差项,
AiLearning/src/py2.x/dl/lstm.py,142,迭代计算每个时刻的误差项,
AiLearning/src/py2.x/dl/lstm.py,161,获得k时刻前向计算的值,
AiLearning/src/py2.x/dl/lstm.py,171,根据式9计算delta_o,
AiLearning/src/py2.x/dl/lstm.py,190,保存全部delta值,
AiLearning/src/py2.x/dl/lstm.py,198,初始化遗忘门权重梯度矩阵和偏置项,
AiLearning/src/py2.x/dl/lstm.py,201,初始化输入门权重梯度矩阵和偏置项,
AiLearning/src/py2.x/dl/lstm.py,204,初始化输出门权重梯度矩阵和偏置项,
AiLearning/src/py2.x/dl/lstm.py,207,初始化单元状态权重梯度矩阵和偏置项,
AiLearning/src/py2.x/dl/lstm.py,211,计算对上一次输出h的权重梯度,
AiLearning/src/py2.x/dl/lstm.py,213,计算各个时刻的梯度,
AiLearning/src/py2.x/dl/lstm.py,219,实际梯度是各时刻梯度之和,
AiLearning/src/py2.x/dl/lstm.py,229,计算对本次输入x的权重梯度,
AiLearning/src/py2.x/dl/lstm.py,264,当前时刻初始化为t0,
AiLearning/src/py2.x/dl/lstm.py,266,各个时刻的单元状态向量c,
AiLearning/src/py2.x/dl/lstm.py,268,各个时刻的输出向量h,
AiLearning/src/py2.x/dl/lstm.py,270,各个时刻的遗忘门f,
AiLearning/src/py2.x/dl/lstm.py,272,各个时刻的输入门i,
AiLearning/src/py2.x/dl/lstm.py,274,各个时刻的输出门o,
AiLearning/src/py2.x/dl/lstm.py,276,各个时刻的即时状态c~,
AiLearning/src/py2.x/dl/lstm.py,291,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py2.x/dl/lstm.py,296,计算forward值,
AiLearning/src/py2.x/dl/lstm.py,301,求取sensitivity map,
AiLearning/src/py2.x/dl/lstm.py,304,计算梯度,
AiLearning/src/py2.x/dl/lstm.py,307,检查梯度,
AiLearning/src/py2.x/dl/perceptron.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/perceptron.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/perceptron.py,4,神经元 / 感知器,
AiLearning/src/py2.x/dl/perceptron.py,27,设置的激活函数,
AiLearning/src/py2.x/dl/perceptron.py,29,权重向量初始化为 0,
AiLearning/src/py2.x/dl/perceptron.py,31,偏置项初始化为 0,
AiLearning/src/py2.x/dl/perceptron.py,56,将输入向量的计算结果返回,
AiLearning/src/py2.x/dl/perceptron.py,57,调用 激活函数 activator ，将输入向量输入，计算感知器的结果,
AiLearning/src/py2.x/dl/perceptron.py,58,reduce() 函数是 python 2 的内置函数，从 python 3 开始移到了 functools 模块,
AiLearning/src/py2.x/dl/perceptron.py,59,"reduce() 从左到右对一个序列的项累计地应用有两个参数的函数，以此合并序列到一个单一值，例如 reduce(lambda x,y: x+y, [1,2,3,4,5]) 计算的就是 ((((1+2)+3)+4)+5)",
AiLearning/src/py2.x/dl/perceptron.py,60,"map() 接收一个函数 f 和一个 list ，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 返回。比如我们的 f 函数是计算平方， map(f, [1,2,3,4,5]) ===> 返回 [1,4,9,16,25]",
AiLearning/src/py2.x/dl/perceptron.py,61,"zip() 接收任意多个（包括 0 个和 1个）序列作为参数，返回一个 tuple 列表。例：x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]",
AiLearning/src/py2.x/dl/perceptron.py,92,"zip() 接收任意多个（包括 0 个和 1个）序列作为参数，返回一个 tuple 列表。例：x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]",
AiLearning/src/py2.x/dl/perceptron.py,94,对每个样本，按照感知器规则更新权重,
AiLearning/src/py2.x/dl/perceptron.py,96,计算感知器在当前权重下的输出,
AiLearning/src/py2.x/dl/perceptron.py,98,更新权重,
AiLearning/src/py2.x/dl/perceptron.py,113,利用感知器规则更新权重,
AiLearning/src/py2.x/dl/perceptron.py,115,"map() 接收一个函数 f 和一个 list ，并通过把函数 f 依次作用在 list 的每个元素上，得到一个新的 list 返回。比如我们的 f 函数是计算平方， map(f, [1,2,3,4,5]) ===> 返回 [1,4,9,16,25]",
AiLearning/src/py2.x/dl/perceptron.py,116,"zip() 接收任意多个（包括 0 个和 1个）序列作为参数，返回一个 tuple 列表。例：x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]",
AiLearning/src/py2.x/dl/perceptron.py,118,更新 bias,
AiLearning/src/py2.x/dl/perceptron.py,145,构建训练数据，输入向量的列表,
AiLearning/src/py2.x/dl/perceptron.py,147,期望的输出列表，也就是上面的输入向量的列表中数据对应的标签，是一一对应的,
AiLearning/src/py2.x/dl/perceptron.py,161,创建感知器，输入参数的个数是 2 个（因为 and 是个二元函数），激活函数为 f,
AiLearning/src/py2.x/dl/perceptron.py,163,进行训练，迭代 10 轮，学习速率是我们设定的 rate ，为 0.1,
AiLearning/src/py2.x/dl/perceptron.py,166,返回训练好的感知器,
AiLearning/src/py2.x/dl/perceptron.py,179,训练 and 感知器,
AiLearning/src/py2.x/dl/perceptron.py,181,打印训练获得的权重,
AiLearning/src/py2.x/dl/perceptron.py,183,测试,
AiLearning/src/py2.x/dl/linear_unit.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/linear_unit.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/linear_unit.py,4,引入 Perceptron 类,
AiLearning/src/py2.x/dl/linear_unit.py,8,定义激活函数 f,
AiLearning/src/py2.x/dl/linear_unit.py,29,初始化我们的感知器类，设置输入参数的个数 input_num 和 激活函数 f,
AiLearning/src/py2.x/dl/linear_unit.py,32,构造简单的数据集,
AiLearning/src/py2.x/dl/linear_unit.py,43,构建数据集，输入向量列表，每一项是工作年限,
AiLearning/src/py2.x/dl/linear_unit.py,45,期望的输出列表，也就是输入向量的对应的标签，与工作年限对应的收入年薪,
AiLearning/src/py2.x/dl/linear_unit.py,50,使用我们的训练数据集对线性单元进行训练,
AiLearning/src/py2.x/dl/linear_unit.py,60,创建感知器对象，输入参数的个数也就是特征数为 1（工作年限）,
AiLearning/src/py2.x/dl/linear_unit.py,62,获取构建的数据集,
AiLearning/src/py2.x/dl/linear_unit.py,64,训练感知器，迭代 10 轮，学习率为 0.01,
AiLearning/src/py2.x/dl/linear_unit.py,66,返回训练好的线性单元,
AiLearning/src/py2.x/dl/linear_unit.py,70,将图像画出来,
AiLearning/src/py2.x/dl/linear_unit.py,80,引入绘图的库,
AiLearning/src/py2.x/dl/linear_unit.py,82,获取训练数据：特征 input_vecs 与 对应的标签 labels,
AiLearning/src/py2.x/dl/linear_unit.py,84,figure() 创建一个 Figure 对象，与用户交互的整个窗口，这个 figure 中容纳着 subplots,
AiLearning/src/py2.x/dl/linear_unit.py,86,在 figure 对象中创建 1行1列中的第一个图,
AiLearning/src/py2.x/dl/linear_unit.py,88,"scatter(x, y) 绘制散点图，其中的 x,y 是相同长度的数组序列",
AiLearning/src/py2.x/dl/linear_unit.py,90,设置权重,
AiLearning/src/py2.x/dl/linear_unit.py,92,设置偏置项,
AiLearning/src/py2.x/dl/linear_unit.py,94,"range(start, stop, step) 从 start 开始，到 stop 结束，步长为 step",
AiLearning/src/py2.x/dl/linear_unit.py,96,计算感知器对输入计算得到的值,
AiLearning/src/py2.x/dl/linear_unit.py,98,将图画出来,
AiLearning/src/py2.x/dl/linear_unit.py,100,将最终的图展示出来,
AiLearning/src/py2.x/dl/linear_unit.py,113,首先训练我们的线性单元,
AiLearning/src/py2.x/dl/linear_unit.py,115,打印训练获得的权重 和 偏置,
AiLearning/src/py2.x/dl/linear_unit.py,117,测试,
AiLearning/src/py2.x/dl/rnn.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/rnn.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/rnn.py,10,Python 2,
AiLearning/src/py2.x/dl/rnn.py,11,Python 3,
AiLearning/src/py2.x/dl/rnn.py,26,当前时刻初始化为t0,
AiLearning/src/py2.x/dl/rnn.py,27,保存各个时刻的state,
AiLearning/src/py2.x/dl/rnn.py,29,初始化s0,
AiLearning/src/py2.x/dl/rnn.py,31,初始化U,
AiLearning/src/py2.x/dl/rnn.py,33,初始化W,
AiLearning/src/py2.x/dl/rnn.py,61,用来保存各个时刻的误差项,
AiLearning/src/py2.x/dl/rnn.py,66,迭代计算每个时刻的误差项,
AiLearning/src/py2.x/dl/rnn.py,82,保存各个时刻的权重梯度,
AiLearning/src/py2.x/dl/rnn.py,88,实际的梯度是各个时刻梯度之和,
AiLearning/src/py2.x/dl/rnn.py,91,[0]被初始化为0且没有被修改过,
AiLearning/src/py2.x/dl/rnn.py,102,当前时刻初始化为t0,
AiLearning/src/py2.x/dl/rnn.py,103,保存各个时刻的state,
AiLearning/src/py2.x/dl/rnn.py,105,初始化s0,
AiLearning/src/py2.x/dl/rnn.py,119,设计一个误差函数，取所有节点输出项之和,
AiLearning/src/py2.x/dl/rnn.py,124,计算forward值,
AiLearning/src/py2.x/dl/rnn.py,129,求取sensitivity map,
AiLearning/src/py2.x/dl/rnn.py,132,计算梯度,
AiLearning/src/py2.x/dl/rnn.py,135,检查梯度,
AiLearning/src/py2.x/dl/fc.py,1,!/usr/bin/env python,
AiLearning/src/py2.x/dl/fc.py,2,-*- coding: UTF-8 -*-,
AiLearning/src/py2.x/dl/fc.py,11,Python 2,
AiLearning/src/py2.x/dl/fc.py,12,Python 3,
AiLearning/src/py2.x/dl/fc.py,16,全连接层实现类,
AiLearning/src/py2.x/dl/fc.py,29,权重数组W,
AiLearning/src/py2.x/dl/fc.py,32,偏置项b,
AiLearning/src/py2.x/dl/fc.py,34,输出向量,
AiLearning/src/py2.x/dl/fc.py,42,式2,
AiLearning/src/py2.x/dl/fc.py,52,式8,
AiLearning/src/py2.x/dl/fc.py,69,神经网络类,
AiLearning/src/py2.x/dl/fc.py,141,获取网络在当前样本下每个连接的梯度,
AiLearning/src/py2.x/dl/fc.py,145,检查梯度,
