file path,line #,comment,satd
transformers/setup.py,50,Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466,not
transformers/setup.py,102,dataclasses for Python versions that don't have it,not
transformers/setup.py,104,filesystem locks e.g. to prevent parallel downloads,not
transformers/setup.py,106,for downloading models over HTTPS,not
transformers/setup.py,108,progress bars in model download and training scripts,not
transformers/setup.py,110,for OpenAI GPT,not
transformers/setup.py,112,for XLNet,not
transformers/setup.py,114,for XLM,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,2,Copyright 2018 XXX Authors.,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,3,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/tokenization_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,7,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,9,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/tokenization_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/tokenization_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/tokenization_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,27,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,28,"In this template, replace all the XXX (various casings) with your model name",SATD
transformers/templates/adding_a_new_model/tokenization_xxx.py,29,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,31,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,32,Mapping from the keyword arguments names of Tokenizer `__init__`,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,33,to file names for serializing Tokenizer instances,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,34,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,37,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,38,Mapping from the keyword arguments names of Tokenizer `__init__`,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,39,to pretrained vocabulary URL for all the model shortcut names.,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,40,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,48,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,49,Mapping from model shortcut names to max length of inputs,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,50,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,56,,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,57,Mapping from model shortcut names to a dictionary of additional,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,58,keyword arguments for Tokenizer `__init__`.,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,59,To be used for checkpoint specific configurations.,not
transformers/templates/adding_a_new_model/tokenization_xxx.py,60,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,4,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,6,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,7,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,8,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,10,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,14,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,15,limitations under the License.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,18,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,19,"In this template, replace all the XXX (various casings) with your model name",SATD
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,20,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,34,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,35,This dict contrains shortcut names and associated url,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,36,for the pretrained weights provided with the models,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,37,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,44,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,45,TF 2.0 Models are constructed using Keras imperative API by sub-classing,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,46,- tf.keras.layers.Layer for the layers and,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,47,- TFPreTrainedModel for the models (itself a sub-class of tf.keras.Model),not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,48,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,50,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,51,Here is an example of typical layer in a TF 2.0 model of the library,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,52,The classes are usually identical to the PyTorch ones and prefixed with 'TF'.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,53,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,54,Note that class __init__ parameters includes **kwargs (send to 'super').,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,55,This let us have a control on class scope and variable names:,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,56,"More precisely, we set the names of the class attributes (lower level layers) to",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,57,to the equivalent attributes names in the PyTorch model so we can have equivalent,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,58,class and scope structure between PyTorch and TF 2.0 models and easily load one in the other.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,59,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,60,See the conversion methods in modeling_tf_pytorch_utils.py for more details,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,61,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,84,add attentions if we output them,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,88,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,89,The full model without a specific pretrained or finetuning head is,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,90,"provided as a tf.keras.layers.Layer usually called ""TFXxxMainLayer""",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,91,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,97,Not implemented yet in the library fr TF 2.0 models,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,100,Not implemented yet in the library fr TF 2.0 models,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,105,We allow three types of multi-inputs:,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,106,- traditional keyword arguments in the call method,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,107,- all the arguments provided as a dict in the first positional argument of call,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,108,- all the arguments provided as a list/tuple (ordered) in the first positional argument of call,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,109,The last two options are useful to use the tf.keras fit() method.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,133,We create a 3D attention mask from a 2D tensor mask.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,134,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,135,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,136,this attention mask is more simple than the triangular masking of causal attention,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,137,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,140,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,141,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,142,positions we want to attend and -10000.0 for masked positions.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,143,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,144,effectively the same as removing these entirely.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,149,Prepare head mask if needed,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,150,1.0 in head_mask indicate we keep the head,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,151,attention_probs has shape bsz x n_heads x N x N,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,152,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,153,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,158,head_mask = tf.constant([0] * self.num_hidden_layers),not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,160,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,161,Replace this with your model code,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,165,add hidden_states and attentions if they are here,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,167,"sequence_output, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,170,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,171,TFXxxPreTrainedModel is a sub-class of tf.keras.Model,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,172,which take care of loading and saving pretrained weights,SATD
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,173,and various common utilities.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,174,Here you just need to specify a few (self-explanatory),not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,175,pointers for your model.,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,176,,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,362,Add hidden states and attention if they are here,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,364,"prediction_scores, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,417,add hidden states and attention if they are here,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,419,"logits, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,472,add hidden states and attention if they are here,not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,474,"scores, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_tf_xxx.py,532,"start_logits, end_logits, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/configuration_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/configuration_xxx.py,2,"Copyright 2010, XXX authors",not
transformers/templates/adding_a_new_model/configuration_xxx.py,3,,not
transformers/templates/adding_a_new_model/configuration_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/configuration_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/configuration_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/configuration_xxx.py,7,,not
transformers/templates/adding_a_new_model/configuration_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/configuration_xxx.py,9,,not
transformers/templates/adding_a_new_model/configuration_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/configuration_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/configuration_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/configuration_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/configuration_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/modeling_xxx.py,2,Copyright 2018 XXX Authors,not
transformers/templates/adding_a_new_model/modeling_xxx.py,3,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/modeling_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/modeling_xxx.py,7,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/modeling_xxx.py,9,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/modeling_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/modeling_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/modeling_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/modeling_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,17,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,18,"In this template, replace all the XXX (various casings) with your model name",SATD
transformers/templates/adding_a_new_model/modeling_xxx.py,19,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,36,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,37,This dict contrains shortcut names and associated url,not
transformers/templates/adding_a_new_model/modeling_xxx.py,38,for the pretrained weights provided with the models,not
transformers/templates/adding_a_new_model/modeling_xxx.py,39,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,46,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,47,This is a conversion method from TF 1.0 to PyTorch,not
transformers/templates/adding_a_new_model/modeling_xxx.py,48,More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28,not
transformers/templates/adding_a_new_model/modeling_xxx.py,49,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,65,Load weights from TF model,not
transformers/templates/adding_a_new_model/modeling_xxx.py,77,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,not
transformers/templates/adding_a_new_model/modeling_xxx.py,78,which are not required for using pretrained model,not
transformers/templates/adding_a_new_model/modeling_xxx.py,122,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,123,PyTorch Models are constructed by sub-classing,not
transformers/templates/adding_a_new_model/modeling_xxx.py,124,- torch.nn.Module for the layers and,not
transformers/templates/adding_a_new_model/modeling_xxx.py,125,- PreTrainedModel for the models (itself a sub-class of torch.nn.Module),not
transformers/templates/adding_a_new_model/modeling_xxx.py,126,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,128,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,129,Here is an example of typical layer in a PyTorch model of the library,not
transformers/templates/adding_a_new_model/modeling_xxx.py,130,The classes are usually identical to the TF 2.0 ones without the 'TF' prefix.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,131,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,132,See the conversion methods in modeling_tf_pytorch_utils.py for more details,not
transformers/templates/adding_a_new_model/modeling_xxx.py,133,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,154,add attentions if we output them,not
transformers/templates/adding_a_new_model/modeling_xxx.py,158,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,159,PreTrainedModel is a sub-class of torch.nn.Module,not
transformers/templates/adding_a_new_model/modeling_xxx.py,160,which take care of loading and saving pretrained weights,SATD
transformers/templates/adding_a_new_model/modeling_xxx.py,161,and various common utilities.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,162,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,163,Here you just need to specify a few (self-explanatory),not
transformers/templates/adding_a_new_model/modeling_xxx.py,164,pointers for your model and the weights initialization,not
transformers/templates/adding_a_new_model/modeling_xxx.py,165,method if its not fully covered by PreTrainedModel's default method,not
transformers/templates/adding_a_new_model/modeling_xxx.py,166,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,190,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/templates/adding_a_new_model/modeling_xxx.py,191,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/templates/adding_a_new_model/modeling_xxx.py,351,We create a 3D attention mask from a 2D tensor mask.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,352,(this can be done with self.invert_attention_mask),not
transformers/templates/adding_a_new_model/modeling_xxx.py,353,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/templates/adding_a_new_model/modeling_xxx.py,354,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/templates/adding_a_new_model/modeling_xxx.py,355,this attention mask is more simple than the triangular masking of causal attention,not
transformers/templates/adding_a_new_model/modeling_xxx.py,356,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/templates/adding_a_new_model/modeling_xxx.py,360,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/templates/adding_a_new_model/modeling_xxx.py,361,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/templates/adding_a_new_model/modeling_xxx.py,362,positions we want to attend and -10000.0 for masked positions.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,363,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/templates/adding_a_new_model/modeling_xxx.py,364,effectively the same as removing these entirely.,not
transformers/templates/adding_a_new_model/modeling_xxx.py,365,fp16 compatibility,not
transformers/templates/adding_a_new_model/modeling_xxx.py,368,Prepare head mask if needed,not
transformers/templates/adding_a_new_model/modeling_xxx.py,369,1.0 in head_mask indicate we keep the head,not
transformers/templates/adding_a_new_model/modeling_xxx.py,370,attention_probs has shape bsz x n_heads x N x N,not
transformers/templates/adding_a_new_model/modeling_xxx.py,371,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/templates/adding_a_new_model/modeling_xxx.py,372,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/templates/adding_a_new_model/modeling_xxx.py,375,,not
transformers/templates/adding_a_new_model/modeling_xxx.py,376,Replace this with your model code,not
transformers/templates/adding_a_new_model/modeling_xxx.py,382,add hidden_states and attentions if they are here,not
transformers/templates/adding_a_new_model/modeling_xxx.py,384,"sequence_output, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_xxx.py,455,Add hidden states and attention if they are here,not
transformers/templates/adding_a_new_model/modeling_xxx.py,461,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_xxx.py,537,add hidden states and attention if they are here,not
transformers/templates/adding_a_new_model/modeling_xxx.py,541,We are doing regression,not
transformers/templates/adding_a_new_model/modeling_xxx.py,549,"(loss), logits, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_xxx.py,623,add hidden states and attention if they are here,not
transformers/templates/adding_a_new_model/modeling_xxx.py,626,Only keep active parts of the loss,not
transformers/templates/adding_a_new_model/modeling_xxx.py,636,"(loss), scores, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/modeling_xxx.py,726,"If we are on multi-GPU, split add a dimension",not
transformers/templates/adding_a_new_model/modeling_xxx.py,731,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/templates/adding_a_new_model/modeling_xxx.py,742,"(loss), start_logits, end_logits, (hidden_states), (attentions)",not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,30,Initialise PyTorch model,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,35,Load weights from tf checkpoint,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,38,Save pytorch-model,not
transformers/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py,45,Required parameters,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,2,Copyright 2018 XXX Authors.,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,3,,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,7,,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,9,,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/tests/test_modeling_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,2,Copyright 2018 XXX Authors.,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,3,,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,7,,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,9,,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/tests/test_tokenization_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,2,Copyright 2018 XXX Authors.,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,3,,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,7,,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,9,,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_model/tests/test_modeling_tf_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_example_script/run_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_example_script/run_xxx.py,2,Copyright 2018 XXX.  All rights reserved.,not
transformers/templates/adding_a_new_example_script/run_xxx.py,3,,not
transformers/templates/adding_a_new_example_script/run_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_example_script/run_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_example_script/run_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_example_script/run_xxx.py,7,,not
transformers/templates/adding_a_new_example_script/run_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_example_script/run_xxx.py,9,,not
transformers/templates/adding_a_new_example_script/run_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_example_script/run_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_example_script/run_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_example_script/run_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_example_script/run_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_example_script/run_xxx.py,48,The follwing import is the official SQuAD evaluation script (2.0).,not
transformers/templates/adding_a_new_example_script/run_xxx.py,49,You can remove it from the dependencies if you are using this script outside of the library,not
transformers/templates/adding_a_new_example_script/run_xxx.py,50,We've added it here for automated tests (see examples/test_examples.py file),not
transformers/templates/adding_a_new_example_script/run_xxx.py,96,Prepare optimizer and schedule (linear warmup and decay),not
transformers/templates/adding_a_new_example_script/run_xxx.py,116,multi-gpu training (should be after apex fp16 initialization),not
transformers/templates/adding_a_new_example_script/run_xxx.py,120,Distributed training (should be after apex fp16 initialization),not
transformers/templates/adding_a_new_example_script/run_xxx.py,126,Train!,not
transformers/templates/adding_a_new_example_script/run_xxx.py,144,Added here for reproductibility,not
transformers/templates/adding_a_new_example_script/run_xxx.py,161,model outputs are always tuple in transformers (see doc),not
transformers/templates/adding_a_new_example_script/run_xxx.py,164,mean() to average on multi-gpu parallel (not distributed) training,not
transformers/templates/adding_a_new_example_script/run_xxx.py,182,Update learning rate schedule,not
transformers/templates/adding_a_new_example_script/run_xxx.py,187,Log metrics,not
transformers/templates/adding_a_new_example_script/run_xxx.py,190,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/templates/adding_a_new_example_script/run_xxx.py,199,Save model checkpoint,not
transformers/templates/adding_a_new_example_script/run_xxx.py,205,Take care of distributed/parallel training,SATD
transformers/templates/adding_a_new_example_script/run_xxx.py,230,Note that DistributedSampler samples randomly,not
transformers/templates/adding_a_new_example_script/run_xxx.py,234,Eval!,not
transformers/templates/adding_a_new_example_script/run_xxx.py,245,XLM don't use segment_ids,not
transformers/templates/adding_a_new_example_script/run_xxx.py,255,XLNet uses a more complex post-processing procedure,not
transformers/templates/adding_a_new_example_script/run_xxx.py,270,Compute predictions,not
transformers/templates/adding_a_new_example_script/run_xxx.py,279,XLNet uses a more complex post-processing procedure,not
transformers/templates/adding_a_new_example_script/run_xxx.py,312,Evaluate with the official SQuAD script,not
transformers/templates/adding_a_new_example_script/run_xxx.py,322,"Make sure only the first process in distributed training process the dataset,",not
transformers/templates/adding_a_new_example_script/run_xxx.py,323,and the others will use the cache,not
transformers/templates/adding_a_new_example_script/run_xxx.py,325,Load data features from cache or dataset file,not
transformers/templates/adding_a_new_example_script/run_xxx.py,356,"Make sure only the first process in distributed training process the dataset,",not
transformers/templates/adding_a_new_example_script/run_xxx.py,357,and the others will use the cache,not
transformers/templates/adding_a_new_example_script/run_xxx.py,359,Convert to Tensors and build dataset,not
transformers/templates/adding_a_new_example_script/run_xxx.py,391,Required parameters,not
transformers/templates/adding_a_new_example_script/run_xxx.py,424,Other parameters,not
transformers/templates/adding_a_new_example_script/run_xxx.py,571,Setup distant debugging if needed,not
transformers/templates/adding_a_new_example_script/run_xxx.py,573,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/templates/adding_a_new_example_script/run_xxx.py,580,"Setup CUDA, GPU & distributed training",not
transformers/templates/adding_a_new_example_script/run_xxx.py,584,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/templates/adding_a_new_example_script/run_xxx.py,591,Setup logging,not
transformers/templates/adding_a_new_example_script/run_xxx.py,606,Set seed,not
transformers/templates/adding_a_new_example_script/run_xxx.py,609,Load pretrained model and tokenizer,not
transformers/templates/adding_a_new_example_script/run_xxx.py,611,Make sure only the first process in distributed training will,not
transformers/templates/adding_a_new_example_script/run_xxx.py,612,download model & vocab,not
transformers/templates/adding_a_new_example_script/run_xxx.py,632,Make sure only the first process in distributed training will,not
transformers/templates/adding_a_new_example_script/run_xxx.py,633,download model & vocab,not
transformers/templates/adding_a_new_example_script/run_xxx.py,639,"Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum",not
transformers/templates/adding_a_new_example_script/run_xxx.py,640,"if args.fp16 is set. Otherwise it'll default to ""promote"" mode, and we'll get fp32 operations.",not
transformers/templates/adding_a_new_example_script/run_xxx.py,641,"Note that running `--fp16_opt_level=""O2""` will remove the need for this code, but it is still valid.",not
transformers/templates/adding_a_new_example_script/run_xxx.py,650,Training,not
transformers/templates/adding_a_new_example_script/run_xxx.py,656,Save the trained model and the tokenizer,not
transformers/templates/adding_a_new_example_script/run_xxx.py,658,Create output directory if needed,not
transformers/templates/adding_a_new_example_script/run_xxx.py,663,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/templates/adding_a_new_example_script/run_xxx.py,664,They can then be reloaded using `from_pretrained()`,not
transformers/templates/adding_a_new_example_script/run_xxx.py,667,Take care of distributed/parallel training,SATD
transformers/templates/adding_a_new_example_script/run_xxx.py,671,Good practice: save your training arguments together with the trained model,not
transformers/templates/adding_a_new_example_script/run_xxx.py,674,Load a trained model and vocabulary that you have fine-tuned,not
transformers/templates/adding_a_new_example_script/run_xxx.py,679,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,not
transformers/templates/adding_a_new_example_script/run_xxx.py,687,Reduce model loading logs,not
transformers/templates/adding_a_new_example_script/run_xxx.py,692,Reload the model,not
transformers/templates/adding_a_new_example_script/run_xxx.py,697,Evaluate,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,1,coding=utf-8,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,2,Copyright 2018 XXX.  All rights reserved.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,3,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,5,you may not use this file except in compliance with the License.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,6,You may obtain a copy of the License at,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,7,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,9,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,13,See the License for the specific language governing permissions and,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,14,limitations under the License.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,25,Required by XLNet evaluation method to compute optimal threshold (see write_predictions_extended() method),not
transformers/templates/adding_a_new_example_script/utils_xxx.py,158,Only add answers where the text can be exactly recovered from the,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,159,document. If this CAN'T happen it's likely due to weird Unicode,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,160,stuff so we will just skip the example.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,161,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,162,"Note that this means for training mode, every example is NOT",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,163,guaranteed to be preserved.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,207,"cnt_pos, cnt_neg = 0, 0",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,208,"max_N, max_M = 1024, 1024",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,209,"f = np.zeros((max_N, max_M), dtype=np.float32)",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,214,if example_index % 100 == 0:,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,215,"logger.info('Converting %s/%s pos %s neg %s', example_index, len(examples), cnt_pos, cnt_neg)",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,247,"The -3 accounts for [CLS], [SEP] and [SEP]",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,250,We can have documents that are longer than the maximum sequence length.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,251,"To deal with this we do a sliding window approach, where we take chunks",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,252,of the up to our max length with a stride of `doc_stride`.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,253,pylint: disable=invalid-name,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,271,p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer),not
transformers/templates/adding_a_new_example_script/utils_xxx.py,272,Original TF implem also keep the classification token (set to 0) (not sure why...),not
transformers/templates/adding_a_new_example_script/utils_xxx.py,275,CLS token at the beginning,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,282,Query,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,288,SEP token,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,293,Paragraph,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,305,SEP token,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,310,CLS token at the end,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,315,Index of classification token,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,319,The mask has 1 for real tokens and 0 for padding tokens. Only real,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,320,tokens are attended to.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,323,Zero-pad up to the sequence length.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,338,"For training, if our document chunk does not contain an annotation",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,339,"we throw it out, since there is nothing to predict.",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,409,The SQuAD annotations are character based. We first project them to,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,410,"whitespace-tokenized words. But then after WordPiece tokenization, we can",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,411,"often find a ""better match"". For example:",SATD
transformers/templates/adding_a_new_example_script/utils_xxx.py,412,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,413,Question: What year was John Smith born?,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,414,Context: The leader was John Smith (1895-1943).,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,415,Answer: 1895,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,416,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,417,"The original whitespace-tokenized answer will be ""(1895-1943)."". However",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,418,"after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,419,"the exact answer, 1895.",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,420,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,421,"However, this is not always possible. Consider the following:",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,422,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,423,Question: What country is the top exporter of electornics?,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,424,Context: The Japanese electronics industry is the lagest in the world.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,425,Answer: Japan,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,426,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,427,"In this case, the annotator chose ""Japan"" as a character sub-span of",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,428,"the word ""Japanese"". Since our WordPiece tokenizer does not split",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,429,"""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,430,"in SQuAD, but does happen.",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,445,"Because of the sliding window approach taken to scoring documents, a single",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,446,token can appear in multiple documents. E.g.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,447,Doc: the man went to the store and bought a gallon of milk,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,448,Span A: the man went to the,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,449,Span B: to the store and bought,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,450,Span C: and bought a gallon of,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,451,...,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,452,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,453,Now the word 'bought' will have two scores from spans B and C. We only,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,454,"want to consider the score with ""maximum context"", which we define as",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,455,the *minimum* of its left and right context (the *sum* of left and,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,456,"right context will always be the same, of course).",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,457,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,458,In the example the maximum context for 'bought' would be span C since,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,459,"it has 1 left context and 3 right context, while span B has 4 left context",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,460,and 0 right context.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,508,pylint: disable=invalid-name,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,520,keep track of the minimum score of null start+end of position 0,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,521,large and positive,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,522,the paragraph slice with min null score,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,523,the start logit at the slice with min null score,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,524,the end logit at the slice with min null score,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,529,"if we could have irrelevant answers, get the min score of irrelevant",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,539,"We could hypothetically create invalid predictions, e.g., predict",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,540,that the start of the span is in the question. We throw out all,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,541,invalid predictions.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,578,pylint: disable=invalid-name,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,588,this is a non-null prediction,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,595,De-tokenize WordPieces that have been split off.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,599,Clean whitespace,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,614,"if we didn't include the empty option in the n-best, include it",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,619,In very rare edge cases we could only have single null prediction.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,620,So we just create a nonce prediction in this case to avoid failure.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,624,In very rare edge cases we could have no valid predictions. So we,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,625,just create a nonce prediction in this case to avoid failure.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,655,"predict """" iff the null score - the score of best non-null > threshold",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,677,For XLNet (and XLM which uses the same head),not
transformers/templates/adding_a_new_example_script/utils_xxx.py,705,pylint: disable=invalid-name,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,709,pylint: disable=invalid-name,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,714,"logger.info(""Writing nbest to: %s"" % (output_nbest_file))",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,732,keep track of the minimum score of null start+end of position 0,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,733,large and positive,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,740,"if we could have irrelevant answers, get the min score of irrelevant",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,753,"We could hypothetically create invalid predictions, e.g., predict",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,754,that the start of the span is in the question. We throw out all,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,755,invalid predictions.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,790,XLNet un-tokenizer,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,791,Let's keep it simple for now and see if we need all this later.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,792,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,793,tok_start_to_orig_index = feature.tok_start_to_orig_index,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,794,tok_end_to_orig_index = feature.tok_end_to_orig_index,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,795,start_orig_pos = tok_start_to_orig_index[pred.start_index],not
transformers/templates/adding_a_new_example_script/utils_xxx.py,796,end_orig_pos = tok_end_to_orig_index[pred.end_index],not
transformers/templates/adding_a_new_example_script/utils_xxx.py,797,paragraph_text = example.paragraph_text,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,798,final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip(),not
transformers/templates/adding_a_new_example_script/utils_xxx.py,800,Previously used Bert untokenizer,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,807,Clean whitespace,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,823,In very rare edge cases we could have no valid predictions. So we,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,824,just create a nonce prediction in this case to avoid failure.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,851,note(zhiliny): always predict best_non_null_entry,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,852,and the evaluation script will search for the best threshold,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,882,"When we created the data, we kept track of the alignment between original",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,883,(whitespace tokenized) tokens and our WordPiece tokenized tokens. So,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,884,now `orig_text` contains the span of our original text corresponding to the,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,885,span that we predicted.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,886,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,887,"However, `orig_text` may contain extra characters that we don't want in",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,888,our prediction.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,889,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,890,"For example, let's say:",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,891,pred_text = steve smith,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,892,orig_text = Steve Smith's,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,893,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,894,"We don't want to return `orig_text` because it contains the extra ""'s"".",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,895,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,896,We don't want to return `pred_text` because it's already been normalized,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,897,(the SQuAD eval script also does punctuation stripping/lower casing but,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,898,our tokenizer does additional normalization like stripping accent,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,899,characters).,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,900,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,901,"What we really want to return is ""Steve Smith"".",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,902,,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,903,"Therefore, we have to apply a semi-complicated alignment heuristic between",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,904,`pred_text` and `orig_text` to get a character-to-character alignment. This,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,905,can fail in certain cases in which case we just return `orig_text`.,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,918,"We first tokenize `orig_text`, strip whitespace from the result",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,919,"and `pred_text`, and check if they are the same length. If they are",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,920,"NOT the same length, the heuristic has failed. If they are the same",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,921,"length, we assume the characters are one-to-one aligned.",not
transformers/templates/adding_a_new_example_script/utils_xxx.py,941,We then project the characters in `pred_text` back to `orig_text` using,not
transformers/templates/adding_a_new_example_script/utils_xxx.py,942,the character-to-character alignment.,not
transformers/src/transformers/file_utils.py,30,pylint: disable=invalid-name,not
transformers/src/transformers/file_utils.py,38,pylint: disable=invalid-name,not
transformers/src/transformers/file_utils.py,44,pylint: disable=invalid-name,not
transformers/src/transformers/file_utils.py,54,pylint: disable=invalid-name,not
transformers/src/transformers/file_utils.py,60,pylint: disable=invalid-name,not
transformers/src/transformers/file_utils.py,251,"URL, so get it from the cache (downloading if necessary)",not
transformers/src/transformers/file_utils.py,262,"File, and it exists.",not
transformers/src/transformers/file_utils.py,265,"File, but it doesn't exist.",not
transformers/src/transformers/file_utils.py,268,Something unknown,not
transformers/src/transformers/file_utils.py,275,Path where we extract compressed archives,not
transformers/src/transformers/file_utils.py,276,"We avoid '.' in dir name and add ""-extracted"" at the end: ""./model.zip"" => ""./model-zip-extracted/""",not
transformers/src/transformers/file_utils.py,284,Prevent parallel extractions,not
transformers/src/transformers/file_utils.py,319,Range not satisfiable,not
transformers/src/transformers/file_utils.py,332,filter out keep-alive new chunks,not
transformers/src/transformers/file_utils.py,370,etag is already None,not
transformers/src/transformers/file_utils.py,375,get cache path to put the file,not
transformers/src/transformers/file_utils.py,378,"etag is None = we don't have a connection, or url doesn't exist, or is otherwise inaccessible.",not
transformers/src/transformers/file_utils.py,379,try to get the last downloaded one,not
transformers/src/transformers/file_utils.py,392,"If files cannot be found and local_files_only=True,",not
transformers/src/transformers/file_utils.py,393,the models might've been found if local_files_only=False,not
transformers/src/transformers/file_utils.py,394,Notify the user about that,not
transformers/src/transformers/file_utils.py,403,"From now on, etag is not None.",not
transformers/src/transformers/file_utils.py,407,Prevent parallel downloads of the same file with a lock.,not
transformers/src/transformers/file_utils.py,411,If the download just completed while the lock was activated.,not
transformers/src/transformers/file_utils.py,413,"Even if returning early like here, the lock will be released.",not
transformers/src/transformers/file_utils.py,433,"Download to temporary file, then copy to cache dir once finished.",not
transformers/src/transformers/file_utils.py,434,Otherwise you get corrupt cache entries if the download gets interrupted.,not
transformers/src/transformers/file_utils.py,462,See docs.python.org/3/howto/descriptor.html#properties,not
transformers/src/transformers/file_utils.py,476,Chose a different decorator name than in tests so it's clear they are not the same.,not
transformers/src/transformers/file_utils.py,488,Chose a different decorator name than in tests so it's clear they are not the same.,not
transformers/src/transformers/modeling_tf_openai.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_openai.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_openai.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_openai.py,4,,not
transformers/src/transformers/modeling_tf_openai.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_openai.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_openai.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_openai.py,8,,not
transformers/src/transformers/modeling_tf_openai.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_openai.py,10,,not
transformers/src/transformers/modeling_tf_openai.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_openai.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_openai.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_openai.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_openai.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_openai.py,71,in Attention: n_state=768 (nx=n_embd),not
transformers/src/transformers/modeling_tf_openai.py,72,[switch nx => n_state from Block to Attention to keep identical to TF implem],not
transformers/src/transformers/modeling_tf_openai.py,100,"q, k, v have shape [batch, heads, sequence, features]",not
transformers/src/transformers/modeling_tf_openai.py,103,scale attention_scores,not
transformers/src/transformers/modeling_tf_openai.py,106,"w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.",not
transformers/src/transformers/modeling_tf_openai.py,113,Apply the attention mask,not
transformers/src/transformers/modeling_tf_openai.py,119,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_openai.py,138,"(batch, head, seq_length, head_features)",not
transformers/src/transformers/modeling_tf_openai.py,157,"a, (attentions)",not
transformers/src/transformers/modeling_tf_openai.py,189,"output_attn: a, (attentions)",not
transformers/src/transformers/modeling_tf_openai.py,196,"x, (attentions)",not
transformers/src/transformers/modeling_tf_openai.py,275,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_tf_openai.py,276,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_tf_openai.py,277,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_openai.py,278,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_tf_openai.py,279,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_tf_openai.py,282,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_openai.py,283,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_openai.py,284,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_openai.py,285,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_openai.py,286,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_openai.py,293,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_openai.py,294,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_openai.py,295,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_openai.py,296,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_openai.py,297,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_tf_openai.py,302,head_mask = tf.constant([0] * self.num_hidden_layers),not
transformers/src/transformers/modeling_tf_openai.py,331,Add last hidden state,not
transformers/src/transformers/modeling_tf_openai.py,339,let the number of heads free (-1) so we can extract attention even after head pruning,not
transformers/src/transformers/modeling_tf_openai.py,343,"last hidden state, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_openai.py,519,"lm_logits, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_openai.py,660,"lm logits, mc logits, (all hidden_states), (attentions)",not
transformers/src/transformers/training_args_tf.py,71,If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`,not
transformers/src/transformers/modelcard.py,1,coding=utf-8,not
transformers/src/transformers/modelcard.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/modelcard.py,3,,not
transformers/src/transformers/modelcard.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modelcard.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modelcard.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modelcard.py,7,,not
transformers/src/transformers/modelcard.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modelcard.py,9,,not
transformers/src/transformers/modelcard.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modelcard.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modelcard.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modelcard.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modelcard.py,14,limitations under the License.,not
transformers/src/transformers/modelcard.py,56,Recomended attributes from https://arxiv.org/abs/1810.03993 (see papers),not
transformers/src/transformers/modelcard.py,67,Open additional attributes,not
transformers/src/transformers/modelcard.py,79,"If we save using the predefined names, we can load using `from_pretrained`",not
transformers/src/transformers/modelcard.py,135,For simplicity we use the same pretrained url than the configuration files,not
transformers/src/transformers/modelcard.py,136,but with a different suffix (modelcard.json). This suffix is replaced below.,not
transformers/src/transformers/modelcard.py,151,Load from URL or cache if already cached,not
transformers/src/transformers/modelcard.py,163,Load model card,not
transformers/src/transformers/modelcard.py,167,We fall back on creating an empty model card,not
transformers/src/transformers/modelcard.py,170,Update model card with kwargs if needed,not
transformers/src/transformers/modeling_mmbt.py,1,coding=utf-8,not
transformers/src/transformers/modeling_mmbt.py,2,"Copyright (c) Facebook, Inc. and its affiliates.",not
transformers/src/transformers/modeling_mmbt.py,3,Copyright (c) HuggingFace Inc. team.,not
transformers/src/transformers/modeling_mmbt.py,4,,not
transformers/src/transformers/modeling_mmbt.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_mmbt.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_mmbt.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_mmbt.py,8,,not
transformers/src/transformers/modeling_mmbt.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_mmbt.py,10,,not
transformers/src/transformers/modeling_mmbt.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_mmbt.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_mmbt.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_mmbt.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_mmbt.py,15,limitations under the License.,not
transformers/src/transformers/modeling_mmbt.py,265,add hidden_states and attentions if they are here,not
transformers/src/transformers/modeling_mmbt.py,266,"sequence_output, pooled_output, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_mmbt.py,355,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_mmbt.py,359,We are doing regression,not
transformers/src/transformers/modeling_mmbt.py,367,"(loss), logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_bert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_bert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_bert.py,4,,not
transformers/src/transformers/modeling_bert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_bert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_bert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_bert.py,8,,not
transformers/src/transformers/modeling_bert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_bert.py,10,,not
transformers/src/transformers/modeling_bert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_bert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_bert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_bert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_bert.py,15,limitations under the License.,not
transformers/src/transformers/modeling_bert.py,76,Load weights from TF model,not
transformers/src/transformers/modeling_bert.py,88,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,not
transformers/src/transformers/modeling_bert.py,89,which are not required for using pretrained model,not
transformers/src/transformers/modeling_bert.py,153,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,not
transformers/src/transformers/modeling_bert.py,154,any TensorFlow checkpoint file,not
transformers/src/transformers/modeling_bert.py,218,"If this is instantiated as a cross-attention module, the keys",not
transformers/src/transformers/modeling_bert.py,219,and values come from an encoder; the attention mask needs to be,not
transformers/src/transformers/modeling_bert.py,220,such that the encoder's padding tokens are not attended to.,not
transformers/src/transformers/modeling_bert.py,233,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",not
transformers/src/transformers/modeling_bert.py,237,Apply the attention mask is (precomputed for all layers in BertModel forward() function),not
transformers/src/transformers/modeling_bert.py,240,Normalize the attention scores to probabilities.,not
transformers/src/transformers/modeling_bert.py,243,"This is actually dropping out entire tokens to attend to, which might",not
transformers/src/transformers/modeling_bert.py,244,"seem a bit unusual, but is taken from the original Transformer paper.",not
transformers/src/transformers/modeling_bert.py,247,Mask heads if we want to,not
transformers/src/transformers/modeling_bert.py,286,Convert to set and remove already pruned heads,not
transformers/src/transformers/modeling_bert.py,288,Compute how many pruned heads are before the head and move the index accordingly,not
transformers/src/transformers/modeling_bert.py,294,Prune linear layers,not
transformers/src/transformers/modeling_bert.py,300,Update hyper params and store pruned heads,not
transformers/src/transformers/modeling_bert.py,317,add attentions if we output them,not
transformers/src/transformers/modeling_bert.py,370,add self attentions if we output attention weights,not
transformers/src/transformers/modeling_bert.py,377,add cross attentions if we output attention weights,not
transformers/src/transformers/modeling_bert.py,414,Add last layer,not
transformers/src/transformers/modeling_bert.py,423,"last-layer hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_bert.py,433,"We ""pool"" the model by simply taking the hidden state corresponding",not
transformers/src/transformers/modeling_bert.py,434,to the first token.,not
transformers/src/transformers/modeling_bert.py,463,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_bert.py,464,an output-only bias for each token.,not
transformers/src/transformers/modeling_bert.py,469,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,not
transformers/src/transformers/modeling_bert.py,523,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_bert.py,524,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_bert.py,704,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_bert.py,705,ourselves in which case we just need to make it broadcastable to all heads.,not
transformers/src/transformers/modeling_bert.py,710,If a 2D ou 3D attention mask is provided for the cross-attention,not
transformers/src/transformers/modeling_bert.py,711,"we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]",not
transformers/src/transformers/modeling_bert.py,721,Prepare head mask if needed,not
transformers/src/transformers/modeling_bert.py,722,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_bert.py,723,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_bert.py,724,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_bert.py,725,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_bert.py,743,add hidden_states and attentions if they are here,not
transformers/src/transformers/modeling_bert.py,744,"sequence_output, pooled_output, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,839,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bert.py,848,"(loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,939,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bert.py,941,"Although this may seem awkward, BertForMaskedLM supports two scenarios:",not
transformers/src/transformers/modeling_bert.py,942,"1. If a tensor that contains the indices of masked labels is provided,",not
transformers/src/transformers/modeling_bert.py,943,the cross-entropy is the MLM cross-entropy that measures the likelihood,not
transformers/src/transformers/modeling_bert.py,944,of predictions for masked words.,not
transformers/src/transformers/modeling_bert.py,945,2. If `lm_labels` is provided we are in a causal scenario where we,not
transformers/src/transformers/modeling_bert.py,946,try to predict the next token for each input in the decoder.,not
transformers/src/transformers/modeling_bert.py,948,-100 index = padding token,not
transformers/src/transformers/modeling_bert.py,953,we are doing next-token prediction; shift prediction scores and input ids by one,not
transformers/src/transformers/modeling_bert.py,960,"(ltr_lm_loss), (masked_lm_loss), prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,966,"if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly",not
transformers/src/transformers/modeling_bert.py,970,if model is does not use a causal mask then add a dummy token,not
transformers/src/transformers/modeling_bert.py,1061,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bert.py,1067,"(next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,1152,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bert.py,1156,We are doing regression,not
transformers/src/transformers/modeling_bert.py,1164,"(loss), logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,1257,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bert.py,1264,"(loss), reshaped_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,1347,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bert.py,1350,Only keep active parts of the loss,not
transformers/src/transformers/modeling_bert.py,1362,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_bert.py,1460,"If we are on multi-GPU, split add a dimension",not
transformers/src/transformers/modeling_bert.py,1465,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/src/transformers/modeling_bert.py,1476,"(loss), start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/configuration_transfo_xl.py,1,coding=utf-8,not
transformers/src/transformers/configuration_transfo_xl.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/configuration_transfo_xl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_transfo_xl.py,4,,not
transformers/src/transformers/configuration_transfo_xl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_transfo_xl.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_transfo_xl.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_transfo_xl.py,8,,not
transformers/src/transformers/configuration_transfo_xl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_transfo_xl.py,10,,not
transformers/src/transformers/configuration_transfo_xl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_transfo_xl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_transfo_xl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_transfo_xl.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_transfo_xl.py,15,limitations under the License.,not
transformers/src/transformers/configuration_transfo_xl.py,195,Backward compatibility,not
transformers/src/transformers/configuration_transfo_xl.py,199,Backward compatibility,not
transformers/src/transformers/configuration_gpt2.py,1,coding=utf-8,not
transformers/src/transformers/configuration_gpt2.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/configuration_gpt2.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_gpt2.py,4,,not
transformers/src/transformers/configuration_gpt2.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_gpt2.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_gpt2.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_gpt2.py,8,,not
transformers/src/transformers/configuration_gpt2.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_gpt2.py,10,,not
transformers/src/transformers/configuration_gpt2.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_gpt2.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_gpt2.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_gpt2.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_gpt2.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_transfo_xl.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_transfo_xl.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_transfo_xl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/tokenization_transfo_xl.py,4,,not
transformers/src/transformers/tokenization_transfo_xl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_transfo_xl.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_transfo_xl.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_transfo_xl.py,8,,not
transformers/src/transformers/tokenization_transfo_xl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_transfo_xl.py,10,,not
transformers/src/transformers/tokenization_transfo_xl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_transfo_xl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_transfo_xl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_transfo_xl.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_transfo_xl.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_transfo_xl.py,116,noqa: W605,not
transformers/src/transformers/tokenization_transfo_xl.py,122,"Hack because, honestly this tokenizer was not made to be used",SATD
transformers/src/transformers/tokenization_transfo_xl.py,123,"in a library like ours, at all.",not
transformers/src/transformers/tokenization_transfo_xl.py,143,noqa: W605,not
transformers/src/transformers/tokenization_transfo_xl.py,284,logger.info('encounter unk {}'.format(sym)),not
transformers/src/transformers/tokenization_transfo_xl.py,285,assert '<eos>' not in sym,not
transformers/src/transformers/tokenization_transfo_xl.py,288,Backward compatibility with pre-trained models,not
transformers/src/transformers/tokenization_transfo_xl.py,313,convert to lower case,not
transformers/src/transformers/tokenization_transfo_xl.py,317,empty delimiter '' will evaluate False,not
transformers/src/transformers/tokenization_transfo_xl.py,323,lm1b,not
transformers/src/transformers/tokenization_transfo_xl.py,331,add spaces before punctuation symbols as should be done in transfo-xl,not
transformers/src/transformers/tokenization_transfo_xl.py,336,searches until the first occurence of a punctuation symbol without surrounding spaces,not
transformers/src/transformers/tokenization_transfo_xl.py,367,Create the correct normalization path,not
transformers/src/transformers/tokenization_transfo_xl.py,370,Include unicode normalization,not
transformers/src/transformers/tokenization_transfo_xl.py,374,Include case normalization,not
transformers/src/transformers/tokenization_transfo_xl.py,378,Strip normalizer at the end,not
transformers/src/transformers/tokenization_transfo_xl.py,384,Setup the splitter,not
transformers/src/transformers/tokenization_transfo_xl.py,478,Work out how cleanly we can divide the dataset into bsz parts.,not
transformers/src/transformers/tokenization_transfo_xl.py,481,Trim off any extra elements that wouldn't cleanly fit (remainders).,not
transformers/src/transformers/tokenization_transfo_xl.py,484,Evenly divide the data across the bsz batches.,not
transformers/src/transformers/tokenization_transfo_xl.py,487,Number of mini-batches,not
transformers/src/transformers/tokenization_transfo_xl.py,541,index iterator,not
transformers/src/transformers/tokenization_transfo_xl.py,544,sentence iterator,not
transformers/src/transformers/tokenization_transfo_xl.py,549,streams for each data in the batch,not
transformers/src/transformers/tokenization_transfo_xl.py,558,data   : [n_retain+bptt x bsz],not
transformers/src/transformers/tokenization_transfo_xl.py,559,target : [bptt x bsz],not
transformers/src/transformers/tokenization_transfo_xl.py,571,number of new tokens to fill in,not
transformers/src/transformers/tokenization_transfo_xl.py,573,first n_retain tokens are retained from last batch,not
transformers/src/transformers/tokenization_transfo_xl.py,596,sent_stream is an iterator,not
transformers/src/transformers/tokenization_transfo_xl.py,629,sent_stream is an iterator,not
transformers/src/transformers/tokenization_transfo_xl.py,646,"redirect to the cache, if necessary",not
transformers/src/transformers/tokenization_transfo_xl.py,666,Instantiate tokenizer.,not
transformers/src/transformers/tokenization_transfo_xl.py,704,the vocab will load from file when build_vocab() is called,not
transformers/src/transformers/modeling_tf_distilbert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_distilbert.py,2,"Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.",not
transformers/src/transformers/modeling_tf_distilbert.py,3,,not
transformers/src/transformers/modeling_tf_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_distilbert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_distilbert.py,7,,not
transformers/src/transformers/modeling_tf_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_distilbert.py,9,,not
transformers/src/transformers/modeling_tf_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_distilbert.py,14,limitations under the License.,not
transformers/src/transformers/modeling_tf_distilbert.py,44,UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE,not
transformers/src/transformers/modeling_tf_distilbert.py,77,padding_idx=0),not
transformers/src/transformers/modeling_tf_distilbert.py,91,Create and initialize weights. The random normal initializer was chosen,not
transformers/src/transformers/modeling_tf_distilbert.py,92,"arbitrarily, and works well.",not
transformers/src/transformers/modeling_tf_distilbert.py,148,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,150,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,151,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,152,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,219,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,220,assert key.size() == value.size(),not
transformers/src/transformers/modeling_tf_distilbert.py,234,"(bs, n_heads, q_length, dim_per_head)",not
transformers/src/transformers/modeling_tf_distilbert.py,235,"(bs, n_heads, k_length, dim_per_head)",not
transformers/src/transformers/modeling_tf_distilbert.py,236,"(bs, n_heads, k_length, dim_per_head)",not
transformers/src/transformers/modeling_tf_distilbert.py,238,"(bs, n_heads, q_length, dim_per_head)",not
transformers/src/transformers/modeling_tf_distilbert.py,239,"(bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_tf_distilbert.py,240,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_distilbert.py,241,"scores.masked_fill_(mask, -float('inf'))            # (bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_tf_distilbert.py,244,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_distilbert.py,245,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_distilbert.py,247,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_distilbert.py,251,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_distilbert.py,252,"(bs, q_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,253,"(bs, q_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,305,"removed: src_enc=None, src_len=None",not
transformers/src/transformers/modeling_tf_distilbert.py,321,Self-Attention,not
transformers/src/transformers/modeling_tf_distilbert.py,324,"(bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)",not
transformers/src/transformers/modeling_tf_distilbert.py,325,To handle these `output_attention` or `output_hidden_states` cases returning tuples,not
transformers/src/transformers/modeling_tf_distilbert.py,326,assert type(sa_output) == tuple,not
transformers/src/transformers/modeling_tf_distilbert.py,328,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,330,Feed Forward Network,not
transformers/src/transformers/modeling_tf_distilbert.py,331,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,332,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,389,Add last layer,not
transformers/src/transformers/modeling_tf_distilbert.py,398,"last-layer hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_tf_distilbert.py,406,Embeddings,not
transformers/src/transformers/modeling_tf_distilbert.py,407,Encoder,not
transformers/src/transformers/modeling_tf_distilbert.py,444,"(bs, seq_length)",not
transformers/src/transformers/modeling_tf_distilbert.py,447,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_distilbert.py,448,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_distilbert.py,449,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_distilbert.py,450,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_distilbert.py,451,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_tf_distilbert.py,457,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,460,"last-layer hidden-state, (all hidden_states), (all attentions)",not
transformers/src/transformers/modeling_tf_distilbert.py,463,INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL,not
transformers/src/transformers/modeling_tf_distilbert.py,542,Embeddings,not
transformers/src/transformers/modeling_tf_distilbert.py,582,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_distilbert.py,583,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_distilbert.py,650,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,651,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,652,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,653,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,657,"logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_distilbert.py,714,"(bs, seq_len, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,715,"(bs, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,716,"(bs, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,717,"(bs, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,718,"(bs, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,721,"logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_distilbert.py,776,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_distilbert.py,778,"scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_distilbert.py,831,"(bs, max_query_len, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,832,"(bs, max_query_len, dim)",not
transformers/src/transformers/modeling_tf_distilbert.py,833,"(bs, max_query_len, 2)",not
transformers/src/transformers/modeling_tf_distilbert.py,839,"start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/tokenization_t5.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_t5.py,2,Copyright 2018 T5 Authors and HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_t5.py,3,,not
transformers/src/transformers/tokenization_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_t5.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_t5.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_t5.py,7,,not
transformers/src/transformers/tokenization_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_t5.py,9,,not
transformers/src/transformers/tokenization_t5.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_t5.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_t5.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_t5.py,30,,not
transformers/src/transformers/tokenization_t5.py,31,Mapping from the keyword arguments names of Tokenizer `__init__`,not
transformers/src/transformers/tokenization_t5.py,32,to file names for serializing Tokenizer instances,not
transformers/src/transformers/tokenization_t5.py,33,,not
transformers/src/transformers/tokenization_t5.py,36,,not
transformers/src/transformers/tokenization_t5.py,37,Mapping from the keyword arguments names of Tokenizer `__init__`,not
transformers/src/transformers/tokenization_t5.py,38,to pretrained vocabulary URL for all the model shortcut names.,not
transformers/src/transformers/tokenization_t5.py,39,,not
transformers/src/transformers/tokenization_t5.py,50,,not
transformers/src/transformers/tokenization_t5.py,51,Mapping from model shortcut names to max length of inputs,not
transformers/src/transformers/tokenization_t5.py,52,,not
transformers/src/transformers/tokenization_t5.py,108,Add extra_ids to the special token list,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,4,,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,8,,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,10,,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,156,No probability for the head cluster,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,171,Add the training-time loss value to the layer using `self.add_loss()`.,not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,174,"Log the loss as a metric (we could log arbitrary metrics,",not
transformers/src/transformers/modeling_tf_transfo_xl_utilities.py,175,including different metrics for training and inference.,not
transformers/src/transformers/modeling_electra.py,41,Load weights from TF model,not
transformers/src/transformers/modeling_electra.py,65,"print(original_name, name)",not
transformers/src/transformers/modeling_electra.py,66,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,not
transformers/src/transformers/modeling_electra.py,67,which are not required for using pretrained model,not
transformers/src/transformers/modeling_electra.py,116,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,not
transformers/src/transformers/modeling_electra.py,117,any TensorFlow checkpoint file,not
transformers/src/transformers/modeling_electra.py,424,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_electra.py,509,Masked language modeling softmax layer,not
transformers/src/transformers/modeling_electra.py,511,-100 index = padding token,not
transformers/src/transformers/modeling_electra.py,517,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_electra.py,598,Only keep active parts of the loss,not
transformers/src/transformers/modeling_electra.py,611,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/tokenization_openai.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_openai.py,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_openai.py,3,,not
transformers/src/transformers/tokenization_openai.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_openai.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_openai.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_openai.py,7,,not
transformers/src/transformers/tokenization_openai.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_openai.py,9,,not
transformers/src/transformers/tokenization_openai.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_openai.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_openai.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_openai.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_openai.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_openai.py,178,Using BERT's BasicTokenizer,not
transformers/src/transformers/tokenization_openai.py,183,Using SpaCy & ftfy (original tokenization process of OpenAI GPT),not
transformers/src/transformers/tokenization_flaubert.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_flaubert.py,2,"Copyright 2019-present CNRS, Facebook Inc. and the HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_flaubert.py,3,,not
transformers/src/transformers/tokenization_flaubert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_flaubert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_flaubert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_flaubert.py,7,,not
transformers/src/transformers/tokenization_flaubert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_flaubert.py,9,,not
transformers/src/transformers/tokenization_flaubert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_flaubert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_flaubert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_flaubert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_flaubert.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_flaubert.py,67,six_ensure_text is copied from https://github.com/benjaminp/six,not
transformers/src/transformers/optimization_tf.py,1,Copyright 2019 The TensorFlow Authors. All Rights Reserved.,not
transformers/src/transformers/optimization_tf.py,2,,not
transformers/src/transformers/optimization_tf.py,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/optimization_tf.py,4,you may not use this file except in compliance with the License.,not
transformers/src/transformers/optimization_tf.py,5,You may obtain a copy of the License at,not
transformers/src/transformers/optimization_tf.py,6,,not
transformers/src/transformers/optimization_tf.py,7,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/optimization_tf.py,8,,not
transformers/src/transformers/optimization_tf.py,9,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/optimization_tf.py,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/optimization_tf.py,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/optimization_tf.py,12,See the License for the specific language governing permissions and,not
transformers/src/transformers/optimization_tf.py,13,limitations under the License.,not
transformers/src/transformers/optimization_tf.py,14,==============================================================================,not
transformers/src/transformers/optimization_tf.py,38,"Implements polynomial warmup. i.e., if global_step < warmup_steps, the",not
transformers/src/transformers/optimization_tf.py,39,learning rate will be `global_step/num_warmup_steps * init_lr`.,not
transformers/src/transformers/optimization_tf.py,63,Implements linear decay of the learning rate.,not
transformers/src/transformers/optimization_tf.py,184,Extracted from https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/optimizers/utils.py,not
transformers/src/transformers/optimization_tf.py,193,We use the ON_READ synchronization policy so that no synchronization is,not
transformers/src/transformers/optimization_tf.py,194,"performed on assignment. To get the value, we call .value() which returns the",not
transformers/src/transformers/optimization_tf.py,195,value on the current replica without synchronization.,not
transformers/src/transformers/optimization_tf.py,225,Create the step variable.,not
transformers/src/transformers/modeling_tf_xlm.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_xlm.py,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_tf_xlm.py,3,,not
transformers/src/transformers/modeling_tf_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_xlm.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_xlm.py,7,,not
transformers/src/transformers/modeling_tf_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_xlm.py,9,,not
transformers/src/transformers/modeling_tf_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_xlm.py,14,limitations under the License.,not
transformers/src/transformers/modeling_tf_xlm.py,73,assert lengths.max().item() <= slen,not
transformers/src/transformers/modeling_tf_xlm.py,77,"attention mask is the same as mask, or triangular inferior attention (causal)",not
transformers/src/transformers/modeling_tf_xlm.py,85,sanity check,not
transformers/src/transformers/modeling_tf_xlm.py,86,"assert shape_list(mask) == [bs, slen]",not
transformers/src/transformers/modeling_tf_xlm.py,123,"Input is (bs, qlen, dim)",not
transformers/src/transformers/modeling_tf_xlm.py,124,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",not
transformers/src/transformers/modeling_tf_xlm.py,130,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",not
transformers/src/transformers/modeling_tf_xlm.py,143,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,145,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,146,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,149,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,150,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,156,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,157,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,162,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,163,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_xlm.py,164,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_xlm.py,165,"scores.masked_fill_(mask, -float('inf'))                            # (bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_xlm.py,168,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_xlm.py,169,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_xlm.py,171,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_xlm.py,175,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_xlm.py,176,"(bs, qlen, dim)",not
transformers/src/transformers/modeling_tf_xlm.py,206,"encoder / decoder, output layer",not
transformers/src/transformers/modeling_tf_xlm.py,211,self.with_output = with_output,not
transformers/src/transformers/modeling_tf_xlm.py,214,dictionary / languages,not
transformers/src/transformers/modeling_tf_xlm.py,220,self.dico = dico,not
transformers/src/transformers/modeling_tf_xlm.py,221,self.id2lang = config.id2lang,not
transformers/src/transformers/modeling_tf_xlm.py,222,self.lang2id = config.lang2id,not
transformers/src/transformers/modeling_tf_xlm.py,223,assert len(self.dico) == self.n_words,not
transformers/src/transformers/modeling_tf_xlm.py,224,assert len(self.id2lang) == len(self.lang2id) == self.n_langs,not
transformers/src/transformers/modeling_tf_xlm.py,226,model parameters,not
transformers/src/transformers/modeling_tf_xlm.py,227,512 by default,not
transformers/src/transformers/modeling_tf_xlm.py,228,2048 by default,not
transformers/src/transformers/modeling_tf_xlm.py,229,8 by default,not
transformers/src/transformers/modeling_tf_xlm.py,233,embeddings,not
transformers/src/transformers/modeling_tf_xlm.py,245,"create_sinusoidal_embeddings(config.max_position_embeddings, self.dim, out=self.position_embeddings.weight)",not
transformers/src/transformers/modeling_tf_xlm.py,255,padding_idx=self.pad_index),not
transformers/src/transformers/modeling_tf_xlm.py,258,transformer layers,not
transformers/src/transformers/modeling_tf_xlm.py,263,if self.is_decoder:,not
transformers/src/transformers/modeling_tf_xlm.py,264,self.layer_norm15 = [],not
transformers/src/transformers/modeling_tf_xlm.py,265,self.encoder_attn = [],not
transformers/src/transformers/modeling_tf_xlm.py,274,if self.is_decoder:,not
transformers/src/transformers/modeling_tf_xlm.py,275,"self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))",not
transformers/src/transformers/modeling_tf_xlm.py,276,"self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))",not
transformers/src/transformers/modeling_tf_xlm.py,316,"removed: src_enc=None, src_len=None",not
transformers/src/transformers/modeling_tf_xlm.py,356,mask = input_ids != self.pad_index,not
transformers/src/transformers/modeling_tf_xlm.py,358,check inputs,not
transformers/src/transformers/modeling_tf_xlm.py,359,assert shape_list(lengths)[0] == bs,not
transformers/src/transformers/modeling_tf_xlm.py,361,assert lengths.max().item() <= slen,not
transformers/src/transformers/modeling_tf_xlm.py,362,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",not
transformers/src/transformers/modeling_tf_xlm.py,363,assert (src_enc is None) == (src_len is None),not
transformers/src/transformers/modeling_tf_xlm.py,364,if src_enc is not None:,not
transformers/src/transformers/modeling_tf_xlm.py,365,assert self.is_decoder,not
transformers/src/transformers/modeling_tf_xlm.py,366,assert src_enc.size(0) == bs,not
transformers/src/transformers/modeling_tf_xlm.py,368,generate masks,not
transformers/src/transformers/modeling_tf_xlm.py,370,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_tf_xlm.py,371,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",not
transformers/src/transformers/modeling_tf_xlm.py,373,position_ids,not
transformers/src/transformers/modeling_tf_xlm.py,377,"assert shape_list(position_ids) == [bs, slen]  # (slen, bs)",not
transformers/src/transformers/modeling_tf_xlm.py,379,"position_ids = position_ids.transpose(0, 1)",not
transformers/src/transformers/modeling_tf_xlm.py,381,langs,not
transformers/src/transformers/modeling_tf_xlm.py,383,"assert shape_list(langs) == [bs, slen]  # (slen, bs)",not
transformers/src/transformers/modeling_tf_xlm.py,385,"langs = langs.transpose(0, 1)",not
transformers/src/transformers/modeling_tf_xlm.py,387,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_xlm.py,388,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_xlm.py,389,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_xlm.py,390,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_xlm.py,391,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen],not
transformers/src/transformers/modeling_tf_xlm.py,397,do not recompute cached elements,not
transformers/src/transformers/modeling_tf_xlm.py,407,embeddings,not
transformers/src/transformers/modeling_tf_xlm.py,420,transformer layers,not
transformers/src/transformers/modeling_tf_xlm.py,427,self attention,not
transformers/src/transformers/modeling_tf_xlm.py,436,encoder attention (for decoder only),not
transformers/src/transformers/modeling_tf_xlm.py,437,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_tf_xlm.py,438,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",not
transformers/src/transformers/modeling_tf_xlm.py,439,"attn = F.dropout(attn, p=self.dropout, training=self.training)",not
transformers/src/transformers/modeling_tf_xlm.py,440,tensor = tensor + attn,not
transformers/src/transformers/modeling_tf_xlm.py,441,tensor = self.layer_norm15[i](tensor),not
transformers/src/transformers/modeling_tf_xlm.py,443,FFN,not
transformers/src/transformers/modeling_tf_xlm.py,448,Add last hidden state,not
transformers/src/transformers/modeling_tf_xlm.py,452,update cache length,not
transformers/src/transformers/modeling_tf_xlm.py,456,move back sequence length to dimension 0,SATD
transformers/src/transformers/modeling_tf_xlm.py,457,"tensor = tensor.transpose(0, 1)",not
transformers/src/transformers/modeling_tf_xlm.py,464,"outputs, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_xlm.py,478,Sometimes XLM has language embeddings so don't forget to build them as well if needed,not
transformers/src/transformers/modeling_tf_xlm.py,628,self.proj = nn.AdaptiveLogSoftmaxWithLoss(,not
transformers/src/transformers/modeling_tf_xlm.py,629,"in_features=dim,",not
transformers/src/transformers/modeling_tf_xlm.py,630,"n_classes=config.n_words,",not
transformers/src/transformers/modeling_tf_xlm.py,631,"cutoffs=config.asm_cutoffs,",not
transformers/src/transformers/modeling_tf_xlm.py,632,"div_value=config.asm_div_value,",not
transformers/src/transformers/modeling_tf_xlm.py,633,"head_bias=True,  # default is False",not
transformers/src/transformers/modeling_tf_xlm.py,634,),not
transformers/src/transformers/modeling_tf_xlm.py,637,"The output weights are the same as the input embeddings, but there is an output-only bias for each token.",not
transformers/src/transformers/modeling_tf_xlm.py,710,Keep new_mems and attention/hidden states if they are here,not
transformers/src/transformers/modeling_tf_xlm.py,765,Keep new_mems and attention/hidden states if they are here,not
transformers/src/transformers/modeling_tf_xlm.py,826,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_tf_xlm.py,828,"start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,2,Copyright 2020 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,32,set parameter of one layer,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,41,set torch weights for 1-to-1 comparison,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,59,set torch weights for 1-to-1 comparison,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,80,layernorm 1,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,88,lsh weights + output,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,95,intermediate weighs,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,98,Chunked Feed Forward,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,102,layernorm 2,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,109,intermediate dense,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,118,intermediate out,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,129,reformer model,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,132,word embeds,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,155,output layer norm,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,164,output embeddings,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,175,Initialise PyTorch model,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,185,Save pytorch-model,not
transformers/src/transformers/convert_reformer_trax_checkpoint_to_pytorch.py,192,Required parameters,not
transformers/src/transformers/configuration_electra.py,1,coding=utf-8,not
transformers/src/transformers/configuration_electra.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_electra.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_electra.py,4,,not
transformers/src/transformers/configuration_electra.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_electra.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_electra.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_electra.py,8,,not
transformers/src/transformers/configuration_electra.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_electra.py,10,,not
transformers/src/transformers/configuration_electra.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_electra.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_electra.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_electra.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_electra.py,15,limitations under the License.,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,30,Initialise PyTorch model,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,35,Load weights from tf checkpoint,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,38,Save pytorch-model,not
transformers/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py,45,Required parameters,not
transformers/src/transformers/configuration_flaubert.py,1,coding=utf-8,not
transformers/src/transformers/configuration_flaubert.py,2,"Copyright 2019-present CNRS, Facebook Inc. and the HuggingFace Inc. team.",not
transformers/src/transformers/configuration_flaubert.py,3,,not
transformers/src/transformers/configuration_flaubert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_flaubert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_flaubert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_flaubert.py,7,,not
transformers/src/transformers/configuration_flaubert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_flaubert.py,9,,not
transformers/src/transformers/configuration_flaubert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_flaubert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_flaubert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_flaubert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_flaubert.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_xlnet.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_xlnet.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_xlnet.py,3,,not
transformers/src/transformers/tokenization_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_xlnet.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_xlnet.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_xlnet.py,7,,not
transformers/src/transformers/tokenization_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_xlnet.py,9,,not
transformers/src/transformers/tokenization_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_xlnet.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_xlnet.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_xlnet.py,45,Segments (not really needed),not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,30,Construct model,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,37,Load weights from numpy,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,40,Save pytorch-model,not
transformers/src/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py,52,Required parameters,not
transformers/src/transformers/tokenization_marian.py,22,Example URL https://s3.amazonaws.com/models.huggingface.co/bert/Helsinki-NLP/opus-mt-en-de/vocab.json,not
transformers/src/transformers/tokenization_marian.py,43,"actually attention_mask, decoder_attention_mask",not
transformers/src/transformers/tokenization_marian.py,59,"bos_token=bos_token,",not
transformers/src/transformers/tokenization_marian.py,74,load SentencePiece model for pre-processing,not
transformers/src/transformers/tokenization_marian.py,83,Multilingual target side: default to using first supported language code.,not
transformers/src/transformers/tokenization_marian.py,85,will not be used unless it is set through prepare_translation_batch,not
transformers/src/transformers/tokenization_marian.py,87,Note(SS): sentence_splitter would require lots of book-keeping.,not
transformers/src/transformers/tokenization_marian.py,114,"We don't expect to process pairs, but leave the pair logic for API consistency",not
transformers/src/transformers/configuration_xlnet.py,1,coding=utf-8,not
transformers/src/transformers/configuration_xlnet.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/configuration_xlnet.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_xlnet.py,4,,not
transformers/src/transformers/configuration_xlnet.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_xlnet.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_xlnet.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_xlnet.py,8,,not
transformers/src/transformers/configuration_xlnet.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_xlnet.py,10,,not
transformers/src/transformers/configuration_xlnet.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_xlnet.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_xlnet.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_xlnet.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_xlnet.py,15,limitations under the License.,not
transformers/src/transformers/configuration_xlnet.py,203,Backward compatibility,not
transformers/src/transformers/configuration_xlnet.py,207,Backward compatibility,not
transformers/src/transformers/modeling_encoder_decoder.py,1,coding=utf-8,not
transformers/src/transformers/modeling_encoder_decoder.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_encoder_decoder.py,3,,not
transformers/src/transformers/modeling_encoder_decoder.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_encoder_decoder.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_encoder_decoder.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_encoder_decoder.py,7,,not
transformers/src/transformers/modeling_encoder_decoder.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_encoder_decoder.py,9,,not
transformers/src/transformers/modeling_encoder_decoder.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_encoder_decoder.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_encoder_decoder.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_encoder_decoder.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_encoder_decoder.py,14,limitations under the License.,not
transformers/src/transformers/modeling_encoder_decoder.py,54,initialize with config,not
transformers/src/transformers/modeling_encoder_decoder.py,74,for now no weights tying in encoder-decoder,not
transformers/src/transformers/modeling_encoder_decoder.py,141,Load and initialize the encoder and decoder,not
transformers/src/transformers/modeling_encoder_decoder.py,142,The distinction between encoder and decoder at the model level is made,not
transformers/src/transformers/modeling_encoder_decoder.py,143,by the value of the flag `is_decoder` that we need to set correctly.,not
transformers/src/transformers/modeling_encoder_decoder.py,273,Decode,not
transformers/src/transformers/modeling_encoder_decoder.py,291,first step,not
transformers/src/transformers/modeling_encoder_decoder.py,307,as a default encoder-decoder models do not re-order the past.,not
transformers/src/transformers/modeling_encoder_decoder.py,308,"TODO(PVP): might have to be updated, e.g. if GPT2 is to be used as a decoder",SATD
transformers/src/transformers/configuration_marian.py,1,coding=utf-8,not
transformers/src/transformers/configuration_marian.py,2,"Copyright 2020 The OPUS-NMT Team, Marian team, and The HuggingFace Inc. team.",not
transformers/src/transformers/configuration_marian.py,3,,not
transformers/src/transformers/configuration_marian.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_marian.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_marian.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_marian.py,7,,not
transformers/src/transformers/configuration_marian.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_marian.py,9,,not
transformers/src/transformers/configuration_marian.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_marian.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_marian.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_marian.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_marian.py,14,limitations under the License.,not
transformers/src/transformers/modeling_albert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_albert.py,2,"Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_albert.py,3,,not
transformers/src/transformers/modeling_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_albert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_albert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_albert.py,7,,not
transformers/src/transformers/modeling_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_albert.py,9,,not
transformers/src/transformers/modeling_albert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_albert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_albert.py,14,limitations under the License.,not
transformers/src/transformers/modeling_albert.py,60,Load weights from TF model,not
transformers/src/transformers/modeling_albert.py,76,If saved from the TF HUB module,not
transformers/src/transformers/modeling_albert.py,79,Renaming and simplifying,not
transformers/src/transformers/modeling_albert.py,88,The feed forward layer had an 'intermediate' step which has been abstracted away,not
transformers/src/transformers/modeling_albert.py,92,ALBERT attention was split between self and output which have been abstracted away,not
transformers/src/transformers/modeling_albert.py,96,The pooler is a linear layer,not
transformers/src/transformers/modeling_albert.py,99,The classifier was simplified to predictions from cls/predictions,not
transformers/src/transformers/modeling_albert.py,103,Naming was changed to be more explicit,not
transformers/src/transformers/modeling_albert.py,108,Classifier,not
transformers/src/transformers/modeling_albert.py,112,No ALBERT model currently handles the next sentence prediction task,not
transformers/src/transformers/modeling_albert.py,119,Ignore the gradients applied by the LAMB/ADAM optimizers.,not
transformers/src/transformers/modeling_albert.py,201,Convert to set and emove already pruned heads,not
transformers/src/transformers/modeling_albert.py,203,Compute how many pruned heads are before the head and move the index accordingly,not
transformers/src/transformers/modeling_albert.py,209,Prune linear layers,not
transformers/src/transformers/modeling_albert.py,215,Update hyper params and store pruned heads,not
transformers/src/transformers/modeling_albert.py,229,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",not
transformers/src/transformers/modeling_albert.py,233,Apply the attention mask is (precomputed for all layers in BertModel forward() function),not
transformers/src/transformers/modeling_albert.py,236,Normalize the attention scores to probabilities.,not
transformers/src/transformers/modeling_albert.py,239,"This is actually dropping out entire tokens to attend to, which might",not
transformers/src/transformers/modeling_albert.py,240,"seem a bit unusual, but is taken from the original Transformer paper.",not
transformers/src/transformers/modeling_albert.py,243,Mask heads if we want to,not
transformers/src/transformers/modeling_albert.py,251,Should find a better way to do this,SATD
transformers/src/transformers/modeling_albert.py,283,add attentions if we output them,not
transformers/src/transformers/modeling_albert.py,313,"last-layer hidden state, (layer hidden states), (layer attentions)",not
transformers/src/transformers/modeling_albert.py,335,Number of layers in a hidden group,not
transformers/src/transformers/modeling_albert.py,338,Index of the hidden group,not
transformers/src/transformers/modeling_albert.py,359,"last-layer hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_albert.py,375,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_albert.py,376,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_albert.py,553,fp16 compatibility,not
transformers/src/transformers/modeling_albert.py,568,add hidden_states and attentions if they are here,not
transformers/src/transformers/modeling_albert.py,669,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_albert.py,678,"(loss), prediction_scores, sop_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_albert.py,691,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,not
transformers/src/transformers/modeling_albert.py,797,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_albert.py,886,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_albert.py,890,We are doing regression,not
transformers/src/transformers/modeling_albert.py,898,"(loss), logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_albert.py,981,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_albert.py,985,Only keep active parts of the loss,not
transformers/src/transformers/modeling_albert.py,995,"(loss), logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_albert.py,1089,"If we are on multi-GPU, split add a dimension",not
transformers/src/transformers/modeling_albert.py,1094,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/src/transformers/modeling_albert.py,1105,"(loss), start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_gpt2.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_gpt2.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_gpt2.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_gpt2.py,4,,not
transformers/src/transformers/modeling_tf_gpt2.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_gpt2.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_gpt2.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_gpt2.py,8,,not
transformers/src/transformers/modeling_tf_gpt2.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_gpt2.py,10,,not
transformers/src/transformers/modeling_tf_gpt2.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_gpt2.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_gpt2.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_gpt2.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_gpt2.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_gpt2.py,67,in Attention: n_state=768 (nx=n_embd),not
transformers/src/transformers/modeling_tf_gpt2.py,68,[switch nx => n_state from Block to Attention to keep identical to TF implem],not
transformers/src/transformers/modeling_tf_gpt2.py,96,"q, k, v have shape [batch, heads, sequence, features]",not
transformers/src/transformers/modeling_tf_gpt2.py,99,scale attention_scores,not
transformers/src/transformers/modeling_tf_gpt2.py,102,"w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.",not
transformers/src/transformers/modeling_tf_gpt2.py,109,Apply the attention mask,not
transformers/src/transformers/modeling_tf_gpt2.py,115,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_gpt2.py,134,"(batch, head, seq_length, head_features)",not
transformers/src/transformers/modeling_tf_gpt2.py,149,to cope with keras serialization,not
transformers/src/transformers/modeling_tf_gpt2.py,150,we need to cast `use_cache` to correct bool,not
transformers/src/transformers/modeling_tf_gpt2.py,151,if it is a tensor,not
transformers/src/transformers/modeling_tf_gpt2.py,171,"a, present, (attentions)",not
transformers/src/transformers/modeling_tf_gpt2.py,204,"output_attn: a, present, (attentions)",not
transformers/src/transformers/modeling_tf_gpt2.py,212,"x, present, (attentions)",not
transformers/src/transformers/modeling_tf_gpt2.py,287,"If using past key value states, only the last tokens",not
transformers/src/transformers/modeling_tf_gpt2.py,288,should be given as an input,not
transformers/src/transformers/modeling_tf_gpt2.py,316,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_tf_gpt2.py,317,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_tf_gpt2.py,318,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_gpt2.py,319,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_tf_gpt2.py,320,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_tf_gpt2.py,323,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_gpt2.py,324,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_gpt2.py,325,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_gpt2.py,326,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_gpt2.py,327,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_gpt2.py,334,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_gpt2.py,335,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_gpt2.py,336,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_gpt2.py,337,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_gpt2.py,338,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_tf_gpt2.py,343,head_mask = tf.constant([0] * self.num_hidden_layers),not
transformers/src/transformers/modeling_tf_gpt2.py,378,Add last hidden state,not
transformers/src/transformers/modeling_tf_gpt2.py,389,let the number of heads free (-1) so we can extract attention even after head pruning,not
transformers/src/transformers/modeling_tf_gpt2.py,393,"last hidden state, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_gpt2.py,542,only last token for inputs_ids if past is defined in kwargs,not
transformers/src/transformers/modeling_tf_gpt2.py,591,"lm_logits, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_gpt2.py,743,"lm logits, mc logits, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_albert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_albert.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_albert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_albert.py,4,,not
transformers/src/transformers/modeling_tf_albert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_albert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_albert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_albert.py,8,,not
transformers/src/transformers/modeling_tf_albert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_albert.py,10,,not
transformers/src/transformers/modeling_tf_albert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_albert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_albert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_albert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_albert.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_albert.py,65,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,not
transformers/src/transformers/modeling_tf_albert.py,66,any TensorFlow checkpoint file,not
transformers/src/transformers/modeling_tf_albert.py,73,Create and initialize weights. The random normal initializer was chosen,not
transformers/src/transformers/modeling_tf_albert.py,74,"arbitrarily, and works well.",not
transformers/src/transformers/modeling_tf_albert.py,186,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",not
transformers/src/transformers/modeling_tf_albert.py,187,"(batch size, num_heads, seq_len_q, seq_len_k)",not
transformers/src/transformers/modeling_tf_albert.py,189,scale attention_scores,not
transformers/src/transformers/modeling_tf_albert.py,194,Apply the attention mask is (precomputed for all layers in TFAlbertModel call() function),not
transformers/src/transformers/modeling_tf_albert.py,197,Normalize the attention scores to probabilities.,not
transformers/src/transformers/modeling_tf_albert.py,200,"This is actually dropping out entire tokens to attend to, which might",not
transformers/src/transformers/modeling_tf_albert.py,201,"seem a bit unusual, but is taken from the original Transformer paper.",not
transformers/src/transformers/modeling_tf_albert.py,204,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_albert.py,213,"(batch_size, seq_len_q, all_head_size)",not
transformers/src/transformers/modeling_tf_albert.py,263,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",not
transformers/src/transformers/modeling_tf_albert.py,264,"(batch size, num_heads, seq_len_q, seq_len_k)",not
transformers/src/transformers/modeling_tf_albert.py,266,scale attention_scores,not
transformers/src/transformers/modeling_tf_albert.py,271,Apply the attention mask is (precomputed for all layers in TFBertModel call() function),not
transformers/src/transformers/modeling_tf_albert.py,274,Normalize the attention scores to probabilities.,not
transformers/src/transformers/modeling_tf_albert.py,277,"This is actually dropping out entire tokens to attend to, which might",not
transformers/src/transformers/modeling_tf_albert.py,278,"seem a bit unusual, but is taken from the original Transformer paper.",not
transformers/src/transformers/modeling_tf_albert.py,281,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_albert.py,290,"(batch_size, seq_len_q, all_head_size)",not
transformers/src/transformers/modeling_tf_albert.py,300,add attentions if we output them,not
transformers/src/transformers/modeling_tf_albert.py,338,add attentions if we output them,not
transformers/src/transformers/modeling_tf_albert.py,374,"last-layer hidden state, (layer hidden states), (layer attentions)",not
transformers/src/transformers/modeling_tf_albert.py,405,Number of layers in a hidden group,not
transformers/src/transformers/modeling_tf_albert.py,408,Index of the hidden group,not
transformers/src/transformers/modeling_tf_albert.py,433,"last-layer hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_tf_albert.py,462,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_albert.py,463,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_albert.py,554,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_tf_albert.py,555,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_tf_albert.py,556,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_albert.py,557,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_tf_albert.py,558,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_tf_albert.py,561,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_albert.py,562,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_albert.py,563,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_albert.py,564,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_albert.py,565,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_albert.py,570,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_albert.py,571,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_albert.py,572,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_albert.py,573,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_albert.py,574,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_tf_albert.py,579,head_mask = tf.constant([0] * self.num_hidden_layers),not
transformers/src/transformers/modeling_tf_albert.py,587,add hidden_states and attentions if they are here,not
transformers/src/transformers/modeling_tf_albert.py,589,"sequence_output, pooled_output, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_albert.py,833,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_albert.py,836,"prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_albert.py,892,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_albert.py,894,"logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_albert.py,959,"start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_albert.py,1081,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_albert.py,1083,"reshaped_logits, (hidden_states), (attentions)",not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,2,Copyright 2020 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,121,logits,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,122,no classification heads to worry about,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,132,an existing summarization ckpt,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,138,Check results,not
transformers/src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py,147,Required parameters,not
transformers/src/transformers/modeling_xlm_roberta.py,1,coding=utf-8,not
transformers/src/transformers/modeling_xlm_roberta.py,2,Copyright 2019 Facebook AI Research and the HuggingFace Inc. team.,not
transformers/src/transformers/modeling_xlm_roberta.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_xlm_roberta.py,4,,not
transformers/src/transformers/modeling_xlm_roberta.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_xlm_roberta.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_xlm_roberta.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_xlm_roberta.py,8,,not
transformers/src/transformers/modeling_xlm_roberta.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_xlm_roberta.py,10,,not
transformers/src/transformers/modeling_xlm_roberta.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_xlm_roberta.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_xlm_roberta.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_xlm_roberta.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_xlm_roberta.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_bert_japanese.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_bert_japanese.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_bert_japanese.py,3,,not
transformers/src/transformers/tokenization_bert_japanese.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_bert_japanese.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_bert_japanese.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_bert_japanese.py,7,,not
transformers/src/transformers/tokenization_bert_japanese.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_bert_japanese.py,9,,not
transformers/src/transformers/tokenization_bert_japanese.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_bert_japanese.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_bert_japanese.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_bert_japanese.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_bert_japanese.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_bert_japanese.py,121,"^^ We call the grandparent's init, not the parent's.",not
transformers/src/transformers/modeling_camembert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_camembert.py,2,"Copyright 2019 Inria, Facebook AI Research and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_camembert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_camembert.py,4,,not
transformers/src/transformers/modeling_camembert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_camembert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_camembert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_camembert.py,8,,not
transformers/src/transformers/modeling_camembert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_camembert.py,10,,not
transformers/src/transformers/modeling_camembert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_camembert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_camembert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_camembert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_camembert.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_transfo_xl.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_transfo_xl.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_tf_transfo_xl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_transfo_xl.py,4,,not
transformers/src/transformers/modeling_tf_transfo_xl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_transfo_xl.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_transfo_xl.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_transfo_xl.py,8,,not
transformers/src/transformers/modeling_tf_transfo_xl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_transfo_xl.py,10,,not
transformers/src/transformers/modeling_tf_transfo_xl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_transfo_xl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_transfo_xl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_transfo_xl.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_transfo_xl.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_transfo_xl.py,75,layer normalization + positionwise feed-forward,not
transformers/src/transformers/modeling_tf_transfo_xl.py,82,residual connection,not
transformers/src/transformers/modeling_tf_transfo_xl.py,85,positionwise feed-forward,not
transformers/src/transformers/modeling_tf_transfo_xl.py,91,residual connection + layer normalization,not
transformers/src/transformers/modeling_tf_transfo_xl.py,140,Biases are shared,not
transformers/src/transformers/modeling_tf_transfo_xl.py,152,Biases are not shared,not
transformers/src/transformers/modeling_tf_transfo_xl.py,196,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,197,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,198,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,200,qlen x n_head x d_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,202,compute attention score,not
transformers/src/transformers/modeling_tf_transfo_xl.py,203,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,204,qlen x klen x bsz x n_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,207,qlen x klen x bsz x n_head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,210,[qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_tf_transfo_xl.py,214,compute attention probability,not
transformers/src/transformers/modeling_tf_transfo_xl.py,219,[qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_tf_transfo_xl.py,223,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_transfo_xl.py,227,compute attention vector,not
transformers/src/transformers/modeling_tf_transfo_xl.py,230,[qlen x bsz x n_head x d_head],not
transformers/src/transformers/modeling_tf_transfo_xl.py,234,linear projection,not
transformers/src/transformers/modeling_tf_transfo_xl.py,239,residual connection,not
transformers/src/transformers/modeling_tf_transfo_xl.py,242,residual connection + layer normalization,not
transformers/src/transformers/modeling_tf_transfo_xl.py,329,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_tf_transfo_xl.py,358,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_tf_transfo_xl.py,421,the default attention,not
transformers/src/transformers/modeling_tf_transfo_xl.py,443,learnable embeddings and absolute embeddings,not
transformers/src/transformers/modeling_tf_transfo_xl.py,444,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_tf_transfo_xl.py,449,default attention,not
transformers/src/transformers/modeling_tf_transfo_xl.py,451,learnable embeddings and absolute embeddings,not
transformers/src/transformers/modeling_tf_transfo_xl.py,452,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_tf_transfo_xl.py,493,does not deal with None,not
transformers/src/transformers/modeling_tf_transfo_xl.py,497,mems is not None,not
transformers/src/transformers/modeling_tf_transfo_xl.py,500,There are `mlen + qlen` steps that can be cached into mems,not
transformers/src/transformers/modeling_tf_transfo_xl.py,501,"For the next step, the last `ext_len` of the `qlen` tokens",not
transformers/src/transformers/modeling_tf_transfo_xl.py,502,"will be used as the extended context. Hence, we only cache",not
transformers/src/transformers/modeling_tf_transfo_xl.py,503,the tokens from `mlen + qlen - self.ext_len - self.mem_len`,not
transformers/src/transformers/modeling_tf_transfo_xl.py,504,to `mlen + qlen - self.ext_len`.,not
transformers/src/transformers/modeling_tf_transfo_xl.py,532,"the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library",not
transformers/src/transformers/modeling_tf_transfo_xl.py,533,"so we transpose here from shape [bsz, len] to shape [len, bsz]",not
transformers/src/transformers/modeling_tf_transfo_xl.py,548,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_transfo_xl.py,549,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_transfo_xl.py,550,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_transfo_xl.py,551,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),not
transformers/src/transformers/modeling_tf_transfo_xl.py,552,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_tf_transfo_xl.py,574,::: PyTorch masking code for reference :::,not
transformers/src/transformers/modeling_tf_transfo_xl.py,575,if self.same_length:,not
transformers/src/transformers/modeling_tf_transfo_xl.py,576,"all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)",not
transformers/src/transformers/modeling_tf_transfo_xl.py,577,mask_len = klen - self.mem_len,not
transformers/src/transformers/modeling_tf_transfo_xl.py,578,if mask_len > 0:,not
transformers/src/transformers/modeling_tf_transfo_xl.py,579,mask_shift_len = qlen - mask_len,not
transformers/src/transformers/modeling_tf_transfo_xl.py,580,else:,not
transformers/src/transformers/modeling_tf_transfo_xl.py,581,mask_shift_len = qlen,not
transformers/src/transformers/modeling_tf_transfo_xl.py,582,"dec_attn_mask = (torch.triu(all_ones, 1+mlen)",not
transformers/src/transformers/modeling_tf_transfo_xl.py,583,"+ torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1",not
transformers/src/transformers/modeling_tf_transfo_xl.py,584,else:,not
transformers/src/transformers/modeling_tf_transfo_xl.py,585,dec_attn_mask = torch.triu(,not
transformers/src/transformers/modeling_tf_transfo_xl.py,586,"word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]",not
transformers/src/transformers/modeling_tf_transfo_xl.py,590,default,not
transformers/src/transformers/modeling_tf_transfo_xl.py,606,learnable embeddings and absolute embeddings,not
transformers/src/transformers/modeling_tf_transfo_xl.py,607,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_tf_transfo_xl.py,613,"We transpose back here to shape [bsz, len, hidden_dim]",not
transformers/src/transformers/modeling_tf_transfo_xl.py,616,"Add last layer and transpose to library standard shape [bsz, len, hidden_dim]",not
transformers/src/transformers/modeling_tf_transfo_xl.py,621,"Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]",not
transformers/src/transformers/modeling_tf_transfo_xl.py,624,"last hidden state, new_mems, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_tf_transfo_xl.py,742,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_transfo_xl.py,743,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_transfo_xl.py,853,"logits, new_mems, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_tf_transfo_xl.py,858,if past is defined in model kwargs then use it for faster decoding,not
transformers/src/transformers/tokenization_utils.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_utils.py,2,Copyright 2020 The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_utils.py,3,,not
transformers/src/transformers/tokenization_utils.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_utils.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_utils.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_utils.py,7,,not
transformers/src/transformers/tokenization_utils.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_utils.py,9,,not
transformers/src/transformers/tokenization_utils.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_utils.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_utils.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_utils.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_utils.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_utils.py,48,This is used to set the max input length for a model with infinite size input,not
transformers/src/transformers/tokenization_utils.py,49,This is used when we need something big but slightly smaller than VERY_LARGE_INTEGER,not
transformers/src/transformers/tokenization_utils.py,51,Define type aliases and NamedTuples,not
transformers/src/transformers/tokenization_utils.py,130,Handle all the truncation and padding stuff,not
transformers/src/transformers/tokenization_utils.py,153,"TODO(morgan, anthony): once we have a simple way to serialize tokenizers maybe store and restore the state afterward",SATD
transformers/src/transformers/tokenization_utils.py,154,to avoid destructing the padding / truncation strategy as we do now.,not
transformers/src/transformers/tokenization_utils.py,210,After this point:,not
transformers/src/transformers/tokenization_utils.py,211,Extended properties and methods only available for fast (Rust-based) tokenizers,not
transformers/src/transformers/tokenization_utils.py,212,provided by HuggingFace tokenizers library.,not
transformers/src/transformers/tokenization_utils.py,820,For backward compatibility we fallback to set model_max_length from max_len if provided,not
transformers/src/transformers/tokenization_utils.py,824,"Padding side is right by default and overridden in subclasses. If specified in the kwargs, it is changed.",not
transformers/src/transformers/tokenization_utils.py,832,Added tokens,not
transformers/src/transformers/tokenization_utils.py,837,inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``),not
transformers/src/transformers/tokenization_utils.py,912,Get the vocabulary from AWS S3 bucket,not
transformers/src/transformers/tokenization_utils.py,921,Get the vocabulary from local files,not
transformers/src/transformers/tokenization_utils.py,943,At this point pretrained_model_name_or_path is either a directory or a model identifier name,not
transformers/src/transformers/tokenization_utils.py,949,Look for the tokenizer main vocabulary files + the additional tokens files,not
transformers/src/transformers/tokenization_utils.py,963,"Get files from url, cache, or disk depending on the case",not
transformers/src/transformers/tokenization_utils.py,1013,Prepare tokenizer initialization kwargs,not
transformers/src/transformers/tokenization_utils.py,1014,Did we saved some inputs and kwargs to reload ?,not
transformers/src/transformers/tokenization_utils.py,1025,Update with newly provided kwargs,not
transformers/src/transformers/tokenization_utils.py,1028,Set max length if needed,not
transformers/src/transformers/tokenization_utils.py,1030,"if we're using a pretrained model, ensure the tokenizer",not
transformers/src/transformers/tokenization_utils.py,1031,wont index sequences longer than the number of positional embeddings,not
transformers/src/transformers/tokenization_utils.py,1036,Merge resolved_vocab_files arguments in init_kwargs.,not
transformers/src/transformers/tokenization_utils.py,1049,Instantiate tokenizer.,not
transformers/src/transformers/tokenization_utils.py,1058,Save inputs and kwargs for saving and re-loading with ``save_pretrained``,not
transformers/src/transformers/tokenization_utils.py,1062,update unique_added_tokens_encoder with special tokens for correct tokenization,not
transformers/src/transformers/tokenization_utils.py,1065,Add supplementary tokens.,not
transformers/src/transformers/tokenization_utils.py,1263,TODO: should this be in the base class?,SATD
transformers/src/transformers/tokenization_utils.py,1265,convert non-special tokens to lowercase,not
transformers/src/transformers/tokenization_utils.py,1557,Throw an error if we can pad because there is no padding token,not
transformers/src/transformers/tokenization_utils.py,1709,Throw an error if we can pad because there is no padding token,not
transformers/src/transformers/tokenization_utils.py,1749,"Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by",not
transformers/src/transformers/tokenization_utils.py,1750,"the model. It adds special tokens, truncates sequences if overflowing while taking into account",not
transformers/src/transformers/tokenization_utils.py,1751,the special tokens and manages a window stride for overflowing tokens,not
transformers/src/transformers/tokenization_utils.py,1765,We will convert the whole batch to tensors at the end,not
transformers/src/transformers/tokenization_utils.py,1779,Do the tensor conversion in batch,not
transformers/src/transformers/tokenization_utils.py,1884,Load from model defaults,not
transformers/src/transformers/tokenization_utils.py,1892,Truncation: Handle max sequence length,not
transformers/src/transformers/tokenization_utils.py,1906,Add special tokens,not
transformers/src/transformers/tokenization_utils.py,1914,Build output dictionnary,not
transformers/src/transformers/tokenization_utils.py,1924,Check lengths,not
transformers/src/transformers/tokenization_utils.py,1933,Padding,not
transformers/src/transformers/tokenization_utils.py,1980,Prepare model inputs as tensors if asked,not
transformers/src/transformers/tokenization_utils.py,2157,To avoid mixing byte-level and unicode for byte-level BPT,not
transformers/src/transformers/tokenization_utils.py,2158,we need to build string separatly for added tokens and byte-level tokens,not
transformers/src/transformers/tokenization_utils.py,2159,cf. https://github.com/huggingface/transformers/issues/1133,not
transformers/src/transformers/tokenization_utils.py,2267,Initialize all the rest of the kwargs,not
transformers/src/transformers/tokenization_utils.py,2390,Map special tokens to class attributes (self.pad_token...),not
transformers/src/transformers/tokenization_utils.py,2393,If the backend tokenizer the only specificities of special tokens are that,not
transformers/src/transformers/tokenization_utils.py,2394,"- they will never be processed by the model, and",not
transformers/src/transformers/tokenization_utils.py,2395,- they will be removed while decoding.,not
transformers/src/transformers/tokenization_utils.py,2396,But they are not mapped to special attributes in the backend so we can just,not
transformers/src/transformers/tokenization_utils.py,2397,send a list.,not
transformers/src/transformers/tokenization_utils.py,2437,Needed if we have to return a tensor,not
transformers/src/transformers/tokenization_utils.py,2440,Throw an error if we can pad because there is no padding token,not
transformers/src/transformers/tokenization_utils.py,2444,Set the truncation and padding strategy and restore the initial configuration,not
transformers/src/transformers/tokenization_utils.py,2457,Check for the pretokenized path,not
transformers/src/transformers/tokenization_utils.py,2461,Iterate over each sample (we don't know yet if they are pairs or simple input,not
transformers/src/transformers/tokenization_utils.py,2471,Test if we have a pair of sentences by checking the depth of nesting,not
transformers/src/transformers/tokenization_utils.py,2474,Take care of the first sequence - we multi-thread over the words,SATD
transformers/src/transformers/tokenization_utils.py,2480,Take care of the second sequence if we have a pair,SATD
transformers/src/transformers/tokenization_utils.py,2489,Post-process - truncate/pad and add special tokens,not
transformers/src/transformers/tokenization_utils.py,2493,Classical path with strings input,not
transformers/src/transformers/tokenization_utils.py,2495,Avoid thread overhead if only one example.,not
transformers/src/transformers/tokenization_utils.py,2511,Convert encoding to dict,not
transformers/src/transformers/tokenization_utils.py,2512,"`Tokens` has type: List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]]",not
transformers/src/transformers/tokenization_utils.py,2513,"with nested dimensions corresponding to batch, overflows, sequence length",not
transformers/src/transformers/tokenization_utils.py,2527,Sanitize the output to have dict[list] from list[dict],not
transformers/src/transformers/tokenization_utils.py,2530,"To List[List[List[int]]] of shape (batch, overflows, sequence length)",not
transformers/src/transformers/tokenization_utils.py,2536,elif not return_tensors and len(stack) == 1:,not
transformers/src/transformers/tokenization_utils.py,2537,stack = stack[0],not
transformers/src/transformers/tokenization_utils.py,2541,"If returning overflowing tokens, we need to return a mapping",not
transformers/src/transformers/tokenization_utils.py,2542,from the batch idx to the original sample,not
transformers/src/transformers/tokenization_utils.py,2568,"Check for pretokenized path (ie [token1, token2, ..., tokenN] -> [id1, id2, ..., idN]",not
transformers/src/transformers/tokenization_utils.py,2572,Encode through encode_batch with sequence of only one word which will be merged after hand,not
transformers/src/transformers/tokenization_utils.py,2576,Let's do the same for pairs if provided,not
transformers/src/transformers/tokenization_utils.py,2578,We prepend empty string before each word so that encoding is aware content is a pair,not
transformers/src/transformers/tokenization_utils.py,2591,"Post process and if asked to do so, insert special tokens where needed",not
transformers/src/transformers/tokenization_utils.py,2629,"Return tensor is None, then we can remove the leading batch axis",not
transformers/src/transformers/modeling_auto.py,1,coding=utf-8,not
transformers/src/transformers/modeling_auto.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_auto.py,3,,not
transformers/src/transformers/modeling_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_auto.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_auto.py,7,,not
transformers/src/transformers/modeling_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_auto.py,9,,not
transformers/src/transformers/modeling_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_auto.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_auto.py,14,limitations under the License.,not
transformers/src/transformers/training_args.py,135,if n_gpu is > 1 we'll use nn.DataParallel.,not
transformers/src/transformers/training_args.py,136,If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`,not
transformers/src/transformers/training_args.py,140,"Here, we'll use torch.distributed.",not
transformers/src/transformers/training_args.py,141,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/src/transformers/modeling_flaubert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_flaubert.py,2,"Copyright 2019-present CNRS, Facebook Inc. and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_flaubert.py,3,,not
transformers/src/transformers/modeling_flaubert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_flaubert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_flaubert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_flaubert.py,7,,not
transformers/src/transformers/modeling_flaubert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_flaubert.py,9,,not
transformers/src/transformers/modeling_flaubert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_flaubert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_flaubert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_flaubert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_flaubert.py,14,limitations under the License.,not
transformers/src/transformers/modeling_flaubert.py,114,", dico, is_encoder, with_output):",not
transformers/src/transformers/modeling_flaubert.py,161,"removed: src_enc=None, src_len=None",not
transformers/src/transformers/modeling_flaubert.py,172,mask = input_ids != self.pad_index,not
transformers/src/transformers/modeling_flaubert.py,174,check inputs,not
transformers/src/transformers/modeling_flaubert.py,177,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",not
transformers/src/transformers/modeling_flaubert.py,178,assert (src_enc is None) == (src_len is None),not
transformers/src/transformers/modeling_flaubert.py,179,if src_enc is not None:,not
transformers/src/transformers/modeling_flaubert.py,180,assert self.is_decoder,not
transformers/src/transformers/modeling_flaubert.py,181,assert src_enc.size(0) == bs,not
transformers/src/transformers/modeling_flaubert.py,183,generate masks,not
transformers/src/transformers/modeling_flaubert.py,185,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_flaubert.py,186,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",not
transformers/src/transformers/modeling_flaubert.py,190,position_ids,not
transformers/src/transformers/modeling_flaubert.py,195,"(slen, bs)",not
transformers/src/transformers/modeling_flaubert.py,196,"position_ids = position_ids.transpose(0, 1)",not
transformers/src/transformers/modeling_flaubert.py,198,langs,not
transformers/src/transformers/modeling_flaubert.py,200,"(slen, bs)",not
transformers/src/transformers/modeling_flaubert.py,201,"langs = langs.transpose(0, 1)",not
transformers/src/transformers/modeling_flaubert.py,203,Prepare head mask if needed,not
transformers/src/transformers/modeling_flaubert.py,206,do not recompute cached elements,not
transformers/src/transformers/modeling_flaubert.py,216,embeddings,not
transformers/src/transformers/modeling_flaubert.py,229,transformer layers,not
transformers/src/transformers/modeling_flaubert.py,233,LayerDrop,not
transformers/src/transformers/modeling_flaubert.py,241,self attention,not
transformers/src/transformers/modeling_flaubert.py,259,encoder attention (for decoder only),not
transformers/src/transformers/modeling_flaubert.py,260,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_flaubert.py,261,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",not
transformers/src/transformers/modeling_flaubert.py,262,"attn = F.dropout(attn, p=self.dropout, training=self.training)",not
transformers/src/transformers/modeling_flaubert.py,263,tensor = tensor + attn,not
transformers/src/transformers/modeling_flaubert.py,264,tensor = self.layer_norm15[i](tensor),not
transformers/src/transformers/modeling_flaubert.py,266,FFN,not
transformers/src/transformers/modeling_flaubert.py,276,Add last hidden state,not
transformers/src/transformers/modeling_flaubert.py,280,update cache length,not
transformers/src/transformers/modeling_flaubert.py,284,move back sequence length to dimension 0,SATD
transformers/src/transformers/modeling_flaubert.py,285,"tensor = tensor.transpose(0, 1)",not
transformers/src/transformers/modeling_flaubert.py,292,"outputs, (hidden_states), (attentions)",not
transformers/src/transformers/tokenization_electra.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_electra.py,2,"Copyright 2020 The Google AI Team, Stanford University and The HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_electra.py,3,,not
transformers/src/transformers/tokenization_electra.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_electra.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_electra.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_electra.py,7,,not
transformers/src/transformers/tokenization_electra.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_electra.py,9,,not
transformers/src/transformers/tokenization_electra.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_electra.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_electra.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_electra.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_electra.py,14,limitations under the License.,not
transformers/src/transformers/trainer.py,81,^^ safe to call this function even if cuda is not available,not
transformers/src/transformers/trainer.py,162,Create output directory if needed,not
transformers/src/transformers/trainer.py,166,Set an xla_device flag on the model's config.,not
transformers/src/transformers/trainer.py,167,We'll find a more elegant and not need to do this in the future.,not
transformers/src/transformers/trainer.py,216,We use the same batch_size as for eval.,not
transformers/src/transformers/trainer.py,244,Prepare optimizer and schedule (linear warmup and decay),not
transformers/src/transformers/trainer.py,269,keep track of model topology and gradients,not
transformers/src/transformers/trainer.py,303,Check if saved optimizer or scheduler states exist,not
transformers/src/transformers/trainer.py,309,Load in optimizer and scheduler states,not
transformers/src/transformers/trainer.py,320,multi-gpu training (should be after apex fp16 initialization),not
transformers/src/transformers/trainer.py,324,Distributed training (should be after apex fp16 initialization),not
transformers/src/transformers/trainer.py,339,Train!,not
transformers/src/transformers/trainer.py,359,Check if continuing training from a checkpoint,not
transformers/src/transformers/trainer.py,361,set global_step to global_step of last saved checkpoint from model path,not
transformers/src/transformers/trainer.py,387,Skip past any already trained steps if resuming training,not
transformers/src/transformers/trainer.py,395,last step in epoch but step is always smaller than gradient_accumulation_steps,not
transformers/src/transformers/trainer.py,439,"In all cases (even distributed/parallel), self.model is always a reference",not
transformers/src/transformers/trainer.py,440,to the model we want to save.,not
transformers/src/transformers/trainer.py,445,Save model checkpoint,not
transformers/src/transformers/trainer.py,461,"tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)",not
transformers/src/transformers/trainer.py,478,model outputs are always tuple in transformers (see doc),not
transformers/src/transformers/trainer.py,481,mean() to average on multi-gpu parallel training,not
transformers/src/transformers/trainer.py,523,Save a trained model and configuration using `save_pretrained()`.,not
transformers/src/transformers/trainer.py,524,They can then be reloaded using `from_pretrained()`,not
transformers/src/transformers/trainer.py,529,Good practice: save your training arguments together with the trained model,not
transformers/src/transformers/trainer.py,553,Check if we should delete older checkpoint(s),not
transformers/src/transformers/trainer.py,586,"tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)",not
transformers/src/transformers/trainer.py,612,multi-gpu eval,not
transformers/src/transformers/trainer.py,657,tpu-comment: Get all predictions and labels from all worker shards of eval dataset,not
transformers/src/transformers/configuration_bart.py,1,coding=utf-8,not
transformers/src/transformers/configuration_bart.py,2,Copyright 2020 The Fairseq Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_bart.py,3,,not
transformers/src/transformers/configuration_bart.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_bart.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_bart.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_bart.py,7,,not
transformers/src/transformers/configuration_bart.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_bart.py,9,,not
transformers/src/transformers/configuration_bart.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_bart.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_bart.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_bart.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_bart.py,14,limitations under the License.,not
transformers/src/transformers/configuration_bart.py,90,encoder_embed_dim and decoder_embed_dim,not
transformers/src/transformers/configuration_bart.py,100,"Normal(0, this parameter)",not
transformers/src/transformers/configuration_bart.py,103,Params introduced for Mbart,not
transformers/src/transformers/configuration_bart.py,104,scale factor will be sqrt(d_model) if True,not
transformers/src/transformers/configuration_bart.py,105,"True for mbart, False otherwise",not
transformers/src/transformers/configuration_bart.py,106,combo of fairseq's encoder_ and decoder_normalize_before,not
transformers/src/transformers/configuration_bart.py,109,Params introduced for Marian,not
transformers/src/transformers/configuration_bart.py,113,3 Types of Dropout,not
transformers/src/transformers/configuration_bart.py,118,Classifier stuff,not
transformers/src/transformers/convert_marian_to_pytorch.py,21,or whatever,not
transformers/src/transformers/convert_marian_to_pytorch.py,30,"besides embeddings, everything must be transposed.",not
transformers/src/transformers/convert_marian_to_pytorch.py,50,+ cant be loaded.,not
transformers/src/transformers/convert_marian_to_pytorch.py,91,this one better,SATD
transformers/src/transformers/convert_marian_to_pytorch.py,126,dont convert BPE models.,not
transformers/src/transformers/convert_marian_to_pytorch.py,237,"Dropout, add, normalize",not
transformers/src/transformers/convert_marian_to_pytorch.py,254,for each encoder and decoder layer,not
transformers/src/transformers/convert_marian_to_pytorch.py,271,Decoder Cross Attention,not
transformers/src/transformers/convert_marian_to_pytorch.py,296,self.state_dict['Wemb'].sha,not
transformers/src/transformers/convert_marian_to_pytorch.py,306,Process decoder.yml,not
transformers/src/transformers/convert_marian_to_pytorch.py,326,see opus-mt-train repo/transformer-dropout param.,not
transformers/src/transformers/convert_marian_to_pytorch.py,327,"default: add_final_layer_norm=False,",not
transformers/src/transformers/convert_marian_to_pytorch.py,374,handle tensors not associated with layers,not
transformers/src/transformers/convert_marian_to_pytorch.py,418,"save_json(opus_state.cfg, dest_dir / ""marian_original_config.json"")",not
transformers/src/transformers/convert_marian_to_pytorch.py,419,^^ Save human readable marian config for debugging,not
transformers/src/transformers/convert_marian_to_pytorch.py,423,sanity check,not
transformers/src/transformers/convert_marian_to_pytorch.py,428,Required parameters,not
transformers/src/transformers/modeling_tf_auto.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_auto.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_auto.py,3,,not
transformers/src/transformers/modeling_tf_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_auto.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_auto.py,7,,not
transformers/src/transformers/modeling_tf_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_auto.py,9,,not
transformers/src/transformers/modeling_tf_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_auto.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_auto.py,14,limitations under the License.,not
transformers/src/transformers/modeling_t5.py,1,coding=utf-8,not
transformers/src/transformers/modeling_t5.py,2,"Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.",not
transformers/src/transformers/modeling_t5.py,3,,not
transformers/src/transformers/modeling_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_t5.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_t5.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_t5.py,7,,not
transformers/src/transformers/modeling_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_t5.py,9,,not
transformers/src/transformers/modeling_t5.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_t5.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_t5.py,14,limitations under the License.,not
transformers/src/transformers/modeling_t5.py,35,,not
transformers/src/transformers/modeling_t5.py,36,This dict contrains shortcut names and associated url,not
transformers/src/transformers/modeling_t5.py,37,for the pretrained weights provided with the models,not
transformers/src/transformers/modeling_t5.py,38,,not
transformers/src/transformers/modeling_t5.py,48,,not
transformers/src/transformers/modeling_t5.py,49,This is a conversion method from TF 1.0 to PyTorch,not
transformers/src/transformers/modeling_t5.py,50,More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28,not
transformers/src/transformers/modeling_t5.py,51,,not
transformers/src/transformers/modeling_t5.py,67,Load weights from TF model,not
transformers/src/transformers/modeling_t5.py,79,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,not
transformers/src/transformers/modeling_t5.py,80,which are not required for using pretrained model,not
transformers/src/transformers/modeling_t5.py,101,elif scope_names[0] == 'scale':,not
transformers/src/transformers/modeling_t5.py,102,"pointer = getattr(pointer, 'weight')",not
transformers/src/transformers/modeling_t5.py,103,elif scope_names[0] == 'output_bias' or scope_names[0] == 'beta':,not
transformers/src/transformers/modeling_t5.py,104,"pointer = getattr(pointer, 'bias')",not
transformers/src/transformers/modeling_t5.py,105,elif scope_names[0] == 'squad':,not
transformers/src/transformers/modeling_t5.py,106,"pointer = getattr(pointer, 'classifier')",not
transformers/src/transformers/modeling_t5.py,131,"logger.info(""Weights not copied to PyTorch model: {}"".format(', '.join(tf_weights.keys())))",not
transformers/src/transformers/modeling_t5.py,135,,not
transformers/src/transformers/modeling_t5.py,136,PyTorch Models are constructed by sub-classing,not
transformers/src/transformers/modeling_t5.py,137,- torch.nn.Module for the layers and,not
transformers/src/transformers/modeling_t5.py,138,- PreTrainedModel for the models (it-self a sub-class of torch.nn.Module),not
transformers/src/transformers/modeling_t5.py,139,,not
transformers/src/transformers/modeling_t5.py,200,Mesh TensorFlow initialization to avoid scaling before softmax,not
transformers/src/transformers/modeling_t5.py,220,Prune linear layers,not
transformers/src/transformers/modeling_t5.py,225,Update hyper params,not
transformers/src/transformers/modeling_t5.py,259,"mtf.to_int32(mtf.less(n, 0)) * num_buckets",not
transformers/src/transformers/modeling_t5.py,263,"now n is in the range [0, inf)",not
transformers/src/transformers/modeling_t5.py,265,half of the buckets are for exact increments in positions,not
transformers/src/transformers/modeling_t5.py,269,The other half of the buckets are for logarithmically bigger bins in positions up to max_distance,not
transformers/src/transformers/modeling_t5.py,282,"shape (qlen, klen)",not
transformers/src/transformers/modeling_t5.py,284,"shape (qlen, klen)",not
transformers/src/transformers/modeling_t5.py,289,"shape (qlen, klen, num_heads)",not
transformers/src/transformers/modeling_t5.py,290,"shape (1, num_heads, qlen, klen)",not
transformers/src/transformers/modeling_t5.py,307,"Input is (bs, qlen, dim)",not
transformers/src/transformers/modeling_t5.py,308,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",not
transformers/src/transformers/modeling_t5.py,309,"past_key_value_state[0] is (bs, n_heads, q_len - 1, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,336,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,339,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,340,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,343,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,344,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,349,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,350,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,359,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_t5.py,366,if key and values are already calculated,not
transformers/src/transformers/modeling_t5.py,367,we want only the last query position bias,not
transformers/src/transformers/modeling_t5.py,372,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_t5.py,375,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_t5.py,376,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_t5.py,378,Mask heads if we want to,not
transformers/src/transformers/modeling_t5.py,382,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_t5.py,383,"(bs, qlen, dim)",not
transformers/src/transformers/modeling_t5.py,423,add attentions if we output them,not
transformers/src/transformers/modeling_t5.py,458,add attentions if we output them,not
transformers/src/transformers/modeling_t5.py,511,Keep self-attention outputs and relative position weights,not
transformers/src/transformers/modeling_t5.py,514,the actual query length is unknown for cross attention,not
transformers/src/transformers/modeling_t5.py,515,if using past key value states. Need to inject it here,not
transformers/src/transformers/modeling_t5.py,532,Combine self attn and cross attn key value states,not
transformers/src/transformers/modeling_t5.py,536,Keep cross-attention outputs and relative position weights,not
transformers/src/transformers/modeling_t5.py,539,Apply Feed Forward layer,not
transformers/src/transformers/modeling_t5.py,543,Add attentions if we output them,not
transformers/src/transformers/modeling_t5.py,545,"hidden-states, present_key_value_states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",not
transformers/src/transformers/modeling_t5.py,571,Used for testing weights initialization,not
transformers/src/transformers/modeling_t5.py,575,Mesh TensorFlow embeddings initialization,not
transformers/src/transformers/modeling_t5.py,576,See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624,not
transformers/src/transformers/modeling_t5.py,579,Mesh TensorFlow FF initialization,not
transformers/src/transformers/modeling_t5.py,580,See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56,not
transformers/src/transformers/modeling_t5.py,581,and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89,not
transformers/src/transformers/modeling_t5.py,589,Mesh TensorFlow attention initialization to avoid scaling before softmax,not
transformers/src/transformers/modeling_t5.py,590,See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136,not
transformers/src/transformers/modeling_t5.py,609,shift inputs to the right,not
transformers/src/transformers/modeling_t5.py,615,replace possible -100 values in lm_labels by `pad_token_id`,not
transformers/src/transformers/modeling_t5.py,684,required mask seq length can be calculated via length of past,not
transformers/src/transformers/modeling_t5.py,685,key value states and seq_length = 1 for the last token,not
transformers/src/transformers/modeling_t5.py,696,initialize past_key_value_states with `None` if past does not exist,not
transformers/src/transformers/modeling_t5.py,700,ourselves in which case we just need to make it broadcastable to all heads.,not
transformers/src/transformers/modeling_t5.py,708,Prepare head mask if needed,not
transformers/src/transformers/modeling_t5.py,733,layer_outputs is a tuple with:,not
transformers/src/transformers/modeling_t5.py,734,"hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",not
transformers/src/transformers/modeling_t5.py,737,We share the position biases between the layers - the first layer store them,not
transformers/src/transformers/modeling_t5.py,738,"layer_outputs = hidden-states, key-value-states (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",not
transformers/src/transformers/modeling_t5.py,742,append next layer key value states,not
transformers/src/transformers/modeling_t5.py,746,We keep only self-attention weights for now,not
transformers/src/transformers/modeling_t5.py,751,Add last layer,not
transformers/src/transformers/modeling_t5.py,763,"last-layer hidden state, (presents,) (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_t5.py,923,"Encode if needed (training, first prediction pass)",not
transformers/src/transformers/modeling_t5.py,931,"If decoding with past key value states, only the last tokens",not
transformers/src/transformers/modeling_t5.py,932,should be given as an input,not
transformers/src/transformers/modeling_t5.py,939,Decode,not
transformers/src/transformers/modeling_t5.py,1052,"Encode if needed (training, first prediction pass)",not
transformers/src/transformers/modeling_t5.py,1054,Convert encoder inputs in embeddings if needed,not
transformers/src/transformers/modeling_t5.py,1062,get decoder inputs from shifting lm labels to the right,not
transformers/src/transformers/modeling_t5.py,1065,"If decoding with past key value states, only the last tokens",not
transformers/src/transformers/modeling_t5.py,1066,should be given as an input,not
transformers/src/transformers/modeling_t5.py,1074,Decode,not
transformers/src/transformers/modeling_t5.py,1086,insert decoder past at right place,not
transformers/src/transformers/modeling_t5.py,1087,to speed up decoding,not
transformers/src/transformers/modeling_t5.py,1093,Rescale output before projecting on vocab,not
transformers/src/transformers/modeling_t5.py,1094,See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586,not
transformers/src/transformers/modeling_t5.py,1098,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_t5.py,1102,TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666,SATD
transformers/src/transformers/modeling_t5.py,1110,first step,not
transformers/src/transformers/modeling_t5.py,1125,if decoder past is not included in output,not
transformers/src/transformers/modeling_t5.py,1126,speedy decoding is disabled and no need to reorder,not
transformers/src/transformers/modeling_t5.py,1135,get the correct batch idx from layer past batch dim,not
transformers/src/transformers/modeling_t5.py,1136,batch dim of `past` is at 2nd position,not
transformers/src/transformers/modeling_t5.py,1139,need to set correct `past` for each of the four key / value states,not
transformers/src/transformers/tokenization_camembert.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_camembert.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_camembert.py,3,,not
transformers/src/transformers/tokenization_camembert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_camembert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_camembert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_camembert.py,7,,not
transformers/src/transformers/tokenization_camembert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_camembert.py,9,,not
transformers/src/transformers/tokenization_camembert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_camembert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_camembert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_camembert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_camembert.py,14,limitations under the License,not
transformers/src/transformers/tokenization_camembert.py,44,Load with,not
transformers/src/transformers/tokenization_camembert.py,45,"`tokenizer = AutoTokenizer.from_pretrained(""username/pretrained_model"")`",not
transformers/src/transformers/tokenization_camembert.py,135,HACK: These tokens were added by fairseq but don't seem to be actually used when duplicated in the actual,SATD
transformers/src/transformers/tokenization_camembert.py,136,sentencepiece vocabulary (this is the case for <s> and </s>,not
transformers/src/transformers/tokenization_camembert.py,235,Convert sentence piece unk token to fairseq unk token index,not
transformers/src/transformers/modeling_tf_xlnet.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_xlnet.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_tf_xlnet.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_xlnet.py,4,,not
transformers/src/transformers/modeling_tf_xlnet.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_xlnet.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_xlnet.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_xlnet.py,8,,not
transformers/src/transformers/modeling_tf_xlnet.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_xlnet.py,10,,not
transformers/src/transformers/modeling_tf_xlnet.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_xlnet.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_xlnet.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_xlnet.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_xlnet.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_xlnet.py,128,"x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))",not
transformers/src/transformers/modeling_tf_xlnet.py,137,content based attention score,not
transformers/src/transformers/modeling_tf_xlnet.py,140,position based attention score,not
transformers/src/transformers/modeling_tf_xlnet.py,144,segment based attention score,not
transformers/src/transformers/modeling_tf_xlnet.py,151,merge attention scores and perform masking,not
transformers/src/transformers/modeling_tf_xlnet.py,154,attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask,not
transformers/src/transformers/modeling_tf_xlnet.py,160,attention probability,not
transformers/src/transformers/modeling_tf_xlnet.py,165,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_xlnet.py,169,attention output,not
transformers/src/transformers/modeling_tf_xlnet.py,179,post-attention projection (back to `d_model`),not
transformers/src/transformers/modeling_tf_xlnet.py,196,Two-stream attention with relative positional encoding.,not
transformers/src/transformers/modeling_tf_xlnet.py,197,content based attention score,not
transformers/src/transformers/modeling_tf_xlnet.py,203,content-based key head,not
transformers/src/transformers/modeling_tf_xlnet.py,206,content-based value head,not
transformers/src/transformers/modeling_tf_xlnet.py,209,position-based key head,not
transformers/src/transformers/modeling_tf_xlnet.py,212,h-stream,not
transformers/src/transformers/modeling_tf_xlnet.py,213,content-stream query head,not
transformers/src/transformers/modeling_tf_xlnet.py,216,core attention ops,not
transformers/src/transformers/modeling_tf_xlnet.py,224,post processing,not
transformers/src/transformers/modeling_tf_xlnet.py,227,g-stream,not
transformers/src/transformers/modeling_tf_xlnet.py,228,query-stream query head,not
transformers/src/transformers/modeling_tf_xlnet.py,231,core attention ops,not
transformers/src/transformers/modeling_tf_xlnet.py,250,post processing,not
transformers/src/transformers/modeling_tf_xlnet.py,257,Multi-head attention with relative positional encoding,not
transformers/src/transformers/modeling_tf_xlnet.py,263,content heads,not
transformers/src/transformers/modeling_tf_xlnet.py,268,positional heads,not
transformers/src/transformers/modeling_tf_xlnet.py,271,core attention ops,not
transformers/src/transformers/modeling_tf_xlnet.py,279,post processing,not
transformers/src/transformers/modeling_tf_xlnet.py,331,Add again attentions if there are there,not
transformers/src/transformers/modeling_tf_xlnet.py,339,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_xlnet.py,340,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_xlnet.py,454,"beg, end = klen - 1, -qlen",not
transformers/src/transformers/modeling_tf_xlnet.py,457,"beg, end = klen - 1, -1",not
transformers/src/transformers/modeling_tf_xlnet.py,475,"With bi_data, the batch size should be divisible by 2.",not
transformers/src/transformers/modeling_tf_xlnet.py,535,"the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end",not
transformers/src/transformers/modeling_tf_xlnet.py,536,but we want a unified interface in the library with the batch size on the first dimension,not
transformers/src/transformers/modeling_tf_xlnet.py,537,so we move here the first dimension (batch) to the end,not
transformers/src/transformers/modeling_tf_xlnet.py,561,Attention mask,not
transformers/src/transformers/modeling_tf_xlnet.py,562,causal attention mask,not
transformers/src/transformers/modeling_tf_xlnet.py,571,data mask: input mask & perm mask,not
transformers/src/transformers/modeling_tf_xlnet.py,588,all mems can be attended to,not
transformers/src/transformers/modeling_tf_xlnet.py,606,Word embeddings and prepare h & g hidden states,not
transformers/src/transformers/modeling_tf_xlnet.py,614,else:  # We removed the inp_q input which was same as target mapping,not
transformers/src/transformers/modeling_tf_xlnet.py,615,"inp_q_ext = inp_q[:, :, None]",not
transformers/src/transformers/modeling_tf_xlnet.py,616,word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k,not
transformers/src/transformers/modeling_tf_xlnet.py,621,Segment embedding,not
transformers/src/transformers/modeling_tf_xlnet.py,623,Convert `token_type_ids` to one-hot `seg_mat`,not
transformers/src/transformers/modeling_tf_xlnet.py,627,`1` indicates not in the same segment [qlen x klen x bsz],not
transformers/src/transformers/modeling_tf_xlnet.py,633,Positional encoding,not
transformers/src/transformers/modeling_tf_xlnet.py,637,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_xlnet.py,638,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_xlnet.py,639,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_xlnet.py,640,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),not
transformers/src/transformers/modeling_tf_xlnet.py,641,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_tf_xlnet.py,650,switch to fload if need + fp16 compatibility,not
transformers/src/transformers/modeling_tf_xlnet.py,661,cache new mems,not
transformers/src/transformers/modeling_tf_xlnet.py,675,Add last hidden state,not
transformers/src/transformers/modeling_tf_xlnet.py,681,"Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)",not
transformers/src/transformers/modeling_tf_xlnet.py,697,"outputs, (new_mems), (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_xlnet.py,856,Add dummy token at the end (no attention on this one),not
transformers/src/transformers/modeling_tf_xlnet.py,862,Build permutation mask so that previous tokens don't see last token,not
transformers/src/transformers/modeling_tf_xlnet.py,868,We'll only predict the last token,not
transformers/src/transformers/modeling_tf_xlnet.py,880,if past is defined in model kwargs then use it for faster decoding,not
transformers/src/transformers/modeling_tf_xlnet.py,933,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_tf_xlnet.py,935,"return logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_tf_xlnet.py,997,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_tf_xlnet.py,999,"return logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_tf_xlnet.py,1056,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_tf_xlnet.py,1058,"return logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_tf_xlnet.py,1124,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_tf_xlnet.py,1126,"start_logits, end_logits, (mems), (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_xlnet.py,1129,"@add_start_docstrings(""""""XLNet Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of",not
transformers/src/transformers/modeling_tf_xlnet.py,1130,"the hidden-states output to compute `span start logits` and `span end logits`). """""",",not
transformers/src/transformers/modeling_tf_xlnet.py,1131,"XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)",not
transformers/src/transformers/modeling_tf_xlnet.py,1132,class TFXLNetForQuestionAnswering(TFXLNetPreTrainedModel):,not
transformers/src/transformers/modeling_tf_xlnet.py,1133,"r""""""",not
transformers/src/transformers/modeling_tf_xlnet.py,1134,Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:,not
transformers/src/transformers/modeling_tf_xlnet.py,1135,"**start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",not
transformers/src/transformers/modeling_tf_xlnet.py,1136,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top)``",not
transformers/src/transformers/modeling_tf_xlnet.py,1137,Log probabilities for the top config.start_n_top start token possibilities (beam-search).,not
transformers/src/transformers/modeling_tf_xlnet.py,1138,"**start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",not
transformers/src/transformers/modeling_tf_xlnet.py,1139,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top)``",not
transformers/src/transformers/modeling_tf_xlnet.py,1140,Indices for the top config.start_n_top start token possibilities (beam-search).,not
transformers/src/transformers/modeling_tf_xlnet.py,1141,"**end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",not
transformers/src/transformers/modeling_tf_xlnet.py,1142,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``",not
transformers/src/transformers/modeling_tf_xlnet.py,1143,Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).,not
transformers/src/transformers/modeling_tf_xlnet.py,1144,"**end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",not
transformers/src/transformers/modeling_tf_xlnet.py,1145,"``tf.Tensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``",not
transformers/src/transformers/modeling_tf_xlnet.py,1146,Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).,not
transformers/src/transformers/modeling_tf_xlnet.py,1147,"**cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)",not
transformers/src/transformers/modeling_tf_xlnet.py,1148,"``tf.Tensor`` of shape ``(batch_size,)``",not
transformers/src/transformers/modeling_tf_xlnet.py,1149,Log probabilities for the ``is_impossible`` label of the answers.,not
transformers/src/transformers/modeling_tf_xlnet.py,1150,**mems**:,not
transformers/src/transformers/modeling_tf_xlnet.py,1151,list of ``tf.Tensor`` (one for each layer):,not
transformers/src/transformers/modeling_tf_xlnet.py,1152,that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model,not
transformers/src/transformers/modeling_tf_xlnet.py,1153,if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.,not
transformers/src/transformers/modeling_tf_xlnet.py,1154,See details in the docstring of the `mems` input above.,not
transformers/src/transformers/modeling_tf_xlnet.py,1155,"**hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)",not
transformers/src/transformers/modeling_tf_xlnet.py,1156,list of ``tf.Tensor`` (one for the output of each layer + the output of the embeddings),not
transformers/src/transformers/modeling_tf_xlnet.py,1157,"of shape ``(batch_size, sequence_length, hidden_size)``:",not
transformers/src/transformers/modeling_tf_xlnet.py,1158,Hidden-states of the model at the output of each layer plus the initial embedding outputs.,not
transformers/src/transformers/modeling_tf_xlnet.py,1159,"**attentions**: (`optional`, returned when ``config.output_attentions=True``)",not
transformers/src/transformers/modeling_tf_xlnet.py,1160,"list of ``tf.Tensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:",not
transformers/src/transformers/modeling_tf_xlnet.py,1161,"Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.",not
transformers/src/transformers/modeling_tf_xlnet.py,1163,Examples::,not
transformers/src/transformers/modeling_tf_xlnet.py,1165,# For example purposes. Not runnable.,not
transformers/src/transformers/modeling_tf_xlnet.py,1166,tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048'),not
transformers/src/transformers/modeling_tf_xlnet.py,1167,model = XLMForQuestionAnswering.from_pretrained('xlnet-large-cased'),not
transformers/src/transformers/modeling_tf_xlnet.py,1168,"input_ids = tf.constant(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True))[None, :]  # Batch size 1",not
transformers/src/transformers/modeling_tf_xlnet.py,1169,start_positions = tf.constant([1]),not
transformers/src/transformers/modeling_tf_xlnet.py,1170,end_positions = tf.constant([3]),not
transformers/src/transformers/modeling_tf_xlnet.py,1171,"outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)",not
transformers/src/transformers/modeling_tf_xlnet.py,1172,"loss, start_scores, end_scores = outputs[:2]",not
transformers/src/transformers/modeling_tf_xlnet.py,1174,"""""""",not
transformers/src/transformers/modeling_tf_xlnet.py,1175,"def __init__(self, config, *inputs, **kwargs):",not
transformers/src/transformers/modeling_tf_xlnet.py,1176,"super().__init__(config, *inputs, **kwargs)",not
transformers/src/transformers/modeling_tf_xlnet.py,1177,self.start_n_top = config.start_n_top,not
transformers/src/transformers/modeling_tf_xlnet.py,1178,self.end_n_top = config.end_n_top,not
transformers/src/transformers/modeling_tf_xlnet.py,1180,"self.transformer = TFXLNetMainLayer(config, name='transformer')",not
transformers/src/transformers/modeling_tf_xlnet.py,1181,"self.start_logits = TFPoolerStartLogits(config, name='start_logits')",not
transformers/src/transformers/modeling_tf_xlnet.py,1182,"self.end_logits = TFPoolerEndLogits(config, name='end_logits')",not
transformers/src/transformers/modeling_tf_xlnet.py,1183,"self.answer_class = TFPoolerAnswerClass(config, name='answer_class')",not
transformers/src/transformers/modeling_tf_xlnet.py,1185,"def call(self, inputs, training=False):",not
transformers/src/transformers/modeling_tf_xlnet.py,1186,"transformer_outputs = self.transformer(inputs, training=training)",not
transformers/src/transformers/modeling_tf_xlnet.py,1187,hidden_states = transformer_outputs[0],not
transformers/src/transformers/modeling_tf_xlnet.py,1188,"start_logits = self.start_logits(hidden_states, p_mask=p_mask)",not
transformers/src/transformers/modeling_tf_xlnet.py,1190,"outputs = transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_tf_xlnet.py,1192,if start_positions is not None and end_positions is not None:,not
transformers/src/transformers/modeling_tf_xlnet.py,1193,"# If we are on multi-GPU, let's remove the dimension added by batch splitting",not
transformers/src/transformers/modeling_tf_xlnet.py,1194,"for x in (start_positions, end_positions, cls_index, is_impossible):",not
transformers/src/transformers/modeling_tf_xlnet.py,1195,if x is not None and x.dim() > 1:,not
transformers/src/transformers/modeling_tf_xlnet.py,1196,x.squeeze_(-1),not
transformers/src/transformers/modeling_tf_xlnet.py,1198,"# during training, compute the end logits based on the ground truth of the start position",not
transformers/src/transformers/modeling_tf_xlnet.py,1199,"end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)",not
transformers/src/transformers/modeling_tf_xlnet.py,1201,loss_fct = CrossEntropyLoss(),not
transformers/src/transformers/modeling_tf_xlnet.py,1202,"start_loss = loss_fct(start_logits, start_positions)",not
transformers/src/transformers/modeling_tf_xlnet.py,1203,"end_loss = loss_fct(end_logits, end_positions)",not
transformers/src/transformers/modeling_tf_xlnet.py,1204,total_loss = (start_loss + end_loss) / 2,not
transformers/src/transformers/modeling_tf_xlnet.py,1206,if cls_index is not None and is_impossible is not None:,not
transformers/src/transformers/modeling_tf_xlnet.py,1207,# Predict answerability from the representation of CLS and START,not
transformers/src/transformers/modeling_tf_xlnet.py,1208,"cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)",not
transformers/src/transformers/modeling_tf_xlnet.py,1209,loss_fct_cls = nn.BCEWithLogitsLoss(),not
transformers/src/transformers/modeling_tf_xlnet.py,1210,"cls_loss = loss_fct_cls(cls_logits, is_impossible)",not
transformers/src/transformers/modeling_tf_xlnet.py,1212,# note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss,not
transformers/src/transformers/modeling_tf_xlnet.py,1213,total_loss += cls_loss * 0.5,not
transformers/src/transformers/modeling_tf_xlnet.py,1215,"outputs = (total_loss,) + outputs",not
transformers/src/transformers/modeling_tf_xlnet.py,1217,else:,not
transformers/src/transformers/modeling_tf_xlnet.py,1218,"# during inference, compute the end logits based on beam search",not
transformers/src/transformers/modeling_tf_xlnet.py,1219,"bsz, slen, hsz = hidden_states.size()",not
transformers/src/transformers/modeling_tf_xlnet.py,1220,"start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)",not
transformers/src/transformers/modeling_tf_xlnet.py,1222,"start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)",not
transformers/src/transformers/modeling_tf_xlnet.py,1223,"start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)",not
transformers/src/transformers/modeling_tf_xlnet.py,1224,"start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)",not
transformers/src/transformers/modeling_tf_xlnet.py,1225,"start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)",not
transformers/src/transformers/modeling_tf_xlnet.py,1227,"hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)",not
transformers/src/transformers/modeling_tf_xlnet.py,1228,p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None,not
transformers/src/transformers/modeling_tf_xlnet.py,1229,"end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)",not
transformers/src/transformers/modeling_tf_xlnet.py,1230,"end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)",not
transformers/src/transformers/modeling_tf_xlnet.py,1232,"end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)",not
transformers/src/transformers/modeling_tf_xlnet.py,1233,"end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)",not
transformers/src/transformers/modeling_tf_xlnet.py,1234,"end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)",not
transformers/src/transformers/modeling_tf_xlnet.py,1236,"start_states = torch.einsum(""blh,bl->bh"", hidden_states, start_log_probs)  # get the representation of START as weighted sum of hidden states",not
transformers/src/transformers/modeling_tf_xlnet.py,1237,"cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)  # Shape (batch size,): one single `cls_logits` for each sample",not
transformers/src/transformers/modeling_tf_xlnet.py,1239,"outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs",not
transformers/src/transformers/modeling_tf_xlnet.py,1241,"# return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits",not
transformers/src/transformers/modeling_tf_xlnet.py,1242,"# or (if labels are provided) (total_loss,)",not
transformers/src/transformers/modeling_tf_xlnet.py,1243,return outputs,not
transformers/src/transformers/configuration_camembert.py,1,coding=utf-8,not
transformers/src/transformers/configuration_camembert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_camembert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_camembert.py,4,,not
transformers/src/transformers/configuration_camembert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_camembert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_camembert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_camembert.py,8,,not
transformers/src/transformers/configuration_camembert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_camembert.py,10,,not
transformers/src/transformers/configuration_camembert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_camembert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_camembert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_camembert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_camembert.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_albert.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_albert.py,2,"Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_albert.py,3,,not
transformers/src/transformers/tokenization_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_albert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_albert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_albert.py,7,,not
transformers/src/transformers/tokenization_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_albert.py,9,,not
transformers/src/transformers/tokenization_albert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_albert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_albert.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_roberta.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_roberta.py,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_roberta.py,3,,not
transformers/src/transformers/tokenization_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_roberta.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_roberta.py,7,,not
transformers/src/transformers/tokenization_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_roberta.py,9,,not
transformers/src/transformers/tokenization_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_roberta.py,14,limitations under the License.,not
transformers/src/transformers/modeling_marian.py,1,coding=utf-8,not
transformers/src/transformers/modeling_marian.py,2,Copyright 2020 Marian Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_marian.py,3,,not
transformers/src/transformers/modeling_marian.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_marian.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_marian.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_marian.py,7,,not
transformers/src/transformers/modeling_marian.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_marian.py,9,,not
transformers/src/transformers/modeling_marian.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_marian.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_marian.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_marian.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_marian.py,14,limitations under the License.,not
transformers/src/transformers/modeling_marian.py,44,see https://huggingface.co/models?search=Helsinki-NLP,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,2,Copyright 2019 Facebook AI Research and the HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_xlm_roberta.py,4,,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_xlm_roberta.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,8,,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,10,,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_xlm_roberta.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_xlm_roberta.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_xlm_roberta.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_xlm_roberta.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_auto.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_auto.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_auto.py,3,,not
transformers/src/transformers/tokenization_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_auto.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_auto.py,7,,not
transformers/src/transformers/tokenization_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_auto.py,9,,not
transformers/src/transformers/tokenization_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_auto.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_auto.py,14,limitations under the License.,not
transformers/src/transformers/modeling_tf_electra.py,49,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,not
transformers/src/transformers/modeling_tf_electra.py,50,any TensorFlow checkpoint file,not
transformers/src/transformers/modeling_tf_electra.py,57,Create and initialize weights. The random normal initializer was chosen,not
transformers/src/transformers/modeling_tf_electra.py,58,"arbitrarily, and works well.",not
transformers/src/transformers/modeling_tf_electra.py,170,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_tf_electra.py,171,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_tf_electra.py,172,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_electra.py,173,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_tf_electra.py,174,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_tf_electra.py,177,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_electra.py,178,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_electra.py,179,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_electra.py,180,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_electra.py,181,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_electra.py,455,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_electra.py,550,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_electra.py,616,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/configuration_bert.py,1,coding=utf-8,not
transformers/src/transformers/configuration_bert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_bert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_bert.py,4,,not
transformers/src/transformers/configuration_bert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_bert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_bert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_bert.py,8,,not
transformers/src/transformers/configuration_bert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_bert.py,10,,not
transformers/src/transformers/configuration_bert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_bert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_bert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_bert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_bert.py,15,limitations under the License.,not
transformers/src/transformers/modeling_roberta.py,1,coding=utf-8,not
transformers/src/transformers/modeling_roberta.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_roberta.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_roberta.py,4,,not
transformers/src/transformers/modeling_roberta.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_roberta.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_roberta.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_roberta.py,8,,not
transformers/src/transformers/modeling_roberta.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_roberta.py,10,,not
transformers/src/transformers/modeling_roberta.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_roberta.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_roberta.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_roberta.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_roberta.py,15,limitations under the License.,not
transformers/src/transformers/modeling_roberta.py,59,Create the position ids from the input token ids. Any padded tokens remain padded.,not
transformers/src/transformers/modeling_roberta.py,237,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_roberta.py,244,"(masked_lm_loss), prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_roberta.py,258,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,not
transformers/src/transformers/modeling_roberta.py,266,project back to size of vocabulary with bias,not
transformers/src/transformers/modeling_roberta.py,352,We are doing regression,not
transformers/src/transformers/modeling_roberta.py,360,"(loss), logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_roberta.py,452,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_roberta.py,459,"(loss), reshaped_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_roberta.py,544,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_roberta.py,548,Only keep active parts of the loss,not
transformers/src/transformers/modeling_roberta.py,560,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_roberta.py,573,take <s> token (equiv. to [CLS]),not
transformers/src/transformers/modeling_roberta.py,681,"If we are on multi-GPU, split add a dimension",not
transformers/src/transformers/modeling_roberta.py,686,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/src/transformers/modeling_roberta.py,697,"(loss), start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_bert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_bert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_bert.py,4,,not
transformers/src/transformers/modeling_tf_bert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_bert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_bert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_bert.py,8,,not
transformers/src/transformers/modeling_tf_bert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_bert.py,10,,not
transformers/src/transformers/modeling_tf_bert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_bert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_bert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_bert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_bert.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_bert.py,116,self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load,not
transformers/src/transformers/modeling_tf_bert.py,117,any TensorFlow checkpoint file,not
transformers/src/transformers/modeling_tf_bert.py,124,Create and initialize weights. The random normal initializer was chosen,not
transformers/src/transformers/modeling_tf_bert.py,125,"arbitrarily, and works well.",not
transformers/src/transformers/modeling_tf_bert.py,239,"Take the dot product between ""query"" and ""key"" to get the raw attention scores.",not
transformers/src/transformers/modeling_tf_bert.py,242,"(batch size, num_heads, seq_len_q, seq_len_k)",not
transformers/src/transformers/modeling_tf_bert.py,243,scale attention_scores,not
transformers/src/transformers/modeling_tf_bert.py,247,Apply the attention mask is (precomputed for all layers in TFBertModel call() function),not
transformers/src/transformers/modeling_tf_bert.py,250,Normalize the attention scores to probabilities.,not
transformers/src/transformers/modeling_tf_bert.py,253,"This is actually dropping out entire tokens to attend to, which might",not
transformers/src/transformers/modeling_tf_bert.py,254,"seem a bit unusual, but is taken from the original Transformer paper.",not
transformers/src/transformers/modeling_tf_bert.py,257,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_bert.py,266,"(batch_size, seq_len_q, all_head_size)",not
transformers/src/transformers/modeling_tf_bert.py,304,add attentions if we output them,not
transformers/src/transformers/modeling_tf_bert.py,357,add attentions if we output them,not
transformers/src/transformers/modeling_tf_bert.py,383,Add last layer,not
transformers/src/transformers/modeling_tf_bert.py,392,"outputs, (hidden states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,406,"We ""pool"" the model by simply taking the hidden state corresponding",not
transformers/src/transformers/modeling_tf_bert.py,407,to the first token.,not
transformers/src/transformers/modeling_tf_bert.py,438,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_bert.py,439,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_bert.py,543,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_tf_bert.py,544,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_tf_bert.py,545,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_bert.py,546,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_tf_bert.py,547,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_tf_bert.py,550,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_bert.py,551,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_bert.py,552,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_bert.py,553,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_bert.py,554,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_bert.py,559,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_bert.py,560,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_bert.py,561,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_bert.py,562,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_bert.py,563,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_tf_bert.py,568,head_mask = tf.constant([0] * self.num_hidden_layers),not
transformers/src/transformers/modeling_tf_bert.py,578,add hidden_states and attentions if they are here,not
transformers/src/transformers/modeling_tf_bert.py,579,"sequence_output, pooled_output, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,769,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_bert.py,771,"prediction_scores, seq_relationship_score, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,820,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_bert.py,822,"prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,870,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_bert.py,872,"seq_relationship_score, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,928,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_bert.py,930,"logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,1047,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_bert.py,1049,"reshaped_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,1105,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_bert.py,1107,"scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_bert.py,1174,"start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_xlm.py,1,coding=utf-8,not
transformers/src/transformers/modeling_xlm.py,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_xlm.py,3,,not
transformers/src/transformers/modeling_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_xlm.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_xlm.py,7,,not
transformers/src/transformers/modeling_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_xlm.py,9,,not
transformers/src/transformers/modeling_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_xlm.py,14,limitations under the License.,not
transformers/src/transformers/modeling_xlm.py,70,"attention mask is the same as mask, or triangular inferior attention (causal)",not
transformers/src/transformers/modeling_xlm.py,77,sanity check,not
transformers/src/transformers/modeling_xlm.py,114,Prune linear layers,not
transformers/src/transformers/modeling_xlm.py,119,Update hyper params,not
transformers/src/transformers/modeling_xlm.py,128,"Input is (bs, qlen, dim)",not
transformers/src/transformers/modeling_xlm.py,129,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",not
transformers/src/transformers/modeling_xlm.py,135,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",not
transformers/src/transformers/modeling_xlm.py,148,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,150,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,151,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,154,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,155,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,161,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,162,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,167,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,168,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_xlm.py,169,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_xlm.py,170,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_xlm.py,172,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_xlm.py,173,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_xlm.py,175,Mask heads if we want to,not
transformers/src/transformers/modeling_xlm.py,179,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_xlm.py,180,"(bs, qlen, dim)",not
transformers/src/transformers/modeling_xlm.py,314,", dico, is_encoder, with_output):",not
transformers/src/transformers/modeling_xlm.py,319,"encoder / decoder, output layer",not
transformers/src/transformers/modeling_xlm.py,324,self.with_output = with_output,not
transformers/src/transformers/modeling_xlm.py,327,dictionary / languages,not
transformers/src/transformers/modeling_xlm.py,333,self.dico = dico,not
transformers/src/transformers/modeling_xlm.py,334,self.id2lang = config.id2lang,not
transformers/src/transformers/modeling_xlm.py,335,self.lang2id = config.lang2id,not
transformers/src/transformers/modeling_xlm.py,336,assert len(self.dico) == self.n_words,not
transformers/src/transformers/modeling_xlm.py,337,assert len(self.id2lang) == len(self.lang2id) == self.n_langs,not
transformers/src/transformers/modeling_xlm.py,339,model parameters,not
transformers/src/transformers/modeling_xlm.py,340,512 by default,not
transformers/src/transformers/modeling_xlm.py,341,2048 by default,not
transformers/src/transformers/modeling_xlm.py,342,8 by default,not
transformers/src/transformers/modeling_xlm.py,348,embeddings,not
transformers/src/transformers/modeling_xlm.py,357,transformer layers,not
transformers/src/transformers/modeling_xlm.py,362,if self.is_decoder:,not
transformers/src/transformers/modeling_xlm.py,363,self.layer_norm15 = nn.ModuleList(),not
transformers/src/transformers/modeling_xlm.py,364,self.encoder_attn = nn.ModuleList(),not
transformers/src/transformers/modeling_xlm.py,369,if self.is_decoder:,not
transformers/src/transformers/modeling_xlm.py,370,"self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))",not
transformers/src/transformers/modeling_xlm.py,371,"self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))",not
transformers/src/transformers/modeling_xlm.py,450,mask = input_ids != self.pad_index,not
transformers/src/transformers/modeling_xlm.py,452,check inputs,not
transformers/src/transformers/modeling_xlm.py,455,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",not
transformers/src/transformers/modeling_xlm.py,456,assert (src_enc is None) == (src_len is None),not
transformers/src/transformers/modeling_xlm.py,457,if src_enc is not None:,not
transformers/src/transformers/modeling_xlm.py,458,assert self.is_decoder,not
transformers/src/transformers/modeling_xlm.py,459,assert src_enc.size(0) == bs,not
transformers/src/transformers/modeling_xlm.py,461,generate masks,not
transformers/src/transformers/modeling_xlm.py,463,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_xlm.py,464,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",not
transformers/src/transformers/modeling_xlm.py,468,position_ids,not
transformers/src/transformers/modeling_xlm.py,473,"(slen, bs)",not
transformers/src/transformers/modeling_xlm.py,474,"position_ids = position_ids.transpose(0, 1)",not
transformers/src/transformers/modeling_xlm.py,476,langs,not
transformers/src/transformers/modeling_xlm.py,478,"(slen, bs)",not
transformers/src/transformers/modeling_xlm.py,479,"langs = langs.transpose(0, 1)",not
transformers/src/transformers/modeling_xlm.py,481,Prepare head mask if needed,not
transformers/src/transformers/modeling_xlm.py,484,do not recompute cached elements,not
transformers/src/transformers/modeling_xlm.py,494,embeddings,not
transformers/src/transformers/modeling_xlm.py,507,transformer layers,not
transformers/src/transformers/modeling_xlm.py,514,self attention,not
transformers/src/transformers/modeling_xlm.py,523,encoder attention (for decoder only),not
transformers/src/transformers/modeling_xlm.py,524,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_xlm.py,525,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",not
transformers/src/transformers/modeling_xlm.py,526,"attn = F.dropout(attn, p=self.dropout, training=self.training)",not
transformers/src/transformers/modeling_xlm.py,527,tensor = tensor + attn,not
transformers/src/transformers/modeling_xlm.py,528,tensor = self.layer_norm15[i](tensor),not
transformers/src/transformers/modeling_xlm.py,530,FFN,not
transformers/src/transformers/modeling_xlm.py,535,Add last hidden state,not
transformers/src/transformers/modeling_xlm.py,539,update cache length,not
transformers/src/transformers/modeling_xlm.py,543,move back sequence length to dimension 0,SATD
transformers/src/transformers/modeling_xlm.py,544,"tensor = tensor.transpose(0, 1)",not
transformers/src/transformers/modeling_xlm.py,551,"outputs, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_xlm.py,574,default is False,not
transformers/src/transformers/modeling_xlm.py,692,Keep new_mems and attention/hidden states if they are here,not
transformers/src/transformers/modeling_xlm.py,779,Keep new_mems and attention/hidden states if they are here,not
transformers/src/transformers/modeling_xlm.py,783,We are doing regression,not
transformers/src/transformers/modeling_xlm.py,891,"If we are on multi-GPU, split add a dimension",not
transformers/src/transformers/modeling_xlm.py,896,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/src/transformers/modeling_xlm.py,907,Keep new_mems and attention/hidden states if they are here,not
transformers/src/transformers/modeling_xlm.py,1024,Keep new_mems and attention/hidden states if they are here,not
transformers/src/transformers/modeling_xlm.py,1106,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_xlm.py,1109,Only keep active parts of the loss,not
transformers/src/transformers/modeling_xlm.py,1121,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/tokenization_bert.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_bert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_bert.py,3,,not
transformers/src/transformers/tokenization_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_bert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_bert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_bert.py,7,,not
transformers/src/transformers/tokenization_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_bert.py,9,,not
transformers/src/transformers/tokenization_bert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_bert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_bert.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_bert.py,381,"This was added on November 1st, 2018 for the multilingual and Chinese",not
transformers/src/transformers/tokenization_bert.py,382,"models. This is also applied to the English models now, but it doesn't",not
transformers/src/transformers/tokenization_bert.py,383,matter since the English models were not trained on any Chinese data,not
transformers/src/transformers/tokenization_bert.py,384,and generally don't have any Chinese data in them (there are Chinese,not
transformers/src/transformers/tokenization_bert.py,385,characters in the vocabulary because Wikipedia does have some Chinese,not
transformers/src/transformers/tokenization_bert.py,386,words in the English Wikipedia.).,not
transformers/src/transformers/tokenization_bert.py,448,"This defines a ""chinese character"" as anything in the CJK Unicode block:",not
transformers/src/transformers/tokenization_bert.py,449,https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block),not
transformers/src/transformers/tokenization_bert.py,450,,not
transformers/src/transformers/tokenization_bert.py,451,"Note that the CJK Unicode block is NOT all Japanese and Korean characters,",not
transformers/src/transformers/tokenization_bert.py,452,"despite its name. The modern Korean Hangul alphabet is a different block,",not
transformers/src/transformers/tokenization_bert.py,453,as is Japanese Hiragana and Katakana. Those alphabets are used to write,not
transformers/src/transformers/tokenization_bert.py,454,"space-separated words, so they are not treated specially and handled",not
transformers/src/transformers/tokenization_bert.py,455,like the all of the other languages.,not
transformers/src/transformers/tokenization_bert.py,458,,not
transformers/src/transformers/tokenization_bert.py,459,,not
transformers/src/transformers/tokenization_bert.py,460,,not
transformers/src/transformers/tokenization_bert.py,461,,not
transformers/src/transformers/tokenization_bert.py,462,,not
transformers/src/transformers/tokenization_bert.py,464,,not
transformers/src/transformers/tokenization_bert.py,465,,not
transformers/src/transformers/tokenization_bert.py,546,"\t, \n, and \r are technically contorl characters but we treat them",not
transformers/src/transformers/tokenization_bert.py,547,as whitespace since they are generally considered as such.,not
transformers/src/transformers/tokenization_bert.py,558,These are technically control characters but we count them as whitespace,not
transformers/src/transformers/tokenization_bert.py,559,characters.,not
transformers/src/transformers/tokenization_bert.py,571,We treat all non-letter/number ASCII as punctuation.,not
transformers/src/transformers/tokenization_bert.py,572,"Characters such as ""^"", ""$"", and ""`"" are not in the Unicode",not
transformers/src/transformers/tokenization_bert.py,573,"Punctuation class but we treat them as punctuation anyways, for",not
transformers/src/transformers/tokenization_bert.py,574,consistency.,not
transformers/src/transformers/tokenization_bart.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_bart.py,2,Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_bart.py,3,,not
transformers/src/transformers/tokenization_bart.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_bart.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_bart.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_bart.py,7,,not
transformers/src/transformers/tokenization_bart.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_bart.py,9,,not
transformers/src/transformers/tokenization_bart.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_bart.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_bart.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_bart.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_bart.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_bart.py,25,vocab and merges same as roberta,not
transformers/src/transformers/tokenization_bart.py,34,merges and vocab same as Roberta,not
transformers/src/transformers/configuration_xlm_roberta.py,1,coding=utf-8,not
transformers/src/transformers/configuration_xlm_roberta.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_xlm_roberta.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_xlm_roberta.py,4,,not
transformers/src/transformers/configuration_xlm_roberta.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_xlm_roberta.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_xlm_roberta.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_xlm_roberta.py,8,,not
transformers/src/transformers/configuration_xlm_roberta.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_xlm_roberta.py,10,,not
transformers/src/transformers/configuration_xlm_roberta.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_xlm_roberta.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_xlm_roberta.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_xlm_roberta.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_xlm_roberta.py,15,limitations under the License.,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,54,Initialise PyTorch model,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,69,Load weights from tf checkpoint,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,72,Save pytorch-model,not
transformers/src/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py,84,Required parameters,not
transformers/src/transformers/modeling_tf_camembert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_camembert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_camembert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_camembert.py,4,,not
transformers/src/transformers/modeling_tf_camembert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_camembert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_camembert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_camembert.py,8,,not
transformers/src/transformers/modeling_tf_camembert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_camembert.py,10,,not
transformers/src/transformers/modeling_tf_camembert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_camembert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_camembert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_camembert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_camembert.py,15,limitations under the License.,not
transformers/src/transformers/modeling_reformer.py,1,coding=utf-8,not
transformers/src/transformers/modeling_reformer.py,2,Copyright 2020 The Trax Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_reformer.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_reformer.py,4,,not
transformers/src/transformers/modeling_reformer.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_reformer.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_reformer.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_reformer.py,8,,not
transformers/src/transformers/modeling_reformer.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_reformer.py,10,,not
transformers/src/transformers/modeling_reformer.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_reformer.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_reformer.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_reformer.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_reformer.py,15,limitations under the License.,not
transformers/src/transformers/modeling_reformer.py,58,Define named tuples for nn.Modules here,not
transformers/src/transformers/modeling_reformer.py,106,create weights,not
transformers/src/transformers/modeling_reformer.py,108,create expanded shapes,not
transformers/src/transformers/modeling_reformer.py,113,create tensor and init,not
transformers/src/transformers/modeling_reformer.py,117,broadcast weights to correct shape,not
transformers/src/transformers/modeling_reformer.py,133,permute weights so that 2D correctly drops dims 1 and 2,not
transformers/src/transformers/modeling_reformer.py,135,drop entire matrix of last two dims (prev dims 1 and 2),not
transformers/src/transformers/modeling_reformer.py,156,reshape axial encodings and use only until sequence_length,not
transformers/src/transformers/modeling_reformer.py,216,dropout,not
transformers/src/transformers/modeling_reformer.py,219,add positional embeddings,not
transformers/src/transformers/modeling_reformer.py,302,projection matrices,not
transformers/src/transformers/modeling_reformer.py,306,save mask value here. Need fp32 and fp16 mask values,not
transformers/src/transformers/modeling_reformer.py,325,num hashes can optionally be overwritten by user,not
transformers/src/transformers/modeling_reformer.py,328,project hidden_states to query_key and value,not
transformers/src/transformers/modeling_reformer.py,332,free memory,not
transformers/src/transformers/modeling_reformer.py,351,"set `num_buckets` on the fly, recommended way to do it",not
transformers/src/transformers/modeling_reformer.py,355,use cached buckets for backprop only,not
transformers/src/transformers/modeling_reformer.py,357,hash query key vectors into buckets,not
transformers/src/transformers/modeling_reformer.py,368,make sure bucket idx is not longer then sequence length,not
transformers/src/transformers/modeling_reformer.py,371,cluster query key value vectors according to hashed buckets,not
transformers/src/transformers/modeling_reformer.py,387,scale key vectors,not
transformers/src/transformers/modeling_reformer.py,390,get attention probs,not
transformers/src/transformers/modeling_reformer.py,399,free memory,not
transformers/src/transformers/modeling_reformer.py,402,sort clusters back to correct ordering,not
transformers/src/transformers/modeling_reformer.py,407,sum up all hash rounds,not
transformers/src/transformers/modeling_reformer.py,418,free memory,not
transformers/src/transformers/modeling_reformer.py,421,free memory,not
transformers/src/transformers/modeling_reformer.py,441,See https://arxiv.org/pdf/1509.02897.pdf,not
transformers/src/transformers/modeling_reformer.py,442,We sample a different random rotation for each round of hashing to,not
transformers/src/transformers/modeling_reformer.py,443,decrease the probability of hash misses.,not
transformers/src/transformers/modeling_reformer.py,451,Factorize the hash if self.num_buckets is a list or tuple,not
transformers/src/transformers/modeling_reformer.py,460,remove gradient,not
transformers/src/transformers/modeling_reformer.py,464,for determinism,not
transformers/src/transformers/modeling_reformer.py,468,create a random self.attention_head_size x num_hashes x num_buckets/2,not
transformers/src/transformers/modeling_reformer.py,471,Output dim: Batch_Size x Num_Attn_Heads x Num_Hashes x Seq_Len x Num_Buckets/2,not
transformers/src/transformers/modeling_reformer.py,478,Get the buckets for them and combine.,not
transformers/src/transformers/modeling_reformer.py,492,buckets is now (Batch_size x Num_Attn_Heads x Num_Hashes x Seq_Len).,not
transformers/src/transformers/modeling_reformer.py,493,Next we add offsets so that bucket numbers from different hashing rounds don't overlap.,not
transformers/src/transformers/modeling_reformer.py,497,expand to batch size and num attention heads,not
transformers/src/transformers/modeling_reformer.py,504,no gradients are needed,not
transformers/src/transformers/modeling_reformer.py,508,arange and expand,not
transformers/src/transformers/modeling_reformer.py,512,scale buckets,not
transformers/src/transformers/modeling_reformer.py,515,remove gradient,not
transformers/src/transformers/modeling_reformer.py,518,Hash-based sort,not
transformers/src/transformers/modeling_reformer.py,521,"create simple indices to scatter to, to have undo sort",not
transformers/src/transformers/modeling_reformer.py,528,get undo sort,not
transformers/src/transformers/modeling_reformer.py,535,recommended `num_buckets` from paper,not
transformers/src/transformers/modeling_reformer.py,538,factorize `num_buckets` if `num_buckets` becomes too large,not
transformers/src/transformers/modeling_reformer.py,552,get logits and dots,not
transformers/src/transformers/modeling_reformer.py,555,free memory,not
transformers/src/transformers/modeling_reformer.py,563,get correct mask values depending on precision,not
transformers/src/transformers/modeling_reformer.py,576,free memory,not
transformers/src/transformers/modeling_reformer.py,579,Self mask is ALWAYS applied.,not
transformers/src/transformers/modeling_reformer.py,580,From the reformer paper (https://arxiv.org/pdf/2001.04451.pdf):,not
transformers/src/transformers/modeling_reformer.py,581,""" While attention to the future is not allowed, typical implementations of the",not
transformers/src/transformers/modeling_reformer.py,582,Transformer do allow a position to attend to itself.,not
transformers/src/transformers/modeling_reformer.py,583,Such behavior is undesirable in a shared-QK formulation because the dot-product,not
transformers/src/transformers/modeling_reformer.py,584,of a query vector with itself will almost always be greater than the dot product of a,not
transformers/src/transformers/modeling_reformer.py,585,query vector with a vector at another position. We therefore modify the masking,not
transformers/src/transformers/modeling_reformer.py,586,"to forbid a token from attending to itself, except in situations",not
transformers/src/transformers/modeling_reformer.py,587,"where a token has no other valid attention targets (e.g. the first token in a sequence) """,not
transformers/src/transformers/modeling_reformer.py,593,apply self_mask,not
transformers/src/transformers/modeling_reformer.py,596,free memory,not
transformers/src/transformers/modeling_reformer.py,600,"dots shape is `[batch_size, num_attn_heads, num_hashes * seq_len // chunk_length, chunk_length, chunk_length * (1 + num_chunks_before + num_chunks_after)]`",not
transformers/src/transformers/modeling_reformer.py,603,free memory,not
transformers/src/transformers/modeling_reformer.py,606,dropout,not
transformers/src/transformers/modeling_reformer.py,609,Mask heads if we want to,not
transformers/src/transformers/modeling_reformer.py,613,attend values,not
transformers/src/transformers/modeling_reformer.py,616,free memory,not
transformers/src/transformers/modeling_reformer.py,619,merge chunk length,not
transformers/src/transformers/modeling_reformer.py,628,Causal mask,not
transformers/src/transformers/modeling_reformer.py,632,"Attention mask: chunk, look up correct mask value from key_value_bucket_idx",not
transformers/src/transformers/modeling_reformer.py,633,IMPORTANT: official trax code does not use a mask for LSH Atttention. Not sure why.,not
transformers/src/transformers/modeling_reformer.py,636,expand attn_mask to fit with key_value_bucket_idx shape,not
transformers/src/transformers/modeling_reformer.py,640,expand to query_key_dots shape: duplicate along query axis since key sorting is the same for each query position in chunk,not
transformers/src/transformers/modeling_reformer.py,642,free memory,not
transformers/src/transformers/modeling_reformer.py,645,multiply by casaul mask if necessary,not
transformers/src/transformers/modeling_reformer.py,691,save sorted_bucket_idx for backprop,not
transformers/src/transformers/modeling_reformer.py,696,undo sort to have correct order for next layer,not
transformers/src/transformers/modeling_reformer.py,704,get parameters saved in ctx,not
transformers/src/transformers/modeling_reformer.py,708,get real gradient shape,not
transformers/src/transformers/modeling_reformer.py,709,shape is BatchSize x NumAttnHeads x ChunkLen * NumHashes,not
transformers/src/transformers/modeling_reformer.py,711,shape is BatchSize x NumAttnHeads x ChunkLen * NumHashes x ChunkLen,not
transformers/src/transformers/modeling_reformer.py,714,split gradient vectors and sorted bucket idxs by concatenated chunk dimension to gather correct indices,not
transformers/src/transformers/modeling_reformer.py,715,shape is BatchSize x NumAttnHeads x NumHashes x ChunkLen,not
transformers/src/transformers/modeling_reformer.py,717,shape is BatchSize x NumAttnHeads x NumHashes x ChunkLen x ChunkLen,not
transformers/src/transformers/modeling_reformer.py,722,reshape and expand,not
transformers/src/transformers/modeling_reformer.py,725,reverse sort of forward,not
transformers/src/transformers/modeling_reformer.py,729,reshape into correct shape,not
transformers/src/transformers/modeling_reformer.py,733,return grad and `None` fillers for last 3 forward args,not
transformers/src/transformers/modeling_reformer.py,752,projection matrices,not
transformers/src/transformers/modeling_reformer.py,759,save mask value here,not
transformers/src/transformers/modeling_reformer.py,767,"project hidden_states to query, key and value",not
transformers/src/transformers/modeling_reformer.py,772,split last dim into `config.num_attention_heads` and `config.attention_head_size`,not
transformers/src/transformers/modeling_reformer.py,798,normalize key vectors,not
transformers/src/transformers/modeling_reformer.py,803,chunk vectors,not
transformers/src/transformers/modeling_reformer.py,804,B x Num_Attn_Head x Seq_Len // chunk_len x chunk_len  x  attn_head_size,not
transformers/src/transformers/modeling_reformer.py,815,chunk indices,not
transformers/src/transformers/modeling_reformer.py,822,append chunks before and after,not
transformers/src/transformers/modeling_reformer.py,829,free memory,not
transformers/src/transformers/modeling_reformer.py,835,get mask tensor depending on half precision or not,not
transformers/src/transformers/modeling_reformer.py,843,free memory,not
transformers/src/transformers/modeling_reformer.py,846,softmax,not
transformers/src/transformers/modeling_reformer.py,850,free memory,not
transformers/src/transformers/modeling_reformer.py,853,dropout,not
transformers/src/transformers/modeling_reformer.py,856,Mask heads if we want to,not
transformers/src/transformers/modeling_reformer.py,860,attend values,not
transformers/src/transformers/modeling_reformer.py,863,free memory,not
transformers/src/transformers/modeling_reformer.py,866,merge chunk length,not
transformers/src/transformers/modeling_reformer.py,881,chunk attention mask and look before and after,not
transformers/src/transformers/modeling_reformer.py,887,Causal mask,not
transformers/src/transformers/modeling_reformer.py,891,Attention mask,not
transformers/src/transformers/modeling_reformer.py,893,create attn_mask,not
transformers/src/transformers/modeling_reformer.py,895,multiply by casaul mask if necessary,not
transformers/src/transformers/modeling_reformer.py,930,get correct attn layers,not
transformers/src/transformers/modeling_reformer.py,954,use cached buckets for backprob if buckets not None for LSHSelfAttention,not
transformers/src/transformers/modeling_reformer.py,965,add buckets if necessary,not
transformers/src/transformers/modeling_reformer.py,1033,dropout requires to have the same,not
transformers/src/transformers/modeling_reformer.py,1034,seed for forward and backward pass,not
transformers/src/transformers/modeling_reformer.py,1049,randomize seeds,not
transformers/src/transformers/modeling_reformer.py,1051,GPU,not
transformers/src/transformers/modeling_reformer.py,1056,CPU,not
transformers/src/transformers/modeling_reformer.py,1069,randomize seeds,not
transformers/src/transformers/modeling_reformer.py,1071,GPU,not
transformers/src/transformers/modeling_reformer.py,1076,CPU,not
transformers/src/transformers/modeling_reformer.py,1090,every forward pass we sample a different seed,not
transformers/src/transformers/modeling_reformer.py,1091,for dropout and save for forward fn in backward pass,not
transformers/src/transformers/modeling_reformer.py,1092,to have correct dropout,not
transformers/src/transformers/modeling_reformer.py,1103,Implementation of RevNet (see Fig. 6 in https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0),not
transformers/src/transformers/modeling_reformer.py,1104,Y_1 = X_1 + f(X_2),not
transformers/src/transformers/modeling_reformer.py,1107,free memory,not
transformers/src/transformers/modeling_reformer.py,1110,every forward pass we sample a different seed,not
transformers/src/transformers/modeling_reformer.py,1111,for dropout and save seed for forward fn in backward,not
transformers/src/transformers/modeling_reformer.py,1112,to have correct dropout,not
transformers/src/transformers/modeling_reformer.py,1114,Y_2 = X_2 + g(Y_1),not
transformers/src/transformers/modeling_reformer.py,1134,Implements the backward pass for reversible ResNets.,not
transformers/src/transformers/modeling_reformer.py,1135,A good blog post on how this works can be found here:,not
transformers/src/transformers/modeling_reformer.py,1136,Implementation of RevNet (see Fig. 6 in https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0),not
transformers/src/transformers/modeling_reformer.py,1137,This code is heavily inspired by https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reversible.py,not
transformers/src/transformers/modeling_reformer.py,1142,set seed to have correct dropout,not
transformers/src/transformers/modeling_reformer.py,1144,g(Y_1),not
transformers/src/transformers/modeling_reformer.py,1149,X_2 = Y_2 - g(Y_1),not
transformers/src/transformers/modeling_reformer.py,1159,set seed to have correct dropout,not
transformers/src/transformers/modeling_reformer.py,1161,f(X_2),not
transformers/src/transformers/modeling_reformer.py,1162,use cached buckets for backprob if buckets not None for LSHSelfAttention,not
transformers/src/transformers/modeling_reformer.py,1169,X_1 = Y_1 - f(X_2),not
transformers/src/transformers/modeling_reformer.py,1209,split duplicated tensor,not
transformers/src/transformers/modeling_reformer.py,1231,Add last layer,not
transformers/src/transformers/modeling_reformer.py,1235,attach params to ctx for backward,not
transformers/src/transformers/modeling_reformer.py,1242,Concatenate 2 RevNet outputs,not
transformers/src/transformers/modeling_reformer.py,1249,retrieve params from ctx for backward,not
transformers/src/transformers/modeling_reformer.py,1252,create tuple,not
transformers/src/transformers/modeling_reformer.py,1260,free memory,not
transformers/src/transformers/modeling_reformer.py,1269,pop last buckets from stack,not
transformers/src/transformers/modeling_reformer.py,1273,backprop,not
transformers/src/transformers/modeling_reformer.py,1287,num of return vars has to match num of forward() args,not
transformers/src/transformers/modeling_reformer.py,1288,return gradient for hidden_states arg and None for other args,not
transformers/src/transformers/modeling_reformer.py,1298,"Reformer is using Rev Nets, thus last layer outputs are concatenated and",not
transformers/src/transformers/modeling_reformer.py,1299,Layer Norm is done over 2 * hidden_size,not
transformers/src/transformers/modeling_reformer.py,1311,hidden_states and attention lists to be filled if wished,not
transformers/src/transformers/modeling_reformer.py,1315,concat same tensor for reversible ResNet,not
transformers/src/transformers/modeling_reformer.py,1329,Apply layer norm to concatenated hidden states,not
transformers/src/transformers/modeling_reformer.py,1332,Apply dropout,not
transformers/src/transformers/modeling_reformer.py,1343,"Reformer is using Rev Nets, thus last layer outputs are concatenated and",not
transformers/src/transformers/modeling_reformer.py,1344,Layer Norm is done over 2 * hidden_size,not
transformers/src/transformers/modeling_reformer.py,1350,Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`,not
transformers/src/transformers/modeling_reformer.py,1388,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_reformer.py,1389,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_reformer.py,1531,TODO(PVP): delete when PR to change output_attentions is made,SATD
transformers/src/transformers/modeling_reformer.py,1538,noqa: F841,not
transformers/src/transformers/modeling_reformer.py,1541,noqa: F841,not
transformers/src/transformers/modeling_reformer.py,1550,prepare head mask,not
transformers/src/transformers/modeling_reformer.py,1553,original sequence length for padding,not
transformers/src/transformers/modeling_reformer.py,1556,if needs padding,not
transformers/src/transformers/modeling_reformer.py,1570,pad input,not
transformers/src/transformers/modeling_reformer.py,1594,if padding was applied,not
transformers/src/transformers/modeling_reformer.py,1599,TODO(PVP): Replace by named tuple after namedtuples are introduced in the library.,SATD
transformers/src/transformers/modeling_reformer.py,1627,Extend `attention_mask`,not
transformers/src/transformers/modeling_reformer.py,1645,Extend `input_ids` with padding to match least common multiple chunk_length,not
transformers/src/transformers/modeling_reformer.py,1650,Pad position ids if given,not
transformers/src/transformers/modeling_reformer.py,1656,Extend `input_embeds` with padding to match least common multiple chunk_length,not
transformers/src/transformers/modeling_reformer.py,1677,word embeddings are not tied in Reformer,not
transformers/src/transformers/modeling_reformer.py,1748,Shift so that tokens < n predict n,not
transformers/src/transformers/modeling_reformer.py,1751,Flatten the tokens,not
transformers/src/transformers/modeling_reformer.py,1755,"(lm_loss), lm_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_reformer.py,1758,TODO(PVP): Add smart caching,SATD
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,39,We do this to be able to load python 2 datasets pickles,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,40,See e.g. https://stackoverflow.com/questions/2121874/python-pickling-after-changing-a-modules-directory/2121918#2121918,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,51,Convert a pre-processed corpus (see original TensorFlow repo),not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,54,Save vocabulary and dataset cache as Dictionaries (should be better than pickles for the long-term),not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,67,Convert a pre-trained TensorFlow model,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,72,Initialise PyTorch model,not
transformers/src/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py,81,Save pytorch-model,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,30,Initialise PyTorch model,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,35,Load weights from tf checkpoint,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,38,Save pytorch-model,not
transformers/src/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py,45,Required parameters,not
transformers/src/transformers/modeling_xlnet.py,1,coding=utf-8,not
transformers/src/transformers/modeling_xlnet.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_xlnet.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_xlnet.py,4,,not
transformers/src/transformers/modeling_xlnet.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_xlnet.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_xlnet.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_xlnet.py,8,,not
transformers/src/transformers/modeling_xlnet.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_xlnet.py,10,,not
transformers/src/transformers/modeling_xlnet.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_xlnet.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_xlnet.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_xlnet.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_xlnet.py,15,limitations under the License.,not
transformers/src/transformers/modeling_xlnet.py,51,We will load also the output bias,not
transformers/src/transformers/modeling_xlnet.py,54,We will load also the sequence summary,not
transformers/src/transformers/modeling_xlnet.py,65,Now load the rest of the transformer,not
transformers/src/transformers/modeling_xlnet.py,68,Embeddings and output,not
transformers/src/transformers/modeling_xlnet.py,76,Transformer blocks,not
transformers/src/transformers/modeling_xlnet.py,97,Relative positioning biases,not
transformers/src/transformers/modeling_xlnet.py,136,Load weights from TF model,not
transformers/src/transformers/modeling_xlnet.py,144,Build TF to PyTorch weights loading map,not
transformers/src/transformers/modeling_xlnet.py,153,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,not
transformers/src/transformers/modeling_xlnet.py,154,which are not required for using pretrained model,not
transformers/src/transformers/modeling_xlnet.py,159,Here we will split the TF weights,not
transformers/src/transformers/modeling_xlnet.py,233,"x = x[:, 0:klen, :, :]",not
transformers/src/transformers/modeling_xlnet.py,245,Note: the tensor-slice form was faster in my testing than torch.index_select,not
transformers/src/transformers/modeling_xlnet.py,246,"However, tracing doesn't like the nature of the slice, and if klen changes",not
transformers/src/transformers/modeling_xlnet.py,247,"during the run then it'll fail, whereas index_select will be fine.",not
transformers/src/transformers/modeling_xlnet.py,249,"x = x[:, :, :, :klen]",not
transformers/src/transformers/modeling_xlnet.py,256,content based attention score,not
transformers/src/transformers/modeling_xlnet.py,259,position based attention score,not
transformers/src/transformers/modeling_xlnet.py,263,segment based attention score,not
transformers/src/transformers/modeling_xlnet.py,270,merge attention scores and perform masking,not
transformers/src/transformers/modeling_xlnet.py,273,attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask,not
transformers/src/transformers/modeling_xlnet.py,279,attention probability,not
transformers/src/transformers/modeling_xlnet.py,283,Mask heads if we want to,not
transformers/src/transformers/modeling_xlnet.py,287,attention output,not
transformers/src/transformers/modeling_xlnet.py,297,post-attention projection (back to `d_model`),not
transformers/src/transformers/modeling_xlnet.py,309,Two-stream attention with relative positional encoding.,not
transformers/src/transformers/modeling_xlnet.py,310,content based attention score,not
transformers/src/transformers/modeling_xlnet.py,316,content-based key head,not
transformers/src/transformers/modeling_xlnet.py,319,content-based value head,not
transformers/src/transformers/modeling_xlnet.py,322,position-based key head,not
transformers/src/transformers/modeling_xlnet.py,325,h-stream,not
transformers/src/transformers/modeling_xlnet.py,326,content-stream query head,not
transformers/src/transformers/modeling_xlnet.py,329,core attention ops,not
transformers/src/transformers/modeling_xlnet.py,337,post processing,not
transformers/src/transformers/modeling_xlnet.py,340,g-stream,not
transformers/src/transformers/modeling_xlnet.py,341,query-stream query head,not
transformers/src/transformers/modeling_xlnet.py,344,core attention ops,not
transformers/src/transformers/modeling_xlnet.py,363,post processing,not
transformers/src/transformers/modeling_xlnet.py,370,Multi-head attention with relative positional encoding,not
transformers/src/transformers/modeling_xlnet.py,376,content heads,not
transformers/src/transformers/modeling_xlnet.py,381,positional heads,not
transformers/src/transformers/modeling_xlnet.py,384,core attention ops,not
transformers/src/transformers/modeling_xlnet.py,392,post processing,not
transformers/src/transformers/modeling_xlnet.py,452,Add again attentions if there are there,not
transformers/src/transformers/modeling_xlnet.py,470,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_xlnet.py,471,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_xlnet.py,630,cache hidden states into memory.,not
transformers/src/transformers/modeling_xlnet.py,653,create relative positional encoding.,not
transformers/src/transformers/modeling_xlnet.py,658,"beg, end = klen - 1, -qlen",not
transformers/src/transformers/modeling_xlnet.py,661,"beg, end = klen - 1, -1",not
transformers/src/transformers/modeling_xlnet.py,741,"the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end",not
transformers/src/transformers/modeling_xlnet.py,742,but we want a unified interface in the library with the batch size on the first dimension,not
transformers/src/transformers/modeling_xlnet.py,743,so we move here the first dimension (batch) to the end,not
transformers/src/transformers/modeling_xlnet.py,767,Attention mask,not
transformers/src/transformers/modeling_xlnet.py,768,causal attention mask,not
transformers/src/transformers/modeling_xlnet.py,777,data mask: input mask & perm mask,not
transformers/src/transformers/modeling_xlnet.py,792,all mems can be attended to,not
transformers/src/transformers/modeling_xlnet.py,812,Word embeddings and prepare h & g hidden states,not
transformers/src/transformers/modeling_xlnet.py,820,else:  # We removed the inp_q input which was same as target mapping,not
transformers/src/transformers/modeling_xlnet.py,821,"inp_q_ext = inp_q[:, :, None]",not
transformers/src/transformers/modeling_xlnet.py,822,word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k,not
transformers/src/transformers/modeling_xlnet.py,827,Segment embedding,not
transformers/src/transformers/modeling_xlnet.py,829,Convert `token_type_ids` to one-hot `seg_mat`,not
transformers/src/transformers/modeling_xlnet.py,836,`1` indicates not in the same segment [qlen x klen x bsz],not
transformers/src/transformers/modeling_xlnet.py,842,Positional encoding,not
transformers/src/transformers/modeling_xlnet.py,846,Prepare head mask if needed,not
transformers/src/transformers/modeling_xlnet.py,847,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_xlnet.py,848,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_xlnet.py,849,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),not
transformers/src/transformers/modeling_xlnet.py,850,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_xlnet.py,859,switch to fload if need + fp16 compatibility,not
transformers/src/transformers/modeling_xlnet.py,871,cache new mems,not
transformers/src/transformers/modeling_xlnet.py,891,Add last hidden state,not
transformers/src/transformers/modeling_xlnet.py,897,"Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)",not
transformers/src/transformers/modeling_xlnet.py,911,"when target_mapping is provided, there are 2-tuple of attentions",not
transformers/src/transformers/modeling_xlnet.py,919,"outputs, (new_mems), (hidden_states), (attentions)",not
transformers/src/transformers/modeling_xlnet.py,942,Add dummy token at the end (no attention on this one),not
transformers/src/transformers/modeling_xlnet.py,948,Build permutation mask so that previous tokens don't see last token,not
transformers/src/transformers/modeling_xlnet.py,955,We'll only predict the last token,not
transformers/src/transformers/modeling_xlnet.py,968,if past is defined in model kwargs then use it for faster decoding,not
transformers/src/transformers/modeling_xlnet.py,1067,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_xlnet.py,1070,Flatten the tokens,not
transformers/src/transformers/modeling_xlnet.py,1075,"return (loss), logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_xlnet.py,1169,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_xlnet.py,1173,We are doing regression,not
transformers/src/transformers/modeling_xlnet.py,1181,"return (loss), logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_xlnet.py,1275,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_xlnet.py,1278,Only keep active parts of the loss,not
transformers/src/transformers/modeling_xlnet.py,1290,"return (loss), logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_xlnet.py,1396,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_xlnet.py,1403,"return (loss), logits, (mems), (hidden states), (attentions)",not
transformers/src/transformers/modeling_xlnet.py,1510,"If we are on multi-GPU, split add a dimension",not
transformers/src/transformers/modeling_xlnet.py,1515,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/src/transformers/modeling_xlnet.py,1526,"(loss), start_logits, end_logits, (mems), (hidden_states), (attentions)",not
transformers/src/transformers/modeling_xlnet.py,1643,"Keep mems, hidden states, attentions if there are in it",not
transformers/src/transformers/modeling_xlnet.py,1646,"If we are on multi-GPU, let's remove the dimension added by batch splitting",not
transformers/src/transformers/modeling_xlnet.py,1651,"during training, compute the end logits based on the ground truth of the start position",not
transformers/src/transformers/modeling_xlnet.py,1660,Predict answerability from the representation of CLS and START,not
transformers/src/transformers/modeling_xlnet.py,1665,note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss,not
transformers/src/transformers/modeling_xlnet.py,1671,"during inference, compute the end logits based on beam search",not
transformers/src/transformers/modeling_xlnet.py,1673,"shape (bsz, slen)",not
transformers/src/transformers/modeling_xlnet.py,1677,"shape (bsz, start_n_top)",not
transformers/src/transformers/modeling_xlnet.py,1678,"shape (bsz, start_n_top, hsz)",not
transformers/src/transformers/modeling_xlnet.py,1679,"shape (bsz, start_n_top, hsz)",not
transformers/src/transformers/modeling_xlnet.py,1680,"shape (bsz, slen, start_n_top, hsz)",not
transformers/src/transformers/modeling_xlnet.py,1684,"shape (bsz, slen, start_n_top, hsz)",not
transformers/src/transformers/modeling_xlnet.py,1687,"shape (bsz, slen, start_n_top)",not
transformers/src/transformers/modeling_xlnet.py,1691,"shape (bsz, end_n_top, start_n_top)",not
transformers/src/transformers/modeling_xlnet.py,1697,get the representation of START as weighted sum of hidden states,not
transformers/src/transformers/modeling_xlnet.py,1700,"Shape (batch size,): one single `cls_logits` for each sample",not
transformers/src/transformers/modeling_xlnet.py,1704,"return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits",not
transformers/src/transformers/modeling_xlnet.py,1705,"or (if labels are provided) (total_loss,)",not
transformers/src/transformers/tokenization_xlm.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_xlm.py,2,Copyright 2019 The Open AI Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_xlm.py,3,,not
transformers/src/transformers/tokenization_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_xlm.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_xlm.py,7,,not
transformers/src/transformers/tokenization_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_xlm.py,9,,not
transformers/src/transformers/tokenization_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_xlm.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_xlm.py,518,https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/normalise-romanian.py,not
transformers/src/transformers/tokenization_xlm.py,521,https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/remove-diacritics.py,not
transformers/src/transformers/tokenization_xlm.py,522,s-comma,not
transformers/src/transformers/tokenization_xlm.py,523,t-comma,not
transformers/src/transformers/tokenization_xlm.py,632,cache of sm.MosesPunctNormalizer instance,not
transformers/src/transformers/tokenization_xlm.py,634,cache of sm.MosesTokenizer instance,not
transformers/src/transformers/tokenization_xlm.py,637,"True for current supported model (v1.2.0), False for XLM-17 & 100",not
transformers/src/transformers/tokenization_xlm.py,792,"TODO: make sure we are using `xlm-mlm-enro-1024`, since XLM-100 doesn't have this step",SATD
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,1,coding=utf-8,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,3,,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,7,,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,9,,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,14,limitations under the License.,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,351,Initialise TF model,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,360,Load weights from tf checkpoint,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,365,Load PyTorch checkpoint in tf2 model:,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,369,build the network,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,385,Save pytorch-model,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,469,Required parameters,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,513,if args.pytorch_checkpoint_path is not None:,not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,514,"convert_pt_checkpoint_to_tf(args.model_type.lower(),",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,515,"args.pytorch_checkpoint_path,",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,516,"args.config_file if args.config_file is not None else args.pytorch_checkpoint_path,",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,517,"args.tf_dump_path,",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,518,"compare_with_pt_model=args.compare_with_pt_model,",not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,519,use_cached_models=args.use_cached_models),not
transformers/src/transformers/convert_pytorch_checkpoint_to_tf2.py,520,else:,not
transformers/src/transformers/modeling_tf_utils.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_utils.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_utils.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_utils.py,4,,not
transformers/src/transformers/modeling_tf_utils.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_utils.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_utils.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_utils.py,8,,not
transformers/src/transformers/modeling_tf_utils.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_utils.py,10,,not
transformers/src/transformers/modeling_tf_utils.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_utils.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_utils.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_utils.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_utils.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_utils.py,78,"normal layer construction, call with unchanged args (config is already in there)",not
transformers/src/transformers/modeling_tf_utils.py,81,"Keras deserialization, convert dict to config",not
transformers/src/transformers/modeling_tf_utils.py,147,Save config in model,not
transformers/src/transformers/modeling_tf_utils.py,172,Overwrite for models with output embeddings,not
transformers/src/transformers/modeling_tf_utils.py,188,if new_num_tokens is None:,not
transformers/src/transformers/modeling_tf_utils.py,189,return old_embeddings,not
transformers/src/transformers/modeling_tf_utils.py,191,"old_num_tokens, old_embedding_dim = old_embeddings.weight.size()",not
transformers/src/transformers/modeling_tf_utils.py,192,if old_num_tokens == new_num_tokens:,not
transformers/src/transformers/modeling_tf_utils.py,193,return old_embeddings,not
transformers/src/transformers/modeling_tf_utils.py,195,# Build new embeddings,not
transformers/src/transformers/modeling_tf_utils.py,196,"new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)",not
transformers/src/transformers/modeling_tf_utils.py,197,new_embeddings.to(old_embeddings.weight.device),not
transformers/src/transformers/modeling_tf_utils.py,199,# initialize all new embeddings (in particular added tokens),not
transformers/src/transformers/modeling_tf_utils.py,200,self._init_weights(new_embeddings),not
transformers/src/transformers/modeling_tf_utils.py,202,# Copy token embeddings from the previous weights,not
transformers/src/transformers/modeling_tf_utils.py,203,"num_tokens_to_copy = min(old_num_tokens, new_num_tokens)",not
transformers/src/transformers/modeling_tf_utils.py,204,"new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]",not
transformers/src/transformers/modeling_tf_utils.py,206,return new_embeddings,not
transformers/src/transformers/modeling_tf_utils.py,240,Save configuration file,not
transformers/src/transformers/modeling_tf_utils.py,243,"If we save using the predefined names, we can load using `from_pretrained`",not
transformers/src/transformers/modeling_tf_utils.py,324,Load config if we don't provide a configuration,not
transformers/src/transformers/modeling_tf_utils.py,339,Load model,not
transformers/src/transformers/modeling_tf_utils.py,345,Load from a TF 2.0 checkpoint,not
transformers/src/transformers/modeling_tf_utils.py,348,Load from a PyTorch checkpoint,not
transformers/src/transformers/modeling_tf_utils.py,367,"redirect to the cache, if necessary",not
transformers/src/transformers/modeling_tf_utils.py,397,Instantiate model.,not
transformers/src/transformers/modeling_tf_utils.py,401,Load from a PyTorch checkpoint,not
transformers/src/transformers/modeling_tf_utils.py,404,build the network with dummy inputs,not
transformers/src/transformers/modeling_tf_utils.py,407,'by_name' allow us to do transfer learning by skipping/adding layers,not
transformers/src/transformers/modeling_tf_utils.py,408,see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357,not
transformers/src/transformers/modeling_tf_utils.py,417,Make sure restore ops are run,not
transformers/src/transformers/modeling_tf_utils.py,419,Check if the models are the same to output loading informations,not
transformers/src/transformers/modeling_tf_utils.py,601,We cannot generate if the model does not have a LM head,not
transformers/src/transformers/modeling_tf_utils.py,634,overriden by the input batch_size,not
transformers/src/transformers/modeling_tf_utils.py,674,not allow to duplicate outputs when greedy decoding,not
transformers/src/transformers/modeling_tf_utils.py,677,no_beam_search greedy generation conditions,not
transformers/src/transformers/modeling_tf_utils.py,683,beam_search greedy generation conditions,not
transformers/src/transformers/modeling_tf_utils.py,688,create attention mask if necessary,not
transformers/src/transformers/modeling_tf_utils.py,689,TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140,SATD
transformers/src/transformers/modeling_tf_utils.py,701,current position and vocab size,not
transformers/src/transformers/modeling_tf_utils.py,705,set effective batch size and effective batch multiplier according to do_sample,not
transformers/src/transformers/modeling_tf_utils.py,723,get encoder and store encoder outputs,not
transformers/src/transformers/modeling_tf_utils.py,728,Expand input ids if num_beams > 1 or num_return_sequences > 1,not
transformers/src/transformers/modeling_tf_utils.py,739,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",not
transformers/src/transformers/modeling_tf_utils.py,742,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",not
transformers/src/transformers/modeling_tf_utils.py,746,create empty decoder_input_ids,not
transformers/src/transformers/modeling_tf_utils.py,754,expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1),not
transformers/src/transformers/modeling_tf_utils.py,759,expand encoder_outputs,not
transformers/src/transformers/modeling_tf_utils.py,846,length of generated sentences / unfinished sentences,not
transformers/src/transformers/modeling_tf_utils.py,850,"defined for encoder-decoder models, None for decoder-only models",not
transformers/src/transformers/modeling_tf_utils.py,859,"if model has past, then set the past variable to speed up decoding",not
transformers/src/transformers/modeling_tf_utils.py,863,repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858),not
transformers/src/transformers/modeling_tf_utils.py,871,calculate a list of banned tokens to prevent repetitively generating the same ngrams,not
transformers/src/transformers/modeling_tf_utils.py,872,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,not
transformers/src/transformers/modeling_tf_utils.py,874,create banned_tokens boolean mask,not
transformers/src/transformers/modeling_tf_utils.py,886,calculate a list of banned tokens according to bad words,not
transformers/src/transformers/modeling_tf_utils.py,899,set eos token prob to zero if min_length is not reached,not
transformers/src/transformers/modeling_tf_utils.py,901,create eos_token_id boolean mask,not
transformers/src/transformers/modeling_tf_utils.py,912,Temperature (higher temperature => more likely to sample low probability tokens),not
transformers/src/transformers/modeling_tf_utils.py,915,Top-p/top-k filtering,not
transformers/src/transformers/modeling_tf_utils.py,917,Sample,not
transformers/src/transformers/modeling_tf_utils.py,922,Greedy decoding,not
transformers/src/transformers/modeling_tf_utils.py,925,update generations and finished sentences,not
transformers/src/transformers/modeling_tf_utils.py,927,pad finished sentences if eos_token_id exist,not
transformers/src/transformers/modeling_tf_utils.py,936,"if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length",not
transformers/src/transformers/modeling_tf_utils.py,945,unfinished_sents is set to zero if eos in sentence,not
transformers/src/transformers/modeling_tf_utils.py,948,"stop when there is a </s> in each sentence, or if we exceed the maximul length",not
transformers/src/transformers/modeling_tf_utils.py,952,extend attention_mask for new generated input if only decoder,not
transformers/src/transformers/modeling_tf_utils.py,960,"if there are different sentences lengths in the batch, some batches have to be padded",not
transformers/src/transformers/modeling_tf_utils.py,965,finished sents are filled with pad_token,not
transformers/src/transformers/modeling_tf_utils.py,968,create length masks for tf.where operation,not
transformers/src/transformers/modeling_tf_utils.py,1012,generated hypotheses,not
transformers/src/transformers/modeling_tf_utils.py,1018,for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times,not
transformers/src/transformers/modeling_tf_utils.py,1028,cache compute states,not
transformers/src/transformers/modeling_tf_utils.py,1031,done sentences,not
transformers/src/transformers/modeling_tf_utils.py,1038,"(batch_size * num_beams, cur_len, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1039,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1041,"if model has past, then set the past variable to speed up decoding",not
transformers/src/transformers/modeling_tf_utils.py,1045,repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858),not
transformers/src/transformers/modeling_tf_utils.py,1052,Temperature (higher temperature => more likely to sample low probability tokens),not
transformers/src/transformers/modeling_tf_utils.py,1056,calculate log softmax score,not
transformers/src/transformers/modeling_tf_utils.py,1057,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1059,set eos token prob to zero if min_length is not reached,not
transformers/src/transformers/modeling_tf_utils.py,1061,create eos_token_id boolean mask,not
transformers/src/transformers/modeling_tf_utils.py,1072,calculate a list of banned tokens to prevent repetitively generating the same ngrams,not
transformers/src/transformers/modeling_tf_utils.py,1073,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,not
transformers/src/transformers/modeling_tf_utils.py,1078,create banned_tokens boolean mask,not
transformers/src/transformers/modeling_tf_utils.py,1090,calculate a list of banned tokens according to bad words,not
transformers/src/transformers/modeling_tf_utils.py,1108,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1110,Top-p/top-k filtering,not
transformers/src/transformers/modeling_tf_utils.py,1113,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1114,Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search),not
transformers/src/transformers/modeling_tf_utils.py,1119,"(batch_size, 2 * num_beams)",not
transformers/src/transformers/modeling_tf_utils.py,1120,Compute next scores,not
transformers/src/transformers/modeling_tf_utils.py,1121,"(batch_size, 2 * num_beams)",not
transformers/src/transformers/modeling_tf_utils.py,1123,sort the sampled vector to make sure that the first num_beams samples are the best,not
transformers/src/transformers/modeling_tf_utils.py,1125,"(batch_size, num_beams * 2)",not
transformers/src/transformers/modeling_tf_utils.py,1126,"(batch_size, num_beams * 2)",not
transformers/src/transformers/modeling_tf_utils.py,1128,Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product),not
transformers/src/transformers/modeling_tf_utils.py,1131,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1133,re-organize to group the beam together (we are keeping top hypothesis accross beams),not
transformers/src/transformers/modeling_tf_utils.py,1136,"(batch_size, num_beams * vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1142,next batch beam content,not
transformers/src/transformers/modeling_tf_utils.py,1145,for each sentence,not
transformers/src/transformers/modeling_tf_utils.py,1148,if we are done with this sentence,not
transformers/src/transformers/modeling_tf_utils.py,1156,pad the batch,not
transformers/src/transformers/modeling_tf_utils.py,1159,next sentence beam content,not
transformers/src/transformers/modeling_tf_utils.py,1162,next tokens for this sentence,not
transformers/src/transformers/modeling_tf_utils.py,1166,get beam and token IDs,not
transformers/src/transformers/modeling_tf_utils.py,1171,add to generated hypotheses if end of sentence or last iteration,not
transformers/src/transformers/modeling_tf_utils.py,1173,"if beam_token does not belong to top num_beams tokens, it should not be added",not
transformers/src/transformers/modeling_tf_utils.py,1181,add next predicted token if it is not eos_token,not
transformers/src/transformers/modeling_tf_utils.py,1184,the beam for next step is full,not
transformers/src/transformers/modeling_tf_utils.py,1188,Check if were done so that we can save a pad step if all(done),not
transformers/src/transformers/modeling_tf_utils.py,1193,update next beam content,not
transformers/src/transformers/modeling_tf_utils.py,1198,stop when we are done with each sentence,not
transformers/src/transformers/modeling_tf_utils.py,1202,sanity check / prepare next batch,not
transformers/src/transformers/modeling_tf_utils.py,1208,re-order batch,not
transformers/src/transformers/modeling_tf_utils.py,1211,re-order internal states,not
transformers/src/transformers/modeling_tf_utils.py,1215,extend attention_mask for new generated input if only decoder,not
transformers/src/transformers/modeling_tf_utils.py,1221,update current length,not
transformers/src/transformers/modeling_tf_utils.py,1224,finalize all open beam hypotheses and end to generated hypotheses,not
transformers/src/transformers/modeling_tf_utils.py,1226,Add all open beam hypothesis to generated_hyps,not
transformers/src/transformers/modeling_tf_utils.py,1229,test that beam scores match previously calculated scores if not eos and batch_idx not done,not
transformers/src/transformers/modeling_tf_utils.py,1239,need to add best num_beams hypotheses to generated hyps,not
transformers/src/transformers/modeling_tf_utils.py,1246,depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch,not
transformers/src/transformers/modeling_tf_utils.py,1250,select the best hypotheses,not
transformers/src/transformers/modeling_tf_utils.py,1254,retrieve best hypotheses,not
transformers/src/transformers/modeling_tf_utils.py,1267,shorter batches are filled with pad_token,not
transformers/src/transformers/modeling_tf_utils.py,1273,fill with hypothesis and eos_token_id if necessary,not
transformers/src/transformers/modeling_tf_utils.py,1276,if sent_length is max_len do not pad,not
transformers/src/transformers/modeling_tf_utils.py,1280,else pad to sent_max_len,not
transformers/src/transformers/modeling_tf_utils.py,1285,finish sentence with EOS token,not
transformers/src/transformers/modeling_tf_utils.py,1292,add to list,not
transformers/src/transformers/modeling_tf_utils.py,1297,none of the hypotheses have an eos_token,not
transformers/src/transformers/modeling_tf_utils.py,1309,create logit penalties for already seen input_ids,not
transformers/src/transformers/modeling_tf_utils.py,1315,if previous logit score is < 0 then multiply repetition penalty else divide,not
transformers/src/transformers/modeling_tf_utils.py,1323,"Copied from fairseq for no_repeat_ngram in beam_search""""""",not
transformers/src/transformers/modeling_tf_utils.py,1325,return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet,not
transformers/src/transformers/modeling_tf_utils.py,1336,"Before decoding the next token, prevent decoding of ngrams that have already appeared",not
transformers/src/transformers/modeling_tf_utils.py,1350,if bad word tokens is just one token always ban it,not
transformers/src/transformers/modeling_tf_utils.py,1353,if bad word tokens are longer then prev input_ids they can't be equal,not
transformers/src/transformers/modeling_tf_utils.py,1357,if tokens match,not
transformers/src/transformers/modeling_tf_utils.py,1371,if tokens do not match continue,not
transformers/src/transformers/modeling_tf_utils.py,1394,Safety check,not
transformers/src/transformers/modeling_tf_utils.py,1395,Remove all tokens with a probability less than the last token of the top-k,not
transformers/src/transformers/modeling_tf_utils.py,1403,"expects logits to be of dim (batch_size, vocab_size)",not
transformers/src/transformers/modeling_tf_utils.py,1407,Remove tokens with cumulative probability above the threshold (token with 0 are kept),not
transformers/src/transformers/modeling_tf_utils.py,1411,Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below),not
transformers/src/transformers/modeling_tf_utils.py,1420,Shift the indices to the right to keep also the first token above the threshold,not
transformers/src/transformers/modeling_tf_utils.py,1425,scatter sorted tensors to original indexing,not
transformers/src/transformers/modeling_tf_utils.py,1433,broadcast batch dim to shape,not
transformers/src/transformers/modeling_tf_utils.py,1435,transform batch_indices to pair_indices,not
transformers/src/transformers/modeling_tf_utils.py,1437,scatter values to pair indices,not
transformers/src/transformers/modeling_tf_utils.py,1442,create value_tensor since tensor value assignment is not possible in TF,not
transformers/src/transformers/modeling_tf_utils.py,1452,ignoring bos_token,not
transformers/src/transformers/modeling_tf_utils.py,1606,We should use a standard multi-head attention module with absolute positional embedding for that.,not
transformers/src/transformers/modeling_tf_utils.py,1607,Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276,not
transformers/src/transformers/modeling_tf_utils.py,1608,We can probably just use the multi-head attention module of PyTorch >=1.1.0,not
transformers/src/transformers/modeling_tf_utils.py,1658,"e.g. [batch, num choices, seq length, hidden dims]",not
transformers/src/transformers/modeling_tf_utils.py,1662,"A tensor full of shape [batch] or [batch, num choices] full of sequence length",not
transformers/src/transformers/modeling_tf_utils.py,1666,else:,not
transformers/src/transformers/modeling_tf_utils.py,1667,"cls_index = cls_index[..., tf.newaxis]",not
transformers/src/transformers/modeling_tf_utils.py,1668,"cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))",not
transformers/src/transformers/modeling_tf_utils.py,1669,"shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states",not
transformers/src/transformers/modeling_tf_utils.py,1673,"shape of output: (batch, num choices, hidden_size)",not
transformers/src/transformers/configuration_auto.py,1,coding=utf-8,not
transformers/src/transformers/configuration_auto.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_auto.py,3,,not
transformers/src/transformers/configuration_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_auto.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_auto.py,7,,not
transformers/src/transformers/configuration_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_auto.py,9,,not
transformers/src/transformers/configuration_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_auto.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_auto.py,14,limitations under the License.,not
transformers/src/transformers/configuration_auto.py,201,Fallback: use pattern matching on the string.,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The T5 authors and HuggingFace Inc. team.,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,30,Initialise PyTorch model,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,35,Load weights from tf checkpoint,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,38,Save pytorch-model,not
transformers/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py,45,Required parameters,not
transformers/src/transformers/modeling_utils.py,1,coding=utf-8,not
transformers/src/transformers/modeling_utils.py,2,"Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.",not
transformers/src/transformers/modeling_utils.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_utils.py,4,,not
transformers/src/transformers/modeling_utils.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_utils.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_utils.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_utils.py,8,,not
transformers/src/transformers/modeling_utils.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_utils.py,10,,not
transformers/src/transformers/modeling_utils.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_utils.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_utils.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_utils.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_utils.py,15,limitations under the License.,not
transformers/src/transformers/modeling_utils.py,46,Older PyTorch compatibility,not
transformers/src/transformers/modeling_utils.py,125,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposition",not
transformers/src/transformers/modeling_utils.py,126,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow,not
transformers/src/transformers/modeling_utils.py,127,/transformer/transformer_layers.py#L270,not
transformers/src/transformers/modeling_utils.py,128,encoder_extended_attention_mask = (encoder_extended_attention_mask ==,not
transformers/src/transformers/modeling_utils.py,129,"encoder_extended_attention_mask.transpose(-1, -2))",not
transformers/src/transformers/modeling_utils.py,130,fp16 compatibility,not
transformers/src/transformers/modeling_utils.py,145,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_utils.py,146,ourselves in which case we just need to make it broadcastable to all heads.,not
transformers/src/transformers/modeling_utils.py,150,"Provided a padding mask of dimensions [batch_size, seq_length]",not
transformers/src/transformers/modeling_utils.py,151,"- if the model is a decoder, apply a causal mask in addition to the padding mask",not
transformers/src/transformers/modeling_utils.py,152,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]",not
transformers/src/transformers/modeling_utils.py,157,causal and attention masks must have same type with pytorch version < 1.3,not
transformers/src/transformers/modeling_utils.py,169,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_utils.py,170,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_utils.py,171,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_utils.py,172,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_utils.py,173,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_utils.py,174,fp16 compatibility,not
transformers/src/transformers/modeling_utils.py,205,We can specify head_mask for each layer,not
transformers/src/transformers/modeling_utils.py,207,switch to fload if need + fp16 compatibility,not
transformers/src/transformers/modeling_utils.py,251,Save config in model,not
transformers/src/transformers/modeling_utils.py,294,Overwrite for models with output embeddings,not
transformers/src/transformers/modeling_utils.py,337,get the base model if needed,not
transformers/src/transformers/modeling_utils.py,342,Update base model and current model config,not
transformers/src/transformers/modeling_utils.py,346,Tie weights again if needed,not
transformers/src/transformers/modeling_utils.py,378,Build new embeddings,not
transformers/src/transformers/modeling_utils.py,382,initialize all new embeddings (in particular added tokens),not
transformers/src/transformers/modeling_utils.py,385,Copy token embeddings from the previous weights,not
transformers/src/transformers/modeling_utils.py,393,Initialize weights,not
transformers/src/transformers/modeling_utils.py,396,Prune heads if needed,not
transformers/src/transformers/modeling_utils.py,400,Tie weights if needed,not
transformers/src/transformers/modeling_utils.py,411,save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads,not
transformers/src/transformers/modeling_utils.py,414,Unfortunately we have to store it as list for JSON,not
transformers/src/transformers/modeling_utils.py,429,Only save the model itself if we are using distributed training,not
transformers/src/transformers/modeling_utils.py,432,Attach architecture to the config,not
transformers/src/transformers/modeling_utils.py,435,"If we save using the predefined names, we can load using `from_pretrained`",not
transformers/src/transformers/modeling_utils.py,442,Save configuration file,not
transformers/src/transformers/modeling_utils.py,444,xm.save takes care of saving only from master,not
transformers/src/transformers/modeling_utils.py,534,Load config if we don't provide a configuration,not
transformers/src/transformers/modeling_utils.py,551,Load model,not
transformers/src/transformers/modeling_utils.py,557,Load from a TF 1.0 checkpoint,not
transformers/src/transformers/modeling_utils.py,560,Load from a TF 2.0 checkpoint,not
transformers/src/transformers/modeling_utils.py,563,Load from a PyTorch checkpoint,not
transformers/src/transformers/modeling_utils.py,588,"redirect to the cache, if necessary",not
transformers/src/transformers/modeling_utils.py,621,Instantiate model.,not
transformers/src/transformers/modeling_utils.py,639,Load from a TensorFlow 1.X checkpoint - provided by original authors,not
transformers/src/transformers/modeling_utils.py,640,Remove the '.index',not
transformers/src/transformers/modeling_utils.py,642,Load from our TensorFlow 2.0 checkpoints,not
transformers/src/transformers/modeling_utils.py,654,Convert old format to new format if needed from a PyTorch state_dict,not
transformers/src/transformers/modeling_utils.py,669,copy state_dict so _load_from_state_dict can modify it,not
transformers/src/transformers/modeling_utils.py,675,PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants,not
transformers/src/transformers/modeling_utils.py,676,so we need to apply the function recursively.,not
transformers/src/transformers/modeling_utils.py,686,Make sure we are able to load base models as well as derived models (with heads),not
transformers/src/transformers/modeling_utils.py,723,make sure token embedding weights are still tied if needed,not
transformers/src/transformers/modeling_utils.py,725,Set model in evaluation mode to deactivate DropOut modules by default,not
transformers/src/transformers/modeling_utils.py,762,if score < 0 then repetition penalty has to multiplied to reduce the previous token probability,not
transformers/src/transformers/modeling_utils.py,915,We cannot generate if the model does not have a LM head,not
transformers/src/transformers/modeling_utils.py,948,overriden by the input batch_size,not
transformers/src/transformers/modeling_utils.py,993,not allow to duplicate outputs when greedy decoding,not
transformers/src/transformers/modeling_utils.py,996,no_beam_search greedy generation conditions,not
transformers/src/transformers/modeling_utils.py,1002,beam_search greedy generation conditions,not
transformers/src/transformers/modeling_utils.py,1007,create attention mask if necessary,not
transformers/src/transformers/modeling_utils.py,1008,TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140,SATD
transformers/src/transformers/modeling_utils.py,1014,set pad_token_id to eos_token_id if not set. Important that this is done after,not
transformers/src/transformers/modeling_utils.py,1015,attention_mask is created,not
transformers/src/transformers/modeling_utils.py,1022,current position and vocab size,not
transformers/src/transformers/modeling_utils.py,1032,set effective batch size and effective batch multiplier according to do_sample,not
transformers/src/transformers/modeling_utils.py,1050,get encoder and store encoder outputs,not
transformers/src/transformers/modeling_utils.py,1055,Expand input ids if num_beams > 1 or num_return_sequences > 1,not
transformers/src/transformers/modeling_utils.py,1065,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",not
transformers/src/transformers/modeling_utils.py,1068,"shape: (batch_size * num_return_sequences * num_beams, cur_len)",not
transformers/src/transformers/modeling_utils.py,1071,create empty decoder_input_ids,not
transformers/src/transformers/modeling_utils.py,1084,expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1),not
transformers/src/transformers/modeling_utils.py,1092,expand encoder_outputs,not
transformers/src/transformers/modeling_utils.py,1179,length of generated sentences / unfinished sentences,not
transformers/src/transformers/modeling_utils.py,1183,"defined for encoder-decoder models, None for decoder-only models",not
transformers/src/transformers/modeling_utils.py,1193,"if model has past, then set the past variable to speed up decoding",not
transformers/src/transformers/modeling_utils.py,1197,repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858),not
transformers/src/transformers/modeling_utils.py,1202,calculate a list of banned tokens to prevent repetitively generating the same ngrams,not
transformers/src/transformers/modeling_utils.py,1203,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,not
transformers/src/transformers/modeling_utils.py,1209,calculate a list of banned tokens according to bad words,not
transformers/src/transformers/modeling_utils.py,1215,set eos token prob to zero if min_length is not reached,not
transformers/src/transformers/modeling_utils.py,1220,Temperature (higher temperature => more likely to sample low probability tokens),not
transformers/src/transformers/modeling_utils.py,1223,Top-p/top-k filtering,not
transformers/src/transformers/modeling_utils.py,1225,Sample,not
transformers/src/transformers/modeling_utils.py,1229,Greedy decoding,not
transformers/src/transformers/modeling_utils.py,1232,update generations and finished sentences,not
transformers/src/transformers/modeling_utils.py,1234,pad finished sentences if eos_token_id exist,not
transformers/src/transformers/modeling_utils.py,1243,"if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length",not
transformers/src/transformers/modeling_utils.py,1246,unfinished_sents is set to zero if eos in sentence,not
transformers/src/transformers/modeling_utils.py,1249,"stop when there is a </s> in each sentence, or if we exceed the maximul length",not
transformers/src/transformers/modeling_utils.py,1253,extend attention_mask for new generated input if only decoder,not
transformers/src/transformers/modeling_utils.py,1261,"if there are different sentences lengths in the batch, some batches have to be padded",not
transformers/src/transformers/modeling_utils.py,1264,finished sents are filled with pad_token,not
transformers/src/transformers/modeling_utils.py,1305,generated hypotheses,not
transformers/src/transformers/modeling_utils.py,1311,scores for each sentence in the beam,not
transformers/src/transformers/modeling_utils.py,1314,for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times,not
transformers/src/transformers/modeling_utils.py,1317,"shape (batch_size * num_beams,)",not
transformers/src/transformers/modeling_utils.py,1319,cache compute states,not
transformers/src/transformers/modeling_utils.py,1320,"defined for encoder-decoder models, None for decoder-only models",not
transformers/src/transformers/modeling_utils.py,1322,done sentences,not
transformers/src/transformers/modeling_utils.py,1329,"(batch_size * num_beams, cur_len, vocab_size)",not
transformers/src/transformers/modeling_utils.py,1330,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_utils.py,1332,"if model has past, then set the past variable to speed up decoding",not
transformers/src/transformers/modeling_utils.py,1336,repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858),not
transformers/src/transformers/modeling_utils.py,1345,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_utils.py,1347,TODO (PVP) still a bit hacky here - there might be a better solutino,SATD
transformers/src/transformers/modeling_utils.py,1350,set eos token prob to zero if min_length is not reached,not
transformers/src/transformers/modeling_utils.py,1355,calculate a list of banned tokens to prevent repetitively generating the same ngrams,not
transformers/src/transformers/modeling_utils.py,1357,from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345,not
transformers/src/transformers/modeling_utils.py,1365,calculate a list of banned tokens according to bad words,not
transformers/src/transformers/modeling_utils.py,1376,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_utils.py,1377,Top-p/top-k filtering,not
transformers/src/transformers/modeling_utils.py,1380,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_utils.py,1381,re-organize to group the beam together to sample from all beam_idxs,not
transformers/src/transformers/modeling_utils.py,1384,"(batch_size, num_beams * vocab_size)",not
transformers/src/transformers/modeling_utils.py,1386,Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search),not
transformers/src/transformers/modeling_utils.py,1388,"(batch_size, num_beams * 2)",not
transformers/src/transformers/modeling_utils.py,1389,Compute next scores,not
transformers/src/transformers/modeling_utils.py,1390,"(batch_size, num_beams * 2)",not
transformers/src/transformers/modeling_utils.py,1391,sort the sampled vector to make sure that the first num_beams samples are the best,not
transformers/src/transformers/modeling_utils.py,1393,"(batch_size, num_beams * 2)",not
transformers/src/transformers/modeling_utils.py,1396,"(batch_size * num_beams, vocab_size)",not
transformers/src/transformers/modeling_utils.py,1398,re-organize to group the beam together (we are keeping top hypothesis accross beams),not
transformers/src/transformers/modeling_utils.py,1401,"(batch_size, num_beams * vocab_size)",not
transformers/src/transformers/modeling_utils.py,1407,next batch beam content,not
transformers/src/transformers/modeling_utils.py,1410,for each sentence,not
transformers/src/transformers/modeling_utils.py,1413,if we are done with this sentence,not
transformers/src/transformers/modeling_utils.py,1421,pad the batch,not
transformers/src/transformers/modeling_utils.py,1424,next sentence beam content,not
transformers/src/transformers/modeling_utils.py,1427,next tokens for this sentence,not
transformers/src/transformers/modeling_utils.py,1431,get beam and token IDs,not
transformers/src/transformers/modeling_utils.py,1436,add to generated hypotheses if end of sentence or last iteration,not
transformers/src/transformers/modeling_utils.py,1438,"if beam_token does not belong to top num_beams tokens, it should not be added",not
transformers/src/transformers/modeling_utils.py,1446,add next predicted token if it is not eos_token,not
transformers/src/transformers/modeling_utils.py,1449,the beam for next step is full,not
transformers/src/transformers/modeling_utils.py,1453,Check if were done so that we can save a pad step if all(done),not
transformers/src/transformers/modeling_utils.py,1458,update next beam content,not
transformers/src/transformers/modeling_utils.py,1463,stop when we are done with each sentence,not
transformers/src/transformers/modeling_utils.py,1467,sanity check / prepare next batch,not
transformers/src/transformers/modeling_utils.py,1473,re-order batch,not
transformers/src/transformers/modeling_utils.py,1476,re-order internal states,not
transformers/src/transformers/modeling_utils.py,1480,extend attention_mask for new generated input if only decoder,not
transformers/src/transformers/modeling_utils.py,1486,update current length,not
transformers/src/transformers/modeling_utils.py,1489,finalize all open beam hypotheses and end to generated hypotheses,not
transformers/src/transformers/modeling_utils.py,1494,test that beam scores match previously calculated scores if not eos and batch_idx not done,not
transformers/src/transformers/modeling_utils.py,1504,need to add best num_beams hypotheses to generated hyps,not
transformers/src/transformers/modeling_utils.py,1511,depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch,not
transformers/src/transformers/modeling_utils.py,1515,select the best hypotheses,not
transformers/src/transformers/modeling_utils.py,1519,retrieve best hypotheses,not
transformers/src/transformers/modeling_utils.py,1528,shorter batches are filled with pad_token,not
transformers/src/transformers/modeling_utils.py,1534,fill with hypothesis and eos_token_id if necessary,not
transformers/src/transformers/modeling_utils.py,1540,none of the hypotheses have an eos_token,not
transformers/src/transformers/modeling_utils.py,1554,return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet,not
transformers/src/transformers/modeling_utils.py,1565,"Before decoding the next token, prevent decoding of ngrams that have already appeared",not
transformers/src/transformers/modeling_utils.py,1579,if bad word tokens is just one token always ban it,not
transformers/src/transformers/modeling_utils.py,1582,if bad word tokens are longer then prev input_ids they can't be equal,not
transformers/src/transformers/modeling_utils.py,1586,if tokens match,not
transformers/src/transformers/modeling_utils.py,1600,if tokens do not match continue,not
transformers/src/transformers/modeling_utils.py,1621,Safety check,not
transformers/src/transformers/modeling_utils.py,1622,Remove all tokens with a probability less than the last token of the top-k,not
transformers/src/transformers/modeling_utils.py,1630,Remove tokens with cumulative probability above the threshold (token with 0 are kept),not
transformers/src/transformers/modeling_utils.py,1633,Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below),not
transformers/src/transformers/modeling_utils.py,1635,Shift the indices to the right to keep also the first token above the threshold,not
transformers/src/transformers/modeling_utils.py,1639,scatter sorted tensors to original indexing,not
transformers/src/transformers/modeling_utils.py,1650,ignoring bos_token,not
transformers/src/transformers/modeling_utils.py,1767,"shape (bsz, 1, hsz)",not
transformers/src/transformers/modeling_utils.py,1768,"shape (bsz, 1, hsz)",not
transformers/src/transformers/modeling_utils.py,1769,"shape (bsz, slen, hsz)",not
transformers/src/transformers/modeling_utils.py,1816,"shape (bsz, 1, hsz)",not
transformers/src/transformers/modeling_utils.py,1817,"shape (bsz, hsz)",not
transformers/src/transformers/modeling_utils.py,1820,"shape (bsz, 1, hsz)",not
transformers/src/transformers/modeling_utils.py,1821,"shape (bsz, hsz)",not
transformers/src/transformers/modeling_utils.py,1823,"shape (bsz, hsz)",not
transformers/src/transformers/modeling_utils.py,1890,"If we are on multi-GPU, let's remove the dimension added by batch splitting",not
transformers/src/transformers/modeling_utils.py,1895,"during training, compute the end logits based on the ground truth of the start position",not
transformers/src/transformers/modeling_utils.py,1904,Predict answerability from the representation of CLS and START,not
transformers/src/transformers/modeling_utils.py,1909,note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss,not
transformers/src/transformers/modeling_utils.py,1915,"during inference, compute the end logits based on beam search",not
transformers/src/transformers/modeling_utils.py,1917,"shape (bsz, slen)",not
transformers/src/transformers/modeling_utils.py,1921,"shape (bsz, start_n_top)",not
transformers/src/transformers/modeling_utils.py,1922,"shape (bsz, start_n_top, hsz)",not
transformers/src/transformers/modeling_utils.py,1923,"shape (bsz, start_n_top, hsz)",not
transformers/src/transformers/modeling_utils.py,1924,"shape (bsz, slen, start_n_top, hsz)",not
transformers/src/transformers/modeling_utils.py,1928,"shape (bsz, slen, start_n_top, hsz)",not
transformers/src/transformers/modeling_utils.py,1931,"shape (bsz, slen, start_n_top)",not
transformers/src/transformers/modeling_utils.py,1935,"shape (bsz, end_n_top, start_n_top)",not
transformers/src/transformers/modeling_utils.py,1944,"return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits",not
transformers/src/transformers/modeling_utils.py,1945,"or (if labels are provided) (total_loss,)",not
transformers/src/transformers/modeling_utils.py,1970,We should use a standard multi-head attention module with absolute positional embedding for that.,not
transformers/src/transformers/modeling_utils.py,1971,Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276,not
transformers/src/transformers/modeling_utils.py,1972,We can probably just use the multi-head attention module of PyTorch >=1.1.0,not
transformers/src/transformers/modeling_utils.py,2013,"shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states",not
transformers/src/transformers/modeling_utils.py,2014,"shape (bsz, XX, hidden_size)",not
transformers/src/transformers/modeling_utils.py,2034,The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.,not
transformers/src/transformers/modeling_utils.py,2138,inspect.signature exist since python 3.5 and is a python method -> no problem with backward compability,not
transformers/src/transformers/modeling_utils.py,2155,chunk input tensor into tuples,not
transformers/src/transformers/modeling_utils.py,2157,apply forward fn to every tuple,not
transformers/src/transformers/modeling_utils.py,2159,concatenate output at same dimension,not
transformers/src/transformers/tokenization_reformer.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_reformer.py,2,Copyright 2020 The Trax Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_reformer.py,3,,not
transformers/src/transformers/tokenization_reformer.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_reformer.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_reformer.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_reformer.py,7,,not
transformers/src/transformers/tokenization_reformer.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_reformer.py,9,,not
transformers/src/transformers/tokenization_reformer.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_reformer.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_reformer.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_reformer.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_reformer.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_reformer.py,30,,not
transformers/src/transformers/tokenization_reformer.py,31,Mapping from the keyword arguments names of Tokenizer `__init__`,not
transformers/src/transformers/tokenization_reformer.py,32,to file names for serializing Tokenizer instances,not
transformers/src/transformers/tokenization_reformer.py,33,,not
transformers/src/transformers/tokenization_reformer.py,36,,not
transformers/src/transformers/tokenization_reformer.py,37,Mapping from the keyword arguments names of Tokenizer `__init__`,not
transformers/src/transformers/tokenization_reformer.py,38,to pretrained vocabulary URL for all the model shortcut names.,not
transformers/src/transformers/tokenization_reformer.py,39,,not
transformers/src/transformers/tokenization_reformer.py,46,,not
transformers/src/transformers/tokenization_reformer.py,47,Mapping from model shortcut names to max length of inputs,not
transformers/src/transformers/tokenization_reformer.py,48,,not
transformers/src/transformers/modeling_distilbert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_distilbert.py,2,"Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.",not
transformers/src/transformers/modeling_distilbert.py,3,,not
transformers/src/transformers/modeling_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_distilbert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_distilbert.py,7,,not
transformers/src/transformers/modeling_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_distilbert.py,9,,not
transformers/src/transformers/modeling_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_distilbert.py,14,limitations under the License.,not
transformers/src/transformers/modeling_distilbert.py,50,UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE,not
transformers/src/transformers/modeling_distilbert.py,87,(max_seq_length),not
transformers/src/transformers/modeling_distilbert.py,88,"(bs, max_seq_length)",not
transformers/src/transformers/modeling_distilbert.py,90,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,91,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,93,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,94,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,95,"(bs, max_seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,128,Prune linear layers,not
transformers/src/transformers/modeling_distilbert.py,133,Update hyper params,not
transformers/src/transformers/modeling_distilbert.py,156,"assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)",not
transformers/src/transformers/modeling_distilbert.py,157,assert key.size() == value.size(),not
transformers/src/transformers/modeling_distilbert.py,171,"(bs, n_heads, q_length, dim_per_head)",not
transformers/src/transformers/modeling_distilbert.py,172,"(bs, n_heads, k_length, dim_per_head)",not
transformers/src/transformers/modeling_distilbert.py,173,"(bs, n_heads, k_length, dim_per_head)",not
transformers/src/transformers/modeling_distilbert.py,175,"(bs, n_heads, q_length, dim_per_head)",not
transformers/src/transformers/modeling_distilbert.py,176,"(bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_distilbert.py,177,"(bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_distilbert.py,178,"(bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_distilbert.py,180,"(bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_distilbert.py,181,"(bs, n_heads, q_length, k_length)",not
transformers/src/transformers/modeling_distilbert.py,183,Mask heads if we want to,not
transformers/src/transformers/modeling_distilbert.py,187,"(bs, n_heads, q_length, dim_per_head)",not
transformers/src/transformers/modeling_distilbert.py,188,"(bs, q_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,189,"(bs, q_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,244,Self-Attention,not
transformers/src/transformers/modeling_distilbert.py,247,"(bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)",not
transformers/src/transformers/modeling_distilbert.py,248,To handle these `output_attention` or `output_hidden_states` cases returning tuples,not
transformers/src/transformers/modeling_distilbert.py,251,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,253,Feed Forward Network,not
transformers/src/transformers/modeling_distilbert.py,254,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,255,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,311,Add last layer,not
transformers/src/transformers/modeling_distilbert.py,320,"last-layer hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_distilbert.py,323,INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL,not
transformers/src/transformers/modeling_distilbert.py,396,Embeddings,not
transformers/src/transformers/modeling_distilbert.py,397,Encoder,not
transformers/src/transformers/modeling_distilbert.py,460,"(bs, seq_length)",not
transformers/src/transformers/modeling_distilbert.py,462,Prepare head mask if needed,not
transformers/src/transformers/modeling_distilbert.py,466,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,471,"last-layer hidden-state, (all hidden_states), (all attentions)",not
transformers/src/transformers/modeling_distilbert.py,537,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,538,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,539,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,540,"(bs, seq_length, dim)",not
transformers/src/transformers/modeling_distilbert.py,541,"(bs, seq_length, vocab_size)",not
transformers/src/transformers/modeling_distilbert.py,550,"(mlm_loss), prediction_logits, (all hidden_states), (all attentions)",not
transformers/src/transformers/modeling_distilbert.py,613,"(bs, seq_len, dim)",not
transformers/src/transformers/modeling_distilbert.py,614,"(bs, dim)",not
transformers/src/transformers/modeling_distilbert.py,615,"(bs, dim)",not
transformers/src/transformers/modeling_distilbert.py,616,"(bs, dim)",not
transformers/src/transformers/modeling_distilbert.py,617,"(bs, dim)",not
transformers/src/transformers/modeling_distilbert.py,618,"(bs, dim)",not
transformers/src/transformers/modeling_distilbert.py,630,"(loss), logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_distilbert.py,706,"(bs, max_query_len, dim)",not
transformers/src/transformers/modeling_distilbert.py,708,"(bs, max_query_len, dim)",not
transformers/src/transformers/modeling_distilbert.py,709,"(bs, max_query_len, 2)",not
transformers/src/transformers/modeling_distilbert.py,711,"(bs, max_query_len)",not
transformers/src/transformers/modeling_distilbert.py,712,"(bs, max_query_len)",not
transformers/src/transformers/modeling_distilbert.py,716,"If we are on multi-GPU, split add a dimension",not
transformers/src/transformers/modeling_distilbert.py,721,"sometimes the start/end positions are outside our model inputs, we ignore these terms",not
transformers/src/transformers/modeling_distilbert.py,732,"(loss), start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_distilbert.py,799,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_distilbert.py,802,Only keep active parts of the loss,not
transformers/src/transformers/modeling_distilbert.py,814,"(loss), scores, (hidden_states), (attentions)",not
transformers/src/transformers/pipelines.py,1,coding=utf-8,not
transformers/src/transformers/pipelines.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/pipelines.py,3,,not
transformers/src/transformers/pipelines.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/pipelines.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/pipelines.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/pipelines.py,7,,not
transformers/src/transformers/pipelines.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/pipelines.py,9,,not
transformers/src/transformers/pipelines.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/pipelines.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/pipelines.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/pipelines.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/pipelines.py,14,limitations under the License.,not
transformers/src/transformers/pipelines.py,70,Both framework are available but the user supplied a model class instance.,not
transformers/src/transformers/pipelines.py,71,Try to guess which framework to use from the model classname,not
transformers/src/transformers/pipelines.py,80,framework = 'tf' if is_tf_available() else 'pt',not
transformers/src/transformers/pipelines.py,112,"Only one argument, let's do case by case",not
transformers/src/transformers/pipelines.py,121,"Multiple arguments (x1, x2, ...)",not
transformers/src/transformers/pipelines.py,126,"If not instance of list, then it should instance of iterable",not
transformers/src/transformers/pipelines.py,274,Split for multi-columns,not
transformers/src/transformers/pipelines.py,279,Dictionary to map arguments,not
transformers/src/transformers/pipelines.py,284,No dictionary to map arguments,not
transformers/src/transformers/pipelines.py,389,Special handling,not
transformers/src/transformers/pipelines.py,393,Update config with task specific parameters,not
transformers/src/transformers/pipelines.py,457,Parse arguments,not
transformers/src/transformers/pipelines.py,478,Encode for forward,not
transformers/src/transformers/pipelines.py,481,TODO trace model,SATD
transformers/src/transformers/pipelines.py,570,Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia,not
transformers/src/transformers/pipelines.py,571,in https://github.com/rusiaaman/XLNet-gen#methodology,not
transformers/src/transformers/pipelines.py,572,and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e,not
transformers/src/transformers/pipelines.py,613,Manage correct placement of the tensors,not
transformers/src/transformers/pipelines.py,620,set input_ids to None to allow empty prompt,not
transformers/src/transformers/pipelines.py,630,Ensure that batch size = 1 (batch generation not allowed for now),not
transformers/src/transformers/pipelines.py,635,BS x SL,not
transformers/src/transformers/pipelines.py,644,Decode text,not
transformers/src/transformers/pipelines.py,651,Remove PADDING prompt of the sequence if XLNet or Transfo-XL model is used,not
transformers/src/transformers/pipelines.py,805,Filter padding out:,not
transformers/src/transformers/pipelines.py,809,Append,not
transformers/src/transformers/pipelines.py,887,Manage correct placement of the tensors,not
transformers/src/transformers/pipelines.py,897,Forward,not
transformers/src/transformers/pipelines.py,921,Append,not
transformers/src/transformers/pipelines.py,941,"Position args, handling is sensibly the same as X and data, so forwarding to avoid duplicating",not
transformers/src/transformers/pipelines.py,948,Generic compatibility with sklearn and Keras,not
transformers/src/transformers/pipelines.py,949,Batched data,not
transformers/src/transformers/pipelines.py,956,Copy to avoid overriding arguments,not
transformers/src/transformers/pipelines.py,973,Tabular input,not
transformers/src/transformers/pipelines.py,1088,Set defaults values,not
transformers/src/transformers/pipelines.py,1102,Convert inputs to features,not
transformers/src/transformers/pipelines.py,1121,Manage tensor allocation on correct device,not
transformers/src/transformers/pipelines.py,1129,Retrieve the score for the context tokens only (removing question tokens),not
transformers/src/transformers/pipelines.py,1134,large and positive,not
transformers/src/transformers/pipelines.py,1137,Normalize logits and spans to retrieve the answer,not
transformers/src/transformers/pipelines.py,1141,Mask padding and question,not
transformers/src/transformers/pipelines.py,1155,Convert the answer (tokens) back to the original text,not
transformers/src/transformers/pipelines.py,1192,Ensure we have batch axis,not
transformers/src/transformers/pipelines.py,1199,"Compute the score of each tuple(start, end) to be the real answer",not
transformers/src/transformers/pipelines.py,1202,Remove candidate with end < start and end - start > max_answer_len,not
transformers/src/transformers/pipelines.py,1205,Inspired by Chen & al. (https://github.com/facebookresearch/DrQA),not
transformers/src/transformers/pipelines.py,1237,Append words if they are in the span,not
transformers/src/transformers/pipelines.py,1247,Stop if we went over the end of the answer,not
transformers/src/transformers/pipelines.py,1251,Append the subtokenization length to the running index,not
transformers/src/transformers/pipelines.py,1255,Join text with spaces,not
transformers/src/transformers/pipelines.py,1512,Register all the supported task here,not
transformers/src/transformers/pipelines.py,1693,Retrieve the task,not
transformers/src/transformers/pipelines.py,1702,Use default model/config/tokenizer for the task if no model is provided,not
transformers/src/transformers/pipelines.py,1707,Try to infer tokenizer from model or config name (if provided as str),not
transformers/src/transformers/pipelines.py,1714,Impossible to guest what is the right tokenizer here,not
transformers/src/transformers/pipelines.py,1721,Try to infer modelcard from model or config name (if provided as str),not
transformers/src/transformers/pipelines.py,1727,Instantiate tokenizer if needed,not
transformers/src/transformers/pipelines.py,1730,"For tuple we have (tokenizer name, {kwargs})",not
transformers/src/transformers/pipelines.py,1735,Instantiate config if needed,not
transformers/src/transformers/pipelines.py,1739,Instantiate modelcard if needed,not
transformers/src/transformers/pipelines.py,1743,Instantiate model if needed,not
transformers/src/transformers/pipelines.py,1745,Handle transparent TF/PT model conversion,not
transformers/src/transformers/hf_argparser.py,46,"field.metadata is not used at all by Data Classes,",not
transformers/src/transformers/hf_argparser.py,47,it is provided as a third-party extension mechanism.,not
transformers/src/transformers/hf_argparser.py,112,in case of duplicate arguments the first one has precedence,not
transformers/src/transformers/hf_argparser.py,113,so we append rather than prepend.,not
transformers/src/transformers/hf_argparser.py,124,additional namespace.,not
transformers/src/transformers/optimization.py,1,coding=utf-8,not
transformers/src/transformers/optimization.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/optimization.py,3,,not
transformers/src/transformers/optimization.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/optimization.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/optimization.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/optimization.py,7,,not
transformers/src/transformers/optimization.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/optimization.py,9,,not
transformers/src/transformers/optimization.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/optimization.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/optimization.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/optimization.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/optimization.py,14,limitations under the License.,not
transformers/src/transformers/optimization.py,140,State initialization,not
transformers/src/transformers/optimization.py,143,Exponential moving average of gradient values,not
transformers/src/transformers/optimization.py,145,Exponential moving average of squared gradient values,not
transformers/src/transformers/optimization.py,153,Decay the first and second moment running average coefficient,not
transformers/src/transformers/optimization.py,154,In-place operations to update the averages at the same time,not
transformers/src/transformers/optimization.py,160,No bias correction for Bert,not
transformers/src/transformers/optimization.py,167,Just adding the square of the weights to the loss function is *not*,not
transformers/src/transformers/optimization.py,168,"the correct way of using L2 regularization/weight decay with Adam,",not
transformers/src/transformers/optimization.py,169,since that will interact with the m and v parameters in strange ways.,not
transformers/src/transformers/optimization.py,170,,not
transformers/src/transformers/optimization.py,171,Instead we want to decay the weights in a manner that doesn't interact,not
transformers/src/transformers/optimization.py,172,with the m/v parameters. This is equivalent to adding the square,not
transformers/src/transformers/optimization.py,173,of the weights to the loss with plain (non-momentum) SGD.,not
transformers/src/transformers/optimization.py,174,Add weight decay at the end (fixed version),not
transformers/src/transformers/tokenization_gpt2.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_gpt2.py,2,Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_gpt2.py,3,,not
transformers/src/transformers/tokenization_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_gpt2.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_gpt2.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_gpt2.py,7,,not
transformers/src/transformers/tokenization_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_gpt2.py,9,,not
transformers/src/transformers/tokenization_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_gpt2.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_gpt2.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_gpt2.py,154,how to handle errors in decoding,not
transformers/src/transformers/tokenization_gpt2.py,163,Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions,not
transformers/src/transformers/tokenization_gpt2.py,221,"Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)",not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,30,Construct model,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,37,Load weights from numpy,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,40,Save pytorch-model,not
transformers/src/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py,52,Required parameters,not
transformers/src/transformers/configuration_albert.py,1,coding=utf-8,not
transformers/src/transformers/configuration_albert.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_albert.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_albert.py,4,,not
transformers/src/transformers/configuration_albert.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_albert.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_albert.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_albert.py,8,,not
transformers/src/transformers/configuration_albert.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_albert.py,10,,not
transformers/src/transformers/configuration_albert.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_albert.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_albert.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_albert.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_albert.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_xlm_roberta.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_xlm_roberta.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/tokenization_xlm_roberta.py,3,,not
transformers/src/transformers/tokenization_xlm_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_xlm_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_xlm_roberta.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_xlm_roberta.py,7,,not
transformers/src/transformers/tokenization_xlm_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_xlm_roberta.py,9,,not
transformers/src/transformers/tokenization_xlm_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_xlm_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_xlm_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_xlm_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_xlm_roberta.py,14,limitations under the License,not
transformers/src/transformers/tokenization_xlm_roberta.py,144,"Original fairseq vocab and spm vocab must be ""aligned"":",not
transformers/src/transformers/tokenization_xlm_roberta.py,145,Vocab    |    0    |    1    |   2    |    3    |  4  |  5  |  6  |   7   |   8   |  9,not
transformers/src/transformers/tokenization_xlm_roberta.py,146,-------- | ------- | ------- | ------ | ------- | --- | --- | --- | ----- | ----- | ----,not
transformers/src/transformers/tokenization_xlm_roberta.py,147,"fairseq  | '<s>'   | '<pad>' | '</s>' | '<unk>' | ',' | '.' | '▁' | 's'   | '▁de' | '-'",not
transformers/src/transformers/tokenization_xlm_roberta.py,148,"spm      | '<unk>' | '<s>'   | '</s>' | ','     | '.' | '▁' | 's' | '▁de' | '-'   | '▁a'",not
transformers/src/transformers/tokenization_xlm_roberta.py,150,Mimic fairseq token-to-id alignment for the first 4 token,not
transformers/src/transformers/tokenization_xlm_roberta.py,153,"The first ""real"" token "","" has position 4 in the original fairseq vocab and position 3 in the spm vocab",not
transformers/src/transformers/tokenization_xlm_roberta.py,262,Add the <mask> token,not
transformers/src/transformers/tokenization_xlm_roberta.py,278,Need to return unknown token if the SP model returned 0,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,30,Initialise PyTorch model,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,41,Load weights from tf checkpoint,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,46,Save pytorch-model,not
transformers/src/transformers/convert_electra_original_tf_checkpoint_to_pytorch.py,53,Required parameters,not
transformers/src/transformers/configuration_t5.py,1,coding=utf-8,not
transformers/src/transformers/configuration_t5.py,2,"Copyright 2010, The T5 Authors and HuggingFace Inc.",not
transformers/src/transformers/configuration_t5.py,3,,not
transformers/src/transformers/configuration_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_t5.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_t5.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_t5.py,7,,not
transformers/src/transformers/configuration_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_t5.py,9,,not
transformers/src/transformers/configuration_t5.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_t5.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_t5.py,14,limitations under the License.,not
transformers/src/transformers/configuration_openai.py,1,coding=utf-8,not
transformers/src/transformers/configuration_openai.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/configuration_openai.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_openai.py,4,,not
transformers/src/transformers/configuration_openai.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_openai.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_openai.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_openai.py,8,,not
transformers/src/transformers/configuration_openai.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_openai.py,10,,not
transformers/src/transformers/configuration_openai.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_openai.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_openai.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_openai.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_openai.py,15,limitations under the License.,not
transformers/src/transformers/configuration_utils.py,1,coding=utf-8,not
transformers/src/transformers/configuration_utils.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_utils.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_utils.py,4,,not
transformers/src/transformers/configuration_utils.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_utils.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_utils.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_utils.py,8,,not
transformers/src/transformers/configuration_utils.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_utils.py,10,,not
transformers/src/transformers/configuration_utils.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_utils.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_utils.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_utils.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_utils.py,15,limitations under the License.,not
transformers/src/transformers/configuration_utils.py,59,Attributes with defaults,not
transformers/src/transformers/configuration_utils.py,62,Not used by all models,not
transformers/src/transformers/configuration_utils.py,63,Only used by PyTorch models,not
transformers/src/transformers/configuration_utils.py,67,Is decoder is used in encoder-decoder models to differentiate encoder from decoder,not
transformers/src/transformers/configuration_utils.py,71,Parameters for sequence generation,not
transformers/src/transformers/configuration_utils.py,86,Fine-tuning task arguments,not
transformers/src/transformers/configuration_utils.py,93,Keys are always strings in JSON so convert ids to int here.,not
transformers/src/transformers/configuration_utils.py,97,Tokenizer arguments TODO: eventually tokenizer and models should share the same config,SATD
transformers/src/transformers/configuration_utils.py,104,task specific arguments,not
transformers/src/transformers/configuration_utils.py,107,TPU arguments,not
transformers/src/transformers/configuration_utils.py,110,Additional attributes without default values,not
transformers/src/transformers/configuration_utils.py,140,"If we save using the predefined names, we can load using `from_pretrained`",not
transformers/src/transformers/configuration_utils.py,242,Load from URL or cache if already cached,not
transformers/src/transformers/configuration_utils.py,251,Load config dict,not
transformers/src/transformers/configuration_utils.py,312,Update config with kwargs if needed,not
transformers/src/transformers/configuration_utils.py,366,get the default config dict,not
transformers/src/transformers/configuration_utils.py,371,only serialize values that differ from the default config,not
transformers/src/transformers/modeling_tf_flaubert.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_flaubert.py,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_tf_flaubert.py,3,,not
transformers/src/transformers/modeling_tf_flaubert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_flaubert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_flaubert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_flaubert.py,7,,not
transformers/src/transformers/modeling_tf_flaubert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_flaubert.py,9,,not
transformers/src/transformers/modeling_tf_flaubert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_flaubert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_flaubert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_flaubert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_flaubert.py,14,limitations under the License.,not
transformers/src/transformers/modeling_tf_flaubert.py,133,"removed: src_enc=None, src_len=None",not
transformers/src/transformers/modeling_tf_flaubert.py,173,mask = input_ids != self.pad_index,not
transformers/src/transformers/modeling_tf_flaubert.py,175,check inputs,not
transformers/src/transformers/modeling_tf_flaubert.py,176,assert shape_list(lengths)[0] == bs,not
transformers/src/transformers/modeling_tf_flaubert.py,178,assert lengths.max().item() <= slen,not
transformers/src/transformers/modeling_tf_flaubert.py,179,"input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0",not
transformers/src/transformers/modeling_tf_flaubert.py,180,assert (src_enc is None) == (src_len is None),not
transformers/src/transformers/modeling_tf_flaubert.py,181,if src_enc is not None:,not
transformers/src/transformers/modeling_tf_flaubert.py,182,assert self.is_decoder,not
transformers/src/transformers/modeling_tf_flaubert.py,183,assert src_enc.size(0) == bs,not
transformers/src/transformers/modeling_tf_flaubert.py,185,generate masks,not
transformers/src/transformers/modeling_tf_flaubert.py,187,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_tf_flaubert.py,188,"src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]",not
transformers/src/transformers/modeling_tf_flaubert.py,190,position_ids,not
transformers/src/transformers/modeling_tf_flaubert.py,194,"assert shape_list(position_ids) == [bs, slen]  # (slen, bs)",not
transformers/src/transformers/modeling_tf_flaubert.py,196,"position_ids = position_ids.transpose(0, 1)",not
transformers/src/transformers/modeling_tf_flaubert.py,198,langs,not
transformers/src/transformers/modeling_tf_flaubert.py,200,"assert shape_list(langs) == [bs, slen]  # (slen, bs)",not
transformers/src/transformers/modeling_tf_flaubert.py,202,"langs = langs.transpose(0, 1)",not
transformers/src/transformers/modeling_tf_flaubert.py,204,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_flaubert.py,205,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_flaubert.py,206,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_flaubert.py,207,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_flaubert.py,208,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen],not
transformers/src/transformers/modeling_tf_flaubert.py,214,do not recompute cached elements,not
transformers/src/transformers/modeling_tf_flaubert.py,224,embeddings,not
transformers/src/transformers/modeling_tf_flaubert.py,237,transformer layers,not
transformers/src/transformers/modeling_tf_flaubert.py,241,LayerDrop,not
transformers/src/transformers/modeling_tf_flaubert.py,249,self attention,not
transformers/src/transformers/modeling_tf_flaubert.py,269,encoder attention (for decoder only),not
transformers/src/transformers/modeling_tf_flaubert.py,270,if self.is_decoder and src_enc is not None:,not
transformers/src/transformers/modeling_tf_flaubert.py,271,"attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)",not
transformers/src/transformers/modeling_tf_flaubert.py,272,"attn = F.dropout(attn, p=self.dropout, training=self.training)",not
transformers/src/transformers/modeling_tf_flaubert.py,273,tensor = tensor + attn,not
transformers/src/transformers/modeling_tf_flaubert.py,274,tensor = self.layer_norm15[i](tensor),not
transformers/src/transformers/modeling_tf_flaubert.py,276,FFN,not
transformers/src/transformers/modeling_tf_flaubert.py,286,Add last hidden state,not
transformers/src/transformers/modeling_tf_flaubert.py,290,update cache length,not
transformers/src/transformers/modeling_tf_flaubert.py,294,move back sequence length to dimension 0,SATD
transformers/src/transformers/modeling_tf_flaubert.py,295,"tensor = tensor.transpose(0, 1)",not
transformers/src/transformers/modeling_tf_flaubert.py,302,"outputs, (hidden_states), (attentions)",not
transformers/src/transformers/configuration_reformer.py,1,coding=utf-8,not
transformers/src/transformers/configuration_reformer.py,2,Copyright 2020 The Trax Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_reformer.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_reformer.py,4,,not
transformers/src/transformers/configuration_reformer.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_reformer.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_reformer.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_reformer.py,8,,not
transformers/src/transformers/configuration_reformer.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_reformer.py,10,,not
transformers/src/transformers/configuration_reformer.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_reformer.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_reformer.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_reformer.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_reformer.py,15,limitations under the License.,not
transformers/src/transformers/tokenization_distilbert.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_distilbert.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_distilbert.py,3,,not
transformers/src/transformers/tokenization_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_distilbert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_distilbert.py,7,,not
transformers/src/transformers/tokenization_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_distilbert.py,9,,not
transformers/src/transformers/tokenization_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_distilbert.py,14,limitations under the License.,not
transformers/src/transformers/trainer_tf.py,23,something similar to a PT Dataset.,not
transformers/src/transformers/trainer_tf.py,24,This is just temporary before to have,not
transformers/src/transformers/trainer_tf.py,25,a framework-agnostic approach for datasets.,not
transformers/src/transformers/trainer_tf.py,140,This is for the case where the optimizer is not Adam-like such as SGD,not
transformers/src/transformers/__init__.py,1,flake8: noqa,not
transformers/src/transformers/__init__.py,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",not
transformers/src/transformers/__init__.py,3,"module, but to preserve other warnings. So, don't check this module at all.",not
transformers/src/transformers/__init__.py,7,Work around to update TensorFlow's absl.logging threshold which alters the,not
transformers/src/transformers/__init__.py,8,default Python logging output behavior when present.,not
transformers/src/transformers/__init__.py,9,see: https://github.com/abseil/abseil-py/issues/99,not
transformers/src/transformers/__init__.py,10,and: https://github.com/tensorflow/tensorflow/issues/26691#issuecomment-500369493,not
transformers/src/transformers/__init__.py,22,Benchmarking,not
transformers/src/transformers/__init__.py,35,Configurations,not
transformers/src/transformers/__init__.py,78,Files and general utilities,not
transformers/src/transformers/__init__.py,96,Model Cards,not
transformers/src/transformers/__init__.py,99,TF 2.0 <=> PyTorch conversion utilities,not
transformers/src/transformers/__init__.py,110,Pipelines,not
transformers/src/transformers/__init__.py,129,Tokenizers,not
transformers/src/transformers/__init__.py,155,pylint: disable=invalid-name,not
transformers/src/transformers/__init__.py,162,Modeling,not
transformers/src/transformers/__init__.py,335,Optimization,not
transformers/src/transformers/__init__.py,345,Trainer,not
transformers/src/transformers/__init__.py,350,TensorFlow,not
transformers/src/transformers/__init__.py,519,Optimization,not
transformers/src/transformers/__init__.py,522,Trainer,not
transformers/src/transformers/configuration_ctrl.py,1,coding=utf-8,not
transformers/src/transformers/configuration_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,not
transformers/src/transformers/configuration_ctrl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_ctrl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_ctrl.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_ctrl.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_ctrl.py,7,,not
transformers/src/transformers/configuration_ctrl.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_ctrl.py,9,,not
transformers/src/transformers/configuration_ctrl.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_ctrl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_ctrl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_ctrl.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_ctrl.py,14,limitations under the License.,not
transformers/src/transformers/modeling_tf_t5.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_t5.py,2,Copyright 2018 T5 Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_t5.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_t5.py,4,,not
transformers/src/transformers/modeling_tf_t5.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_t5.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_t5.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_t5.py,8,,not
transformers/src/transformers/modeling_tf_t5.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_t5.py,10,,not
transformers/src/transformers/modeling_tf_t5.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_t5.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_t5.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_t5.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_t5.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_t5.py,41,,not
transformers/src/transformers/modeling_tf_t5.py,42,TF 2.0 Models are constructed using Keras imperative API by sub-classing,not
transformers/src/transformers/modeling_tf_t5.py,43,- tf.keras.layers.Layer for the layers and,not
transformers/src/transformers/modeling_tf_t5.py,44,- TFPreTrainedModel for the models (it-self a sub-class of tf.keras.Model),not
transformers/src/transformers/modeling_tf_t5.py,45,,not
transformers/src/transformers/modeling_tf_t5.py,113,Mesh TensorFlow initialization to avoid scaling before softmax,not
transformers/src/transformers/modeling_tf_t5.py,162,"now n is in the range [0, inf)",not
transformers/src/transformers/modeling_tf_t5.py,179,"shape (qlen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,183,"shape (qlen, klen, num_heads)",not
transformers/src/transformers/modeling_tf_t5.py,184,"shape (1, num_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,203,"Input is (bs, qlen, dim)",not
transformers/src/transformers/modeling_tf_t5.py,204,"Mask is (bs, klen) (non-causal) or (bs, klen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,205,"past_key_value_state[0] is (bs, n_heads, q_len - 1, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,232,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,235,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,236,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,239,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,240,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,245,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,246,"(bs, n_heads, klen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,250,to cope with keras serialization,not
transformers/src/transformers/modeling_tf_t5.py,251,we need to cast `use_cache` to correct bool,not
transformers/src/transformers/modeling_tf_t5.py,252,if it is a tensor,not
transformers/src/transformers/modeling_tf_t5.py,264,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,271,if key and values are already calculated,not
transformers/src/transformers/modeling_tf_t5.py,272,we want only the last query position bias,not
transformers/src/transformers/modeling_tf_t5.py,277,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,280,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,281,"(bs, n_heads, qlen, klen)",not
transformers/src/transformers/modeling_tf_t5.py,283,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_t5.py,287,"(bs, n_heads, qlen, dim_per_head)",not
transformers/src/transformers/modeling_tf_t5.py,288,"(bs, qlen, dim)",not
transformers/src/transformers/modeling_tf_t5.py,332,add attentions if we output them,not
transformers/src/transformers/modeling_tf_t5.py,371,add attentions if we output them,not
transformers/src/transformers/modeling_tf_t5.py,432,Keep self-attention outputs and relative position weights,not
transformers/src/transformers/modeling_tf_t5.py,435,the actual query length is unknown for cross attention,not
transformers/src/transformers/modeling_tf_t5.py,436,if using past key value states. Need to inject it here,not
transformers/src/transformers/modeling_tf_t5.py,454,Combine self attn and cross attn key value states,not
transformers/src/transformers/modeling_tf_t5.py,458,Keep cross-attention outputs and relative position weights,not
transformers/src/transformers/modeling_tf_t5.py,461,Apply Feed Forward layer,not
transformers/src/transformers/modeling_tf_t5.py,465,Add attentions if we output them,not
transformers/src/transformers/modeling_tf_t5.py,467,"hidden-states, present_key_value_states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",not
transformers/src/transformers/modeling_tf_t5.py,485,"if an abs scope name is given to the embedding variable, call variable from absolute scope",not
transformers/src/transformers/modeling_tf_t5.py,494,"if an abs scope name is given to the embedding variable, call variable from absolute scope",not
transformers/src/transformers/modeling_tf_t5.py,500,,not
transformers/src/transformers/modeling_tf_t5.py,501,The full model without a specific pretrained or finetuning head is,not
transformers/src/transformers/modeling_tf_t5.py,502,"provided as a tf.keras.layers.Layer usually called ""TFT5MainLayer""",not
transformers/src/transformers/modeling_tf_t5.py,503,,not
transformers/src/transformers/modeling_tf_t5.py,533,Not implemented yet in the library fr TF 2.0 models,not
transformers/src/transformers/modeling_tf_t5.py,536,Not implemented yet in the library fr TF 2.0 models,not
transformers/src/transformers/modeling_tf_t5.py,571,required mask seq length can be calculated via length of past,not
transformers/src/transformers/modeling_tf_t5.py,572,key value states and seq_length = 1 for the last token,not
transformers/src/transformers/modeling_tf_t5.py,583,initialize past_key_value_states with `None` if past does not exist,not
transformers/src/transformers/modeling_tf_t5.py,587,"We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_t5.py,588,ourselves in which case we just need to make it broadcastable to all heads.,not
transformers/src/transformers/modeling_tf_t5.py,594,"Provided a padding mask of dimensions [batch_size, mask_seq_length]",not
transformers/src/transformers/modeling_tf_t5.py,595,"- if the model is a decoder, apply a causal mask in addition to the padding mask",not
transformers/src/transformers/modeling_tf_t5.py,596,"- if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]",not
transformers/src/transformers/modeling_tf_t5.py,609,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_t5.py,610,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_t5.py,611,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_t5.py,612,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_t5.py,613,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_t5.py,615,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposistion",not
transformers/src/transformers/modeling_tf_t5.py,616,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270,not
transformers/src/transformers/modeling_tf_t5.py,617,"extended_attention_mask = tf.math.equal(extended_attention_mask,",not
transformers/src/transformers/modeling_tf_t5.py,618,"tf.transpose(extended_attention_mask, perm=(-1, -2)))",not
transformers/src/transformers/modeling_tf_t5.py,623,If a 2D ou 3D attention mask is provided for the cross-attention,not
transformers/src/transformers/modeling_tf_t5.py,624,"we need to make broadcastabe to [batch_size, num_heads, mask_seq_length, mask_seq_length]",not
transformers/src/transformers/modeling_tf_t5.py,625,"we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]",not
transformers/src/transformers/modeling_tf_t5.py,633,"T5 has a mask that can compare sequence ids, we can simulate this here with this transposistion",not
transformers/src/transformers/modeling_tf_t5.py,634,Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270,not
transformers/src/transformers/modeling_tf_t5.py,635,"encoder_extended_attention_mask = tf.math.equal(encoder_extended_attention_mask,",not
transformers/src/transformers/modeling_tf_t5.py,636,"tf.transpose(encoder_extended_attention_mask, perm=(-1, -2)))",not
transformers/src/transformers/modeling_tf_t5.py,642,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_t5.py,643,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_t5.py,644,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_t5.py,645,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads],not
transformers/src/transformers/modeling_tf_t5.py,646,and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length],not
transformers/src/transformers/modeling_tf_t5.py,651,head_mask = tf.constant([0] * self.num_hidden_layers),not
transformers/src/transformers/modeling_tf_t5.py,677,layer_outputs is a tuple with:,not
transformers/src/transformers/modeling_tf_t5.py,678,"hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",not
transformers/src/transformers/modeling_tf_t5.py,681,We share the position biases between the layers - the first layer store them,not
transformers/src/transformers/modeling_tf_t5.py,682,"layer_outputs = hidden-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)",not
transformers/src/transformers/modeling_tf_t5.py,686,append next layer key value states,not
transformers/src/transformers/modeling_tf_t5.py,695,Add last layer,not
transformers/src/transformers/modeling_tf_t5.py,707,"last-layer hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_tf_t5.py,710,,not
transformers/src/transformers/modeling_tf_t5.py,711,TFT5PreTrainedModel is a sub-class of tf.keras.Model,not
transformers/src/transformers/modeling_tf_t5.py,712,which take care of loading and saving pretrained weights,SATD
transformers/src/transformers/modeling_tf_t5.py,713,and various common utilities.,not
transformers/src/transformers/modeling_tf_t5.py,714,Here you just need to specify a few (self-explanatory),not
transformers/src/transformers/modeling_tf_t5.py,715,pointers for your model.,not
transformers/src/transformers/modeling_tf_t5.py,716,,not
transformers/src/transformers/modeling_tf_t5.py,833,retrieve correct absolute scope for embed token wrapper,not
transformers/src/transformers/modeling_tf_t5.py,899,retrieve arguments,not
transformers/src/transformers/modeling_tf_t5.py,911,"Encode if needed (training, first prediction pass)",not
transformers/src/transformers/modeling_tf_t5.py,919,"If decoding with past key value states, only the last tokens",not
transformers/src/transformers/modeling_tf_t5.py,920,should be given as an input,not
transformers/src/transformers/modeling_tf_t5.py,927,Decode,not
transformers/src/transformers/modeling_tf_t5.py,954,retrieve correct absolute scope for embed token wrapper,not
transformers/src/transformers/modeling_tf_t5.py,1025,retrieve arguments,not
transformers/src/transformers/modeling_tf_t5.py,1037,"Encode if needed (training, first prediction pass)",not
transformers/src/transformers/modeling_tf_t5.py,1039,Convert encoder inputs in embeddings if needed,not
transformers/src/transformers/modeling_tf_t5.py,1046,"If decoding with past key value states, only the last tokens",not
transformers/src/transformers/modeling_tf_t5.py,1047,should be given as an input,not
transformers/src/transformers/modeling_tf_t5.py,1054,Decode,not
transformers/src/transformers/modeling_tf_t5.py,1066,insert decoder past at right place,not
transformers/src/transformers/modeling_tf_t5.py,1067,to speed up decoding,not
transformers/src/transformers/modeling_tf_t5.py,1082,first step,not
transformers/src/transformers/modeling_tf_t5.py,1089,"inputs don't have to be defined, but still need to be passed to make Keras.layer.__call__ happy",not
transformers/src/transformers/modeling_tf_t5.py,1090,input_ids are the decoder_input_ids,not
transformers/src/transformers/modeling_tf_t5.py,1098,if decoder past is not included in output,not
transformers/src/transformers/modeling_tf_t5.py,1099,speedy decoding is disabled and no need to reorder,not
transformers/src/transformers/modeling_tf_t5.py,1110,get the correct batch idx from layer past batch dim,not
transformers/src/transformers/modeling_tf_t5.py,1111,batch dim of `past` is at 2nd position,not
transformers/src/transformers/modeling_tf_t5.py,1114,need to set correct `past` for each of the four key / value states,not
transformers/src/transformers/modeling_ctrl.py,1,coding=utf-8,not
transformers/src/transformers/modeling_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_ctrl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_ctrl.py,4,,not
transformers/src/transformers/modeling_ctrl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_ctrl.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_ctrl.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_ctrl.py,8,,not
transformers/src/transformers/modeling_ctrl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_ctrl.py,10,,not
transformers/src/transformers/modeling_ctrl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_ctrl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_ctrl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_ctrl.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_ctrl.py,15,limitations under the License.,not
transformers/src/transformers/modeling_ctrl.py,42,create the sinusoidal pattern for the positional encoding,not
transformers/src/transformers/modeling_ctrl.py,57,calculate attention,not
transformers/src/transformers/modeling_ctrl.py,68,Apply the attention mask,not
transformers/src/transformers/modeling_ctrl.py,73,Mask heads if we want to,not
transformers/src/transformers/modeling_ctrl.py,188,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_ctrl.py,189,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_ctrl.py,347,"If using past key value states, only the last tokens",not
transformers/src/transformers/modeling_ctrl.py,348,should be given as an input,not
transformers/src/transformers/modeling_ctrl.py,379,Attention mask.,not
transformers/src/transformers/modeling_ctrl.py,383,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_ctrl.py,384,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_ctrl.py,385,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_ctrl.py,386,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_ctrl.py,387,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_ctrl.py,390,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_ctrl.py,391,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_ctrl.py,392,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_ctrl.py,393,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_ctrl.py,394,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_ctrl.py,395,fp16 compatibility,not
transformers/src/transformers/modeling_ctrl.py,398,Prepare head mask if needed,not
transformers/src/transformers/modeling_ctrl.py,411,inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded,not
transformers/src/transformers/modeling_ctrl.py,456,let the number of heads free (-1) so we can extract attention even after head pruning,not
transformers/src/transformers/modeling_ctrl.py,480,only last token for inputs_ids if past is defined in kwargs,not
transformers/src/transformers/modeling_ctrl.py,559,Shift so that tokens < n predict n,not
transformers/src/transformers/modeling_ctrl.py,562,Flatten the tokens,not
transformers/src/transformers/modeling_ctrl.py,567,"(loss), lm_logits, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/benchmark_utils.py,23,pylint: disable=invalid-name,not
transformers/src/transformers/benchmark_utils.py,187,Filter events,not
transformers/src/transformers/benchmark_utils.py,194,Filter modules,not
transformers/src/transformers/benchmark_utils.py,199,Filter whitelist of modules to trace,not
transformers/src/transformers/benchmark_utils.py,206,Filter blacklist of modules not to trace,not
transformers/src/transformers/benchmark_utils.py,213,"Record current tracing state (file, location in file...)",not
transformers/src/transformers/benchmark_utils.py,221,Record current memory state (rss memory) and compute difference with previous memory state,not
transformers/src/transformers/benchmark_utils.py,229,Clear GPU caches,not
transformers/src/transformers/benchmark_utils.py,233,See https://github.com/tensorflow/tensorflow/issues/20218#issuecomment-416771802,not
transformers/src/transformers/benchmark_utils.py,235,Sum used memory for all GPUs,not
transformers/src/transformers/benchmark_utils.py,316,order by the total CPU + GPU memory increase,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,1,coding=utf-8,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,3,,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,7,,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,9,,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py,14,limitations under the License.,not
transformers/src/transformers/configuration_roberta.py,1,coding=utf-8,not
transformers/src/transformers/configuration_roberta.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_roberta.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_roberta.py,4,,not
transformers/src/transformers/configuration_roberta.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_roberta.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_roberta.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_roberta.py,8,,not
transformers/src/transformers/configuration_roberta.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_roberta.py,10,,not
transformers/src/transformers/configuration_roberta.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_roberta.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_roberta.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_roberta.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_roberta.py,15,limitations under the License.,not
transformers/src/transformers/modeling_openai.py,1,coding=utf-8,not
transformers/src/transformers/modeling_openai.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_openai.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_openai.py,4,,not
transformers/src/transformers/modeling_openai.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_openai.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_openai.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_openai.py,8,,not
transformers/src/transformers/modeling_openai.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_openai.py,10,,not
transformers/src/transformers/modeling_openai.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_openai.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_openai.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_openai.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_openai.py,15,limitations under the License.,not
transformers/src/transformers/modeling_openai.py,59,This was used when we had a single embedding matrix for positions and tokens,not
transformers/src/transformers/modeling_openai.py,60,"init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)",not
transformers/src/transformers/modeling_openai.py,61,del init_params[1],not
transformers/src/transformers/modeling_openai.py,75,Pop position and token embedding arrays,not
transformers/src/transformers/modeling_openai.py,79,"names[1:n_transfer], init_params[1:n_transfer]):",not
transformers/src/transformers/modeling_openai.py,80,"skip ""model/""",not
transformers/src/transformers/modeling_openai.py,122,in Attention: n_state=768 (nx=n_embd),not
transformers/src/transformers/modeling_openai.py,123,[switch nx => n_state from Block to Attention to keep identical to TF implem],not
transformers/src/transformers/modeling_openai.py,149,Prune conv1d layers,not
transformers/src/transformers/modeling_openai.py,152,Update hyper params,not
transformers/src/transformers/modeling_openai.py,161,w = w * self.bias + -1e9 * (1 - self.bias)  # TF implem method: mask_attn_weights,not
transformers/src/transformers/modeling_openai.py,162,"XD: self.b may be larger than w, so we need to crop it",not
transformers/src/transformers/modeling_openai.py,167,Apply the attention mask,not
transformers/src/transformers/modeling_openai.py,173,Mask heads if we want to,not
transformers/src/transformers/modeling_openai.py,185,in Tensorflow implem: fct merge_states,not
transformers/src/transformers/modeling_openai.py,189,in Tensorflow implem: fct split_states,not
transformers/src/transformers/modeling_openai.py,210,"a, (attentions)",not
transformers/src/transformers/modeling_openai.py,214,in MLP: n_state=3072 (4 * n_embd),not
transformers/src/transformers/modeling_openai.py,263,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_openai.py,264,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_openai.py,403,Code is different from when we had a single embedding matrice from position and token embeddings,not
transformers/src/transformers/modeling_openai.py,408,Attention mask.,not
transformers/src/transformers/modeling_openai.py,410,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_openai.py,411,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_openai.py,412,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_openai.py,413,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_openai.py,414,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_openai.py,417,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_openai.py,418,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_openai.py,419,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_openai.py,420,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_openai.py,421,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_openai.py,422,fp16 compatibility,not
transformers/src/transformers/modeling_openai.py,425,Prepare head mask if needed,not
transformers/src/transformers/modeling_openai.py,452,Add last layer,not
transformers/src/transformers/modeling_openai.py,461,"last hidden state, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_openai.py,546,Shift so that tokens < n predict n,not
transformers/src/transformers/modeling_openai.py,549,Flatten the tokens,not
transformers/src/transformers/modeling_openai.py,554,"(loss), lm_logits, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_openai.py,676,"(lm loss), (mc loss), lm logits, mc logits, (all hidden_states), (attentions)",not
transformers/src/transformers/hf_api.py,1,coding=utf-8,not
transformers/src/transformers/hf_api.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/src/transformers/hf_api.py,3,,not
transformers/src/transformers/hf_api.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/hf_api.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/hf_api.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/hf_api.py,7,,not
transformers/src/transformers/hf_api.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/hf_api.py,9,,not
transformers/src/transformers/hf_api.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/hf_api.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/hf_api.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/hf_api.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/hf_api.py,14,limitations under the License.,not
transformers/src/transformers/hf_api.py,45,mime-type to send to S3.,not
transformers/src/transformers/hf_api.py,55,S3 object key,not
transformers/src/transformers/hf_api.py,59,filename relative to config.json,not
transformers/src/transformers/hf_api.py,76,id of model,not
transformers/src/transformers/hf_api.py,77,S3 object key of config.json,not
transformers/src/transformers/hf_api.py,81,list of files that constitute the model,not
transformers/src/transformers/hf_api.py,152,streaming upload:,not
transformers/src/transformers/hf_api.py,153,https://2.python-requests.org/en/master/user/advanced/#streaming-uploads,not
transformers/src/transformers/hf_api.py,154,,not
transformers/src/transformers/hf_api.py,155,"Even though we presign with the correct content-type,",not
transformers/src/transformers/hf_api.py,156,the client still has to specify it when uploading the file.,not
transformers/src/transformers/modeling_bart.py,1,coding=utf-8,not
transformers/src/transformers/modeling_bart.py,2,Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_bart.py,3,,not
transformers/src/transformers/modeling_bart.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_bart.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_bart.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_bart.py,7,,not
transformers/src/transformers/modeling_bart.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_bart.py,9,,not
transformers/src/transformers/modeling_bart.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_bart.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_bart.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_bart.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_bart.py,14,limitations under the License.,not
transformers/src/transformers/modeling_bart.py,154,"Helper Functions, mostly for making masks",not
transformers/src/transformers/modeling_bart.py,177,Helper Modules,not
transformers/src/transformers/modeling_bart.py,266,mbart has one extra layer_norm,not
transformers/src/transformers/modeling_bart.py,287,check attention mask and invert,not
transformers/src/transformers/modeling_bart.py,297,B x T x C -> T x B x C,not
transformers/src/transformers/modeling_bart.py,304,add LayerDrop (see https://arxiv.org/abs/1909.11556 for description),not
transformers/src/transformers/modeling_bart.py,306,skip the layer,not
transformers/src/transformers/modeling_bart.py,319,T x B x C -> B x T x C,not
transformers/src/transformers/modeling_bart.py,366,Self Attention,not
transformers/src/transformers/modeling_bart.py,371,adds keys to layer state,not
transformers/src/transformers/modeling_bart.py,381,Cross attention,not
transformers/src/transformers/modeling_bart.py,390,mutates layer state,not
transformers/src/transformers/modeling_bart.py,397,Fully Connected,not
transformers/src/transformers/modeling_bart.py,412,"just self_attn weights for now, following t5, layer_state = cache for decoding",not
transformers/src/transformers/modeling_bart.py,444,type: List[DecoderLayer],not
transformers/src/transformers/modeling_bart.py,477,check attention mask and invert,not
transformers/src/transformers/modeling_bart.py,481,embed positions,not
transformers/src/transformers/modeling_bart.py,486,happens after we embed them,not
transformers/src/transformers/modeling_bart.py,487,assert input_ids.ne(self.padding_idx).any(),not
transformers/src/transformers/modeling_bart.py,494,"Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)",not
transformers/src/transformers/modeling_bart.py,498,decoder layers,not
transformers/src/transformers/modeling_bart.py,503,add LayerDrop (see https://arxiv.org/abs/1909.11556 for description),not
transformers/src/transformers/modeling_bart.py,524,last layer of mbart,not
transformers/src/transformers/modeling_bart.py,529,"Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)",not
transformers/src/transformers/modeling_bart.py,557,otherwise self_attention,not
transformers/src/transformers/modeling_bart.py,591,get here for encoder decoder cause of static_kv,not
transformers/src/transformers/modeling_bart.py,592,"reuse k,v and encoder_padding_mask",not
transformers/src/transformers/modeling_bart.py,595,previous time steps are cached - no need to recompute key and value if they are static,not
transformers/src/transformers/modeling_bart.py,622,Update cache,not
transformers/src/transformers/modeling_bart.py,638,This is part of a workaround to get around fork/join parallelism not supporting Optional types.,SATD
transformers/src/transformers/modeling_bart.py,643,don't attend to padding symbols,not
transformers/src/transformers/modeling_bart.py,663,"saved states are stored with shape (bsz, num_heads, seq_len, head_dim)",not
transformers/src/transformers/modeling_bart.py,697,"saved key padding masks have shape (bsz, seq_len)",not
transformers/src/transformers/modeling_bart.py,720,This can trivially be shared with RobertaClassificationHead,not
transformers/src/transformers/modeling_bart.py,750,if padding_idx is specified then offset the embedding ids by,not
transformers/src/transformers/modeling_bart.py,751,this index and adjust num_embeddings appropriately,not
transformers/src/transformers/modeling_bart.py,753,WHY?,SATD
transformers/src/transformers/modeling_bart.py,758,the position is our current step in the decoded sequence,not
transformers/src/transformers/modeling_bart.py,787,Public API,not
transformers/src/transformers/modeling_bart.py,821,make masks if user doesn't supply,not
transformers/src/transformers/modeling_bart.py,837,"decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)",not
transformers/src/transformers/modeling_bart.py,847,Attention and hidden_states will be [] or None if they aren't needed,not
transformers/src/transformers/modeling_bart.py,862,make it on the fly,not
transformers/src/transformers/modeling_bart.py,957,"Add cache, hidden states and attention if they are here",not
transformers/src/transformers/modeling_bart.py,960,TODO(SS): do we need to ignore pad tokens in lm_labels?,SATD
transformers/src/transformers/modeling_bart.py,969,"first step, decoder_cached_states are empty",not
transformers/src/transformers/modeling_bart.py,975,encoder_outputs is defined. input_ids not needed,not
transformers/src/transformers/modeling_bart.py,980,change this to avoid caching (presumably for debugging),not
transformers/src/transformers/modeling_bart.py,1007,get the correct batch idx from decoder layer's batch dim for cross and self-attn,not
transformers/src/transformers/modeling_bart.py,1023,make it on the fly,not
transformers/src/transformers/modeling_bart.py,1093,last hidden state,not
transformers/src/transformers/modeling_bart.py,1099,Prepend logits,not
transformers/src/transformers/modeling_bart.py,1100,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_bart.py,1101,"prepend loss to output,",not
transformers/src/transformers/modeling_bart.py,1126,This line breaks for odd n_pos,not
transformers/src/transformers/modeling_bart.py,1137,called before slicing,not
transformers/src/transformers/modeling_bart.py,1139,"starts at 0, ends at 1-seq_len",not
transformers/src/transformers/modeling_tf_roberta.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_roberta.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_roberta.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_roberta.py,4,,not
transformers/src/transformers/modeling_tf_roberta.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_roberta.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_roberta.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_roberta.py,8,,not
transformers/src/transformers/modeling_tf_roberta.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_roberta.py,10,,not
transformers/src/transformers/modeling_tf_roberta.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_roberta.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_roberta.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_roberta.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_roberta.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_roberta.py,76,Create the position ids from the input token ids. Any padded tokens remain padded.,not
transformers/src/transformers/modeling_tf_roberta.py,240,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_roberta.py,241,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_roberta.py,253,project back to size of vocabulary with bias,not
transformers/src/transformers/modeling_tf_roberta.py,305,Add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_roberta.py,307,"prediction_scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_roberta.py,327,take <s> token (equiv. to [CLS]),not
transformers/src/transformers/modeling_tf_roberta.py,386,"logits, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_roberta.py,442,add hidden states and attention if they are here,not
transformers/src/transformers/modeling_tf_roberta.py,444,"scores, (hidden_states), (attentions)",not
transformers/src/transformers/modeling_tf_roberta.py,509,"start_logits, end_logits, (hidden_states), (attentions)",not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,33,Load checkpoint,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,38,We have the base model one level deeper than the original XLM repository,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,52,Save pytorch-model,not
transformers/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,71,Required parameters,not
transformers/src/transformers/tokenization_ctrl.py,1,coding=utf-8,not
transformers/src/transformers/tokenization_ctrl.py,2,Copyright 2018 Salesforce and The HuggingFace Inc. team.,not
transformers/src/transformers/tokenization_ctrl.py,3,,not
transformers/src/transformers/tokenization_ctrl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/tokenization_ctrl.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/tokenization_ctrl.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/tokenization_ctrl.py,7,,not
transformers/src/transformers/tokenization_ctrl.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/tokenization_ctrl.py,9,,not
transformers/src/transformers/tokenization_ctrl.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/tokenization_ctrl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/tokenization_ctrl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/tokenization_ctrl.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/tokenization_ctrl.py,14,limitations under the License.,not
transformers/src/transformers/tokenization_ctrl.py,263,"def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):",not
transformers/src/transformers/tokenization_ctrl.py,264,"filtered_tokens = ' '.join(self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens))",not
transformers/src/transformers/tokenization_ctrl.py,265,"tokens_generated_so_far = re.sub('(@@ )', '', string=filtered_tokens)",not
transformers/src/transformers/tokenization_ctrl.py,266,"tokens_generated_so_far = re.sub('(@@ ?$)', '', string=tokens_generated_so_far)",not
transformers/src/transformers/tokenization_ctrl.py,267,return ''.join(tokens_generated_so_far),not
transformers/src/transformers/modeling_tf_ctrl.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_ctrl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_ctrl.py,4,,not
transformers/src/transformers/modeling_tf_ctrl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_ctrl.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_ctrl.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_ctrl.py,8,,not
transformers/src/transformers/modeling_tf_ctrl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_ctrl.py,10,,not
transformers/src/transformers/modeling_tf_ctrl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_ctrl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_ctrl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_ctrl.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_ctrl.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_ctrl.py,41,create the sinusoidal pattern for the positional encoding,not
transformers/src/transformers/modeling_tf_ctrl.py,47,"pos_encoding = tf.cast(np.concatenate([sines, cosines], axis=-1)[np.newaxis, ...], dtype=tf.float32)",not
transformers/src/transformers/modeling_tf_ctrl.py,53,calculate attention,not
transformers/src/transformers/modeling_tf_ctrl.py,63,Apply the attention mask,not
transformers/src/transformers/modeling_tf_ctrl.py,68,Mask heads if we want to,not
transformers/src/transformers/modeling_tf_ctrl.py,113,to cope with keras serialization,not
transformers/src/transformers/modeling_tf_ctrl.py,114,we need to cast `use_cache` to correct bool,not
transformers/src/transformers/modeling_tf_ctrl.py,115,if it is a tensor,not
transformers/src/transformers/modeling_tf_ctrl.py,263,"If using past key value states, only the last tokens",not
transformers/src/transformers/modeling_tf_ctrl.py,264,should be given as an input,not
transformers/src/transformers/modeling_tf_ctrl.py,292,Attention mask.,not
transformers/src/transformers/modeling_tf_ctrl.py,294,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_tf_ctrl.py,295,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_tf_ctrl.py,296,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_tf_ctrl.py,297,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_tf_ctrl.py,298,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_tf_ctrl.py,301,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_tf_ctrl.py,302,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_tf_ctrl.py,303,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_tf_ctrl.py,304,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_tf_ctrl.py,305,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_tf_ctrl.py,312,Prepare head mask if needed,not
transformers/src/transformers/modeling_tf_ctrl.py,313,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_tf_ctrl.py,314,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_tf_ctrl.py,315,head_mask has shape n_layer x batch x n_heads x N x N,not
transformers/src/transformers/modeling_tf_ctrl.py,369,let the number of heads free (-1) so we can extract attention even after head pruning,not
transformers/src/transformers/modeling_tf_ctrl.py,518,"The output weights are the same as the input embeddings, but there is",not
transformers/src/transformers/modeling_tf_ctrl.py,519,an output-only bias for each token.,not
transformers/src/transformers/modeling_tf_ctrl.py,548,only last token for inputs_ids if past is defined in kwargs,not
transformers/src/transformers/modeling_tf_ctrl.py,597,"lm_logits, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_gpt2.py,1,coding=utf-8,not
transformers/src/transformers/modeling_gpt2.py,2,Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.,not
transformers/src/transformers/modeling_gpt2.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_gpt2.py,4,,not
transformers/src/transformers/modeling_gpt2.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_gpt2.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_gpt2.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_gpt2.py,8,,not
transformers/src/transformers/modeling_gpt2.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_gpt2.py,10,,not
transformers/src/transformers/modeling_gpt2.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_gpt2.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_gpt2.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_gpt2.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_gpt2.py,15,limitations under the License.,not
transformers/src/transformers/modeling_gpt2.py,57,Load weights from TF model,not
transformers/src/transformers/modeling_gpt2.py,68,"skip ""model/""",not
transformers/src/transformers/modeling_gpt2.py,103,in Attention: n_state=768 (nx=n_embd),not
transformers/src/transformers/modeling_gpt2.py,104,[switch nx => n_state from Block to Attention to keep identical to TF implem],not
transformers/src/transformers/modeling_gpt2.py,124,Convert to set and emove already pruned heads,not
transformers/src/transformers/modeling_gpt2.py,126,Compute how many pruned heads are before the head and move the index accordingly,not
transformers/src/transformers/modeling_gpt2.py,133,Prune conv1d layers,not
transformers/src/transformers/modeling_gpt2.py,137,Update hyper params,not
transformers/src/transformers/modeling_gpt2.py,151,Apply the attention mask,not
transformers/src/transformers/modeling_gpt2.py,157,Mask heads if we want to,not
transformers/src/transformers/modeling_gpt2.py,169,in Tensorflow implem: fct merge_states,not
transformers/src/transformers/modeling_gpt2.py,173,in Tensorflow implem: fct split_states,not
transformers/src/transformers/modeling_gpt2.py,175,"(batch, head, head_features, seq_length)",not
transformers/src/transformers/modeling_gpt2.py,177,"(batch, head, seq_length, head_features)",not
transformers/src/transformers/modeling_gpt2.py,186,transpose back cf below,not
transformers/src/transformers/modeling_gpt2.py,191,transpose to have same shapes for stacking,not
transformers/src/transformers/modeling_gpt2.py,203,"a, present, (attentions)",not
transformers/src/transformers/modeling_gpt2.py,207,in MLP: n_state=3072 (4 * n_embd),not
transformers/src/transformers/modeling_gpt2.py,238,"output_attn: a, present, (attentions)",not
transformers/src/transformers/modeling_gpt2.py,245,"x, present, (attentions)",not
transformers/src/transformers/modeling_gpt2.py,265,Slightly different from the TF version which uses truncated_normal for initialization,not
transformers/src/transformers/modeling_gpt2.py,266,cf https://github.com/pytorch/pytorch/pull/5617,not
transformers/src/transformers/modeling_gpt2.py,413,"If using past key value states, only the last tokens",not
transformers/src/transformers/modeling_gpt2.py,414,should be given as an input,not
transformers/src/transformers/modeling_gpt2.py,450,Attention mask.,not
transformers/src/transformers/modeling_gpt2.py,454,We create a 3D attention mask from a 2D tensor mask.,not
transformers/src/transformers/modeling_gpt2.py,455,"Sizes are [batch_size, 1, 1, to_seq_length]",not
transformers/src/transformers/modeling_gpt2.py,456,"So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]",not
transformers/src/transformers/modeling_gpt2.py,457,this attention mask is more simple than the triangular masking of causal attention,not
transformers/src/transformers/modeling_gpt2.py,458,"used in OpenAI GPT, we just need to prepare the broadcast dimension here.",not
transformers/src/transformers/modeling_gpt2.py,461,Since attention_mask is 1.0 for positions we want to attend and 0.0 for,not
transformers/src/transformers/modeling_gpt2.py,462,"masked positions, this operation will create a tensor which is 0.0 for",not
transformers/src/transformers/modeling_gpt2.py,463,positions we want to attend and -10000.0 for masked positions.,not
transformers/src/transformers/modeling_gpt2.py,464,"Since we are adding it to the raw scores before the softmax, this is",not
transformers/src/transformers/modeling_gpt2.py,465,effectively the same as removing these entirely.,not
transformers/src/transformers/modeling_gpt2.py,466,fp16 compatibility,not
transformers/src/transformers/modeling_gpt2.py,469,Prepare head mask if needed,not
transformers/src/transformers/modeling_gpt2.py,470,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_gpt2.py,471,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_gpt2.py,472,head_mask has shape n_layer x batch x n_heads x N x N,not
transformers/src/transformers/modeling_gpt2.py,512,Add last hidden state,not
transformers/src/transformers/modeling_gpt2.py,522,let the number of heads free (-1) so we can extract attention even after head pruning,not
transformers/src/transformers/modeling_gpt2.py,526,"last hidden state, (presents), (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_gpt2.py,546,only last token for inputs_ids if past is defined in kwargs,not
transformers/src/transformers/modeling_gpt2.py,623,Shift so that tokens < n predict n,not
transformers/src/transformers/modeling_gpt2.py,626,Flatten the tokens,not
transformers/src/transformers/modeling_gpt2.py,631,"(loss), lm_logits, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/modeling_gpt2.py,762,"(lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)",not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,1,coding=utf-8,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,3,,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,7,,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,9,,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,14,limitations under the License.,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,49,disable dropout,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,59,PyTorch default used in fairseq,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,68,Now let's copy all the weights.,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,69,Embeddings,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,74,just zero them out b/c RoBERTa doesn't use them.,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,79,Encoder: start of layer,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,83,self attention,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,99,self-attention output,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,107,intermediate,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,113,output,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,120,end of layer,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,128,LM Head,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,136,Let's check that we get the same results.,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,137,batch of size 1,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,146,~ 1e-7,not
transformers/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py,159,Required parameters,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,1,coding=utf-8,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,4,,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,8,,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,10,,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,15,limitations under the License.,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,26,CUDA_MAJOR = int(torch.version.cuda.split('.')[0]),not
transformers/src/transformers/modeling_transfo_xl_utilities.py,27,CUDA_MINOR = int(torch.version.cuda.split('.')[1]),not
transformers/src/transformers/modeling_transfo_xl_utilities.py,76,if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,79,else:,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,80,"logit = torch.einsum('bd,de,ev->bv', (hidden, proj, weight.t()))",not
transformers/src/transformers/modeling_transfo_xl_utilities.py,81,if bias is not None:,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,82,logit = logit + bias,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,102,Shift so that tokens < n predict n,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,119,construct weights and biases,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,175,No probability for the head cluster,not
transformers/src/transformers/modeling_transfo_xl_utilities.py,210,construct weights and biases,not
transformers/src/transformers/configuration_mmbt.py,1,coding=utf-8,not
transformers/src/transformers/configuration_mmbt.py,2,"Copyright (c) Facebook, Inc. and its affiliates.",not
transformers/src/transformers/configuration_mmbt.py,3,Copyright (c) HuggingFace Inc. team.,not
transformers/src/transformers/configuration_mmbt.py,4,,not
transformers/src/transformers/configuration_mmbt.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_mmbt.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_mmbt.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_mmbt.py,8,,not
transformers/src/transformers/configuration_mmbt.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_mmbt.py,10,,not
transformers/src/transformers/configuration_mmbt.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_mmbt.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_mmbt.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_mmbt.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_mmbt.py,15,limitations under the License.,not
transformers/src/transformers/configuration_xlm.py,1,coding=utf-8,not
transformers/src/transformers/configuration_xlm.py,2,"Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.",not
transformers/src/transformers/configuration_xlm.py,3,,not
transformers/src/transformers/configuration_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_xlm.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_xlm.py,7,,not
transformers/src/transformers/configuration_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_xlm.py,9,,not
transformers/src/transformers/configuration_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_xlm.py,14,limitations under the License.,not
transformers/src/transformers/configuration_xlm.py,240,For backward compatibility,not
transformers/src/transformers/configuration_xlm.py,244,For backward compatibility,not
transformers/src/transformers/configuration_encoder_decoder.py,1,coding=utf-8,not
transformers/src/transformers/configuration_encoder_decoder.py,2,Copyright 2020 The HuggingFace Inc. team.,not
transformers/src/transformers/configuration_encoder_decoder.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/configuration_encoder_decoder.py,4,,not
transformers/src/transformers/configuration_encoder_decoder.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_encoder_decoder.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_encoder_decoder.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_encoder_decoder.py,8,,not
transformers/src/transformers/configuration_encoder_decoder.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_encoder_decoder.py,10,,not
transformers/src/transformers/configuration_encoder_decoder.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_encoder_decoder.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_encoder_decoder.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_encoder_decoder.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_encoder_decoder.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,1,coding=utf-8,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,4,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,8,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,10,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,15,limitations under the License.,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,40,device ids,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,43,'$1___$2' is replaced by $2 (can be used to duplicate or remove layers in TF2.0 vs PyTorch),not
transformers/src/transformers/modeling_tf_pytorch_utils.py,46,'_._' is replaced by a level separation (can be used to convert TF2.0 lists in PyTorch nn.ModulesList),not
transformers/src/transformers/modeling_tf_pytorch_utils.py,47,Remove empty levels at the end,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,48,Convert from TF2.0 '/' separators to PyTorch '.' separators,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,49,Remove level zero,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,51,When should we transpose the weights,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,54,Convert standard TF2.0 names in PyTorch names,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,60,Remove prefix if needed,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,68,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,69,PyTorch => TF 2.0,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,70,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,77,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,78,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,111,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,112,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,125,Make sure model is built,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,127,Adapt state dict - TODO remove this and update the AWS weights files instead,SATD
transformers/src/transformers/modeling_tf_pytorch_utils.py,128,Convert old format to new format if needed from a PyTorch state_dict,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,143,Make sure we are able to load PyTorch base models as well as derived models (with heads),not
transformers/src/transformers/modeling_tf_pytorch_utils.py,144,"TF models always have a prefix, some of PyTorch models (base ones) don't",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,159,Find associated numpy array in pytorch model state dict,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,183,"logger.warning(""Initialize TF weight {}"".format(symbolic_weight.name))",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,191,Make sure restore ops are run,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,200,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,201,TF 2.0 => PyTorch,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,202,,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,211,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,212,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,224,Instantiate and load the associated TF 2.0 model,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,225,"Add ""TF"" at the beggining",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,233,Make sure model is built,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,252,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,253,noqa: F401,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,264,Make sure we are able to load PyTorch base models as well as derived models (with heads),not
transformers/src/transformers/modeling_tf_pytorch_utils.py,265,"TF models always have a prefix, some of PyTorch models (base ones) don't",not
transformers/src/transformers/modeling_tf_pytorch_utils.py,270,Build a map from potential PyTorch weight names to TF 2.0 Variables,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,282,Handle PyTorch shared weight ()not duplicated in TF 2.0,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,287,Find associated numpy array in pytorch model state dict,not
transformers/src/transformers/modeling_tf_pytorch_utils.py,311,"logger.warning(""Initialize PyTorch weight {}"".format(pt_weight_name))",not
transformers/src/transformers/modeling_transfo_xl.py,1,coding=utf-8,not
transformers/src/transformers/modeling_transfo_xl.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/src/transformers/modeling_transfo_xl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/modeling_transfo_xl.py,4,,not
transformers/src/transformers/modeling_transfo_xl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/modeling_transfo_xl.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/modeling_transfo_xl.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/modeling_transfo_xl.py,8,,not
transformers/src/transformers/modeling_transfo_xl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/modeling_transfo_xl.py,10,,not
transformers/src/transformers/modeling_transfo_xl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/modeling_transfo_xl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/modeling_transfo_xl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/modeling_transfo_xl.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/modeling_transfo_xl.py,15,limitations under the License.,not
transformers/src/transformers/modeling_transfo_xl.py,48,We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax,not
transformers/src/transformers/modeling_transfo_xl.py,63,I don't think this is implemented in the TF code,not
transformers/src/transformers/modeling_transfo_xl.py,67,Now load the rest of the transformer,not
transformers/src/transformers/modeling_transfo_xl.py,70,Embeddings,not
transformers/src/transformers/modeling_transfo_xl.py,75,Transformer blocks,not
transformers/src/transformers/modeling_transfo_xl.py,94,Relative positioning biases,not
transformers/src/transformers/modeling_transfo_xl.py,120,Build TF to PyTorch weights loading map,not
transformers/src/transformers/modeling_transfo_xl.py,123,Load weights from TF model,not
transformers/src/transformers/modeling_transfo_xl.py,134,adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v,not
transformers/src/transformers/modeling_transfo_xl.py,135,which are not required for using pretrained model,not
transformers/src/transformers/modeling_transfo_xl.py,139,Here we will split the TF weights,not
transformers/src/transformers/modeling_transfo_xl.py,207,layer normalization + positionwise feed-forward,not
transformers/src/transformers/modeling_transfo_xl.py,210,residual connection,not
transformers/src/transformers/modeling_transfo_xl.py,213,positionwise feed-forward,not
transformers/src/transformers/modeling_transfo_xl.py,216,residual connection + layer normalization,not
transformers/src/transformers/modeling_transfo_xl.py,259,Biases are not shared,not
transformers/src/transformers/modeling_transfo_xl.py,304,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_transfo_xl.py,305,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_transfo_xl.py,306,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_transfo_xl.py,308,qlen x n_head x d_head,not
transformers/src/transformers/modeling_transfo_xl.py,310,compute attention score,not
transformers/src/transformers/modeling_transfo_xl.py,311,qlen x bsz x n_head x d_head,not
transformers/src/transformers/modeling_transfo_xl.py,312,qlen x klen x bsz x n_head,not
transformers/src/transformers/modeling_transfo_xl.py,315,qlen x klen x bsz x n_head,not
transformers/src/transformers/modeling_transfo_xl.py,318,[qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_transfo_xl.py,322,compute attention probability,not
transformers/src/transformers/modeling_transfo_xl.py,324,Switch to bool,not
transformers/src/transformers/modeling_transfo_xl.py,338,[qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_transfo_xl.py,342,Mask heads if we want to,not
transformers/src/transformers/modeling_transfo_xl.py,346,compute attention vector,not
transformers/src/transformers/modeling_transfo_xl.py,349,[qlen x bsz x n_head x d_head],not
transformers/src/transformers/modeling_transfo_xl.py,352,linear projection,not
transformers/src/transformers/modeling_transfo_xl.py,357,residual connection,not
transformers/src/transformers/modeling_transfo_xl.py,360,residual connection + layer normalization,not
transformers/src/transformers/modeling_transfo_xl.py,585,the default attention,not
transformers/src/transformers/modeling_transfo_xl.py,605,learnable embeddings and absolute embeddings are not used in our pretrained checkpoints,not
transformers/src/transformers/modeling_transfo_xl.py,606,Removed them to avoid maintaining dead code,not
transformers/src/transformers/modeling_transfo_xl.py,611,default attention,not
transformers/src/transformers/modeling_transfo_xl.py,613,learnable embeddings and absolute embeddings,not
transformers/src/transformers/modeling_transfo_xl.py,614,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_transfo_xl.py,649,does not deal with None,not
transformers/src/transformers/modeling_transfo_xl.py,653,mems is not None,not
transformers/src/transformers/modeling_transfo_xl.py,656,There are `mlen + qlen` steps that can be cached into mems,not
transformers/src/transformers/modeling_transfo_xl.py,657,"For the next step, the last `ext_len` of the `qlen` tokens",not
transformers/src/transformers/modeling_transfo_xl.py,658,"will be used as the extended context. Hence, we only cache",not
transformers/src/transformers/modeling_transfo_xl.py,659,the tokens from `mlen + qlen - self.ext_len - self.mem_len`,not
transformers/src/transformers/modeling_transfo_xl.py,660,to `mlen + qlen - self.ext_len`.,not
transformers/src/transformers/modeling_transfo_xl.py,707,"the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library",not
transformers/src/transformers/modeling_transfo_xl.py,708,"so we transpose here from shape [bsz, len] to shape [len, bsz]",not
transformers/src/transformers/modeling_transfo_xl.py,723,Prepare head mask if needed,not
transformers/src/transformers/modeling_transfo_xl.py,724,1.0 in head_mask indicate we keep the head,not
transformers/src/transformers/modeling_transfo_xl.py,725,attention_probs has shape bsz x n_heads x N x N,not
transformers/src/transformers/modeling_transfo_xl.py,726,input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer),not
transformers/src/transformers/modeling_transfo_xl.py,727,and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head],not
transformers/src/transformers/modeling_transfo_xl.py,736,switch to fload if need + fp16 compatibility,not
transformers/src/transformers/modeling_transfo_xl.py,754,-1,not
transformers/src/transformers/modeling_transfo_xl.py,762,default,not
transformers/src/transformers/modeling_transfo_xl.py,780,learnable embeddings and absolute embeddings,not
transformers/src/transformers/modeling_transfo_xl.py,781,Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint,not
transformers/src/transformers/modeling_transfo_xl.py,787,"We transpose back here to shape [bsz, len, hidden_dim]",not
transformers/src/transformers/modeling_transfo_xl.py,790,"Add last layer and transpose to library standard shape [bsz, len, hidden_dim]",not
transformers/src/transformers/modeling_transfo_xl.py,795,"Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]",not
transformers/src/transformers/modeling_transfo_xl.py,799,"last hidden state, new_mems, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_transfo_xl.py,915,"(loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)",not
transformers/src/transformers/modeling_transfo_xl.py,928,if past is defined in model kwargs then use it for faster decoding,not
transformers/src/transformers/configuration_distilbert.py,1,coding=utf-8,not
transformers/src/transformers/configuration_distilbert.py,2,"Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.",not
transformers/src/transformers/configuration_distilbert.py,3,,not
transformers/src/transformers/configuration_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/configuration_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/src/transformers/configuration_distilbert.py,6,You may obtain a copy of the License at,not
transformers/src/transformers/configuration_distilbert.py,7,,not
transformers/src/transformers/configuration_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/configuration_distilbert.py,9,,not
transformers/src/transformers/configuration_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/configuration_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/configuration_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/configuration_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/src/transformers/configuration_distilbert.py,14,limitations under the License.,not
transformers/src/transformers/commands/user.py,25,s3,not
transformers/src/transformers/commands/user.py,35,upload,not
transformers/src/transformers/commands/user.py,88,"probably invalid credentials, display error message.",not
transformers/src/transformers/commands/user.py,181,"(filepath, filename)",not
transformers/src/transformers/commands/serving.py,205,Check we don't have empty string,not
transformers/src/transformers/commands/serving.py,210,Forward through the model,not
transformers/src/transformers/commands/run.py,8,pylint: disable=invalid-name,not
transformers/src/transformers/commands/run.py,91,Saving data,not
transformers/src/transformers/commands/env.py,35,deprecated in v2.1,not
transformers/src/transformers/commands/env.py,38,"returns list of devices, convert to bool",not
transformers/src/transformers/commands/train.py,13,TF training parameters,not
transformers/src/transformers/commands/train.py,143,Save trained pipeline,not
transformers/src/transformers/data/data_collator.py,46,In this method we'll make the assumption that all `features` in the batch,not
transformers/src/transformers/data/data_collator.py,47,have the same attributes.,not
transformers/src/transformers/data/data_collator.py,48,So we will look at the first element as a proxy for what attributes exist,not
transformers/src/transformers/data/data_collator.py,49,on the whole batch.,not
transformers/src/transformers/data/data_collator.py,52,Special handling for labels.,not
transformers/src/transformers/data/data_collator.py,53,Ensure that tensor is created with the correct type,not
transformers/src/transformers/data/data_collator.py,54,"(it should be automatically the case, but let's make sure of it.)",not
transformers/src/transformers/data/data_collator.py,70,Handling of all other possible attributes.,not
transformers/src/transformers/data/data_collator.py,71,"Again, we will use the first element to figure out which key/values are not None for this model.",not
transformers/src/transformers/data/data_collator.py,122,We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa),not
transformers/src/transformers/data/data_collator.py,132,We only compute loss on masked tokens,not
transformers/src/transformers/data/data_collator.py,134,"80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])",not
transformers/src/transformers/data/data_collator.py,138,"10% of the time, we replace masked input tokens with random word",not
transformers/src/transformers/data/data_collator.py,143,The rest of the time (10% of the time) we keep the masked input tokens unchanged,not
transformers/src/transformers/data/__init__.py,1,flake8: noqa,not
transformers/src/transformers/data/__init__.py,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",not
transformers/src/transformers/data/__init__.py,3,"module, but to preserve other warnings. So, don't check this module at all.",not
transformers/src/transformers/data/datasets/language_modeling.py,35,"Make sure only the first process in distributed training processes the dataset,",not
transformers/src/transformers/data/datasets/language_modeling.py,36,and the others will use the cache.,not
transformers/src/transformers/data/datasets/language_modeling.py,55,Truncate in block of block_size,not
transformers/src/transformers/data/datasets/language_modeling.py,59,Note that we are losing the last truncated example here for the sake of simplicity (no padding),not
transformers/src/transformers/data/datasets/language_modeling.py,60,"If your dataset is small, first you should loook for a bigger one :-) and second you",not
transformers/src/transformers/data/datasets/language_modeling.py,61,can change this behavior by adding (model specific) padding.,not
transformers/src/transformers/data/datasets/language_modeling.py,85,"Here, we do not cache the features, operating under the assumption",not
transformers/src/transformers/data/datasets/language_modeling.py,86,that we will soon use fast multithreaded tokenizers from the,not
transformers/src/transformers/data/datasets/language_modeling.py,87,`tokenizers` repo everywhere =),not
transformers/src/transformers/data/datasets/glue.py,70,Load data features from cache or dataset file,not
transformers/src/transformers/data/datasets/glue.py,78,"Make sure only the first process in distributed training processes the dataset,",not
transformers/src/transformers/data/datasets/glue.py,79,and the others will use the cache.,not
transformers/src/transformers/data/datasets/glue.py,97,HACK(label indices are swapped in RoBERTa pretrained model),SATD
transformers/src/transformers/data/datasets/glue.py,115,^ This seems to take a lot of time so I want to investigate why and how we can improve.,not
transformers/src/transformers/data/datasets/__init__.py,1,flake8: noqa,not
transformers/src/transformers/data/datasets/__init__.py,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",not
transformers/src/transformers/data/datasets/__init__.py,3,"module, but to preserve other warnings. So, don't check this module at all.",not
transformers/src/transformers/data/metrics/squad_metrics.py,60,"If either is no-answer, then F1 is 1 if they agree, 0 otherwise",not
transformers/src/transformers/data/metrics/squad_metrics.py,82,"For unanswerable questions, only correct answer is empty string",not
transformers/src/transformers/data/metrics/squad_metrics.py,245,"When we created the data, we kept track of the alignment between original",not
transformers/src/transformers/data/metrics/squad_metrics.py,246,(whitespace tokenized) tokens and our WordPiece tokenized tokens. So,not
transformers/src/transformers/data/metrics/squad_metrics.py,247,now `orig_text` contains the span of our original text corresponding to the,not
transformers/src/transformers/data/metrics/squad_metrics.py,248,span that we predicted.,not
transformers/src/transformers/data/metrics/squad_metrics.py,249,,not
transformers/src/transformers/data/metrics/squad_metrics.py,250,"However, `orig_text` may contain extra characters that we don't want in",not
transformers/src/transformers/data/metrics/squad_metrics.py,251,our prediction.,not
transformers/src/transformers/data/metrics/squad_metrics.py,252,,not
transformers/src/transformers/data/metrics/squad_metrics.py,253,"For example, let's say:",not
transformers/src/transformers/data/metrics/squad_metrics.py,254,pred_text = steve smith,not
transformers/src/transformers/data/metrics/squad_metrics.py,255,orig_text = Steve Smith's,not
transformers/src/transformers/data/metrics/squad_metrics.py,256,,not
transformers/src/transformers/data/metrics/squad_metrics.py,257,"We don't want to return `orig_text` because it contains the extra ""'s"".",not
transformers/src/transformers/data/metrics/squad_metrics.py,258,,not
transformers/src/transformers/data/metrics/squad_metrics.py,259,We don't want to return `pred_text` because it's already been normalized,not
transformers/src/transformers/data/metrics/squad_metrics.py,260,(the SQuAD eval script also does punctuation stripping/lower casing but,not
transformers/src/transformers/data/metrics/squad_metrics.py,261,our tokenizer does additional normalization like stripping accent,not
transformers/src/transformers/data/metrics/squad_metrics.py,262,characters).,not
transformers/src/transformers/data/metrics/squad_metrics.py,263,,not
transformers/src/transformers/data/metrics/squad_metrics.py,264,"What we really want to return is ""Steve Smith"".",not
transformers/src/transformers/data/metrics/squad_metrics.py,265,,not
transformers/src/transformers/data/metrics/squad_metrics.py,266,"Therefore, we have to apply a semi-complicated alignment heuristic between",not
transformers/src/transformers/data/metrics/squad_metrics.py,267,`pred_text` and `orig_text` to get a character-to-character alignment. This,not
transformers/src/transformers/data/metrics/squad_metrics.py,268,can fail in certain cases in which case we just return `orig_text`.,not
transformers/src/transformers/data/metrics/squad_metrics.py,281,"We first tokenize `orig_text`, strip whitespace from the result",not
transformers/src/transformers/data/metrics/squad_metrics.py,282,"and `pred_text`, and check if they are the same length. If they are",not
transformers/src/transformers/data/metrics/squad_metrics.py,283,"NOT the same length, the heuristic has failed. If they are the same",not
transformers/src/transformers/data/metrics/squad_metrics.py,284,"length, we assume the characters are one-to-one aligned.",not
transformers/src/transformers/data/metrics/squad_metrics.py,304,We then project the characters in `pred_text` back to `orig_text` using,not
transformers/src/transformers/data/metrics/squad_metrics.py,305,the character-to-character alignment.,not
transformers/src/transformers/data/metrics/squad_metrics.py,402,pylint: disable=invalid-name,not
transformers/src/transformers/data/metrics/squad_metrics.py,414,keep track of the minimum score of null start+end of position 0,not
transformers/src/transformers/data/metrics/squad_metrics.py,415,large and positive,not
transformers/src/transformers/data/metrics/squad_metrics.py,416,the paragraph slice with min null score,not
transformers/src/transformers/data/metrics/squad_metrics.py,417,the start logit at the slice with min null score,not
transformers/src/transformers/data/metrics/squad_metrics.py,418,the end logit at the slice with min null score,not
transformers/src/transformers/data/metrics/squad_metrics.py,423,"if we could have irrelevant answers, get the min score of irrelevant",not
transformers/src/transformers/data/metrics/squad_metrics.py,433,"We could hypothetically create invalid predictions, e.g., predict",not
transformers/src/transformers/data/metrics/squad_metrics.py,434,that the start of the span is in the question. We throw out all,not
transformers/src/transformers/data/metrics/squad_metrics.py,435,invalid predictions.,not
transformers/src/transformers/data/metrics/squad_metrics.py,472,pylint: disable=invalid-name,not
transformers/src/transformers/data/metrics/squad_metrics.py,482,this is a non-null prediction,not
transformers/src/transformers/data/metrics/squad_metrics.py,490,"tok_text = "" "".join(tok_tokens)",not
transformers/src/transformers/data/metrics/squad_metrics.py,491,,not
transformers/src/transformers/data/metrics/squad_metrics.py,492,# De-tokenize WordPieces that have been split off.,not
transformers/src/transformers/data/metrics/squad_metrics.py,493,"tok_text = tok_text.replace("" ##"", """")",not
transformers/src/transformers/data/metrics/squad_metrics.py,494,"tok_text = tok_text.replace(""##"", """")",not
transformers/src/transformers/data/metrics/squad_metrics.py,496,Clean whitespace,not
transformers/src/transformers/data/metrics/squad_metrics.py,511,"if we didn't include the empty option in the n-best, include it",not
transformers/src/transformers/data/metrics/squad_metrics.py,516,In very rare edge cases we could only have single null prediction.,not
transformers/src/transformers/data/metrics/squad_metrics.py,517,So we just create a nonce prediction in this case to avoid failure.,not
transformers/src/transformers/data/metrics/squad_metrics.py,521,In very rare edge cases we could have no valid predictions. So we,not
transformers/src/transformers/data/metrics/squad_metrics.py,522,just create a nonce prediction in this case to avoid failure.,not
transformers/src/transformers/data/metrics/squad_metrics.py,552,"predict """" iff the null score - the score of best non-null > threshold",not
transformers/src/transformers/data/metrics/squad_metrics.py,596,pylint: disable=invalid-name,not
transformers/src/transformers/data/metrics/squad_metrics.py,600,pylint: disable=invalid-name,not
transformers/src/transformers/data/metrics/squad_metrics.py,605,"logger.info(""Writing nbest to: %s"" % (output_nbest_file))",not
transformers/src/transformers/data/metrics/squad_metrics.py,623,keep track of the minimum score of null start+end of position 0,not
transformers/src/transformers/data/metrics/squad_metrics.py,624,large and positive,not
transformers/src/transformers/data/metrics/squad_metrics.py,631,"if we could have irrelevant answers, get the min score of irrelevant",not
transformers/src/transformers/data/metrics/squad_metrics.py,644,"We could hypothetically create invalid predictions, e.g., predict",not
transformers/src/transformers/data/metrics/squad_metrics.py,645,that the start of the span is in the question. We throw out all,not
transformers/src/transformers/data/metrics/squad_metrics.py,646,invalid predictions.,not
transformers/src/transformers/data/metrics/squad_metrics.py,681,XLNet un-tokenizer,not
transformers/src/transformers/data/metrics/squad_metrics.py,682,Let's keep it simple for now and see if we need all this later.,not
transformers/src/transformers/data/metrics/squad_metrics.py,683,,not
transformers/src/transformers/data/metrics/squad_metrics.py,684,tok_start_to_orig_index = feature.tok_start_to_orig_index,not
transformers/src/transformers/data/metrics/squad_metrics.py,685,tok_end_to_orig_index = feature.tok_end_to_orig_index,not
transformers/src/transformers/data/metrics/squad_metrics.py,686,start_orig_pos = tok_start_to_orig_index[pred.start_index],not
transformers/src/transformers/data/metrics/squad_metrics.py,687,end_orig_pos = tok_end_to_orig_index[pred.end_index],not
transformers/src/transformers/data/metrics/squad_metrics.py,688,paragraph_text = example.paragraph_text,not
transformers/src/transformers/data/metrics/squad_metrics.py,689,final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip(),not
transformers/src/transformers/data/metrics/squad_metrics.py,691,Previously used Bert untokenizer,not
transformers/src/transformers/data/metrics/squad_metrics.py,698,Clean whitespace,not
transformers/src/transformers/data/metrics/squad_metrics.py,719,In very rare edge cases we could have no valid predictions. So we,not
transformers/src/transformers/data/metrics/squad_metrics.py,720,just create a nonce prediction in this case to avoid failure.,not
transformers/src/transformers/data/metrics/squad_metrics.py,747,note(zhiliny): always predict best_non_null_entry,not
transformers/src/transformers/data/metrics/squad_metrics.py,748,and the evaluation script will search for the best threshold,not
transformers/src/transformers/data/metrics/__init__.py,1,coding=utf-8,not
transformers/src/transformers/data/metrics/__init__.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/data/metrics/__init__.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/data/metrics/__init__.py,4,,not
transformers/src/transformers/data/metrics/__init__.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/data/metrics/__init__.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/data/metrics/__init__.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/data/metrics/__init__.py,8,,not
transformers/src/transformers/data/metrics/__init__.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/data/metrics/__init__.py,10,,not
transformers/src/transformers/data/metrics/__init__.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/data/metrics/__init__.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/data/metrics/__init__.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/data/metrics/__init__.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/data/metrics/__init__.py,15,limitations under the License.,not
transformers/src/transformers/data/processors/utils.py,1,coding=utf-8,not
transformers/src/transformers/data/processors/utils.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/data/processors/utils.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/data/processors/utils.py,4,,not
transformers/src/transformers/data/processors/utils.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/data/processors/utils.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/data/processors/utils.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/data/processors/utils.py,8,,not
transformers/src/transformers/data/processors/utils.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/data/processors/utils.py,10,,not
transformers/src/transformers/data/processors/utils.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/data/processors/utils.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/data/processors/utils.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/data/processors/utils.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/data/processors/utils.py,15,limitations under the License.,not
transformers/src/transformers/data/processors/utils.py,208,Update examples,not
transformers/src/transformers/data/processors/utils.py,214,Update labels,not
transformers/src/transformers/data/processors/utils.py,273,The mask has 1 for real tokens and 0 for padding tokens. Only real,not
transformers/src/transformers/data/processors/utils.py,274,tokens are attended to.,not
transformers/src/transformers/data/processors/utils.py,277,Zero-pad up to the sequence length.,not
transformers/src/transformers/data/processors/squad.py,60,if len(doc_spans) == 1:,not
transformers/src/transformers/data/processors/squad.py,61,return True,not
transformers/src/transformers/data/processors/squad.py,89,Get start and end position,not
transformers/src/transformers/data/processors/squad.py,93,"If the answer cannot be found in the text, then skip this example.",not
transformers/src/transformers/data/processors/squad.py,194,Identify the position of the CLS token,not
transformers/src/transformers/data/processors/squad.py,197,p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer),not
transformers/src/transformers/data/processors/squad.py,198,Original TF implem also keep the classification token (set to 0) (not sure why...),not
transformers/src/transformers/data/processors/squad.py,204,Limit positive values to one,not
transformers/src/transformers/data/processors/squad.py,209,Set the CLS index to '0',not
transformers/src/transformers/data/processors/squad.py,216,"For training, if our document chunk does not contain an annotation",not
transformers/src/transformers/data/processors/squad.py,217,"we throw it out, since there is nothing to predict.",not
transformers/src/transformers/data/processors/squad.py,245,Can not set unique_id and example_index here. They will be set after multiple processing.,not
transformers/src/transformers/data/processors/squad.py,311,Defining helper methods,not
transformers/src/transformers/data/processors/squad.py,350,Convert to Tensors and build dataset,not
transformers/src/transformers/data/processors/squad.py,401,Why have we split the batch into a tuple? PyTorch just has a list of tensors.,SATD
transformers/src/transformers/data/processors/squad.py,640,Split on whitespace so that different tokens may be attributed to their original position.,not
transformers/src/transformers/data/processors/squad.py,655,Start and end positions only has a value during evaluation.,not
transformers/src/transformers/data/processors/glue.py,1,coding=utf-8,not
transformers/src/transformers/data/processors/glue.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/data/processors/glue.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/data/processors/glue.py,4,,not
transformers/src/transformers/data/processors/glue.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/data/processors/glue.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/data/processors/glue.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/data/processors/glue.py,8,,not
transformers/src/transformers/data/processors/glue.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/data/processors/glue.py,10,,not
transformers/src/transformers/data/processors/glue.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/data/processors/glue.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/data/processors/glue.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/data/processors/glue.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/data/processors/glue.py,15,limitations under the License.,not
transformers/src/transformers/data/processors/__init__.py,1,flake8: noqa,not
transformers/src/transformers/data/processors/__init__.py,2,"There's no way to ignore ""F401 '...' imported but unused"" warnings in this",not
transformers/src/transformers/data/processors/__init__.py,3,"module, but to preserve other warnings. So, don't check this module at all.",not
transformers/src/transformers/data/processors/xnli.py,1,coding=utf-8,not
transformers/src/transformers/data/processors/xnli.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/src/transformers/data/processors/xnli.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/src/transformers/data/processors/xnli.py,4,,not
transformers/src/transformers/data/processors/xnli.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/src/transformers/data/processors/xnli.py,6,you may not use this file except in compliance with the License.,not
transformers/src/transformers/data/processors/xnli.py,7,You may obtain a copy of the License at,not
transformers/src/transformers/data/processors/xnli.py,8,,not
transformers/src/transformers/data/processors/xnli.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/src/transformers/data/processors/xnli.py,10,,not
transformers/src/transformers/data/processors/xnli.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/src/transformers/data/processors/xnli.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/src/transformers/data/processors/xnli.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/src/transformers/data/processors/xnli.py,14,See the License for the specific language governing permissions and,not
transformers/src/transformers/data/processors/xnli.py,15,limitations under the License.,not
transformers/tests/test_tokenization_bert.py,1,coding=utf-8,not
transformers/tests/test_tokenization_bert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_bert.py,3,,not
transformers/tests/test_tokenization_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_bert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_bert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_bert.py,7,,not
transformers/tests/test_tokenization_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_bert.py,9,,not
transformers/tests/test_tokenization_bert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_bert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_bert.py,14,limitations under the License.,not
transformers/tests/test_modeling_marian.py,1,coding=utf-8,not
transformers/tests/test_modeling_marian.py,2,Copyright 2020 HuggingFace Inc. team.,not
transformers/tests/test_modeling_marian.py,3,,not
transformers/tests/test_modeling_marian.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_marian.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_marian.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_marian.py,7,,not
transformers/tests/test_modeling_marian.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_marian.py,9,,not
transformers/tests/test_modeling_marian.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_marian.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_marian.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_marian.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_marian.py,14,limitations under the License.,not
transformers/tests/test_modeling_marian.py,67,"^^ actual C++ output differs slightly: (1) des Deutschen removed, (2) """"-> ""O"", (3) tun -> machen",not
transformers/tests/test_modeling_marian.py,140,pad,not
transformers/tests/test_modeling_marian.py,174,Accents,not
transformers/tests/test_modeling_marian.py,203,Known Issue: This model generates a string of .... at the end of the translation.,not
transformers/tests/test_modeling_marian.py,217,"""Language codes are not yet supported.""",not
transformers/tests/test_modeling_marian.py,221,"""Language codes are not yet supported.""",not
transformers/tests/test_modeling_tf_distilbert.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_distilbert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_distilbert.py,3,,not
transformers/tests/test_modeling_tf_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_distilbert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_distilbert.py,7,,not
transformers/tests/test_modeling_tf_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_distilbert.py,9,,not
transformers/tests/test_modeling_tf_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_distilbert.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_distilbert.py,219,@slow,not
transformers/tests/test_modeling_tf_distilbert.py,220,def test_model_from_pretrained(self):,not
transformers/tests/test_modeling_tf_distilbert.py,221,for model_name in list(DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_tf_distilbert.py,222,model = DistilBertModesss.from_pretrained(model_name),not
transformers/tests/test_modeling_tf_distilbert.py,223,self.assertIsNotNone(model),not
transformers/tests/test_modeling_flaubert.py,1,coding=utf-8,not
transformers/tests/test_modeling_flaubert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_flaubert.py,3,,not
transformers/tests/test_modeling_flaubert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_flaubert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_flaubert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_flaubert.py,7,,not
transformers/tests/test_modeling_flaubert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_flaubert.py,9,,not
transformers/tests/test_modeling_flaubert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_flaubert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_flaubert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_flaubert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_flaubert.py,14,limitations under the License.,not
transformers/tests/test_modeling_flaubert.py,123,small variation of seq_length,not
transformers/tests/test_doc_samples.py,1,coding=utf-8,not
transformers/tests/test_doc_samples.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/tests/test_doc_samples.py,3,,not
transformers/tests/test_doc_samples.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_doc_samples.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_doc_samples.py,6,You may obtain a copy of the License at,not
transformers/tests/test_doc_samples.py,7,,not
transformers/tests/test_doc_samples.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_doc_samples.py,9,,not
transformers/tests/test_doc_samples.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_doc_samples.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_doc_samples.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_doc_samples.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_doc_samples.py,14,limitations under the License.,not
transformers/tests/test_doc_samples.py,32,"Check if the indentation is 0 for the example, so that we don't exit as soon as there's a line return.",not
transformers/tests/test_doc_samples.py,35,If we're back to the example indentation or if it's the end of the docstring.,not
transformers/tests/test_doc_samples.py,37,Exit the example mode and add the example to the examples list,not
transformers/tests/test_doc_samples.py,43,"If line is not empty, add it to the current example",not
transformers/tests/test_doc_samples.py,47,Detect the example from '::' or 'example::',not
transformers/tests/test_doc_samples.py,54,"elif ""::"" in line.lower() and len(line.strip()) == 2:",not
transformers/tests/test_doc_samples.py,55,example_mode = True,not
transformers/tests/test_doc_samples.py,56,"example_indentation = line.lower().find(""::"")",not
transformers/tests/test_doc_samples.py,80,Open all files,not
transformers/tests/test_doc_samples.py,83,Retrieve examples,not
transformers/tests/test_doc_samples.py,90,Some examples are the continuation of others.,not
transformers/tests/test_doc_samples.py,95,"If they contain this line, then they're a continuation of the previous script",not
transformers/tests/test_doc_samples.py,98,"If not, create a new example and increment the index",not
transformers/tests/test_doc_samples.py,105,Execute sub tests with every example.,not
transformers/tests/test_optimization.py,1,coding=utf-8,not
transformers/tests/test_optimization.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_optimization.py,3,,not
transformers/tests/test_optimization.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_optimization.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_optimization.py,6,You may obtain a copy of the License at,not
transformers/tests/test_optimization.py,7,,not
transformers/tests/test_optimization.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_optimization.py,9,,not
transformers/tests/test_optimization.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_optimization.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_optimization.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_optimization.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_optimization.py,14,limitations under the License.,not
transformers/tests/test_optimization.py,73,"No warmup, constant schedule, no gradient clipping",not
transformers/tests/test_optimization.py,79,No zero_grad() function on simple tensors. we do it ourselves.,not
transformers/tests/test_modeling_distilbert.py,1,coding=utf-8,not
transformers/tests/test_modeling_distilbert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_distilbert.py,3,,not
transformers/tests/test_modeling_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_distilbert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_distilbert.py,7,,not
transformers/tests/test_modeling_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_distilbert.py,9,,not
transformers/tests/test_modeling_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_distilbert.py,14,limitations under the License.,not
transformers/tests/test_modeling_distilbert.py,248,@slow,not
transformers/tests/test_modeling_distilbert.py,249,def test_model_from_pretrained(self):,not
transformers/tests/test_modeling_distilbert.py,250,for model_name in list(DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_distilbert.py,251,model = DistilBertModel.from_pretrained(model_name),not
transformers/tests/test_modeling_distilbert.py,252,self.assertIsNotNone(model),not
transformers/tests/test_modeling_ctrl.py,1,coding=utf-8,not
transformers/tests/test_modeling_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,not
transformers/tests/test_modeling_ctrl.py,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_ctrl.py,4,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_ctrl.py,5,You may obtain a copy of the License at,not
transformers/tests/test_modeling_ctrl.py,6,,not
transformers/tests/test_modeling_ctrl.py,7,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_ctrl.py,8,,not
transformers/tests/test_modeling_ctrl.py,9,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_ctrl.py,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_ctrl.py,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_ctrl.py,12,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_ctrl.py,13,limitations under the License.,not
transformers/tests/test_modeling_ctrl.py,119,"intermediate_size=self.intermediate_size,",not
transformers/tests/test_modeling_ctrl.py,120,"hidden_act=self.hidden_act,",not
transformers/tests/test_modeling_ctrl.py,121,"hidden_dropout_prob=self.hidden_dropout_prob,",not
transformers/tests/test_modeling_ctrl.py,122,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",not
transformers/tests/test_modeling_ctrl.py,125,"type_vocab_size=self.type_vocab_size,",not
transformers/tests/test_modeling_ctrl.py,126,initializer_range=self.initializer_range,not
transformers/tests/test_modeling_ctrl.py,224,Legal the president is,not
transformers/tests/test_modeling_ctrl.py,246,Legal the president is a good guy and I don't want to lose my job. \n \n I have a,not
transformers/tests/test_modeling_xlnet.py,1,coding=utf-8,not
transformers/tests/test_modeling_xlnet.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_xlnet.py,3,,not
transformers/tests/test_modeling_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_xlnet.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_xlnet.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_xlnet.py,7,,not
transformers/tests/test_modeling_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_xlnet.py,9,,not
transformers/tests/test_modeling_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_xlnet.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_xlnet.py,14,limitations under the License.,not
transformers/tests/test_modeling_xlnet.py,57,TODO (PVP): Check other models whether language generation is also applicable,SATD
transformers/tests/test_modeling_xlnet.py,92,self.key_len = seq_length + mem_len,not
transformers/tests/test_modeling_xlnet.py,124,Previous tokens don't see last token,not
transformers/tests/test_modeling_xlnet.py,128,predict last token,not
transformers/tests/test_modeling_xlnet.py,689,"In 1991, the remains of Russian Tsar Nicholas II and his family",not
transformers/tests/test_modeling_xlnet.py,690,(except for Alexei and Maria) are discovered.,not
transformers/tests/test_modeling_xlnet.py,691,"The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the",not
transformers/tests/test_modeling_xlnet.py,692,"remainder of the story. 1883 Western Siberia,",not
transformers/tests/test_modeling_xlnet.py,693,a young Grigori Rasputin is asked by his father and a group of men to perform magic.,not
transformers/tests/test_modeling_xlnet.py,694,Rasputin has a vision and denounces one of the men as a horse thief. Although his,not
transformers/tests/test_modeling_xlnet.py,695,"father initially slaps him for making such an accusation, Rasputin watches as the",not
transformers/tests/test_modeling_xlnet.py,696,"man is chased outside and beaten. Twenty years later, Rasputin sees a vision of",not
transformers/tests/test_modeling_xlnet.py,697,"the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,",not
transformers/tests/test_modeling_xlnet.py,698,"with people, even a bishop, begging for his blessing. """"""",not
transformers/tests/test_modeling_xlnet.py,902,"In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria)",not
transformers/tests/test_modeling_xlnet.py,903,"are discovered. The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich,",not
transformers/tests/test_modeling_xlnet.py,904,"narrates the remainder of the story. 1883 Western Siberia, a young Grigori Rasputin",not
transformers/tests/test_modeling_xlnet.py,905,is asked by his father and a group of men to perform magic. Rasputin has a vision and,not
transformers/tests/test_modeling_xlnet.py,906,denounces one of the men as a horse thief. Although his father initially slaps,not
transformers/tests/test_modeling_xlnet.py,907,"him for making such an accusation, Rasputin watches as the man is chased outside and beaten.",not
transformers/tests/test_modeling_xlnet.py,908,"Twenty years later, Rasputin sees a vision of the Virgin Mary, prompting him to become a priest.",not
transformers/tests/test_modeling_xlnet.py,909,"Rasputin quickly becomes famous, with people, even a bishop, begging for his blessing.",not
transformers/tests/test_modeling_xlnet.py,910,"<sep><cls>, Rasputin is asked to perform magic.",not
transformers/tests/test_modeling_xlnet.py,911,"He is not able to perform magic, and his father and",not
transformers/tests/test_modeling_xlnet.py,912,the men are forced to leave the monastery. Rasputin is forced to return to,not
transformers/tests/test_pipelines.py,38,"('xlnet-base-cased', 'xlnet-base-cased', None), # Disabled for now as it crash for TF2",not
transformers/tests/test_pipelines.py,43,"('xlnet-base-cased', 'xlnet-base-cased', None), # Disabled for now as it crash for TF2",not
transformers/tests/test_pipelines.py,482,Test that pipelines can be correctly loaded without any argument,not
transformers/tests/test_pipelines.py,490,Test that pipelines can be correctly loaded without any argument,not
transformers/tests/test_modeling_tf_gpt2.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_gpt2.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_gpt2.py,3,,not
transformers/tests/test_modeling_tf_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_gpt2.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_gpt2.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_gpt2.py,7,,not
transformers/tests/test_modeling_tf_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_gpt2.py,9,,not
transformers/tests/test_modeling_tf_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_gpt2.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_gpt2.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_gpt2.py,124,"intermediate_size=self.intermediate_size,",not
transformers/tests/test_modeling_tf_gpt2.py,125,"hidden_act=self.hidden_act,",not
transformers/tests/test_modeling_tf_gpt2.py,126,"hidden_dropout_prob=self.hidden_dropout_prob,",not
transformers/tests/test_modeling_tf_gpt2.py,127,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",not
transformers/tests/test_modeling_tf_gpt2.py,130,"type_vocab_size=self.type_vocab_size,",not
transformers/tests/test_modeling_tf_gpt2.py,131,initializer_range=self.initializer_range,not
transformers/tests/test_modeling_tf_gpt2.py,159,None is the input for 'past',not
transformers/tests/test_modeling_tf_gpt2.py,174,first forward pass,not
transformers/tests/test_modeling_tf_gpt2.py,177,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_tf_gpt2.py,181,append to next input_ids and token_type_ids,not
transformers/tests/test_modeling_tf_gpt2.py,188,select random slice,not
transformers/tests/test_modeling_tf_gpt2.py,193,test that outputs are equal for slice,not
transformers/tests/test_modeling_tf_gpt2.py,201,create attention mask,not
transformers/tests/test_modeling_tf_gpt2.py,207,first forward pass,not
transformers/tests/test_modeling_tf_gpt2.py,210,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_tf_gpt2.py,213,change a random masked slice from input_ids,not
transformers/tests/test_modeling_tf_gpt2.py,222,append to next input_ids and attn_mask,not
transformers/tests/test_modeling_tf_gpt2.py,226,get two different outputs,not
transformers/tests/test_modeling_tf_gpt2.py,230,select random slice,not
transformers/tests/test_modeling_tf_gpt2.py,235,test that outputs are equal for slice,not
transformers/tests/test_modeling_tf_gpt2.py,335,The dog,not
transformers/tests/test_modeling_tf_gpt2.py,357,The dog was found in a field near the intersection of West and West Streets.\n\nThe dog,not
transformers/tests/test_modeling_tf_gpt2.py,364,The president,not
transformers/tests/test_modeling_tf_gpt2.py,386,"The president of the United States, and the president of the United Kingdom, have been in the White",not
transformers/tests/test_modeling_tf_xlnet.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_xlnet.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_xlnet.py,3,,not
transformers/tests/test_modeling_tf_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_xlnet.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_xlnet.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_xlnet.py,7,,not
transformers/tests/test_modeling_tf_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_xlnet.py,9,,not
transformers/tests/test_modeling_tf_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_xlnet.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_xlnet.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_xlnet.py,56,TODO (PVP): Check other models whether language generation is also applicable,SATD
transformers/tests/test_modeling_tf_xlnet.py,91,self.key_len = seq_length + mem_len,not
transformers/tests/test_modeling_tf_xlnet.py,123,"perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token",not
transformers/tests/test_modeling_tf_xlnet.py,127,"target_mapping[:, 0, -1] = 1.0  # predict last token",not
transformers/tests/test_modeling_tf_xlnet.py,347,'token_type_ids': token_type_ids,not
transformers/tests/test_modeling_tf_xlnet.py,590,"In 1991, the remains of Russian Tsar Nicholas II and his family",not
transformers/tests/test_modeling_tf_xlnet.py,591,(except for Alexei and Maria) are discovered.,not
transformers/tests/test_modeling_tf_xlnet.py,592,"The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the",not
transformers/tests/test_modeling_tf_xlnet.py,593,"remainder of the story. 1883 Western Siberia,",not
transformers/tests/test_modeling_tf_xlnet.py,594,a young Grigori Rasputin is asked by his father and a group of men to perform magic.,not
transformers/tests/test_modeling_tf_xlnet.py,595,Rasputin has a vision and denounces one of the men as a horse thief. Although his,not
transformers/tests/test_modeling_tf_xlnet.py,596,"father initially slaps him for making such an accusation, Rasputin watches as the",not
transformers/tests/test_modeling_tf_xlnet.py,597,"man is chased outside and beaten. Twenty years later, Rasputin sees a vision of",not
transformers/tests/test_modeling_tf_xlnet.py,598,"the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,",not
transformers/tests/test_modeling_tf_xlnet.py,599,"with people, even a bishop, begging for his blessing. """"""",not
transformers/tests/test_modeling_tf_xlnet.py,803,"In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria)",not
transformers/tests/test_modeling_tf_xlnet.py,804,"are discovered. The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich,",not
transformers/tests/test_modeling_tf_xlnet.py,805,"narrates the remainder of the story. 1883 Western Siberia, a young Grigori Rasputin",not
transformers/tests/test_modeling_tf_xlnet.py,806,is asked by his father and a group of men to perform magic. Rasputin has a vision and,not
transformers/tests/test_modeling_tf_xlnet.py,807,denounces one of the men as a horse thief. Although his father initially slaps,not
transformers/tests/test_modeling_tf_xlnet.py,808,"him for making such an accusation, Rasputin watches as the man is chased outside and beaten.",not
transformers/tests/test_modeling_tf_xlnet.py,809,"Twenty years later, Rasputin sees a vision of the Virgin Mary, prompting him to become a priest.",not
transformers/tests/test_modeling_tf_xlnet.py,810,"Rasputin quickly becomes famous, with people, even a bishop, begging for his blessing.",not
transformers/tests/test_modeling_tf_xlnet.py,811,"<sep><cls>, Rasputin is asked to perform magic.",not
transformers/tests/test_modeling_tf_xlnet.py,812,"He is not able to perform magic, and his father and",not
transformers/tests/test_modeling_tf_xlnet.py,813,the men are forced to leave the monastery. Rasputin is forced to return to,not
transformers/tests/test_hf_api.py,1,coding=utf-8,not
transformers/tests/test_hf_api.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/tests/test_hf_api.py,3,,not
transformers/tests/test_hf_api.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_hf_api.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_hf_api.py,6,You may obtain a copy of the License at,not
transformers/tests/test_hf_api.py,7,,not
transformers/tests/test_hf_api.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_hf_api.py,9,,not
transformers/tests/test_hf_api.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_hf_api.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_hf_api.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_hf_api.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_hf_api.py,14,limitations under the License.,not
transformers/tests/test_hf_api.py,35,space is intentional,not
transformers/tests/test_hf_api.py,137,"^^ not an error, we test that the",not
transformers/tests/test_hf_api.py,138,second call does not fail.,not
transformers/tests/test_modeling_tf_transfo_xl.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_transfo_xl.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_transfo_xl.py,3,,not
transformers/tests/test_modeling_tf_transfo_xl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_transfo_xl.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_transfo_xl.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_transfo_xl.py,7,,not
transformers/tests/test_modeling_tf_transfo_xl.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_transfo_xl.py,9,,not
transformers/tests/test_modeling_tf_transfo_xl.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_transfo_xl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_transfo_xl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_transfo_xl.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_transfo_xl.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_transfo_xl.py,41,TODO: add this test when TFTransfoXLLMHead has a linear output layer implemented,SATD
transformers/tests/test_modeling_tf_transfo_xl.py,369,"In 1991 , the remains of Russian Tsar Nicholas II and his family",not
transformers/tests/test_modeling_tf_transfo_xl.py,370,( except for Alexei and Maria ) are discovered .,not
transformers/tests/test_modeling_tf_transfo_xl.py,371,"The voice of Nicholas's young son , Tsarevich Alexei Nikolaevich , narrates the",not
transformers/tests/test_modeling_tf_transfo_xl.py,372,"remainder of the story . 1883 Western Siberia ,",not
transformers/tests/test_modeling_tf_transfo_xl.py,373,a young Grigori Rasputin is asked by his father and a group of men to perform magic .,not
transformers/tests/test_modeling_tf_transfo_xl.py,374,Rasputin has a vision and denounces one of the men as a horse thief . Although his,not
transformers/tests/test_modeling_tf_transfo_xl.py,375,"father initially slaps him for making such an accusation , Rasputin watches as the",not
transformers/tests/test_modeling_tf_transfo_xl.py,376,"man is chased outside and beaten . Twenty years later , Rasputin sees a vision of",not
transformers/tests/test_modeling_tf_transfo_xl.py,377,"the Virgin Mary , prompting him to become a priest . Rasputin quickly becomes famous ,",not
transformers/tests/test_modeling_tf_transfo_xl.py,378,"with people , even a bishop , begging for his blessing . <eod> </s> <eos>",not
transformers/tests/test_modeling_tf_transfo_xl.py,559,"In 1991, the remains of Russian Tsar Nicholas II and his family (",not
transformers/tests/test_modeling_tf_transfo_xl.py,560,"except for Alexei and Maria ) are discovered. The voice of young son,",not
transformers/tests/test_modeling_tf_transfo_xl.py,561,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.",not
transformers/tests/test_modeling_tf_transfo_xl.py,562,"1883 Western Siberia, a young Grigori Rasputin is asked by his father",not
transformers/tests/test_modeling_tf_transfo_xl.py,563,and a group of men to perform magic. Rasputin has a vision and,not
transformers/tests/test_modeling_tf_transfo_xl.py,564,denounces one of the men as a horse thief. Although his father initially,not
transformers/tests/test_modeling_tf_transfo_xl.py,565,"slaps him for making such an accusation, Rasputin watches as the man",not
transformers/tests/test_modeling_tf_transfo_xl.py,566,"is chased outside and beaten. Twenty years later, Rasputin sees a vision",not
transformers/tests/test_modeling_tf_transfo_xl.py,567,"of the Virgin Mary, prompting him to become a priest.",not
transformers/tests/test_modeling_tf_transfo_xl.py,568,"Rasputin quickly becomes famous, with people, even a bishop, begging for",not
transformers/tests/test_modeling_tf_transfo_xl.py,569,"his blessing. <unk> <unk> <eos> In the 1990s, the remains of Russian Tsar",not
transformers/tests/test_modeling_tf_transfo_xl.py,570,"Nicholas II and his family were discovered. The voice of <unk> young son,",not
transformers/tests/test_modeling_tf_transfo_xl.py,571,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.<eos>",not
transformers/tests/test_modeling_reformer.py,1,coding=utf-8 # Copyright 2020 Huggingface,not
transformers/tests/test_modeling_reformer.py,2,,not
transformers/tests/test_modeling_reformer.py,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_reformer.py,4,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_reformer.py,5,You may obtain a copy of the License at,not
transformers/tests/test_modeling_reformer.py,6,,not
transformers/tests/test_modeling_reformer.py,7,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_reformer.py,8,,not
transformers/tests/test_modeling_reformer.py,9,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_reformer.py,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_reformer.py,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_reformer.py,12,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_reformer.py,13,limitations under the License.,not
transformers/tests/test_modeling_reformer.py,180,2 * hidden_size because we use reversible resnet layers,not
transformers/tests/test_modeling_reformer.py,211,no special position embeddings,not
transformers/tests/test_modeling_reformer.py,216,need to set chunk length equal sequence length to be certain that chunking works,not
transformers/tests/test_modeling_reformer.py,222,set all position encodings to zero so that postions don't matter,not
transformers/tests/test_modeling_reformer.py,233,normal padded,not
transformers/tests/test_modeling_reformer.py,239,shifted padded,not
transformers/tests/test_modeling_reformer.py,259,Batch x SeqLen x hiddenSize,not
transformers/tests/test_modeling_reformer.py,261,get random tensors,not
transformers/tests/test_modeling_reformer.py,265,now the random seeds for attention and feed forward is initialized,not
transformers/tests/test_modeling_reformer.py,266,forward tensors with dropout,not
transformers/tests/test_modeling_reformer.py,306,disable dropout,not
transformers/tests/test_modeling_reformer.py,352,Batch x SeqLen x hiddenSize,not
transformers/tests/test_modeling_reformer.py,891,check last grads to cover all proable errors,not
transformers/tests/test_modeling_reformer.py,924,check last grads to cover all proable errors,not
transformers/tests/test_tokenization_distilbert.py,1,coding=utf-8,not
transformers/tests/test_tokenization_distilbert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_distilbert.py,3,,not
transformers/tests/test_tokenization_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_distilbert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_distilbert.py,7,,not
transformers/tests/test_tokenization_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_distilbert.py,9,,not
transformers/tests/test_tokenization_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_distilbert.py,14,limitations under the License.,not
transformers/tests/test_tokenization_common.py,1,coding=utf-8,not
transformers/tests/test_tokenization_common.py,2,Copyright 2019 HuggingFace Inc.,not
transformers/tests/test_tokenization_common.py,3,,not
transformers/tests/test_tokenization_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_common.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_common.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_common.py,7,,not
transformers/tests/test_tokenization_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_common.py,9,,not
transformers/tests/test_tokenization_common.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_common.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_common.py,14,limitations under the License.,not
transformers/tests/test_tokenization_common.py,71,"Switch from batch_encode_plus format:   {'input_ids': [[...], [...]], ...}",not
transformers/tests/test_tokenization_common.py,72,"to the concatenated encode_plus format: [{'input_ids': [...], ...}, {'input_ids': [...], ...}]",not
transformers/tests/test_tokenization_common.py,101,safety check on max_len default value so we are sure the test works,not
transformers/tests/test_tokenization_common.py,105,Now let's start the test,not
transformers/tests/test_tokenization_common.py,146,toks before adding new_toks,not
transformers/tests/test_tokenization_common.py,156,toks0 should be longer,not
transformers/tests/test_tokenization_common.py,159,Check that none of the special tokens are lowercased,not
transformers/tests/test_tokenization_common.py,174,Length should still be the same,not
transformers/tests/test_tokenization_common.py,176,But at least the first non-special tokens should differ,not
transformers/tests/test_tokenization_common.py,304,Method is implemented (e.g. not GPT-2),not
transformers/tests/test_tokenization_common.py,400,Add tokens so that masked token isn't split,not
transformers/tests/test_tokenization_common.py,406,Test first masked sequence,not
transformers/tests/test_tokenization_common.py,413,Test second masked sequence,not
transformers/tests/test_tokenization_common.py,426,Testing single inputs,not
transformers/tests/test_tokenization_common.py,441,Testing inputs pairs,not
transformers/tests/test_tokenization_common.py,457,Testing with already existing special tokens,not
transformers/tests/test_tokenization_common.py,477,check correct behaviour if no pad_token_id exists and add it eventually,not
transformers/tests/test_tokenization_common.py,482,RIGHT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True,not
transformers/tests/test_tokenization_common.py,491,LEFT PADDING - Check that it correctly pads when a maximum length is specified along with the padding flag set to True,not
transformers/tests/test_tokenization_common.py,500,RIGHT & LEFT PADDING - Check that nothing is done when a maximum length is not specified,not
transformers/tests/test_tokenization_common.py,522,check correct behaviour if no pad_token_id exists and add it eventually,not
transformers/tests/test_tokenization_common.py,534,Test right padding,not
transformers/tests/test_tokenization_common.py,552,Test left padding,not
transformers/tests/test_tokenization_common.py,585,This tests that tokenizers don't impact others. Unfortunately the case where it fails is when,not
transformers/tests/test_tokenization_common.py,586,"we're loading an S3 configuration from a pre-trained identifier, and we have no way of testing those today.",not
transformers/tests/test_tokenization_common.py,615,Tests that all encoded values have the correct size,not
transformers/tests/test_tokenization_common.py,631,check correct behaviour if no pad_token_id exists and add it eventually,not
transformers/tests/test_tokenization_common.py,646,Test that padded sequences are equivalent between batch_encode_plus and encode_plus,not
transformers/tests/test_tokenization_common.py,648,Right padding tests,not
transformers/tests/test_tokenization_common.py,658,check correct behaviour if no pad_token_id exists and add it eventually,not
transformers/tests/test_tokenization_common.py,669,Left padding tests,not
transformers/tests/test_tokenization_common.py,681,check correct behaviour if no pad_token_id exists and add it eventually,not
transformers/tests/test_tokenization_common.py,702,A Tensor cannot be build by sequences which are not the same size,not
transformers/tests/test_tokenization_common.py,726,"if tokenizer does not have pad_token_id, an error should be thrown",not
transformers/tests/test_tokenization_common.py,734,add pad_token_id to pass subsequent tests,not
transformers/tests/test_tokenization_common.py,756,Make sure the model contains at least the full vocabulary size in its embedding matrix,not
transformers/tests/test_tokenization_common.py,760,Build sequence,not
transformers/tests/test_tokenization_common.py,765,This should not fail,not
transformers/tests/test_tokenization_common.py,773,This should not fail,not
transformers/tests/test_tokenization_common.py,796,Make sure the model contains at least the full vocabulary size in its embedding matrix,not
transformers/tests/test_tokenization_common.py,799,Build sequence,not
transformers/tests/test_tokenization_common.py,805,This should not fail,not
transformers/tests/test_tokenization_common.py,813,This should not fail,not
transformers/tests/test_tokenization_roberta.py,1,coding=utf-8,not
transformers/tests/test_tokenization_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_roberta.py,3,,not
transformers/tests/test_tokenization_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_roberta.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_roberta.py,7,,not
transformers/tests/test_tokenization_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_roberta.py,9,,not
transformers/tests/test_tokenization_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_roberta.py,14,limitations under the License.,not
transformers/tests/test_tokenization_roberta.py,33,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,not
transformers/tests/test_tokenization_roberta.py,120,Testing encoder arguments,not
transformers/tests/test_tokenization_roberta.py,134,Testing spaces after special tokenss,not
transformers/tests/test_modeling_tf_common.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_common.py,2,Copyright 2019 HuggingFace Inc.,not
transformers/tests/test_modeling_tf_common.py,3,,not
transformers/tests/test_modeling_tf_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_common.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_common.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_common.py,7,,not
transformers/tests/test_modeling_tf_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_common.py,9,,not
transformers/tests/test_modeling_tf_common.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_common.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_common.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_common.py,38,Restrict TensorFlow to only allocate x GB of memory on the GPUs,not
transformers/tests/test_modeling_tf_common.py,46,Virtual devices must be set before GPUs have been initialized,not
transformers/tests/test_modeling_tf_common.py,71,"config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()",not
transformers/tests/test_modeling_tf_common.py,73,configs_no_init = _config_zero_init(config),not
transformers/tests/test_modeling_tf_common.py,74,for model_class in self.all_model_classes:,not
transformers/tests/test_modeling_tf_common.py,75,model = model_class(config=configs_no_init),not
transformers/tests/test_modeling_tf_common.py,76,"for name, param in model.named_parameters():",not
transformers/tests/test_modeling_tf_common.py,77,if param.requires_grad:,not
transformers/tests/test_modeling_tf_common.py,78,"self.assertIn(param.data.mean().item(), [0.0, 1.0],",not
transformers/tests/test_modeling_tf_common.py,79,"msg=""Parameter {} of model {} seems not properly initialized"".format(name, model_class))",not
transformers/tests/test_modeling_tf_common.py,128,Make sure we don't have nans,not
transformers/tests/test_modeling_tf_common.py,147,"Skip the ""TF"" at the beggining",not
transformers/tests/test_modeling_tf_common.py,155,Check we can load pt model in tf and vice-versa with model => model functions,not
transformers/tests/test_modeling_tf_common.py,160,Check predictions on first output (logits/hidden-states) are close enought given low-level computational differences,not
transformers/tests/test_modeling_tf_common.py,165,"need to rename encoder-decoder ""inputs"" for PyTorch",not
transformers/tests/test_modeling_tf_common.py,184,Debug info (remove when fixed),not
transformers/tests/test_modeling_tf_common.py,193,Check we can load pt model in tf and vice-versa with checkpoint => model functions,not
transformers/tests/test_modeling_tf_common.py,203,Check predictions on first output (logits/hidden-states) are close enought given low-level computational differences,not
transformers/tests/test_modeling_tf_common.py,208,"need to rename encoder-decoder ""inputs"" for PyTorch",not
transformers/tests/test_modeling_tf_common.py,243,Prepare our model,not
transformers/tests/test_modeling_tf_common.py,246,Let's load it from the disk to be sure we can use pretrained weights,not
transformers/tests/test_modeling_tf_common.py,248,build the model,not
transformers/tests/test_modeling_tf_common.py,255,Add a dense layer on top to test intetgration with other keras modules,not
transformers/tests/test_modeling_tf_common.py,258,Compile extended model,not
transformers/tests/test_modeling_tf_common.py,324,Check attention is always last and order is fine,not
transformers/tests/test_modeling_tf_common.py,382,"^^ In our TF models, the input_embeddings can take slightly different forms,",not
transformers/tests/test_modeling_tf_common.py,383,so we try a few of them.,not
transformers/tests/test_modeling_tf_common.py,384,We used to fall back to just synthetically creating a dummy tensor of ones:,not
transformers/tests/test_modeling_tf_common.py,427,iterate over all generative models,not
transformers/tests/test_modeling_tf_common.py,432,if bos token id is not defined mobel needs input_ids,not
transformers/tests/test_modeling_tf_common.py,435,num_return_sequences = 1,not
transformers/tests/test_modeling_tf_common.py,438,num_return_sequences = 1,not
transformers/tests/test_modeling_tf_common.py,442,generating multiple sequences when no beam search generation,not
transformers/tests/test_modeling_tf_common.py,443,is not allowed as it would always generate the same sequences,not
transformers/tests/test_modeling_tf_common.py,446,"num_return_sequences > 1, sample",not
transformers/tests/test_modeling_tf_common.py,449,check bad words tokens language generation,not
transformers/tests/test_modeling_tf_common.py,450,create list of 1-seq bad token and list of 2-seq of bad tokens,not
transformers/tests/test_modeling_tf_common.py,455,only count generated tokens,not
transformers/tests/test_modeling_tf_common.py,467,"if bos token id is not defined mobel needs input_ids, num_return_sequences = 1",not
transformers/tests/test_modeling_tf_common.py,470,num_return_sequences = 1,not
transformers/tests/test_modeling_tf_common.py,474,generating more sequences than having beams leads is not possible,not
transformers/tests/test_modeling_tf_common.py,477,"num_return_sequences > 1, sample",not
transformers/tests/test_modeling_tf_common.py,479,"num_return_sequences > 1, greedy",not
transformers/tests/test_modeling_tf_common.py,482,check bad words tokens language generation,not
transformers/tests/test_modeling_tf_common.py,483,create list of 1-seq bad token and list of 2-seq of bad tokens,not
transformers/tests/test_modeling_tf_common.py,488,only count generated tokens,not
transformers/tests/test_modeling_tf_common.py,493,special tokens cannot be bad tokens,not
transformers/tests/test_modeling_tf_common.py,502,create random bad tokens that are not special tokens,not
transformers/tests/test_modeling_tf_common.py,516,for all bad word tokens,not
transformers/tests/test_modeling_tf_common.py,518,for all slices in batch,not
transformers/tests/test_modeling_tf_common.py,520,for all word idx,not
transformers/tests/test_modeling_tf_common.py,522,if tokens match,not
transformers/tests/test_modeling_tf_common.py,549,tests whether the top_k_top_p_filtering function behaves as expected,not
transformers/tests/test_modeling_tf_common.py,554,3rd highest value; idx. 0,not
transformers/tests/test_modeling_tf_common.py,563,5th highest value; idx. 9,not
transformers/tests/test_modeling_tf_common.py,564,2nd highest value; idx. 10,not
transformers/tests/test_modeling_tf_common.py,579,4th highest value; idx. 25,not
transformers/tests/test_modeling_tf_common.py,580,1st highest value; idx. 26,not
transformers/tests/test_modeling_tf_common.py,584,cummulative prob of 5 highest values <= 0.6,not
transformers/tests/test_modeling_tf_common.py,599,4th highest value; idx. 13,not
transformers/tests/test_modeling_tf_common.py,603,2nd highest value; idx. 17,not
transformers/tests/test_modeling_tf_common.py,604,5th highest value; idx. 18,not
transformers/tests/test_modeling_tf_common.py,606,3rd highest value; idx. 20,not
transformers/tests/test_modeling_tf_common.py,613,1st highest value; idx. 27,not
transformers/tests/test_modeling_tf_common.py,616,cummulative prob of 5 highest values <= 0.6,not
transformers/tests/test_modeling_tf_common.py,623,expected non filtered idx as noted above,not
transformers/tests/test_modeling_tf_common.py,628,expected non filtered values as noted above,not
transformers/tests/test_tokenization_utils.py,1,coding=utf-8,not
transformers/tests/test_tokenization_utils.py,2,Copyright 2018 HuggingFace Inc..,not
transformers/tests/test_tokenization_utils.py,3,,not
transformers/tests/test_tokenization_utils.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_utils.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_utils.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_utils.py,7,,not
transformers/tests/test_tokenization_utils.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_utils.py,9,,not
transformers/tests/test_tokenization_utils.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_utils.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_utils.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_utils.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_utils.py,14,limitations under the License.,not
transformers/tests/test_modeling_xlm_roberta.py,1,coding=utf-8,not
transformers/tests/test_modeling_xlm_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_xlm_roberta.py,3,,not
transformers/tests/test_modeling_xlm_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_xlm_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_xlm_roberta.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_xlm_roberta.py,7,,not
transformers/tests/test_modeling_xlm_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_xlm_roberta.py,9,,not
transformers/tests/test_modeling_xlm_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_xlm_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_xlm_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_xlm_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_xlm_roberta.py,14,limitations under the License.,not
transformers/tests/test_modeling_xlm_roberta.py,34,The dog is cute and lives in the garden house,not
transformers/tests/test_modeling_xlm_roberta.py,36,"batch_size, sequence_length, embedding_vector_dim",not
transformers/tests/test_modeling_xlm_roberta.py,40,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')",not
transformers/tests/test_modeling_xlm_roberta.py,41,xlmr.eval(),not
transformers/tests/test_modeling_xlm_roberta.py,42,"expected_output_values_last_dim = xlmr.extract_features(input_ids[0])[:, :, -1]",not
transformers/tests/test_modeling_xlm_roberta.py,46,compare the actual values for a slice of last dim,not
transformers/tests/test_modeling_xlm_roberta.py,53,The dog is cute and lives in the garden house,not
transformers/tests/test_modeling_xlm_roberta.py,55,"batch_size, sequence_length, embedding_vector_dim",not
transformers/tests/test_modeling_xlm_roberta.py,59,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large')",not
transformers/tests/test_modeling_xlm_roberta.py,60,xlmr.eval(),not
transformers/tests/test_modeling_xlm_roberta.py,61,"expected_output_values_last_dim = xlmr.extract_features(input_ids[0])[:, :, -1]",not
transformers/tests/test_modeling_xlm_roberta.py,65,compare the actual values for a slice of last dim,not
transformers/tests/test_tokenization_gpt2.py,1,coding=utf-8,not
transformers/tests/test_tokenization_gpt2.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_gpt2.py,3,,not
transformers/tests/test_tokenization_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_gpt2.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_gpt2.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_gpt2.py,7,,not
transformers/tests/test_tokenization_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_gpt2.py,9,,not
transformers/tests/test_tokenization_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_gpt2.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_gpt2.py,14,limitations under the License.,not
transformers/tests/test_tokenization_gpt2.py,34,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,not
transformers/tests/test_tokenization_gpt2.py,101,Testing tokenization,not
transformers/tests/test_tokenization_gpt2.py,106,Testing conversion to ids without special tokens,not
transformers/tests/test_tokenization_gpt2.py,111,Testing conversion to ids with special tokens,not
transformers/tests/test_tokenization_gpt2.py,117,Testing the unknown token,not
transformers/tests/test_tokenization_fast.py,54,Tokenizer.filter makes it possible to filter which Tokenizer to case based on all the,not
transformers/tests/test_tokenization_fast.py,55,"information available in Tokenizer (name, rust class, python class, vocab key name)",not
transformers/tests/test_tokenization_fast.py,67,Check is_fast is set correctly,not
transformers/tests/test_tokenization_fast.py,71,Check that Rust and Python align,not
transformers/tests/test_tokenization_fast.py,78,TODO: enable for v3.0.0,SATD
transformers/tests/test_tokenization_fast.py,79,"self.assert_empty_output_no_special_tokens(tokenizer_r, tokenizer_p)",not
transformers/tests/test_tokenization_fast.py,82,Ensure None raise an error,not
transformers/tests/test_tokenization_fast.py,108,"words, tokens",not
transformers/tests/test_tokenization_fast.py,117,Assert token_to_word,not
transformers/tests/test_tokenization_fast.py,126,Assert word_to_tokens,not
transformers/tests/test_tokenization_fast.py,135,Assert token_to_chars,not
transformers/tests/test_tokenization_fast.py,144,Assert char_to_token,not
transformers/tests/test_tokenization_fast.py,153,Assert char_to_word,not
transformers/tests/test_tokenization_fast.py,162,Assert word_to_chars,not
transformers/tests/test_tokenization_fast.py,172,Ensure basic input match,not
transformers/tests/test_tokenization_fast.py,185,Ensure truncation match,not
transformers/tests/test_tokenization_fast.py,192,Ensure truncation with stride match,not
transformers/tests/test_tokenization_fast.py,200,Check we have the same number of added_tokens for both pair and non-pair inputs.,not
transformers/tests/test_tokenization_fast.py,205,Check we have the correct max_length for both pair and non-pair inputs.,not
transformers/tests/test_tokenization_fast.py,210,Assert the set of special tokens match.,not
transformers/tests/test_tokenization_fast.py,236,No pair,not
transformers/tests/test_tokenization_fast.py,243,Assert there is the same number of tokens and offsets,not
transformers/tests/test_tokenization_fast.py,246,Assert there is online added_tokens special_tokens,not
transformers/tests/test_tokenization_fast.py,249,Pairs,not
transformers/tests/test_tokenization_fast.py,256,Assert there is the same number of tokens and offsets,not
transformers/tests/test_tokenization_fast.py,259,Assert there is online added_tokens special_tokens,not
transformers/tests/test_tokenization_fast.py,285,Mono sample,not
transformers/tests/test_tokenization_fast.py,298,Multi sample,not
transformers/tests/test_tokenization_fast.py,312,Input string,not
transformers/tests/test_tokenization_fast.py,316,Generate output,not
transformers/tests/test_tokenization_fast.py,321,Generate pair output,not
transformers/tests/test_tokenization_fast.py,326,Input tokens id,not
transformers/tests/test_tokenization_fast.py,330,Generate output,not
transformers/tests/test_tokenization_fast.py,335,Generate pair output,not
transformers/tests/test_tokenization_fast.py,343,Ensure we match max_length,not
transformers/tests/test_tokenization_fast.py,346,Ensure the number of padded tokens is the same,not
transformers/tests/test_tokenization_fast.py,362,Simple input,not
transformers/tests/test_tokenization_fast.py,367,Pair input,not
transformers/tests/test_tokenization_fast.py,376,Simple input,not
transformers/tests/test_tokenization_fast.py,382,Pair input,not
transformers/tests/test_tokenization_fast.py,392,Simple input,not
transformers/tests/test_tokenization_fast.py,401,Pair input,not
transformers/tests/test_tokenization_fast.py,421,Checks it save with the same files,not
transformers/tests/test_tokenization_fast.py,424,Checks everything loads correctly in the same way,not
transformers/tests/test_tokenization_fast.py,427,Check special tokens are set accordingly on Rust and Python,not
transformers/tests/test_tokenization_fast.py,430,"self.assertEqual(getattr(tokenizer_rp, key), getattr(tokenizer_pp, key))",not
transformers/tests/test_tokenization_fast.py,431,"self.assertEqual(getattr(tokenizer_rp, key + ""_id""), getattr(tokenizer_pp, key + ""_id""))",not
transformers/tests/test_tokenization_fast.py,454,pair_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=True),not
transformers/tests/test_tokenization_fast.py,457,tokenize(),not
transformers/tests/test_tokenization_fast.py,462,encode(),not
transformers/tests/test_tokenization_fast.py,467,encode_plus(),not
transformers/tests/test_tokenization_fast.py,475,# batch_encode_plus,not
transformers/tests/test_tokenization_fast.py,515,BERT normalizes this away,not
transformers/tests/test_tokenization_fast.py,516,Append MASK here after lower-casing,not
transformers/tests/test_tokenization_fast.py,524,Check if the tokenizer is uncased,not
transformers/tests/test_tokenization_fast.py,528,Append the special tokens,not
transformers/tests/test_tokenization_fast.py,534,"self.assertEqual([e[0] for e in expected_results], tokens[""offset_mapping""])",not
transformers/tests/test_tokenization_fast.py,547,Rust correctly handles the space before the mask while python doesnt,not
transformers/tests/test_tokenization_fast.py,551,token_type_ids should put 0 everywhere,not
transformers/tests/test_tokenization_fast.py,554,"attention_mask should put 1 everywhere, so sum over length should be 1",not
transformers/tests/test_tokenization_fast.py,560,Rust should have 'Ġ' before <mask> which should be left as an entire token,not
transformers/tests/test_tokenization_fast.py,572,Simple input,not
transformers/tests/test_tokenization_fast.py,581,Simple input tests,not
transformers/tests/test_tokenization_fast.py,584,Simple input,not
transformers/tests/test_tokenization_fast.py,587,Simple input,not
transformers/tests/test_tokenization_fast.py,590,Pair input,not
transformers/tests/test_tokenization_fast.py,593,Pair input,not
transformers/tests/test_tokenization_fast.py,596,Pair input,not
transformers/tests/test_tokenization_albert.py,1,coding=utf-8,not
transformers/tests/test_tokenization_albert.py,2,Copyright 2019 Hugging Face inc.,not
transformers/tests/test_tokenization_albert.py,3,,not
transformers/tests/test_tokenization_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_albert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_albert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_albert.py,7,,not
transformers/tests/test_tokenization_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_albert.py,9,,not
transformers/tests/test_tokenization_albert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_albert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_albert.py,14,limitations under the License.,not
transformers/tests/test_tokenization_albert.py,35,We have a SentencePiece fixture for testing,not
transformers/tests/test_modeling_electra.py,1,coding=utf-8,not
transformers/tests/test_modeling_electra.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_electra.py,3,,not
transformers/tests/test_modeling_electra.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_electra.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_electra.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_electra.py,7,,not
transformers/tests/test_modeling_electra.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_electra.py,9,,not
transformers/tests/test_modeling_electra.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_electra.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_electra.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_electra.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_electra.py,14,limitations under the License.,not
transformers/tests/test_tokenization_bert_japanese.py,1,coding=utf-8,not
transformers/tests/test_tokenization_bert_japanese.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_bert_japanese.py,3,,not
transformers/tests/test_tokenization_bert_japanese.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_bert_japanese.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_bert_japanese.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_bert_japanese.py,7,,not
transformers/tests/test_tokenization_bert_japanese.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_bert_japanese.py,9,,not
transformers/tests/test_tokenization_bert_japanese.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_bert_japanese.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_bert_japanese.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_bert_japanese.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_bert_japanese.py,14,limitations under the License.,not
transformers/tests/test_tokenization_bert_japanese.py,100,"if dict doesn't exist in the system, previous code raises this error.",not
transformers/tests/test_tokenization_bert_japanese.py,142,"2 is for ""[CLS]"", 3 is for ""[SEP]""",not
transformers/tests/test_tokenization_bert_japanese.py,203,"2 is for ""[CLS]"", 3 is for ""[SEP]""",not
transformers/tests/test_modeling_transfo_xl.py,1,coding=utf-8,not
transformers/tests/test_modeling_transfo_xl.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_transfo_xl.py,3,,not
transformers/tests/test_modeling_transfo_xl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_transfo_xl.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_transfo_xl.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_transfo_xl.py,7,,not
transformers/tests/test_modeling_transfo_xl.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_transfo_xl.py,9,,not
transformers/tests/test_modeling_transfo_xl.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_transfo_xl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_transfo_xl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_transfo_xl.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_transfo_xl.py,14,limitations under the License.,not
transformers/tests/test_modeling_transfo_xl.py,370,"In 1991 , the remains of Russian Tsar Nicholas II and his family",not
transformers/tests/test_modeling_transfo_xl.py,371,( except for Alexei and Maria ) are discovered .,not
transformers/tests/test_modeling_transfo_xl.py,372,"The voice of Nicholas's young son , Tsarevich Alexei Nikolaevich , narrates the",not
transformers/tests/test_modeling_transfo_xl.py,373,"remainder of the story . 1883 Western Siberia ,",not
transformers/tests/test_modeling_transfo_xl.py,374,a young Grigori Rasputin is asked by his father and a group of men to perform magic .,not
transformers/tests/test_modeling_transfo_xl.py,375,Rasputin has a vision and denounces one of the men as a horse thief . Although his,not
transformers/tests/test_modeling_transfo_xl.py,376,"father initially slaps him for making such an accusation , Rasputin watches as the",not
transformers/tests/test_modeling_transfo_xl.py,377,"man is chased outside and beaten . Twenty years later , Rasputin sees a vision of",not
transformers/tests/test_modeling_transfo_xl.py,378,"the Virgin Mary , prompting him to become a priest . Rasputin quickly becomes famous ,",not
transformers/tests/test_modeling_transfo_xl.py,380,"with people , even a bishop , begging for his blessing . <eod> </s> <eos>",not
transformers/tests/test_modeling_transfo_xl.py,561,"In 1991, the remains of Russian Tsar Nicholas II and his family (",not
transformers/tests/test_modeling_transfo_xl.py,562,"except for Alexei and Maria ) are discovered. The voice of young son,",not
transformers/tests/test_modeling_transfo_xl.py,563,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.",not
transformers/tests/test_modeling_transfo_xl.py,564,"1883 Western Siberia, a young Grigori Rasputin is asked by his father",not
transformers/tests/test_modeling_transfo_xl.py,565,and a group of men to perform magic. Rasputin has a vision and,not
transformers/tests/test_modeling_transfo_xl.py,566,denounces one of the men as a horse thief. Although his father initially,not
transformers/tests/test_modeling_transfo_xl.py,567,"slaps him for making such an accusation, Rasputin watches as the man",not
transformers/tests/test_modeling_transfo_xl.py,568,"is chased outside and beaten. Twenty years later, Rasputin sees a vision",not
transformers/tests/test_modeling_transfo_xl.py,569,"of the Virgin Mary, prompting him to become a priest.",not
transformers/tests/test_modeling_transfo_xl.py,570,"Rasputin quickly becomes famous, with people, even a bishop, begging for",not
transformers/tests/test_modeling_transfo_xl.py,571,"his blessing. <unk> <unk> <eos> In the 1990s, the remains of Russian Tsar",not
transformers/tests/test_modeling_transfo_xl.py,572,"Nicholas II and his family were discovered. The voice of <unk> young son,",not
transformers/tests/test_modeling_transfo_xl.py,573,"Tsarevich Alexei Nikolaevich, narrates the remainder of the story.<eos>",not
transformers/tests/utils.py,10,"Used to test Auto{Config, Model, Tokenizer} model_type detection.",not
transformers/tests/utils.py,17,"KEY isn't set, default to `default`.",not
transformers/tests/utils.py,20,"KEY is set, convert it to True or False.",not
transformers/tests/utils.py,24,"More values are supported, but let's keep the message simple.",not
transformers/tests/utils.py,98,Set the USE_CUDA environment variable to select a GPU.,not
transformers/tests/test_modeling_roberta.py,1,coding=utf-8,not
transformers/tests/test_modeling_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_roberta.py,3,,not
transformers/tests/test_modeling_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_roberta.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_roberta.py,7,,not
transformers/tests/test_modeling_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_roberta.py,9,,not
transformers/tests/test_modeling_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_roberta.py,14,limitations under the License.,not
transformers/tests/test_modeling_roberta.py,331,compare the actual values for a slice.,not
transformers/tests/test_modeling_roberta.py,336,"roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')",not
transformers/tests/test_modeling_roberta.py,337,roberta.eval(),not
transformers/tests/test_modeling_roberta.py,338,"expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()",not
transformers/tests/test_modeling_roberta.py,348,compare the actual values for a slice.,not
transformers/tests/test_modeling_roberta.py,353,"roberta = torch.hub.load('pytorch/fairseq', 'roberta.base')",not
transformers/tests/test_modeling_roberta.py,354,roberta.eval(),not
transformers/tests/test_modeling_roberta.py,355,"expected_slice = roberta.extract_features(input_ids)[:, :3, :3].detach()",not
transformers/tests/test_modeling_roberta.py,369,"roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')",not
transformers/tests/test_modeling_roberta.py,370,roberta.eval(),not
transformers/tests/test_modeling_roberta.py,371,"expected_tensor = roberta.predict(""mnli"", input_ids, return_logits=True).detach()",not
transformers/tests/test_modeling_tf_camembert.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_camembert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_camembert.py,3,,not
transformers/tests/test_modeling_tf_camembert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_camembert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_camembert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_camembert.py,7,,not
transformers/tests/test_modeling_tf_camembert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_camembert.py,9,,not
transformers/tests/test_modeling_tf_camembert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_camembert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_camembert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_camembert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_camembert.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_camembert.py,37,"J'aime le camembert !""",not
transformers/tests/test_modeling_tf_camembert.py,42,compare the actual values for a slice.,not
transformers/tests/test_modeling_tf_camembert.py,46,"camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')",not
transformers/tests/test_modeling_tf_camembert.py,47,camembert.eval(),not
transformers/tests/test_modeling_tf_camembert.py,48,"expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()",not
transformers/tests/test_modeling_encoder_decoder.py,1,coding=utf-8,not
transformers/tests/test_modeling_encoder_decoder.py,2,Copyright 2020 HuggingFace Inc. team.,not
transformers/tests/test_modeling_encoder_decoder.py,3,,not
transformers/tests/test_modeling_encoder_decoder.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_encoder_decoder.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_encoder_decoder.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_encoder_decoder.py,7,,not
transformers/tests/test_modeling_encoder_decoder.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_encoder_decoder.py,9,,not
transformers/tests/test_modeling_encoder_decoder.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_encoder_decoder.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_encoder_decoder.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_encoder_decoder.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_encoder_decoder.py,14,limitations under the License.,not
transformers/tests/test_modeling_encoder_decoder.py,22,TODO(PVP): this line reruns all the tests in BertModelTest; not sure whether this can be prevented,SATD
transformers/tests/test_modeling_encoder_decoder.py,23,for now only run module with pytest tests/test_modeling_encoder_decoder.py::EncoderDecoderModelTest,not
transformers/tests/test_modeling_encoder_decoder.py,252,check that backprop works,not
transformers/tests/test_modeling_encoder_decoder.py,284,check that backprop works,not
transformers/tests/test_modeling_encoder_decoder.py,296,"Bert does not have a bos token id, so use pad_token_id instead",not
transformers/tests/test_modeling_tf_openai_gpt.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_openai_gpt.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_openai_gpt.py,3,,not
transformers/tests/test_modeling_tf_openai_gpt.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_openai_gpt.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_openai_gpt.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_openai_gpt.py,7,,not
transformers/tests/test_modeling_tf_openai_gpt.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_openai_gpt.py,9,,not
transformers/tests/test_modeling_tf_openai_gpt.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_openai_gpt.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_openai_gpt.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_openai_gpt.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_openai_gpt.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_openai_gpt.py,44,TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly,SATD
transformers/tests/test_modeling_tf_openai_gpt.py,125,"intermediate_size=self.intermediate_size,",not
transformers/tests/test_modeling_tf_openai_gpt.py,126,"hidden_act=self.hidden_act,",not
transformers/tests/test_modeling_tf_openai_gpt.py,127,"hidden_dropout_prob=self.hidden_dropout_prob,",not
transformers/tests/test_modeling_tf_openai_gpt.py,128,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",not
transformers/tests/test_modeling_tf_openai_gpt.py,131,"type_vocab_size=self.type_vocab_size,",not
transformers/tests/test_modeling_tf_openai_gpt.py,132,initializer_range=self.initializer_range,not
transformers/tests/test_modeling_tf_openai_gpt.py,247,the president is,not
transformers/tests/test_modeling_tf_openai_gpt.py,269,"the president is a very good man. "" \n "" i\'m sure he is, "" said the",not
transformers/tests/test_model_card.py,1,coding=utf-8,not
transformers/tests/test_model_card.py,2,Copyright 2019 HuggingFace Inc.,not
transformers/tests/test_model_card.py,3,,not
transformers/tests/test_model_card.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_model_card.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_model_card.py,6,You may obtain a copy of the License at,not
transformers/tests/test_model_card.py,7,,not
transformers/tests/test_model_card.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_model_card.py,9,,not
transformers/tests/test_model_card.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_model_card.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_model_card.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_model_card.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_model_card.py,14,limitations under the License.,not
transformers/tests/test_modeling_bart.py,1,coding=utf-8,not
transformers/tests/test_modeling_bart.py,2,Copyright 2020 Huggingface,not
transformers/tests/test_modeling_bart.py,3,,not
transformers/tests/test_modeling_bart.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_bart.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_bart.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_bart.py,7,,not
transformers/tests/test_modeling_bart.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_bart.py,9,,not
transformers/tests/test_modeling_bart.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_bart.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_bart.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_bart.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_bart.py,14,limitations under the License.,not
transformers/tests/test_modeling_bart.py,19,noqa,not
transformers/tests/test_modeling_bart.py,76,Eos Token,not
transformers/tests/test_modeling_bart.py,116,TODO(SS): fix the below in a separate PR,SATD
transformers/tests/test_modeling_bart.py,120,This requires inputs_dict['input_ids'],not
transformers/tests/test_modeling_bart.py,121,because BartForConditionalGeneration and BartModel now have identical state_dict,not
transformers/tests/test_modeling_bart.py,131,"(config, input_ids, token_type_ids, input_mask, *unused) = \",not
transformers/tests/test_modeling_bart.py,136,test init,not
transformers/tests/test_modeling_bart.py,163,no hidden states or attentions,not
transformers/tests/test_modeling_bart.py,167,some tokens were masked,not
transformers/tests/test_modeling_bart.py,170,Test different encoder attention masks,not
transformers/tests/test_modeling_bart.py,192,same vocab size,not
transformers/tests/test_modeling_bart.py,193,same tokenizer,not
transformers/tests/test_modeling_bart.py,250,"example_english_phrase = "" UN Chief Says There Is No Military Solution in Syria""",not
transformers/tests/test_modeling_bart.py,251,"inputs: dict = tokenizer.batch_encode_plus([example_english_phrase], return_tensors=""pt"",)",not
transformers/tests/test_modeling_bart.py,256,250004,not
transformers/tests/test_modeling_bart.py,283,"TODO(SS): should be  [8274, ..., 2, 250020]",SATD
transformers/tests/test_modeling_bart.py,319,note padding,not
transformers/tests/test_modeling_bart.py,420,"TODO(SS): uneven length batches, empty inputs",SATD
transformers/tests/test_modeling_bart.py,434,need leading spaces for equality,not
transformers/tests/test_modeling_bart.py,477,"never attend to the final token, because its pad",not
transformers/tests/test_modeling_bart.py,545,eval called in from_pre,not
transformers/tests/test_modeling_bart.py,547,Test that model hasn't changed,not
transformers/tests/test_modeling_bart.py,555,Test that padding does not change results,not
transformers/tests/test_modeling_bart.py,566,Forces 1.6GB download from S3 for each model,not
transformers/tests/test_modeling_bart.py,609,@noqa,not
transformers/tests/test_modeling_bart.py,615,The below article tests that we don't add any hypotheses outside of the top n_beams,not
transformers/tests/test_modeling_bart.py,654,"TODO(SS): run fairseq again with num_beams=2, min_len=20.",SATD
transformers/tests/test_modeling_bart.py,655,TODO(SS): add test case that hits max_length,SATD
transformers/tests/test_modeling_bart.py,672,"extra dim to allow broadcasting, feel free to delete!",not
transformers/tests/test_modeling_bart.py,679,odd num_positions is allowed,not
transformers/tests/test_modeling_bart.py,690,"test that forward pass is just a lookup, there is no ignore padding logic",not
transformers/tests/test_tokenization_t5.py,1,coding=utf-8,not
transformers/tests/test_tokenization_t5.py,2,Copyright 2018 Google T5 Authors and HuggingFace Inc. team.,not
transformers/tests/test_tokenization_t5.py,3,,not
transformers/tests/test_tokenization_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_t5.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_t5.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_t5.py,7,,not
transformers/tests/test_tokenization_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_t5.py,9,,not
transformers/tests/test_tokenization_t5.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_t5.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_t5.py,14,limitations under the License.,not
transformers/tests/test_tokenization_t5.py,36,We have a SentencePiece fixture for testing,not
transformers/tests/test_modeling_common.py,1,coding=utf-8,not
transformers/tests/test_modeling_common.py,2,Copyright 2019 HuggingFace Inc.,not
transformers/tests/test_modeling_common.py,3,,not
transformers/tests/test_modeling_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_common.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_common.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_common.py,7,,not
transformers/tests/test_modeling_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_common.py,9,,not
transformers/tests/test_modeling_common.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_common.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_common.py,14,limitations under the License.,not
transformers/tests/test_modeling_common.py,84,Make sure we don't have nans,not
transformers/tests/test_modeling_common.py,161,loss will come first,not
transformers/tests/test_modeling_common.py,162,compute loss,not
transformers/tests/test_modeling_common.py,174,Check attention is always last and order is fine,not
transformers/tests/test_modeling_common.py,220,To be sure we have no Nan,not
transformers/tests/test_modeling_common.py,226,Let's keep only input_ids,not
transformers/tests/test_modeling_common.py,275,To be sure we have no Nan,not
transformers/tests/test_modeling_common.py,281,Prepare head_mask,not
transformers/tests/test_modeling_common.py,282,Set require_grad after having prepared the tensor to avoid error (leaf variable has been moved into the graph interior),not
transformers/tests/test_modeling_common.py,294,Test that we can get a gradient back for importance score computation,not
transformers/tests/test_modeling_common.py,302,Remove Nan,not
transformers/tests/test_modeling_common.py,306,Check we don't have more than 25% nans (arbitrary),not
transformers/tests/test_modeling_common.py,309,remove them (the test is less complete),not
transformers/tests/test_modeling_common.py,510,Retrieve the embeddings and clone theme,not
transformers/tests/test_modeling_common.py,514,Check that resizing the token embeddings with a larger vocab size increases the model's vocab size,not
transformers/tests/test_modeling_common.py,517,Check that it actually resizes the embeddings matrix,not
transformers/tests/test_modeling_common.py,519,Check that the model can still do a forward pass successfully (every parameter should be resized),not
transformers/tests/test_modeling_common.py,522,Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size,not
transformers/tests/test_modeling_common.py,525,Check that it actually resizes the embeddings matrix,not
transformers/tests/test_modeling_common.py,528,Check that the model can still do a forward pass successfully (every parameter should be resized),not
transformers/tests/test_modeling_common.py,529,Input ids should be clamped to the maximum size of the vocabulary,not
transformers/tests/test_modeling_common.py,533,Check that adding and removing tokens has not modified the first part of the embedding matrix.,not
transformers/tests/test_modeling_common.py,594,Check that the embedding layer and decoding layer are the same in size and in value,not
transformers/tests/test_modeling_common.py,596,"self.assertTrue(check_same_values(embeddings, decoding))",not
transformers/tests/test_modeling_common.py,598,"# Check that after modification, they remain the same.",not
transformers/tests/test_modeling_common.py,599,embeddings.weight.data.div_(2),not
transformers/tests/test_modeling_common.py,600,# Check that the embedding layer and decoding layer are the same in size and in value,not
transformers/tests/test_modeling_common.py,601,"self.assertTrue(embeddings.weight.shape, decoding.weight.shape)",not
transformers/tests/test_modeling_common.py,602,"self.assertTrue(check_same_values(embeddings, decoding))",not
transformers/tests/test_modeling_common.py,604,"# Check that after modification, they remain the same.",not
transformers/tests/test_modeling_common.py,605,decoding.weight.data.div_(4),not
transformers/tests/test_modeling_common.py,606,# Check that the embedding layer and decoding layer are the same in size and in value,not
transformers/tests/test_modeling_common.py,607,"self.assertTrue(embeddings.weight.shape, decoding.weight.shape)",not
transformers/tests/test_modeling_common.py,608,"self.assertTrue(check_same_values(embeddings, decoding))",not
transformers/tests/test_modeling_common.py,610,Check that after resize they remain tied.,not
transformers/tests/test_modeling_common.py,616,decoding.weight.data.mul_(20),not
transformers/tests/test_modeling_common.py,617,# Check that the embedding layer and decoding layer are the same in size and in value,not
transformers/tests/test_modeling_common.py,618,"self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)",not
transformers/tests/test_modeling_common.py,619,"self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))",not
transformers/tests/test_modeling_common.py,652,make sure that input_ids is at most of size 15,not
transformers/tests/test_modeling_common.py,655,iterate over all generative models,not
transformers/tests/test_modeling_common.py,661,"if bos token id is not defined, model needs input_ids",not
transformers/tests/test_modeling_common.py,664,num_return_sequences = 1,not
transformers/tests/test_modeling_common.py,667,num_return_sequences = 1,not
transformers/tests/test_modeling_common.py,671,generating multiple sequences when no beam search generation,not
transformers/tests/test_modeling_common.py,672,is not allowed as it would always generate the same sequences,not
transformers/tests/test_modeling_common.py,675,"num_return_sequences > 1, sample",not
transformers/tests/test_modeling_common.py,678,check bad words tokens language generation,not
transformers/tests/test_modeling_common.py,679,create list of 1-seq bad token and list of 2-seq of bad tokens,not
transformers/tests/test_modeling_common.py,687,only count generated tokens,not
transformers/tests/test_modeling_common.py,697,make sure that input_ids is at most of size 15,not
transformers/tests/test_modeling_common.py,705,"if bos token id is not defined mobel needs input_ids, num_return_sequences = 1",not
transformers/tests/test_modeling_common.py,708,num_return_sequences = 1,not
transformers/tests/test_modeling_common.py,712,generating more sequences than having beams leads is not possible,not
transformers/tests/test_modeling_common.py,715,"num_return_sequences > 1, sample",not
transformers/tests/test_modeling_common.py,717,"num_return_sequences > 1, greedy",not
transformers/tests/test_modeling_common.py,720,check bad words tokens language generation,not
transformers/tests/test_modeling_common.py,721,create list of 1-seq bad token and list of 2-seq of bad tokens,not
transformers/tests/test_modeling_common.py,729,only count generated tokens,not
transformers/tests/test_modeling_common.py,734,special tokens cannot be bad tokens,not
transformers/tests/test_modeling_common.py,736,create random bad tokens that are not special tokens,not
transformers/tests/test_modeling_common.py,750,for all bad word tokens,not
transformers/tests/test_modeling_common.py,752,for all slices in batch,not
transformers/tests/test_modeling_common.py,754,for all word idx,not
transformers/tests/test_modeling_common.py,756,if tokens match,not
transformers/tests/test_modeling_common.py,766,Creates a random int32 tensor of the shape within the vocab size,not
transformers/tests/test_modeling_common.py,824,tests whether the top_k_top_p function behaves as expected,not
transformers/tests/test_modeling_common.py,829,3rd highest value; idx. 0,not
transformers/tests/test_modeling_common.py,838,5th highest value; idx. 9,not
transformers/tests/test_modeling_common.py,839,2nd highest value; idx. 10,not
transformers/tests/test_modeling_common.py,854,4th highest value; idx. 25,not
transformers/tests/test_modeling_common.py,855,1st highest value; idx. 26,not
transformers/tests/test_modeling_common.py,859,cummulative prob of 5 highest values <= 0.6,not
transformers/tests/test_modeling_common.py,874,4th highest value; idx. 13,not
transformers/tests/test_modeling_common.py,878,2nd highest value; idx. 17,not
transformers/tests/test_modeling_common.py,879,5th highest value; idx. 18,not
transformers/tests/test_modeling_common.py,881,3rd highest value; idx. 20,not
transformers/tests/test_modeling_common.py,888,1st highest value; idx. 27,not
transformers/tests/test_modeling_common.py,891,cummulative prob of 5 highest values <= 0.6,not
transformers/tests/test_modeling_common.py,901,expected non filtered idx as noted above,not
transformers/tests/test_modeling_common.py,915,expected non filtered values as noted above,not
transformers/tests/test_configuration_auto.py,1,coding=utf-8,not
transformers/tests/test_configuration_auto.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/tests/test_configuration_auto.py,3,,not
transformers/tests/test_configuration_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_configuration_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_configuration_auto.py,6,You may obtain a copy of the License at,not
transformers/tests/test_configuration_auto.py,7,,not
transformers/tests/test_configuration_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_configuration_auto.py,9,,not
transformers/tests/test_configuration_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_configuration_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_configuration_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_configuration_auto.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_configuration_auto.py,14,limitations under the License.,not
transformers/tests/test_configuration_auto.py,51,no key string should be included in a later key string (typical failure case),not
transformers/tests/test_modeling_tf_albert.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_albert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_albert.py,3,,not
transformers/tests/test_modeling_tf_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_albert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_albert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_albert.py,7,,not
transformers/tests/test_modeling_tf_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_albert.py,9,,not
transformers/tests/test_modeling_tf_albert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_albert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_albert.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_albert.py,142,"inputs = {'input_ids': input_ids,",not
transformers/tests/test_modeling_tf_albert.py,143,"'attention_mask': input_mask,",not
transformers/tests/test_modeling_tf_albert.py,144,'token_type_ids': token_type_ids},not
transformers/tests/test_modeling_tf_albert.py,145,"sequence_output, pooled_output = model(**inputs)",not
transformers/tests/test_modeling_tf_electra.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_electra.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_electra.py,3,,not
transformers/tests/test_modeling_tf_electra.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_electra.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_electra.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_electra.py,7,,not
transformers/tests/test_modeling_tf_electra.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_electra.py,9,,not
transformers/tests/test_modeling_tf_electra.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_electra.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_electra.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_electra.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_electra.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_electra.py,224,for model_name in list(TF_ELECTRA_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_t5.py,1,coding=utf-8,not
transformers/tests/test_modeling_t5.py,2,Copyright 2018 Google T5 Authors and HuggingFace Inc. team.,not
transformers/tests/test_modeling_t5.py,3,,not
transformers/tests/test_modeling_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_t5.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_t5.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_t5.py,7,,not
transformers/tests/test_modeling_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_t5.py,9,,not
transformers/tests/test_modeling_t5.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_t5.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_t5.py,14,limitations under the License.,not
transformers/tests/test_modeling_t5.py,138,make sure that lm_labels are correctly padded from the right,not
transformers/tests/test_modeling_t5.py,141,add casaul pad token mask,not
transformers/tests/test_modeling_t5.py,147,first item,not
transformers/tests/test_modeling_t5.py,151,items before diagonal,not
transformers/tests/test_modeling_t5.py,155,pad items after diagonal,not
transformers/tests/test_modeling_t5.py,161,all items after square,not
transformers/tests/test_modeling_t5.py,192,decoder_past[0] should correspond to encoder output,not
transformers/tests/test_modeling_t5.py,194,There should be `num_layers` key value embeddings stored in decoder_past[1],not
transformers/tests/test_modeling_t5.py,196,"There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past[1] tuple",not
transformers/tests/test_modeling_t5.py,229,first forward pass,not
transformers/tests/test_modeling_t5.py,232,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_t5.py,235,append to next input_ids and,not
transformers/tests/test_modeling_t5.py,241,select random slice,not
transformers/tests/test_modeling_t5.py,246,test that outputs are equal for slice,not
transformers/tests/test_modeling_t5.py,256,create attention mask,not
transformers/tests/test_modeling_t5.py,262,first forward pass,not
transformers/tests/test_modeling_t5.py,265,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_t5.py,268,change a random masked slice from input_ids,not
transformers/tests/test_modeling_t5.py,273,append to next input_ids and attn_mask,not
transformers/tests/test_modeling_t5.py,279,get two different outputs,not
transformers/tests/test_modeling_t5.py,285,select random slice,not
transformers/tests/test_modeling_t5.py,290,test that outputs are equal for slice,not
transformers/tests/test_modeling_t5.py,372,@noqa,not
transformers/tests/test_tokenization_ctrl.py,1,coding=utf-8,not
transformers/tests/test_tokenization_ctrl.py,2,Copyright 2018 Salesforce and HuggingFace Inc. team.,not
transformers/tests/test_tokenization_ctrl.py,3,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_ctrl.py,4,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_ctrl.py,5,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_ctrl.py,6,,not
transformers/tests/test_tokenization_ctrl.py,7,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_ctrl.py,8,,not
transformers/tests/test_tokenization_ctrl.py,9,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_ctrl.py,10,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_ctrl.py,11,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_ctrl.py,12,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_ctrl.py,13,limitations under the License.,not
transformers/tests/test_tokenization_ctrl.py,32,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,not
transformers/tests/test_modeling_tf_ctrl.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_ctrl.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_ctrl.py,3,,not
transformers/tests/test_modeling_tf_ctrl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_ctrl.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_ctrl.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_ctrl.py,7,,not
transformers/tests/test_modeling_tf_ctrl.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_ctrl.py,9,,not
transformers/tests/test_modeling_tf_ctrl.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_ctrl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_ctrl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_ctrl.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_ctrl.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_ctrl.py,116,"intermediate_size=self.intermediate_size,",not
transformers/tests/test_modeling_tf_ctrl.py,117,"hidden_act=self.hidden_act,",not
transformers/tests/test_modeling_tf_ctrl.py,118,"hidden_dropout_prob=self.hidden_dropout_prob,",not
transformers/tests/test_modeling_tf_ctrl.py,119,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",not
transformers/tests/test_modeling_tf_ctrl.py,122,"type_vocab_size=self.type_vocab_size,",not
transformers/tests/test_modeling_tf_ctrl.py,123,initializer_range=self.initializer_range,not
transformers/tests/test_modeling_tf_ctrl.py,145,None is the input for 'past',not
transformers/tests/test_modeling_tf_ctrl.py,212,Legal the president is,not
transformers/tests/test_modeling_tf_ctrl.py,234,Legal the president is a good guy and I don't want to lose my job. \n \n I have a,not
transformers/tests/test_modeling_openai.py,1,coding=utf-8,not
transformers/tests/test_modeling_openai.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_openai.py,3,,not
transformers/tests/test_modeling_openai.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_openai.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_openai.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_openai.py,7,,not
transformers/tests/test_modeling_openai.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_openai.py,9,,not
transformers/tests/test_modeling_openai.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_openai.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_openai.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_openai.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_openai.py,14,limitations under the License.,not
transformers/tests/test_modeling_openai.py,45,TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly,SATD
transformers/tests/test_modeling_openai.py,114,"intermediate_size=self.intermediate_size,",not
transformers/tests/test_modeling_openai.py,115,"hidden_act=self.hidden_act,",not
transformers/tests/test_modeling_openai.py,116,"hidden_dropout_prob=self.hidden_dropout_prob,",not
transformers/tests/test_modeling_openai.py,117,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",not
transformers/tests/test_modeling_openai.py,120,"type_vocab_size=self.type_vocab_size,",not
transformers/tests/test_modeling_openai.py,121,initializer_range=self.initializer_range,not
transformers/tests/test_modeling_openai.py,230,the president is,not
transformers/tests/test_modeling_openai.py,252,"the president is a very good man. "" \n "" i\'m sure he is, "" said the",not
transformers/tests/test_modeling_camembert.py,1,coding=utf-8,not
transformers/tests/test_modeling_camembert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_camembert.py,3,,not
transformers/tests/test_modeling_camembert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_camembert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_camembert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_camembert.py,7,,not
transformers/tests/test_modeling_camembert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_camembert.py,9,,not
transformers/tests/test_modeling_camembert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_camembert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_camembert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_camembert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_camembert.py,14,limitations under the License.,not
transformers/tests/test_modeling_camembert.py,36,J'aime le camembert !,not
transformers/tests/test_modeling_camembert.py,40,compare the actual values for a slice.,not
transformers/tests/test_modeling_camembert.py,46,"camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')",not
transformers/tests/test_modeling_camembert.py,47,camembert.eval(),not
transformers/tests/test_modeling_camembert.py,48,"expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()",not
transformers/tests/test_configuration_common.py,1,coding=utf-8,not
transformers/tests/test_configuration_common.py,2,Copyright 2019 HuggingFace Inc.,not
transformers/tests/test_configuration_common.py,3,,not
transformers/tests/test_configuration_common.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_configuration_common.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_configuration_common.py,6,You may obtain a copy of the License at,not
transformers/tests/test_configuration_common.py,7,,not
transformers/tests/test_configuration_common.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_configuration_common.py,9,,not
transformers/tests/test_configuration_common.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_configuration_common.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_configuration_common.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_configuration_common.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_configuration_common.py,14,limitations under the License.,not
transformers/tests/test_tokenization_xlm.py,1,coding=utf-8,not
transformers/tests/test_tokenization_xlm.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_xlm.py,3,,not
transformers/tests/test_tokenization_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_xlm.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_xlm.py,7,,not
transformers/tests/test_tokenization_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_xlm.py,9,,not
transformers/tests/test_tokenization_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_xlm.py,14,limitations under the License.,not
transformers/tests/test_tokenization_xlm.py,34,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,not
transformers/tests/test_trainer.py,52,^ causal lm,not
transformers/tests/test_trainer.py,57,Expect error due to padding token missing on gpt2:,not
transformers/tests/test_trainer.py,70,^ masked lm,not
transformers/tests/test_modeling_gpt2.py,1,coding=utf-8,not
transformers/tests/test_modeling_gpt2.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_gpt2.py,3,,not
transformers/tests/test_modeling_gpt2.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_gpt2.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_gpt2.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_gpt2.py,7,,not
transformers/tests/test_modeling_gpt2.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_gpt2.py,9,,not
transformers/tests/test_modeling_gpt2.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_gpt2.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_gpt2.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_gpt2.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_gpt2.py,14,limitations under the License.,not
transformers/tests/test_modeling_gpt2.py,43,TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly,SATD
transformers/tests/test_modeling_gpt2.py,126,"intermediate_size=self.intermediate_size,",not
transformers/tests/test_modeling_gpt2.py,127,"hidden_act=self.hidden_act,",not
transformers/tests/test_modeling_gpt2.py,128,"hidden_dropout_prob=self.hidden_dropout_prob,",not
transformers/tests/test_modeling_gpt2.py,129,"attention_probs_dropout_prob=self.attention_probs_dropout_prob,",not
transformers/tests/test_modeling_gpt2.py,132,"type_vocab_size=self.type_vocab_size,",not
transformers/tests/test_modeling_gpt2.py,133,initializer_range=self.initializer_range,not
transformers/tests/test_modeling_gpt2.py,178,first forward pass,not
transformers/tests/test_modeling_gpt2.py,181,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_gpt2.py,185,append to next input_ids and token_type_ids,not
transformers/tests/test_modeling_gpt2.py,192,select random slice,not
transformers/tests/test_modeling_gpt2.py,197,test that outputs are equal for slice,not
transformers/tests/test_modeling_gpt2.py,207,create attention mask,not
transformers/tests/test_modeling_gpt2.py,212,first forward pass,not
transformers/tests/test_modeling_gpt2.py,215,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_gpt2.py,218,change a random masked slice from input_ids,not
transformers/tests/test_modeling_gpt2.py,223,append to next input_ids and attn_mask,not
transformers/tests/test_modeling_gpt2.py,229,get two different outputs,not
transformers/tests/test_modeling_gpt2.py,233,select random slice,not
transformers/tests/test_modeling_gpt2.py,238,test that outputs are equal for slice,not
transformers/tests/test_modeling_gpt2.py,346,The dog,not
transformers/tests/test_modeling_gpt2.py,368,The dog was found in a field near the intersection of West and West Streets.\n\nThe dog,not
transformers/tests/test_modeling_gpt2.py,375,The president,not
transformers/tests/test_modeling_gpt2.py,397,"The president of the United States, and the president of the United Kingdom, have been in the White",not
transformers/tests/test_modeling_xlm.py,1,coding=utf-8,not
transformers/tests/test_modeling_xlm.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_xlm.py,3,,not
transformers/tests/test_modeling_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_xlm.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_xlm.py,7,,not
transformers/tests/test_modeling_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_xlm.py,9,,not
transformers/tests/test_modeling_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_xlm.py,14,limitations under the License.,not
transformers/tests/test_modeling_xlm.py,56,TODO (PVP): Check other models whether language generation is also applicable,SATD
transformers/tests/test_modeling_xlm.py,130,small variation of seq_length,not
transformers/tests/test_modeling_xlm.py,437,the president,not
transformers/tests/test_modeling_xlm.py,459,the president the president the president the president the president the president the president the president the president the president,not
transformers/tests/test_modeling_xlm.py,460,TODO(PVP): this and other input_ids I tried for generation give pretty bad results. Not sure why. Model might just not be made for auto-regressive inference,SATD
transformers/tests/test_modeling_bert.py,1,coding=utf-8,not
transformers/tests/test_modeling_bert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_bert.py,3,,not
transformers/tests/test_modeling_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_bert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_bert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_bert.py,7,,not
transformers/tests/test_modeling_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_bert.py,9,,not
transformers/tests/test_modeling_bert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_bert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_bert.py,14,limitations under the License.,not
transformers/tests/test_modeling_bert.py,436,This regression test was failing with PyTorch < 1.3,not
transformers/tests/test_tokenization_xlm_roberta.py,1,coding=utf-8,not
transformers/tests/test_tokenization_xlm_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_xlm_roberta.py,3,,not
transformers/tests/test_tokenization_xlm_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_xlm_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_xlm_roberta.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_xlm_roberta.py,7,,not
transformers/tests/test_tokenization_xlm_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_xlm_roberta.py,9,,not
transformers/tests/test_tokenization_xlm_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_xlm_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_xlm_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_xlm_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_xlm_roberta.py,14,limitations under the License.,not
transformers/tests/test_tokenization_xlm_roberta.py,36,We have a SentencePiece fixture for testing,not
transformers/tests/test_tokenization_xlm_roberta.py,92,^ unk: 2 + 1 = 3                  unk: 2 + 1 = 3 ^,not
transformers/tests/test_tokenization_xlm_roberta.py,130,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')  # xlmr.large has same tokenizer",not
transformers/tests/test_tokenization_xlm_roberta.py,131,xlmr.eval(),not
transformers/tests/test_tokenization_xlm_roberta.py,132,xlmr.encode(symbols),not
transformers/tests/test_tokenization_xlm_roberta.py,191,"4426, # What fairseq tokenizes from ""<unk>"": ""_<""",not
transformers/tests/test_tokenization_xlm_roberta.py,192,"3678, # What fairseq tokenizes from ""<unk>"": ""unk""",not
transformers/tests/test_tokenization_xlm_roberta.py,193,"2740, # What fairseq tokenizes from ""<unk>"": "">""",not
transformers/tests/test_tokenization_xlm_roberta.py,194,"What we tokenize from ""<unk>"": ""<unk>""",not
transformers/tests/test_tokenization_xlm_roberta.py,195,Residue from the tokenization: an extra sentencepiece underline,not
transformers/tests/test_tokenization_xlm_roberta.py,208,"xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.base')  # xlmr.large has same tokenizer",not
transformers/tests/test_tokenization_xlm_roberta.py,209,xlmr.eval(),not
transformers/tests/test_tokenization_xlm_roberta.py,210,xlmr.encode(symbols),not
transformers/tests/test_modeling_tf_xlm.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_xlm.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_xlm.py,3,,not
transformers/tests/test_modeling_tf_xlm.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_xlm.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_xlm.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_xlm.py,7,,not
transformers/tests/test_modeling_tf_xlm.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_xlm.py,9,,not
transformers/tests/test_modeling_tf_xlm.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_xlm.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_xlm.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_xlm.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_xlm.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_xlm.py,48,TODO (PVP): Check other models whether language generation is also applicable,SATD
transformers/tests/test_modeling_tf_xlm.py,122,small variation of seq_length,not
transformers/tests/test_modeling_tf_xlm.py,320,the president,not
transformers/tests/test_modeling_tf_xlm.py,342,the president the president the president the president the president the president the president the president the president the president,not
transformers/tests/test_modeling_tf_xlm.py,343,TODO(PVP): this and other input_ids I tried for generation give pretty bad results. Not sure why. Model might just not be made for auto-regressive inference,SATD
transformers/tests/test_tokenization_xlnet.py,1,coding=utf-8,not
transformers/tests/test_tokenization_xlnet.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_xlnet.py,3,,not
transformers/tests/test_tokenization_xlnet.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_xlnet.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_xlnet.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_xlnet.py,7,,not
transformers/tests/test_tokenization_xlnet.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_xlnet.py,9,,not
transformers/tests/test_tokenization_xlnet.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_xlnet.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_xlnet.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_xlnet.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_xlnet.py,14,limitations under the License.,not
transformers/tests/test_tokenization_xlnet.py,36,We have a SentencePiece fixture for testing,not
transformers/tests/test_modeling_albert.py,1,coding=utf-8,not
transformers/tests/test_modeling_albert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_albert.py,3,,not
transformers/tests/test_modeling_albert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_albert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_albert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_albert.py,7,,not
transformers/tests/test_modeling_albert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_albert.py,9,,not
transformers/tests/test_modeling_albert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_albert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_albert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_albert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_albert.py,14,limitations under the License.,not
transformers/tests/test_tokenization_auto.py,1,coding=utf-8,not
transformers/tests/test_tokenization_auto.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_auto.py,3,,not
transformers/tests/test_tokenization_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_auto.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_auto.py,7,,not
transformers/tests/test_tokenization_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_auto.py,9,,not
transformers/tests/test_tokenization_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_auto.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_auto.py,14,limitations under the License.,not
transformers/tests/test_tokenization_auto.py,33,noqa: F401,not
transformers/tests/test_tokenization_auto.py,37,@slow,not
transformers/tests/test_tokenization_auto.py,84,"Test that the children are placed before the parents in the mappings, as the `instanceof` will be triggered",not
transformers/tests/test_tokenization_auto.py,85,by the parents and will return the wrong configuration type when using auto models,not
transformers/tests/test_tokenization_auto.py,99,Check for Fast tokenizer implementation if provided,not
transformers/tests/test_modeling_tf_roberta.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_roberta.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_roberta.py,3,,not
transformers/tests/test_modeling_tf_roberta.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_roberta.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_roberta.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_roberta.py,7,,not
transformers/tests/test_modeling_tf_roberta.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_roberta.py,9,,not
transformers/tests/test_modeling_tf_roberta.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_roberta.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_roberta.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_roberta.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_roberta.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_roberta.py,249,compare the actual values for a slice.,not
transformers/tests/test_modeling_tf_roberta.py,261,compare the actual values for a slice.,not
transformers/tests/test_tokenization_openai.py,1,coding=utf-8,not
transformers/tests/test_tokenization_openai.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_openai.py,3,,not
transformers/tests/test_tokenization_openai.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_openai.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_openai.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_openai.py,7,,not
transformers/tests/test_tokenization_openai.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_openai.py,9,,not
transformers/tests/test_tokenization_openai.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_openai.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_openai.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_openai.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_openai.py,14,limitations under the License.,not
transformers/tests/test_tokenization_openai.py,33,Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt,not
transformers/tests/test_tokenization_transfo_xl.py,1,coding=utf-8,not
transformers/tests/test_tokenization_transfo_xl.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_tokenization_transfo_xl.py,3,,not
transformers/tests/test_tokenization_transfo_xl.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_tokenization_transfo_xl.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_tokenization_transfo_xl.py,6,You may obtain a copy of the License at,not
transformers/tests/test_tokenization_transfo_xl.py,7,,not
transformers/tests/test_tokenization_transfo_xl.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_tokenization_transfo_xl.py,9,,not
transformers/tests/test_tokenization_transfo_xl.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_tokenization_transfo_xl.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_tokenization_transfo_xl.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_tokenization_transfo_xl.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_tokenization_transfo_xl.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_bert.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_bert.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_bert.py,3,,not
transformers/tests/test_modeling_tf_bert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_bert.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_bert.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_bert.py,7,,not
transformers/tests/test_modeling_tf_bert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_bert.py,9,,not
transformers/tests/test_modeling_tf_bert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_bert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_bert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_bert.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_bert.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_bert.py,314,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_auto.py,1,coding=utf-8,not
transformers/tests/test_modeling_auto.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_auto.py,3,,not
transformers/tests/test_modeling_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_auto.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_auto.py,7,,not
transformers/tests/test_modeling_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_auto.py,9,,not
transformers/tests/test_modeling_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_auto.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_auto.py,14,limitations under the License.,not
transformers/tests/test_modeling_auto.py,155,"Test that the children are placed before the parents in the mappings, as the `instanceof` will be triggered",not
transformers/tests/test_modeling_auto.py,156,by the parents and will return the wrong configuration type when using auto models,not
transformers/tests/test_modeling_tf_t5.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_t5.py,2,Copyright 2018 Google T5 Authors and HuggingFace Inc. team.,not
transformers/tests/test_modeling_tf_t5.py,3,,not
transformers/tests/test_modeling_tf_t5.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_t5.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_t5.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_t5.py,7,,not
transformers/tests/test_modeling_tf_t5.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_t5.py,9,,not
transformers/tests/test_modeling_tf_t5.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_t5.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_t5.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_t5.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_t5.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_t5.py,132,decoder_past[0] should correspond to encoder output,not
transformers/tests/test_modeling_tf_t5.py,134,There should be `num_layers` key value embeddings stored in decoder_past[1],not
transformers/tests/test_modeling_tf_t5.py,136,"There should be a self attn key, a self attn value, a cross attn key and a cross attn value stored in each decoder_past[1] tuple",not
transformers/tests/test_modeling_tf_t5.py,162,first forward pass,not
transformers/tests/test_modeling_tf_t5.py,165,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_tf_t5.py,168,append to next input_ids and,not
transformers/tests/test_modeling_tf_t5.py,174,select random slice,not
transformers/tests/test_modeling_tf_t5.py,179,test that outputs are equal for slice,not
transformers/tests/test_modeling_tf_t5.py,187,create attention mask,not
transformers/tests/test_modeling_tf_t5.py,193,first forward pass,not
transformers/tests/test_modeling_tf_t5.py,196,create hypothetical next token and extent to next_input_ids,not
transformers/tests/test_modeling_tf_t5.py,199,change a random masked slice from input_ids,not
transformers/tests/test_modeling_tf_t5.py,208,append to next input_ids and attn_mask,not
transformers/tests/test_modeling_tf_t5.py,212,get two different outputs,not
transformers/tests/test_modeling_tf_t5.py,218,select random slice,not
transformers/tests/test_modeling_tf_t5.py,223,test that outputs are equal for slice,not
transformers/tests/test_modeling_tf_t5.py,274,@noqa,not
transformers/tests/test_modeling_tf_auto.py,1,coding=utf-8,not
transformers/tests/test_modeling_tf_auto.py,2,Copyright 2018 The Google AI Language Team Authors.,not
transformers/tests/test_modeling_tf_auto.py,3,,not
transformers/tests/test_modeling_tf_auto.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/tests/test_modeling_tf_auto.py,5,you may not use this file except in compliance with the License.,not
transformers/tests/test_modeling_tf_auto.py,6,You may obtain a copy of the License at,not
transformers/tests/test_modeling_tf_auto.py,7,,not
transformers/tests/test_modeling_tf_auto.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/tests/test_modeling_tf_auto.py,9,,not
transformers/tests/test_modeling_tf_auto.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/tests/test_modeling_tf_auto.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/tests/test_modeling_tf_auto.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/tests/test_modeling_tf_auto.py,13,See the License for the specific language governing permissions and,not
transformers/tests/test_modeling_tf_auto.py,14,limitations under the License.,not
transformers/tests/test_modeling_tf_auto.py,52,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_tf_auto.py,69,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_tf_auto.py,82,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_tf_auto.py,95,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/tests/test_modeling_tf_auto.py,108,for model_name in list(TF_BERT_PRETRAINED_MODEL_ARCHIVE_MAP.keys())[:1]:,not
transformers/examples/test_examples.py,1,coding=utf-8,not
transformers/examples/test_examples.py,2,Copyright 2018 HuggingFace Inc..,not
transformers/examples/test_examples.py,3,,not
transformers/examples/test_examples.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/test_examples.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/test_examples.py,6,You may obtain a copy of the License at,not
transformers/examples/test_examples.py,7,,not
transformers/examples/test_examples.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/test_examples.py,9,,not
transformers/examples/test_examples.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/test_examples.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/test_examples.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/test_examples.py,13,See the License for the specific language governing permissions and,not
transformers/examples/test_examples.py,14,limitations under the License.,not
transformers/examples/benchmarks.py,1,coding=utf-8,not
transformers/examples/benchmarks.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/examples/benchmarks.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/benchmarks.py,4,,not
transformers/examples/benchmarks.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/benchmarks.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/benchmarks.py,7,You may obtain a copy of the License at,not
transformers/examples/benchmarks.py,8,,not
transformers/examples/benchmarks.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/benchmarks.py,10,,not
transformers/examples/benchmarks.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/benchmarks.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/benchmarks.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/benchmarks.py,14,See the License for the specific language governing permissions and,not
transformers/examples/benchmarks.py,15,limitations under the License.,not
transformers/examples/benchmarks.py,18,If checking the tensors placement,not
transformers/examples/benchmarks.py,19,tf.debugging.set_log_device_placement(True),not
transformers/examples/benchmarks.py,466,model.add_memory_hooks()  # Forward method tracing (only for PyTorch models),not
transformers/examples/benchmarks.py,468,Line by line memory tracing (all code in the module `transformers`) works for all models/arbitrary code,not
transformers/examples/benchmarks.py,531,To make sure that the model is traced + that the tensors are on the appropriate device,not
transformers/examples/benchmarks.py,535,Line by line memory tracing (all code in the module `transformers`) works for all models/arbitrary code,not
transformers/examples/xla_spawn.py,35,Optional arguments for the launch helper,not
transformers/examples/xla_spawn.py,38,positional,not
transformers/examples/xla_spawn.py,50,rest from the training program,not
transformers/examples/xla_spawn.py,59,Import training_script as a module.,not
transformers/examples/xla_spawn.py,65,Patch sys.argv,not
transformers/examples/lightning_base.py,182,Log results,not
transformers/examples/lightning_base.py,193,Log and save results to file,not
transformers/examples/lightning_base.py,241,init model,not
transformers/examples/question-answering/run_squad.py,1,coding=utf-8,not
transformers/examples/question-answering/run_squad.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/question-answering/run_squad.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/question-answering/run_squad.py,4,,not
transformers/examples/question-answering/run_squad.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/question-answering/run_squad.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/question-answering/run_squad.py,7,You may obtain a copy of the License at,not
transformers/examples/question-answering/run_squad.py,8,,not
transformers/examples/question-answering/run_squad.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/question-answering/run_squad.py,10,,not
transformers/examples/question-answering/run_squad.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/question-answering/run_squad.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/question-answering/run_squad.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/question-answering/run_squad.py,14,See the License for the specific language governing permissions and,not
transformers/examples/question-answering/run_squad.py,15,limitations under the License.,not
transformers/examples/question-answering/run_squad.py,91,Prepare optimizer and schedule (linear warmup and decay),not
transformers/examples/question-answering/run_squad.py,105,Check if saved optimizer or scheduler states exist,not
transformers/examples/question-answering/run_squad.py,109,Load in optimizer and scheduler states,not
transformers/examples/question-answering/run_squad.py,121,multi-gpu training (should be after apex fp16 initialization),not
transformers/examples/question-answering/run_squad.py,125,Distributed training (should be after apex fp16 initialization),not
transformers/examples/question-answering/run_squad.py,131,Train!,not
transformers/examples/question-answering/run_squad.py,148,Check if continuing training from a checkpoint,not
transformers/examples/question-answering/run_squad.py,151,set global_step to gobal_step of last saved checkpoint from model path,not
transformers/examples/question-answering/run_squad.py,169,Added here for reproductibility,not
transformers/examples/question-answering/run_squad.py,176,Skip past any already trained steps if resuming training,not
transformers/examples/question-answering/run_squad.py,205,model outputs are always tuple in transformers (see doc),not
transformers/examples/question-answering/run_squad.py,209,mean() to average on multi-gpu parallel (not distributed) training,not
transformers/examples/question-answering/run_squad.py,227,Update learning rate schedule,not
transformers/examples/question-answering/run_squad.py,231,Log metrics,not
transformers/examples/question-answering/run_squad.py,233,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/examples/question-answering/run_squad.py,242,Save model checkpoint,not
transformers/examples/question-answering/run_squad.py,247,Take care of distributed/parallel training,SATD
transformers/examples/question-answering/run_squad.py,280,Note that DistributedSampler samples randomly,not
transformers/examples/question-answering/run_squad.py,284,multi-gpu evaluate,not
transformers/examples/question-answering/run_squad.py,288,Eval!,not
transformers/examples/question-answering/run_squad.py,312,XLNet and XLM use more arguments for their predictions,not
transformers/examples/question-answering/run_squad.py,315,for lang_id-sensitive xlm models,not
transformers/examples/question-answering/run_squad.py,324,TODO: i and feature_index are the same number! Simplify by removing enumerate?,SATD
transformers/examples/question-answering/run_squad.py,330,"Some models (XLNet, XLM) use 5 arguments for their predictions, while the other ""simpler""",not
transformers/examples/question-answering/run_squad.py,331,models only use two.,not
transformers/examples/question-answering/run_squad.py,357,Compute predictions,not
transformers/examples/question-answering/run_squad.py,366,XLNet and XLM use a more complex post-processing procedure,not
transformers/examples/question-answering/run_squad.py,403,Compute the F1 and exact scores.,not
transformers/examples/question-answering/run_squad.py,410,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/question-answering/run_squad.py,413,Load data features from cache or dataset file,not
transformers/examples/question-answering/run_squad.py,424,Init features and dataset from cache if it exists,not
transformers/examples/question-answering/run_squad.py,470,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/question-answering/run_squad.py,481,Required parameters,not
transformers/examples/question-answering/run_squad.py,504,Other parameters,not
transformers/examples/question-answering/run_squad.py,687,Setup distant debugging if needed,not
transformers/examples/question-answering/run_squad.py,689,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/question-answering/run_squad.py,696,"Setup CUDA, GPU & distributed training",not
transformers/examples/question-answering/run_squad.py,700,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/examples/question-answering/run_squad.py,707,Setup logging,not
transformers/examples/question-answering/run_squad.py,722,Set seed,not
transformers/examples/question-answering/run_squad.py,725,Load pretrained model and tokenizer,not
transformers/examples/question-answering/run_squad.py,727,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/question-answering/run_squad.py,748,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/question-answering/run_squad.py,755,"Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.",not
transformers/examples/question-answering/run_squad.py,756,"Otherwise it'll default to ""promote"" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=""O2""` will",not
transformers/examples/question-answering/run_squad.py,757,"remove the need for this code, but it is still valid.",not
transformers/examples/question-answering/run_squad.py,766,Training,not
transformers/examples/question-answering/run_squad.py,772,Save the trained model and the tokenizer,not
transformers/examples/question-answering/run_squad.py,774,Create output directory if needed,not
transformers/examples/question-answering/run_squad.py,779,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/examples/question-answering/run_squad.py,780,They can then be reloaded using `from_pretrained()`,not
transformers/examples/question-answering/run_squad.py,781,Take care of distributed/parallel training,SATD
transformers/examples/question-answering/run_squad.py,786,Good practice: save your training arguments together with the trained model,not
transformers/examples/question-answering/run_squad.py,789,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/question-answering/run_squad.py,790,", force_download=True)",not
transformers/examples/question-answering/run_squad.py,794,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,not
transformers/examples/question-answering/run_squad.py,805,Reduce model loading logs,not
transformers/examples/question-answering/run_squad.py,813,Reload the model,not
transformers/examples/question-answering/run_squad.py,815,", force_download=True)",not
transformers/examples/question-answering/run_squad.py,818,Evaluate,not
transformers/examples/language-modeling/run_language_modeling.py,1,coding=utf-8,not
transformers/examples/language-modeling/run_language_modeling.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/language-modeling/run_language_modeling.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/language-modeling/run_language_modeling.py,4,,not
transformers/examples/language-modeling/run_language_modeling.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/language-modeling/run_language_modeling.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/language-modeling/run_language_modeling.py,7,You may obtain a copy of the License at,not
transformers/examples/language-modeling/run_language_modeling.py,8,,not
transformers/examples/language-modeling/run_language_modeling.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/language-modeling/run_language_modeling.py,10,,not
transformers/examples/language-modeling/run_language_modeling.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/language-modeling/run_language_modeling.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/language-modeling/run_language_modeling.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/language-modeling/run_language_modeling.py,14,See the License for the specific language governing permissions and,not
transformers/examples/language-modeling/run_language_modeling.py,15,limitations under the License.,not
transformers/examples/language-modeling/run_language_modeling.py,131,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/language-modeling/run_language_modeling.py,132,or by passing the --help flag to this script.,not
transformers/examples/language-modeling/run_language_modeling.py,133,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/language-modeling/run_language_modeling.py,154,Setup logging,not
transformers/examples/language-modeling/run_language_modeling.py,170,Set seed,not
transformers/examples/language-modeling/run_language_modeling.py,173,Load pretrained model and tokenizer,not
transformers/examples/language-modeling/run_language_modeling.py,174,,not
transformers/examples/language-modeling/run_language_modeling.py,175,Distributed training:,not
transformers/examples/language-modeling/run_language_modeling.py,176,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/language-modeling/run_language_modeling.py,177,download model & vocab.,not
transformers/examples/language-modeling/run_language_modeling.py,218,Our input block size will be the max possible for the model,not
transformers/examples/language-modeling/run_language_modeling.py,222,Get datasets,not
transformers/examples/language-modeling/run_language_modeling.py,237,Initialize our Trainer,not
transformers/examples/language-modeling/run_language_modeling.py,247,Training,not
transformers/examples/language-modeling/run_language_modeling.py,256,"For convenience, we also re-save the tokenizer to the same directory,",not
transformers/examples/language-modeling/run_language_modeling.py,257,so that you can share your model easily on huggingface.co/models =),not
transformers/examples/language-modeling/run_language_modeling.py,261,Evaluation,not
transformers/examples/language-modeling/run_language_modeling.py,284,For xla_spawn (TPUs),not
transformers/examples/adversarial/utils_hans.py,1,coding=utf-8,not
transformers/examples/adversarial/utils_hans.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/adversarial/utils_hans.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/adversarial/utils_hans.py,4,,not
transformers/examples/adversarial/utils_hans.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/adversarial/utils_hans.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/adversarial/utils_hans.py,7,You may obtain a copy of the License at,not
transformers/examples/adversarial/utils_hans.py,8,,not
transformers/examples/adversarial/utils_hans.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/adversarial/utils_hans.py,10,,not
transformers/examples/adversarial/utils_hans.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/adversarial/utils_hans.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/adversarial/utils_hans.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/adversarial/utils_hans.py,14,See the License for the specific language governing permissions and,not
transformers/examples/adversarial/utils_hans.py,15,limitations under the License.,not
transformers/examples/adversarial/hans_processors.py,1,coding=utf-8,not
transformers/examples/adversarial/hans_processors.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/adversarial/hans_processors.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/adversarial/hans_processors.py,4,,not
transformers/examples/adversarial/hans_processors.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/adversarial/hans_processors.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/adversarial/hans_processors.py,7,You may obtain a copy of the License at,not
transformers/examples/adversarial/hans_processors.py,8,,not
transformers/examples/adversarial/hans_processors.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/adversarial/hans_processors.py,10,,not
transformers/examples/adversarial/hans_processors.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/adversarial/hans_processors.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/adversarial/hans_processors.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/adversarial/hans_processors.py,14,See the License for the specific language governing permissions and,not
transformers/examples/adversarial/hans_processors.py,15,limitations under the License.,not
transformers/examples/adversarial/hans_processors.py,92,The mask has 1 for real tokens and 0 for padding tokens. Only real,not
transformers/examples/adversarial/hans_processors.py,93,tokens are attended to.,not
transformers/examples/adversarial/hans_processors.py,96,Zero-pad up to the sequence length.,not
transformers/examples/adversarial/test_hans.py,1,coding=utf-8,not
transformers/examples/adversarial/test_hans.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/adversarial/test_hans.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/adversarial/test_hans.py,4,,not
transformers/examples/adversarial/test_hans.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/adversarial/test_hans.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/adversarial/test_hans.py,7,You may obtain a copy of the License at,not
transformers/examples/adversarial/test_hans.py,8,,not
transformers/examples/adversarial/test_hans.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/adversarial/test_hans.py,10,,not
transformers/examples/adversarial/test_hans.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/adversarial/test_hans.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/adversarial/test_hans.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/adversarial/test_hans.py,14,See the License for the specific language governing permissions and,not
transformers/examples/adversarial/test_hans.py,15,limitations under the License.,not
transformers/examples/adversarial/test_hans.py,109,Prepare optimizer and schedule (linear warmup and decay),not
transformers/examples/adversarial/test_hans.py,130,multi-gpu training (should be after apex fp16 initialization),not
transformers/examples/adversarial/test_hans.py,134,Distributed training (should be after apex fp16 initialization),not
transformers/examples/adversarial/test_hans.py,140,Train!,not
transformers/examples/adversarial/test_hans.py,158,Added here for reproductibility (even between python 2 and 3),not
transformers/examples/adversarial/test_hans.py,168,"XLM, DistilBERT and RoBERTa don't use segment_ids",not
transformers/examples/adversarial/test_hans.py,170,model outputs are always tuple in transformers (see doc),not
transformers/examples/adversarial/test_hans.py,173,mean() to average on multi-gpu parallel training,not
transformers/examples/adversarial/test_hans.py,191,Update learning rate schedule,not
transformers/examples/adversarial/test_hans.py,199,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/examples/adversarial/test_hans.py,213,"print(json.dumps({**logs, **{'step': global_step}}))",not
transformers/examples/adversarial/test_hans.py,216,Save model checkpoint,not
transformers/examples/adversarial/test_hans.py,222,Take care of distributed/parallel training,SATD
transformers/examples/adversarial/test_hans.py,241,"Loop to handle MNLI double evaluation (matched, mis-matched)",not
transformers/examples/adversarial/test_hans.py,253,Note that DistributedSampler samples randomly,not
transformers/examples/adversarial/test_hans.py,257,multi-gpu eval,not
transformers/examples/adversarial/test_hans.py,261,Eval!,not
transformers/examples/adversarial/test_hans.py,278,"XLM, DistilBERT and RoBERTa don't use segment_ids",not
transformers/examples/adversarial/test_hans.py,310,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/adversarial/test_hans.py,314,Load data features from cache or dataset file,not
transformers/examples/adversarial/test_hans.py,333,HACK(label indices are swapped in RoBERTa pretrained model),SATD
transformers/examples/adversarial/test_hans.py,344,pad on the left for xlnet,not
transformers/examples/adversarial/test_hans.py,353,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/adversarial/test_hans.py,355,Convert to Tensors and build dataset,not
transformers/examples/adversarial/test_hans.py,372,Required parameters,not
transformers/examples/adversarial/test_hans.py,409,Other parameters,not
transformers/examples/adversarial/test_hans.py,511,Setup distant debugging if needed,not
transformers/examples/adversarial/test_hans.py,513,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/adversarial/test_hans.py,520,"Setup CUDA, GPU & distributed training",not
transformers/examples/adversarial/test_hans.py,524,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/examples/adversarial/test_hans.py,531,Setup logging,not
transformers/examples/adversarial/test_hans.py,546,Set seed,not
transformers/examples/adversarial/test_hans.py,549,Prepare GLUE task,not
transformers/examples/adversarial/test_hans.py,558,Load pretrained model and tokenizer,not
transformers/examples/adversarial/test_hans.py,560,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/adversarial/test_hans.py,583,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/adversarial/test_hans.py,589,Training,not
transformers/examples/adversarial/test_hans.py,595,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",not
transformers/examples/adversarial/test_hans.py,597,Create output directory if needed,not
transformers/examples/adversarial/test_hans.py,602,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/examples/adversarial/test_hans.py,603,They can then be reloaded using `from_pretrained()`,not
transformers/examples/adversarial/test_hans.py,606,Take care of distributed/parallel training,SATD
transformers/examples/adversarial/test_hans.py,610,Good practice: save your training arguments together with the trained model,not
transformers/examples/adversarial/test_hans.py,613,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/adversarial/test_hans.py,618,Evaluation,not
transformers/examples/adversarial/test_hans.py,627,Reduce logging,not
transformers/examples/contrib/run_openai_gpt.py,1,coding=utf-8,not
transformers/examples/contrib/run_openai_gpt.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/examples/contrib/run_openai_gpt.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/contrib/run_openai_gpt.py,4,,not
transformers/examples/contrib/run_openai_gpt.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/contrib/run_openai_gpt.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/contrib/run_openai_gpt.py,7,You may obtain a copy of the License at,not
transformers/examples/contrib/run_openai_gpt.py,8,,not
transformers/examples/contrib/run_openai_gpt.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/contrib/run_openai_gpt.py,10,,not
transformers/examples/contrib/run_openai_gpt.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/contrib/run_openai_gpt.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/contrib/run_openai_gpt.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/contrib/run_openai_gpt.py,14,See the License for the specific language governing permissions and,not
transformers/examples/contrib/run_openai_gpt.py,15,limitations under the License.,not
transformers/examples/contrib/run_openai_gpt.py,67,skip the first line,not
transformers/examples/contrib/run_openai_gpt.py,148,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/contrib/run_openai_gpt.py,170,Load tokenizer and model,not
transformers/examples/contrib/run_openai_gpt.py,171,This loading functions also add new tokens and embeddings called `special tokens`,not
transformers/examples/contrib/run_openai_gpt.py,172,These new embeddings will be fine-tuned on the RocStories dataset,not
transformers/examples/contrib/run_openai_gpt.py,181,Load and encode the datasets,not
transformers/examples/contrib/run_openai_gpt.py,196,Compute the max input length for the Transformer,not
transformers/examples/contrib/run_openai_gpt.py,203,Max size of input for the pre-trained model,not
transformers/examples/contrib/run_openai_gpt.py,205,Prepare inputs tensors and dataloaders,not
transformers/examples/contrib/run_openai_gpt.py,217,Prepare optimizer,not
transformers/examples/contrib/run_openai_gpt.py,262,Save a trained model,not
transformers/examples/contrib/run_openai_gpt.py,264,"Save a trained model, configuration and tokenizer",not
transformers/examples/contrib/run_openai_gpt.py,265,Only save the model itself,not
transformers/examples/contrib/run_openai_gpt.py,267,"If we save using the predefined names, we can load using `from_pretrained`",not
transformers/examples/contrib/run_openai_gpt.py,275,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/contrib/run_camembert.py,8,Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py,not
transformers/examples/contrib/run_camembert.py,10,Batch size 1,not
transformers/examples/contrib/run_camembert.py,11,The last hidden-state is the first element of the output tuple,not
transformers/examples/contrib/run_swag.py,1,coding=utf-8,not
transformers/examples/contrib/run_swag.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/contrib/run_swag.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/contrib/run_swag.py,4,,not
transformers/examples/contrib/run_swag.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/contrib/run_swag.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/contrib/run_swag.py,7,You may obtain a copy of the License at,not
transformers/examples/contrib/run_swag.py,8,,not
transformers/examples/contrib/run_swag.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/contrib/run_swag.py,10,,not
transformers/examples/contrib/run_swag.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/contrib/run_swag.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/contrib/run_swag.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/contrib/run_swag.py,14,See the License for the specific language governing permissions and,not
transformers/examples/contrib/run_swag.py,15,limitations under the License.,not
transformers/examples/contrib/run_swag.py,115,"in the swag dataset, the",not
transformers/examples/contrib/run_swag.py,116,common beginning of each,not
transformers/examples/contrib/run_swag.py,117,"choice is stored in ""sent2"".",not
transformers/examples/contrib/run_swag.py,124,we skip the line with the column names,not
transformers/examples/contrib/run_swag.py,133,"Swag is a multiple choice task. To perform this task using Bert,",not
transformers/examples/contrib/run_swag.py,134,"we will use the formatting proposed in ""Improving Language",SATD
transformers/examples/contrib/run_swag.py,135,"Understanding by Generative Pre-Training"" and suggested by",not
transformers/examples/contrib/run_swag.py,136,@jacobdevlin-google in this issue,not
transformers/examples/contrib/run_swag.py,137,https://github.com/google-research/bert/issues/38.,not
transformers/examples/contrib/run_swag.py,138,,not
transformers/examples/contrib/run_swag.py,139,Each choice will correspond to a sample on which we run the,not
transformers/examples/contrib/run_swag.py,140,"inference. For a given Swag example, we will create the 4",not
transformers/examples/contrib/run_swag.py,141,following inputs:,not
transformers/examples/contrib/run_swag.py,142,- [CLS] context [SEP] choice_1 [SEP],not
transformers/examples/contrib/run_swag.py,143,- [CLS] context [SEP] choice_2 [SEP],not
transformers/examples/contrib/run_swag.py,144,- [CLS] context [SEP] choice_3 [SEP],not
transformers/examples/contrib/run_swag.py,145,- [CLS] context [SEP] choice_4 [SEP],not
transformers/examples/contrib/run_swag.py,146,The model will output a single value for each input. To get the,not
transformers/examples/contrib/run_swag.py,147,"final decision of the model, we will run a softmax over these 4",not
transformers/examples/contrib/run_swag.py,148,outputs.,not
transformers/examples/contrib/run_swag.py,156,We create a copy of the context tokens in order to be,not
transformers/examples/contrib/run_swag.py,157,able to shrink it according to ending_tokens,not
transformers/examples/contrib/run_swag.py,160,Modifies `context_tokens_choice` and `ending_tokens` in,not
transformers/examples/contrib/run_swag.py,161,place so that the total length is less than the,not
transformers/examples/contrib/run_swag.py,162,"specified length.  Account for [CLS], [SEP], [SEP] with",not
transformers/examples/contrib/run_swag.py,163,"""- 3""",not
transformers/examples/contrib/run_swag.py,172,Zero-pad up to the sequence length.,not
transformers/examples/contrib/run_swag.py,205,This is a simple heuristic which will always truncate the longer sequence,not
transformers/examples/contrib/run_swag.py,206,one token at a time. This makes more sense than truncating an equal percent,not
transformers/examples/contrib/run_swag.py,207,"of tokens from each, since if one sequence is very short then each token",not
transformers/examples/contrib/run_swag.py,208,that's truncated likely contains more information than a longer sequence.,not
transformers/examples/contrib/run_swag.py,238,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/contrib/run_swag.py,240,Load data features from cache or dataset file,not
transformers/examples/contrib/run_swag.py,263,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/contrib/run_swag.py,265,Convert to Tensors and build dataset,not
transformers/examples/contrib/run_swag.py,296,Prepare optimizer and schedule (linear warmup and decay),not
transformers/examples/contrib/run_swag.py,316,multi-gpu training (should be after apex fp16 initialization),not
transformers/examples/contrib/run_swag.py,320,Distributed training (should be after apex fp16 initialization),not
transformers/examples/contrib/run_swag.py,326,Train!,not
transformers/examples/contrib/run_swag.py,344,Added here for reproductibility,not
transformers/examples/contrib/run_swag.py,353,"'token_type_ids':  None if args.model_type == 'xlm' else batch[2],",not
transformers/examples/contrib/run_swag.py,357,"if args.model_type in ['xlnet', 'xlm']:",not
transformers/examples/contrib/run_swag.py,358,"inputs.update({'cls_index': batch[5],",not
transformers/examples/contrib/run_swag.py,359,'p_mask':       batch[6]}),not
transformers/examples/contrib/run_swag.py,361,model outputs are always tuple in transformers (see doc),not
transformers/examples/contrib/run_swag.py,364,mean() to average on multi-gpu parallel (not distributed) training,not
transformers/examples/contrib/run_swag.py,379,Update learning rate schedule,not
transformers/examples/contrib/run_swag.py,384,Log metrics,not
transformers/examples/contrib/run_swag.py,387,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/examples/contrib/run_swag.py,396,Save model checkpoint,not
transformers/examples/contrib/run_swag.py,402,Take care of distributed/parallel training,SATD
transformers/examples/contrib/run_swag.py,428,Note that DistributedSampler samples randomly,not
transformers/examples/contrib/run_swag.py,432,Eval!,not
transformers/examples/contrib/run_swag.py,447,'token_type_ids': None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids,not
transformers/examples/contrib/run_swag.py,452,"if args.model_type in ['xlnet', 'xlm']:",not
transformers/examples/contrib/run_swag.py,453,"inputs.update({'cls_index': batch[4],",not
transformers/examples/contrib/run_swag.py,454,'p_mask':    batch[5]}),not
transformers/examples/contrib/run_swag.py,484,Required parameters,not
transformers/examples/contrib/run_swag.py,517,Other parameters,not
transformers/examples/contrib/run_swag.py,613,Setup distant debugging if needed,not
transformers/examples/contrib/run_swag.py,615,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/contrib/run_swag.py,622,"Setup CUDA, GPU & distributed training",not
transformers/examples/contrib/run_swag.py,626,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/examples/contrib/run_swag.py,633,Setup logging,not
transformers/examples/contrib/run_swag.py,648,Set seed,not
transformers/examples/contrib/run_swag.py,651,Load pretrained model and tokenizer,not
transformers/examples/contrib/run_swag.py,653,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/contrib/run_swag.py,666,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/contrib/run_swag.py,672,Training,not
transformers/examples/contrib/run_swag.py,678,Save the trained model and the tokenizer,not
transformers/examples/contrib/run_swag.py,680,Create output directory if needed,not
transformers/examples/contrib/run_swag.py,685,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/examples/contrib/run_swag.py,686,They can then be reloaded using `from_pretrained()`,not
transformers/examples/contrib/run_swag.py,689,Take care of distributed/parallel training,SATD
transformers/examples/contrib/run_swag.py,693,Good practice: save your training arguments together with the trained model,not
transformers/examples/contrib/run_swag.py,696,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/contrib/run_swag.py,701,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,not
transformers/examples/contrib/run_swag.py,707,"if do_train is False and do_eval is true, load model directly from pretrained.",not
transformers/examples/contrib/run_swag.py,714,Reduce model loading logs,not
transformers/examples/contrib/run_swag.py,719,Reload the model,not
transformers/examples/contrib/run_swag.py,725,Evaluate,not
transformers/examples/contrib/run_transfo_xl.py,1,coding=utf-8,not
transformers/examples/contrib/run_transfo_xl.py,2,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/examples/contrib/run_transfo_xl.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/contrib/run_transfo_xl.py,4,,not
transformers/examples/contrib/run_transfo_xl.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/contrib/run_transfo_xl.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/contrib/run_transfo_xl.py,7,You may obtain a copy of the License at,not
transformers/examples/contrib/run_transfo_xl.py,8,,not
transformers/examples/contrib/run_transfo_xl.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/contrib/run_transfo_xl.py,10,,not
transformers/examples/contrib/run_transfo_xl.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/contrib/run_transfo_xl.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/contrib/run_transfo_xl.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/contrib/run_transfo_xl.py,14,See the License for the specific language governing permissions and,not
transformers/examples/contrib/run_transfo_xl.py,15,limitations under the License.,not
transformers/examples/contrib/run_transfo_xl.py,61,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/contrib/run_transfo_xl.py,71,Load a pre-processed dataset,not
transformers/examples/contrib/run_transfo_xl.py,72,You can also build the corpus yourself using TransfoXLCorpus methods,not
transformers/examples/contrib/run_transfo_xl.py,73,The pre-processing involve computing word frequencies to prepare the Adaptive input and SoftMax,not
transformers/examples/contrib/run_transfo_xl.py,74,and tokenizing the dataset,not
transformers/examples/contrib/run_transfo_xl.py,75,The pre-processed corpus is a convertion (using the conversion script ),not
transformers/examples/contrib/run_transfo_xl.py,81,Load a pre-trained model,not
transformers/examples/contrib/run_transfo_xl.py,97,,not
transformers/examples/contrib/run_transfo_xl.py,98,Evaluation code,not
transformers/examples/contrib/run_transfo_xl.py,99,,not
transformers/examples/contrib/run_transfo_xl.py,101,Turn on evaluation mode which disables dropout.,not
transformers/examples/contrib/run_transfo_xl.py,117,Run on test data.,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,1,coding=utf-8,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,2,"Copyright (c) Facebook, Inc. and its affiliates.",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,3,Copyright (c) HuggingFace Inc. team.,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,4,,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,7,You may obtain a copy of the License at,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,8,,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,10,,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,14,See the License for the specific language governing permissions and,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,15,limitations under the License.,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,117,Prepare optimizer and schedule (linear warmup and decay),not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,138,multi-gpu training (should be after apex fp16 initialization),not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,142,Distributed training (should be after apex fp16 initialization),not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,148,Train!,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,167,Added here for reproductibility,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,182,model outputs are always tuple in transformers (see doc),not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,186,mean() to average on multi-gpu parallel training,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,204,Update learning rate schedule,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,212,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,229,Save model checkpoint,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,235,Take care of distributed/parallel training,SATD
transformers/examples/contrib/mm-imdb/run_mmimdb.py,266,"Loop to handle MNLI double evaluation (matched, mis-matched)",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,274,Note that DistributedSampler samples randomly,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,280,multi-gpu eval,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,284,Eval!,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,307,model outputs are always tuple in transformers (see doc),not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,346,Required parameters,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,376,Other parameters,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,483,Setup distant debugging if needed,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,485,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,492,"Setup CUDA, GPU & distributed training",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,496,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,504,Setup logging,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,519,Set seed,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,522,Load pretrained model and tokenizer,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,524,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,526,Setup model,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,547,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,553,Training,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,565,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,567,Create output directory if needed,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,572,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,573,They can then be reloaded using `from_pretrained()`,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,576,Take care of distributed/parallel training,SATD
transformers/examples/contrib/mm-imdb/run_mmimdb.py,580,Good practice: save your training arguments together with the trained model,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,583,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,589,Evaluation,not
transformers/examples/contrib/mm-imdb/run_mmimdb.py,598,Reduce logging,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,1,coding=utf-8,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,2,"Copyright (c) Facebook, Inc. and its affiliates.",not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,3,Copyright (c) HuggingFace Inc. team.,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,4,,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,7,You may obtain a copy of the License at,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,8,,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,10,,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,14,See the License for the specific language governing permissions and,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,15,limitations under the License.,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,41,Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048,not
transformers/examples/contrib/mm-imdb/utils_mmimdb.py,45,BxNx2048,not
transformers/examples/token-classification/run_tf_ner.py,1,coding=utf-8,not
transformers/examples/token-classification/run_tf_ner.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/examples/token-classification/run_tf_ner.py,3,,not
transformers/examples/token-classification/run_tf_ner.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/token-classification/run_tf_ner.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/token-classification/run_tf_ner.py,6,You may obtain a copy of the License at,not
transformers/examples/token-classification/run_tf_ner.py,7,,not
transformers/examples/token-classification/run_tf_ner.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/token-classification/run_tf_ner.py,9,,not
transformers/examples/token-classification/run_tf_ner.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/token-classification/run_tf_ner.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/token-classification/run_tf_ner.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/token-classification/run_tf_ner.py,13,See the License for the specific language governing permissions and,not
transformers/examples/token-classification/run_tf_ner.py,14,limitations under the License.,not
transformers/examples/token-classification/run_tf_ner.py,57,"If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,",SATD
transformers/examples/token-classification/run_tf_ner.py,58,or just modify its tokenizer_config.json.,not
transformers/examples/token-classification/run_tf_ner.py,89,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/token-classification/run_tf_ner.py,90,or by passing the --help flag to this script.,not
transformers/examples/token-classification/run_tf_ner.py,91,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/token-classification/run_tf_ner.py,105,Setup logging,not
transformers/examples/token-classification/run_tf_ner.py,119,Prepare Token Classification task,not
transformers/examples/token-classification/run_tf_ner.py,124,Load pretrained model and tokenizer,not
transformers/examples/token-classification/run_tf_ner.py,125,,not
transformers/examples/token-classification/run_tf_ner.py,126,Distributed training:,not
transformers/examples/token-classification/run_tf_ner.py,127,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/token-classification/run_tf_ner.py,128,download model & vocab.,not
transformers/examples/token-classification/run_tf_ner.py,151,Get datasets,not
transformers/examples/token-classification/run_tf_ner.py,202,Initialize our Trainer,not
transformers/examples/token-classification/run_tf_ner.py,211,Training,not
transformers/examples/token-classification/run_tf_ner.py,217,Evaluation,not
transformers/examples/token-classification/run_tf_ner.py,234,Predict,not
transformers/examples/token-classification/run_tf_ner.py,257,Save predictions,not
transformers/examples/token-classification/run_ner.py,1,coding=utf-8,not
transformers/examples/token-classification/run_ner.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/token-classification/run_ner.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/token-classification/run_ner.py,4,,not
transformers/examples/token-classification/run_ner.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/token-classification/run_ner.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/token-classification/run_ner.py,7,You may obtain a copy of the License at,not
transformers/examples/token-classification/run_ner.py,8,,not
transformers/examples/token-classification/run_ner.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/token-classification/run_ner.py,10,,not
transformers/examples/token-classification/run_ner.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/token-classification/run_ner.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/token-classification/run_ner.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/token-classification/run_ner.py,14,See the License for the specific language governing permissions and,not
transformers/examples/token-classification/run_ner.py,15,limitations under the License.,not
transformers/examples/token-classification/run_ner.py,61,"If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,",SATD
transformers/examples/token-classification/run_ner.py,62,or just modify its tokenizer_config.json.,not
transformers/examples/token-classification/run_ner.py,93,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/token-classification/run_ner.py,94,or by passing the --help flag to this script.,not
transformers/examples/token-classification/run_ner.py,95,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/token-classification/run_ner.py,99,"If we pass only one argument to the script and it's the path to a json file,",not
transformers/examples/token-classification/run_ner.py,100,let's parse it to get our arguments.,not
transformers/examples/token-classification/run_ner.py,115,Setup logging,not
transformers/examples/token-classification/run_ner.py,131,Set seed,not
transformers/examples/token-classification/run_ner.py,134,Prepare CONLL-2003 task,not
transformers/examples/token-classification/run_ner.py,139,Load pretrained model and tokenizer,not
transformers/examples/token-classification/run_ner.py,140,,not
transformers/examples/token-classification/run_ner.py,141,Distributed training:,not
transformers/examples/token-classification/run_ner.py,142,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/token-classification/run_ner.py,143,download model & vocab.,not
transformers/examples/token-classification/run_ner.py,164,Get datasets,not
transformers/examples/token-classification/run_ner.py,218,Initialize our Trainer,not
transformers/examples/token-classification/run_ner.py,227,Training,not
transformers/examples/token-classification/run_ner.py,233,"For convenience, we also re-save the tokenizer to the same directory,",not
transformers/examples/token-classification/run_ner.py,234,so that you can share your model easily on huggingface.co/models =),not
transformers/examples/token-classification/run_ner.py,238,Evaluation,not
transformers/examples/token-classification/run_ner.py,254,Predict,not
transformers/examples/token-classification/run_ner.py,276,Save predictions,not
transformers/examples/token-classification/run_ner.py,296,For xla_spawn (TPUs),not
transformers/examples/token-classification/run_pl_ner.py,41,"XLM and RoBERTa don""t use token_type_ids",not
transformers/examples/token-classification/run_pl_ner.py,88,HACK(we will not use this anymore soon),SATD
transformers/examples/token-classification/run_pl_ner.py,101,"XLM and RoBERTa don""t use token_type_ids",not
transformers/examples/token-classification/run_pl_ner.py,137,when stable,not
transformers/examples/token-classification/run_pl_ner.py,143,updating to test_epoch_end instead of deprecated test_end,not
transformers/examples/token-classification/run_pl_ner.py,146,Converting to the dict required by pl,not
transformers/examples/token-classification/run_pl_ner.py,147,https://github.com/PyTorchLightning/pytorch-lightning/blob/master/\,not
transformers/examples/token-classification/run_pl_ner.py,148,pytorch_lightning/trainer/logging.py#L139,not
transformers/examples/token-classification/run_pl_ner.py,150,`val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`,not
transformers/examples/token-classification/run_pl_ner.py,155,Add NER specific options,not
transformers/examples/token-classification/run_pl_ner.py,196,See https://github.com/huggingface/transformers/issues/3159,not
transformers/examples/token-classification/run_pl_ner.py,197,pl use this format to create a checkpoint:,not
transformers/examples/token-classification/run_pl_ner.py,198,https://github.com/PyTorchLightning/pytorch-lightning/blob/master\,not
transformers/examples/token-classification/run_pl_ner.py,199,/pytorch_lightning/callbacks/model_checkpoint.py#L169,not
transformers/examples/token-classification/utils_ner.py,1,coding=utf-8,not
transformers/examples/token-classification/utils_ner.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/token-classification/utils_ner.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/token-classification/utils_ner.py,4,,not
transformers/examples/token-classification/utils_ner.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/token-classification/utils_ner.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/token-classification/utils_ner.py,7,You may obtain a copy of the License at,not
transformers/examples/token-classification/utils_ner.py,8,,not
transformers/examples/token-classification/utils_ner.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/token-classification/utils_ner.py,10,,not
transformers/examples/token-classification/utils_ner.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/token-classification/utils_ner.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/token-classification/utils_ner.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/token-classification/utils_ner.py,14,See the License for the specific language governing permissions and,not
transformers/examples/token-classification/utils_ner.py,15,limitations under the License.,not
transformers/examples/token-classification/utils_ner.py,81,Use cross entropy ignore_index as padding label id so that only,not
transformers/examples/token-classification/utils_ner.py,82,real label ids contribute to the loss later.,not
transformers/examples/token-classification/utils_ner.py,95,Load data features from cache or dataset file,not
transformers/examples/token-classification/utils_ner.py,101,"Make sure only the first process in distributed training processes the dataset,",not
transformers/examples/token-classification/utils_ner.py,102,and the others will use the cache.,not
transformers/examples/token-classification/utils_ner.py,110,TODO clean up all this to leverage built-in features of tokenizers,SATD
transformers/examples/token-classification/utils_ner.py,117,xlnet has a cls token at the end,not
transformers/examples/token-classification/utils_ner.py,122,"roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805",not
transformers/examples/token-classification/utils_ner.py,150,Use cross entropy ignore_index as padding label id so that only,not
transformers/examples/token-classification/utils_ner.py,151,real label ids contribute to the loss later.,not
transformers/examples/token-classification/utils_ner.py,164,TODO clean up all this to leverage built-in features of tokenizers,SATD
transformers/examples/token-classification/utils_ner.py,171,xlnet has a cls token at the end,not
transformers/examples/token-classification/utils_ner.py,176,"roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805",not
transformers/examples/token-classification/utils_ner.py,255,"Examples could have no label for mode = ""test""",not
transformers/examples/token-classification/utils_ner.py,285,TODO clean up all this to leverage built-in features of tokenizers,SATD
transformers/examples/token-classification/utils_ner.py,299,"bert-base-multilingual-cased sometimes output ""nothing ([]) when calling tokenize with just a space.",not
transformers/examples/token-classification/utils_ner.py,302,"Use the real label id for the first token of the word, and padding ids for the remaining tokens",not
transformers/examples/token-classification/utils_ner.py,305,"Account for [CLS] and [SEP] with ""- 2"" and with ""- 3"" for RoBERTa.",not
transformers/examples/token-classification/utils_ner.py,311,The convention in BERT is:,not
transformers/examples/token-classification/utils_ner.py,312,(a) For sequence pairs:,not
transformers/examples/token-classification/utils_ner.py,313,tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP],not
transformers/examples/token-classification/utils_ner.py,314,type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1,not
transformers/examples/token-classification/utils_ner.py,315,(b) For single sequences:,not
transformers/examples/token-classification/utils_ner.py,316,tokens:   [CLS] the dog is hairy . [SEP],not
transformers/examples/token-classification/utils_ner.py,317,type_ids:   0   0   0   0  0     0   0,not
transformers/examples/token-classification/utils_ner.py,318,,not
transformers/examples/token-classification/utils_ner.py,319,"Where ""type_ids"" are used to indicate whether this is the first",not
transformers/examples/token-classification/utils_ner.py,320,sequence or the second sequence. The embedding vectors for `type=0` and,not
transformers/examples/token-classification/utils_ner.py,321,`type=1` were learned during pre-training and are added to the wordpiece,not
transformers/examples/token-classification/utils_ner.py,322,embedding vector (and position vector). This is not *strictly* necessary,not
transformers/examples/token-classification/utils_ner.py,323,"since the [SEP] token unambiguously separates the sequences, but it makes",not
transformers/examples/token-classification/utils_ner.py,324,it easier for the model to learn the concept of sequences.,not
transformers/examples/token-classification/utils_ner.py,325,,not
transformers/examples/token-classification/utils_ner.py,326,"For classification tasks, the first vector (corresponding to [CLS]) is",not
transformers/examples/token-classification/utils_ner.py,327,"used as as the ""sentence vector"". Note that this only makes sense because",not
transformers/examples/token-classification/utils_ner.py,328,the entire model is fine-tuned.,not
transformers/examples/token-classification/utils_ner.py,332,roberta uses an extra separator b/w pairs of sentences,not
transformers/examples/token-classification/utils_ner.py,348,The mask has 1 for real tokens and 0 for padding tokens. Only real,not
transformers/examples/token-classification/utils_ner.py,349,tokens are attended to.,not
transformers/examples/token-classification/utils_ner.py,352,Zero-pad up to the sequence length.,not
transformers/examples/multiple-choice/run_multiple_choice.py,1,coding=utf-8,not
transformers/examples/multiple-choice/run_multiple_choice.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/multiple-choice/run_multiple_choice.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/multiple-choice/run_multiple_choice.py,4,,not
transformers/examples/multiple-choice/run_multiple_choice.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/multiple-choice/run_multiple_choice.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/multiple-choice/run_multiple_choice.py,7,You may obtain a copy of the License at,not
transformers/examples/multiple-choice/run_multiple_choice.py,8,,not
transformers/examples/multiple-choice/run_multiple_choice.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/multiple-choice/run_multiple_choice.py,10,,not
transformers/examples/multiple-choice/run_multiple_choice.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/multiple-choice/run_multiple_choice.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/multiple-choice/run_multiple_choice.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/multiple-choice/run_multiple_choice.py,14,See the License for the specific language governing permissions and,not
transformers/examples/multiple-choice/run_multiple_choice.py,15,limitations under the License.,not
transformers/examples/multiple-choice/run_multiple_choice.py,87,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/multiple-choice/run_multiple_choice.py,88,or by passing the --help flag to this script.,not
transformers/examples/multiple-choice/run_multiple_choice.py,89,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/multiple-choice/run_multiple_choice.py,104,Setup logging,not
transformers/examples/multiple-choice/run_multiple_choice.py,120,Set seed,not
transformers/examples/multiple-choice/run_multiple_choice.py,130,Load pretrained model and tokenizer,not
transformers/examples/multiple-choice/run_multiple_choice.py,131,,not
transformers/examples/multiple-choice/run_multiple_choice.py,132,Distributed training:,not
transformers/examples/multiple-choice/run_multiple_choice.py,133,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/multiple-choice/run_multiple_choice.py,134,download model & vocab.,not
transformers/examples/multiple-choice/run_multiple_choice.py,153,Get datasets,not
transformers/examples/multiple-choice/run_multiple_choice.py,185,Initialize our Trainer,not
transformers/examples/multiple-choice/run_multiple_choice.py,194,Training,not
transformers/examples/multiple-choice/run_multiple_choice.py,200,"For convenience, we also re-save the tokenizer to the same directory,",not
transformers/examples/multiple-choice/run_multiple_choice.py,201,so that you can share your model easily on huggingface.co/models =),not
transformers/examples/multiple-choice/run_multiple_choice.py,205,Evaluation,not
transformers/examples/multiple-choice/run_multiple_choice.py,225,For xla_spawn (TPUs),not
transformers/examples/multiple-choice/utils_multiple_choice.py,1,coding=utf-8,not
transformers/examples/multiple-choice/utils_multiple_choice.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/multiple-choice/utils_multiple_choice.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/multiple-choice/utils_multiple_choice.py,4,,not
transformers/examples/multiple-choice/utils_multiple_choice.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/multiple-choice/utils_multiple_choice.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/multiple-choice/utils_multiple_choice.py,7,You may obtain a copy of the License at,not
transformers/examples/multiple-choice/utils_multiple_choice.py,8,,not
transformers/examples/multiple-choice/utils_multiple_choice.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/multiple-choice/utils_multiple_choice.py,10,,not
transformers/examples/multiple-choice/utils_multiple_choice.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/multiple-choice/utils_multiple_choice.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/multiple-choice/utils_multiple_choice.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/multiple-choice/utils_multiple_choice.py,14,See the License for the specific language governing permissions and,not
transformers/examples/multiple-choice/utils_multiple_choice.py,15,limitations under the License.,not
transformers/examples/multiple-choice/utils_multiple_choice.py,107,"Make sure only the first process in distributed training processes the dataset,",not
transformers/examples/multiple-choice/utils_multiple_choice.py,108,and the others will use the cache.,not
transformers/examples/multiple-choice/utils_multiple_choice.py,123,TODO clean up all this to leverage built-in features of tokenizers,SATD
transformers/examples/multiple-choice/utils_multiple_choice.py,175,TODO clean up all this to leverage built-in features of tokenizers,SATD
transformers/examples/multiple-choice/utils_multiple_choice.py,312,this is not efficient but convenient,not
transformers/examples/multiple-choice/utils_multiple_choice.py,353,"in the swag dataset, the",not
transformers/examples/multiple-choice/utils_multiple_choice.py,354,common beginning of each,not
transformers/examples/multiple-choice/utils_multiple_choice.py,355,"choice is stored in ""sent2"".",not
transformers/examples/multiple-choice/utils_multiple_choice.py,360,we skip the line with the column names,not
transformers/examples/multiple-choice/utils_multiple_choice.py,404,"in the swag dataset, the",not
transformers/examples/multiple-choice/utils_multiple_choice.py,405,common beginning of each,not
transformers/examples/multiple-choice/utils_multiple_choice.py,406,"choice is stored in ""sent2"".",not
transformers/examples/multiple-choice/utils_multiple_choice.py,411,we skip the line with the column names,not
transformers/examples/multiple-choice/utils_multiple_choice.py,446,There are two types of labels. They should be normalized,not
transformers/examples/multiple-choice/utils_multiple_choice.py,461,we deleted example which has more than or less than four choices,not
transformers/examples/multiple-choice/utils_multiple_choice.py,532,this is for cloze question,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,1,coding=utf-8,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,4,,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,7,You may obtain a copy of the License at,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,8,,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,10,,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,14,See the License for the specific language governing permissions and,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,15,limitations under the License.,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,87,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,88,or by passing the --help flag to this script.,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,89,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,104,Setup logging,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,115,Set seed,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,125,Load pretrained model and tokenizer,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,126,,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,127,Distributed training:,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,128,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,129,download model & vocab.,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,147,Get datasets,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,177,Initialize our Trainer,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,186,Training,not
transformers/examples/multiple-choice/run_tf_multiple_choice.py,191,Evaluation,not
transformers/examples/text-generation/run_generation.py,1,!/usr/bin/env python3,not
transformers/examples/text-generation/run_generation.py,2,coding=utf-8,not
transformers/examples/text-generation/run_generation.py,3,"Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.",not
transformers/examples/text-generation/run_generation.py,4,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/text-generation/run_generation.py,5,,not
transformers/examples/text-generation/run_generation.py,6,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/text-generation/run_generation.py,7,you may not use this file except in compliance with the License.,not
transformers/examples/text-generation/run_generation.py,8,You may obtain a copy of the License at,not
transformers/examples/text-generation/run_generation.py,9,,not
transformers/examples/text-generation/run_generation.py,10,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/text-generation/run_generation.py,11,,not
transformers/examples/text-generation/run_generation.py,12,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/text-generation/run_generation.py,13,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/text-generation/run_generation.py,14,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/text-generation/run_generation.py,15,See the License for the specific language governing permissions and,not
transformers/examples/text-generation/run_generation.py,16,limitations under the License.,not
transformers/examples/text-generation/run_generation.py,48,Hardcoded max length to avoid infinite loop,not
transformers/examples/text-generation/run_generation.py,59,Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia,not
transformers/examples/text-generation/run_generation.py,60,in https://github.com/rusiaaman/XLNet-gen#methodology,not
transformers/examples/text-generation/run_generation.py,61,and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e,not
transformers/examples/text-generation/run_generation.py,81,,not
transformers/examples/text-generation/run_generation.py,82,Functions to prepare models' input,not
transformers/examples/text-generation/run_generation.py,83,,not
transformers/examples/text-generation/run_generation.py,97,"kwargs = {""language"": None, ""mask_token_id"": None}",not
transformers/examples/text-generation/run_generation.py,99,Set the language,not
transformers/examples/text-generation/run_generation.py,111,"kwargs[""language""] = tokenizer.lang2id[language]",not
transformers/examples/text-generation/run_generation.py,113,TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers,SATD
transformers/examples/text-generation/run_generation.py,114,XLM masked-language modeling (MLM) models need masked token,not
transformers/examples/text-generation/run_generation.py,115,"is_xlm_mlm = ""mlm"" in args.model_name_or_path",not
transformers/examples/text-generation/run_generation.py,116,if is_xlm_mlm:,not
transformers/examples/text-generation/run_generation.py,117,"kwargs[""mask_token_id""] = tokenizer.mask_token_id",not
transformers/examples/text-generation/run_generation.py,144,No generation bigger than model size,not
transformers/examples/text-generation/run_generation.py,146,avoid infinite loop,not
transformers/examples/text-generation/run_generation.py,196,Initialize the model and tokenizer,not
transformers/examples/text-generation/run_generation.py,212,Different models need different input formatting and/or extra arguments,not
transformers/examples/text-generation/run_generation.py,240,Remove the batch dimension when returning multiple sequences,not
transformers/examples/text-generation/run_generation.py,250,Decode text,not
transformers/examples/text-generation/run_generation.py,253,Remove all text after the stop token,not
transformers/examples/text-generation/run_generation.py,256,Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing,not
transformers/examples/text-generation/pplm/pplm_classification_head.py,11,"self.mlp1 = torch.nn.Linear(embed_size, embed_size)",not
transformers/examples/text-generation/pplm/pplm_classification_head.py,12,"self.mlp2 = (torch.nn.Linear(embed_size, class_size))",not
transformers/examples/text-generation/pplm/pplm_classification_head.py,16,hidden_state = F.relu(self.mlp1(hidden_state)),not
transformers/examples/text-generation/pplm/pplm_classification_head.py,17,hidden_state = self.mlp2(hidden_state),not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,1,! /usr/bin/env python3,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,2,coding=utf-8,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,4,"Copyright (c) 2019 Uber Technologies, Inc.",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,5,,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,6,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,7,you may not use this file except in compliance with the License.,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,8,You may obtain a copy of the License at,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,9,,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,10,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,11,,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,12,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,13,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,14,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,15,See the License for the specific language governing permissions and,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,16,limitations under the License.,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,105,padding value = 0,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,169,sum up batch loss,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,171,get the index of the max log-probability,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,363,"if dataset == ""generic"":",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,364,This assumes the input dataset is a TSV with the following structure:,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,365,class \t text,not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,471,"torch.save(discriminator.state_dict(),",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,472,"""{}_discriminator_{}.pt"".format(",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,473,"args.dataset, epoch + 1",not
transformers/examples/text-generation/pplm/run_pplm_discrim_train.py,474,)),not
transformers/examples/text-generation/pplm/run_pplm.py,1,! /usr/bin/env python3,not
transformers/examples/text-generation/pplm/run_pplm.py,2,coding=utf-8,not
transformers/examples/text-generation/pplm/run_pplm.py,4,"Copyright (c) 2019 Uber Technologies, Inc.",not
transformers/examples/text-generation/pplm/run_pplm.py,5,,not
transformers/examples/text-generation/pplm/run_pplm.py,6,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/text-generation/pplm/run_pplm.py,7,you may not use this file except in compliance with the License.,not
transformers/examples/text-generation/pplm/run_pplm.py,8,You may obtain a copy of the License at,not
transformers/examples/text-generation/pplm/run_pplm.py,9,,not
transformers/examples/text-generation/pplm/run_pplm.py,10,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/text-generation/pplm/run_pplm.py,11,,not
transformers/examples/text-generation/pplm/run_pplm.py,12,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/text-generation/pplm/run_pplm.py,13,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/text-generation/pplm/run_pplm.py,14,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/text-generation/pplm/run_pplm.py,15,See the License for the specific language governing permissions and,not
transformers/examples/text-generation/pplm/run_pplm.py,16,limitations under the License.,not
transformers/examples/text-generation/pplm/run_pplm.py,124,Generate inital perturbed past,not
transformers/examples/text-generation/pplm/run_pplm.py,135,TODO fix this comment (SUMANTH),SATD
transformers/examples/text-generation/pplm/run_pplm.py,136,Generate a mask is gradient perturbated is based on a past window,not
transformers/examples/text-generation/pplm/run_pplm.py,154,accumulate perturbations for num_iterations,not
transformers/examples/text-generation/pplm/run_pplm.py,163,Compute hidden using perturbed past,not
transformers/examples/text-generation/pplm/run_pplm.py,169,TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth),SATD
transformers/examples/text-generation/pplm/run_pplm.py,185,TODO why we need to do this assignment and not just using unpert_past? (Sumanth),SATD
transformers/examples/text-generation/pplm/run_pplm.py,216,compute gradients,not
transformers/examples/text-generation/pplm/run_pplm.py,219,calculate gradient norms,not
transformers/examples/text-generation/pplm/run_pplm.py,230,normalize gradients,not
transformers/examples/text-generation/pplm/run_pplm.py,236,accumulate gradient,not
transformers/examples/text-generation/pplm/run_pplm.py,239,"reset gradients, just to make sure",not
transformers/examples/text-generation/pplm/run_pplm.py,243,removing past from the graph,not
transformers/examples/text-generation/pplm/run_pplm.py,249,apply the accumulated perturbations to the past,not
transformers/examples/text-generation/pplm/run_pplm.py,458,collect one hot vectors for bags of words,not
transformers/examples/text-generation/pplm/run_pplm.py,467,"Get past/probs for current output, except for last word",not
transformers/examples/text-generation/pplm/run_pplm.py,468,Note that GPT takes 2 inputs: past + current_token,not
transformers/examples/text-generation/pplm/run_pplm.py,470,run model forward to obtain unperturbed,not
transformers/examples/text-generation/pplm/run_pplm.py,479,check if we are abowe grad max length,not
transformers/examples/text-generation/pplm/run_pplm.py,485,modify the past if necessary,not
transformers/examples/text-generation/pplm/run_pplm.py,520,+ SMALL_CONST,not
transformers/examples/text-generation/pplm/run_pplm.py,539,Fuse the modified model and original model,not
transformers/examples/text-generation/pplm/run_pplm.py,544,+ SMALL_CONST,not
transformers/examples/text-generation/pplm/run_pplm.py,545,+ SMALL_CONST,not
transformers/examples/text-generation/pplm/run_pplm.py,547,rescale,not
transformers/examples/text-generation/pplm/run_pplm.py,552,+ SMALL_CONST,not
transformers/examples/text-generation/pplm/run_pplm.py,555,sample or greedy,not
transformers/examples/text-generation/pplm/run_pplm.py,562,update context/output_so_far appending the new token,not
transformers/examples/text-generation/pplm/run_pplm.py,610,set Random seed,not
transformers/examples/text-generation/pplm/run_pplm.py,614,set the device,not
transformers/examples/text-generation/pplm/run_pplm.py,624,load pretrained model,not
transformers/examples/text-generation/pplm/run_pplm.py,629,load tokenizer,not
transformers/examples/text-generation/pplm/run_pplm.py,632,Freeze GPT-2 weights,not
transformers/examples/text-generation/pplm/run_pplm.py,636,figure out conditioning text,not
transformers/examples/text-generation/pplm/run_pplm.py,650,generate unperturbed and perturbed texts,not
transformers/examples/text-generation/pplm/run_pplm.py,652,full_text_generation returns:,not
transformers/examples/text-generation/pplm/run_pplm.py,653,"unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time",not
transformers/examples/text-generation/pplm/run_pplm.py,679,untokenize unperturbed text,not
transformers/examples/text-generation/pplm/run_pplm.py,693,filtering all words in the list composed of more than 1 token,not
transformers/examples/text-generation/pplm/run_pplm.py,695,w[0] because we are sure w has only 1 item because previous fitler,not
transformers/examples/text-generation/pplm/run_pplm.py,698,iterate through the perturbed texts,not
transformers/examples/text-generation/pplm/run_pplm.py,701,untokenize unperturbed text,not
transformers/examples/text-generation/pplm/run_pplm.py,722,"keep the prefix, perturbed seq, original seq for each index",not
transformers/examples/text-classification/run_pl_glue.py,78,We test on dev set to compare to benchmarks without having to submit to GLUE server,not
transformers/examples/text-classification/run_pl_glue.py,136,updating to test_epoch_end instead of deprecated test_end,not
transformers/examples/text-classification/run_pl_glue.py,139,Converting to the dic required by pl,not
transformers/examples/text-classification/run_pl_glue.py,140,https://github.com/PyTorchLightning/pytorch-lightning/blob/master/\,not
transformers/examples/text-classification/run_pl_glue.py,141,pytorch_lightning/trainer/logging.py#L139,not
transformers/examples/text-classification/run_pl_glue.py,143,`val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`,not
transformers/examples/text-classification/run_pl_glue.py,148,Add NER specific options,not
transformers/examples/text-classification/run_pl_glue.py,183,"If output_dir not provided, a folder will be generated in pwd",not
transformers/examples/text-classification/run_pl_glue.py,191,"Optionally, predict on dev set and write to output_dir",not
transformers/examples/text-classification/run_glue.py,1,coding=utf-8,not
transformers/examples/text-classification/run_glue.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/text-classification/run_glue.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/text-classification/run_glue.py,4,,not
transformers/examples/text-classification/run_glue.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/text-classification/run_glue.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/text-classification/run_glue.py,7,You may obtain a copy of the License at,not
transformers/examples/text-classification/run_glue.py,8,,not
transformers/examples/text-classification/run_glue.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/text-classification/run_glue.py,10,,not
transformers/examples/text-classification/run_glue.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/text-classification/run_glue.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/text-classification/run_glue.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/text-classification/run_glue.py,14,See the License for the specific language governing permissions and,not
transformers/examples/text-classification/run_glue.py,15,limitations under the License.,not
transformers/examples/text-classification/run_glue.py,65,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/text-classification/run_glue.py,66,or by passing the --help flag to this script.,not
transformers/examples/text-classification/run_glue.py,67,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/text-classification/run_glue.py,72,"If we pass only one argument to the script and it's the path to a json file,",not
transformers/examples/text-classification/run_glue.py,73,let's parse it to get our arguments.,not
transformers/examples/text-classification/run_glue.py,88,Setup logging,not
transformers/examples/text-classification/run_glue.py,104,Set seed,not
transformers/examples/text-classification/run_glue.py,113,Load pretrained model and tokenizer,not
transformers/examples/text-classification/run_glue.py,114,,not
transformers/examples/text-classification/run_glue.py,115,Distributed training:,not
transformers/examples/text-classification/run_glue.py,116,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/text-classification/run_glue.py,117,download model & vocab.,not
transformers/examples/text-classification/run_glue.py,136,Get datasets,not
transformers/examples/text-classification/run_glue.py,147,Initialize our Trainer,not
transformers/examples/text-classification/run_glue.py,156,Training,not
transformers/examples/text-classification/run_glue.py,162,"For convenience, we also re-save the tokenizer to the same directory,",not
transformers/examples/text-classification/run_glue.py,163,so that you can share your model easily on huggingface.co/models =),not
transformers/examples/text-classification/run_glue.py,167,Evaluation,not
transformers/examples/text-classification/run_glue.py,172,"Loop to handle MNLI double evaluation (matched, mis-matched)",not
transformers/examples/text-classification/run_glue.py,196,For xla_spawn (TPUs),not
transformers/examples/text-classification/run_xnli.py,1,coding=utf-8,not
transformers/examples/text-classification/run_xnli.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/text-classification/run_xnli.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/text-classification/run_xnli.py,4,,not
transformers/examples/text-classification/run_xnli.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/text-classification/run_xnli.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/text-classification/run_xnli.py,7,You may obtain a copy of the License at,not
transformers/examples/text-classification/run_xnli.py,8,,not
transformers/examples/text-classification/run_xnli.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/text-classification/run_xnli.py,10,,not
transformers/examples/text-classification/run_xnli.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/text-classification/run_xnli.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/text-classification/run_xnli.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/text-classification/run_xnli.py,14,See the License for the specific language governing permissions and,not
transformers/examples/text-classification/run_xnli.py,15,limitations under the License.,not
transformers/examples/text-classification/run_xnli.py,94,Prepare optimizer and schedule (linear warmup and decay),not
transformers/examples/text-classification/run_xnli.py,108,Check if saved optimizer or scheduler states exist,not
transformers/examples/text-classification/run_xnli.py,112,Load in optimizer and scheduler states,not
transformers/examples/text-classification/run_xnli.py,123,multi-gpu training (should be after apex fp16 initialization),not
transformers/examples/text-classification/run_xnli.py,127,Distributed training (should be after apex fp16 initialization),not
transformers/examples/text-classification/run_xnli.py,133,Train!,not
transformers/examples/text-classification/run_xnli.py,150,Check if continuing training from a checkpoint,not
transformers/examples/text-classification/run_xnli.py,152,set global_step to gobal_step of last saved checkpoint from model path,not
transformers/examples/text-classification/run_xnli.py,167,Added here for reproductibility,not
transformers/examples/text-classification/run_xnli.py,171,Skip past any already trained steps if resuming training,not
transformers/examples/text-classification/run_xnli.py,182,XLM and DistilBERT don't use segment_ids,not
transformers/examples/text-classification/run_xnli.py,184,model outputs are always tuple in transformers (see doc),not
transformers/examples/text-classification/run_xnli.py,187,mean() to average on multi-gpu parallel training,not
transformers/examples/text-classification/run_xnli.py,205,Update learning rate schedule,not
transformers/examples/text-classification/run_xnli.py,210,Log metrics,not
transformers/examples/text-classification/run_xnli.py,213,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/examples/text-classification/run_xnli.py,222,Save model checkpoint,not
transformers/examples/text-classification/run_xnli.py,228,Take care of distributed/parallel training,SATD
transformers/examples/text-classification/run_xnli.py,264,Note that DistributedSampler samples randomly,not
transformers/examples/text-classification/run_xnli.py,268,multi-gpu eval,not
transformers/examples/text-classification/run_xnli.py,272,Eval!,not
transformers/examples/text-classification/run_xnli.py,289,XLM and DistilBERT don't use segment_ids,not
transformers/examples/text-classification/run_xnli.py,322,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/text-classification/run_xnli.py,326,Load data features from cache or dataset file,not
transformers/examples/text-classification/run_xnli.py,354,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/text-classification/run_xnli.py,356,Convert to Tensors and build dataset,not
transformers/examples/text-classification/run_xnli.py,372,Required parameters,not
transformers/examples/text-classification/run_xnli.py,412,Other parameters,not
transformers/examples/text-classification/run_xnli.py,514,Setup distant debugging if needed,not
transformers/examples/text-classification/run_xnli.py,516,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/text-classification/run_xnli.py,523,"Setup CUDA, GPU & distributed training",not
transformers/examples/text-classification/run_xnli.py,527,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/examples/text-classification/run_xnli.py,534,Setup logging,not
transformers/examples/text-classification/run_xnli.py,549,Set seed,not
transformers/examples/text-classification/run_xnli.py,552,Prepare XNLI task,not
transformers/examples/text-classification/run_xnli.py,561,Load pretrained model and tokenizer,not
transformers/examples/text-classification/run_xnli.py,563,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/text-classification/run_xnli.py,586,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/text-classification/run_xnli.py,592,Training,not
transformers/examples/text-classification/run_xnli.py,598,"Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()",not
transformers/examples/text-classification/run_xnli.py,600,Create output directory if needed,not
transformers/examples/text-classification/run_xnli.py,605,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/examples/text-classification/run_xnli.py,606,They can then be reloaded using `from_pretrained()`,not
transformers/examples/text-classification/run_xnli.py,609,Take care of distributed/parallel training,SATD
transformers/examples/text-classification/run_xnli.py,613,Good practice: save your training arguments together with the trained model,not
transformers/examples/text-classification/run_xnli.py,616,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/text-classification/run_xnli.py,621,Evaluation,not
transformers/examples/text-classification/run_xnli.py,630,Reduce logging,not
transformers/examples/text-classification/run_tf_glue.py,1,coding=utf-8,not
transformers/examples/text-classification/run_tf_glue.py,103,"If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,",SATD
transformers/examples/text-classification/run_tf_glue.py,104,or just modify its tokenizer_config.json.,not
transformers/examples/text-classification/run_tf_glue.py,111,See all possible arguments in src/transformers/training_args.py,not
transformers/examples/text-classification/run_tf_glue.py,112,or by passing the --help flag to this script.,not
transformers/examples/text-classification/run_tf_glue.py,113,"We now keep distinct sets of args, for a cleaner separation of concerns.",not
transformers/examples/text-classification/run_tf_glue.py,127,Setup logging,not
transformers/examples/text-classification/run_tf_glue.py,147,Load pretrained model and tokenizer,not
transformers/examples/text-classification/run_tf_glue.py,148,,not
transformers/examples/text-classification/run_tf_glue.py,149,Distributed training:,not
transformers/examples/text-classification/run_tf_glue.py,150,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/text-classification/run_tf_glue.py,151,download model & vocab.,not
transformers/examples/text-classification/run_tf_glue.py,172,Get datasets,not
transformers/examples/text-classification/run_tf_glue.py,193,Initialize our Trainer,not
transformers/examples/text-classification/run_tf_glue.py,202,Training,not
transformers/examples/text-classification/run_tf_glue.py,208,Evaluation,not
transformers/examples/translation/t5/evaluate_wmt.py,23,update config with summarization specific params,not
transformers/examples/translation/t5/evaluate_wmt.py,85,Read input lines into python,not
transformers/examples/translation/t5/evaluate_wmt.py,91,Read generated lines into python,not
transformers/examples/translation/t5/evaluate_wmt.py,95,Read reference lines into python,not
transformers/examples/summarization/t5/evaluate_cnn.py,25,update config with summarization specific params,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,1,MIT License,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,3,Copyright (c) 2019 Yang Liu and the HuggingFace team,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,5,"Permission is hereby granted, free of charge, to any person obtaining a copy",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,6,"of this software and associated documentation files (the ""Software""), to deal",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,7,"in the Software without restriction, including without limitation the rights",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,8,"to use, copy, modify, merge, publish, distribute, sublicense, and/or sell",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,9,"copies of the Software, and to permit persons to whom the Software is",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,10,"furnished to do so, subject to the following conditions:",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,12,The above copyright notice and this permission notice shall be included in all,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,13,copies or substantial portions of the Software.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,15,"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,16,"IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,17,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,18,"AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,19,"LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,20,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,21,SOFTWARE.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,54,"If pre-trained weights are passed for Bert, load these.",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,156,Basic attributes.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,162,Build TransformerDecoder.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,169,"forward(input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask)",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,170,"def forward(self, input_ids, state, attention_mask=None, memory_lengths=None,",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,171,"step=None, cache=None, encoder_attention_mask=None, encoder_hidden_states=None, memory_masks=None):",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,187,Name conversion,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,192,src_words = state.src,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,198,Decoder padding mask,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,203,Encoder padding mask,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,210,Pass through the embeddings,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,213,len x batch x embedding_dim,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,244,Decoders in transformers return a tuple. Beam search will fail,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,245,if we don't follow this convention.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,246,", state",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,306,"Register self.mask as a saved_state in TransformerDecoderLayer, so",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,307,it gets TransformerDecoderLayer's cuda behavior automatically.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,346,return output,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,458,"1) Project key, value, and query.",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,505,2) Calculate and scale scores.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,513,3) Apply attention dropout and compute context vectors.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,656,,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,657,TRANSLATOR,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,658,The following code is used to generate summaries using the,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,659,pre-trained weights and beam search.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,660,,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,664,we should be able to refactor the global scorer a lot,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,799,Where the beam search lives,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,800,I have no idea why it is being called from the method above,SATD
transformers/examples/summarization/bertabs/modeling_bertabs.py,805,The batch object is funny,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,806,Instead of just looking at the size of the arguments we encapsulate,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,807,a size argument.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,808,Where is it defined?,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,819,Tile states and memory beam_size times.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,826,Give full probability to the first beam on the first step.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,829,Structure that holds finished hypotheses.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,830,noqa: F812,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,833,noqa: F812,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,834,noqa: F812,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,841,Decoder forward.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,846,Generator forward.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,853,Multiply probs by the beam probability.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,859,Flatten probs into a list of possibilities.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,882,Recover log probs.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,885,Resolve beam origin and true word ids.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,889,Map beam_index to batch_index in the flat representation.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,893,Append last prediction.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,899,End condition is top beam is finished.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,901,Save finished hypotheses.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,909,Store finished hypotheses for this batch.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,912,"If the batch reached the end, save the n_best hypotheses.",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,920,"If all sentences are translated, no need to go further.",not
transformers/examples/summarization/bertabs/modeling_bertabs.py,923,Remove finished batches for the next step.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,928,Reorder states.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,978,,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,979,Optimizer for training. We keep this here in case we want to add,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,980,a finetuning script.,not
transformers/examples/summarization/bertabs/modeling_bertabs.py,981,,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,1,coding=utf-8,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,2,Copyright 2019 The HuggingFace Inc. team.,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/summarization/bertabs/configuration_bertabs.py,4,,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/summarization/bertabs/configuration_bertabs.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,7,You may obtain a copy of the License at,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,8,,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,10,,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/summarization/bertabs/configuration_bertabs.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/summarization/bertabs/configuration_bertabs.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/summarization/bertabs/configuration_bertabs.py,14,See the License for the specific language governing permissions and,not
transformers/examples/summarization/bertabs/configuration_bertabs.py,15,limitations under the License.,not
transformers/examples/summarization/bertabs/run_summarization.py,1,! /usr/bin/python3,not
transformers/examples/summarization/bertabs/run_summarization.py,59,Default F1_score,not
transformers/examples/summarization/bertabs/run_summarization.py,64,these (unused) arguments are defined to keep the compatibility,not
transformers/examples/summarization/bertabs/run_summarization.py,65,with the legacy code and will be deleted in a next iteration.,not
transformers/examples/summarization/bertabs/run_summarization.py,113,Prepare the summary file's name,not
transformers/examples/summarization/bertabs/run_summarization.py,180,,not
transformers/examples/summarization/bertabs/run_summarization.py,181,LOAD the dataset,not
transformers/examples/summarization/bertabs/run_summarization.py,182,,not
transformers/examples/summarization/bertabs/run_summarization.py,209,remove empty_files,not
transformers/examples/summarization/bertabs/run_summarization.py,268,EVALUATION options,not
transformers/examples/summarization/bertabs/run_summarization.py,275,BEAM SEARCH arguments,not
transformers/examples/summarization/bertabs/run_summarization.py,296,Select device (distibuted not available),not
transformers/examples/summarization/bertabs/run_summarization.py,299,Check the existence of directories,not
transformers/examples/summarization/bertabs/utils_summarization.py,8,------------,not
transformers/examples/summarization/bertabs/utils_summarization.py,9,Data loading,not
transformers/examples/summarization/bertabs/utils_summarization.py,10,------------,not
transformers/examples/summarization/bertabs/utils_summarization.py,73,"for some unknown reason some lines miss a period, add it",SATD
transformers/examples/summarization/bertabs/utils_summarization.py,76,gather article lines,not
transformers/examples/summarization/bertabs/utils_summarization.py,86,"if ""@highlight"" is absent from the file we pop",not
transformers/examples/summarization/bertabs/utils_summarization.py,87,"all elements until there is None, raising an exception.",not
transformers/examples/summarization/bertabs/utils_summarization.py,90,gather summary lines,not
transformers/examples/summarization/bertabs/utils_summarization.py,105,--------------------------,not
transformers/examples/summarization/bertabs/utils_summarization.py,106,Encoding and preprocessing,not
transformers/examples/summarization/bertabs/utils_summarization.py,107,--------------------------,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,1,coding=utf-8,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,2,Copyright 2019 HuggingFace Inc.,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,3,,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/summarization/bertabs/test_utils_summarization.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,6,You may obtain a copy of the License at,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,7,,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,9,,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/summarization/bertabs/test_utils_summarization.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/summarization/bertabs/test_utils_summarization.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/summarization/bertabs/test_utils_summarization.py,13,See the License for the specific language governing permissions and,not
transformers/examples/summarization/bertabs/test_utils_summarization.py,14,limitations under the License.,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,1,coding=utf-8,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,2,Copyright 2018 The HuggingFace Inc. team.,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,3,,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,6,You may obtain a copy of the License at,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,7,,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,9,,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,13,See the License for the specific language governing permissions and,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,14,limitations under the License.,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,29,The authors' implementation,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,69,Instantiate the authors' model with the pre-trained weights,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,96,-------------------,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,97,Convert the weights,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,98,-------------------,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,105,----------------------------------,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,106,Make sure the outpus are identical,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,107,----------------------------------,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,112,prepare the model inputs,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,120,failsafe to make sure the weights reset does not affect the,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,121,loaded weights.,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,124,forward pass,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,133,The original model does not apply the geneator layer immediatly but rather in,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,134,the beam search (where it combines softmax + linear layer). Since we already,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,135,apply the softmax in our generation process we only apply the linear layer here.,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,136,We make sure that the outputs of the full stack are identical,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,156,The model has been saved with torch.save(model) and this is bound to the exact,not
transformers/examples/summarization/bertabs/convert_bertabs_original_pytorch_checkpoint.py,157,directory structure. We save the state_dict instead.,not
transformers/examples/summarization/bart/finetune.py,69,NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py,not
transformers/examples/summarization/bart/finetune.py,95,write predictions and targets for later rouge evaluation.,not
transformers/examples/summarization/bart/finetune.py,132,Add BART specific options,not
transformers/examples/summarization/bart/finetune.py,160,"If output_dir not provided, a folder will be generated in pwd",not
transformers/examples/summarization/bart/finetune.py,167,"Optionally, predict on dev set and write to output_dir",not
transformers/examples/summarization/bart/finetune.py,169,See https://github.com/huggingface/transformers/issues/3159,not
transformers/examples/summarization/bart/finetune.py,170,pl use this format to create a checkpoint:,not
transformers/examples/summarization/bart/finetune.py,171,https://github.com/PyTorchLightning/pytorch-lightning/blob/master\,not
transformers/examples/summarization/bart/finetune.py,172,/pytorch_lightning/callbacks/model_checkpoint.py#L169,not
transformers/examples/summarization/bart/test_bart_examples.py,74,remove noisy download output from tracebacks,not
transformers/examples/summarization/bart/test_bart_examples.py,123,"args_d.update({""do_train"": False, ""do_predict"": True})",not
transformers/examples/summarization/bart/test_bart_examples.py,124,main(argparse.Namespace(**args_d)),not
transformers/examples/summarization/bart/test_bart_examples.py,142,show that articles were trimmed.,not
transformers/examples/summarization/bart/test_bart_examples.py,144,trimmed significantly,not
transformers/examples/summarization/bart/test_bart_examples.py,146,show that targets were truncated,not
transformers/examples/summarization/bart/test_bart_examples.py,147,Truncated,not
transformers/examples/summarization/bart/test_bart_examples.py,148,Truncated,not
transformers/examples/summarization/bart/evaluate_cnn.py,36,+2 from original because we start at step=1 and stop before max_length,not
transformers/examples/summarization/bart/evaluate_cnn.py,37,+1 from original because we start at step=1,not
transformers/examples/distillation/distiller.py,1,coding=utf-8,not
transformers/examples/distillation/distiller.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",not
transformers/examples/distillation/distiller.py,3,,not
transformers/examples/distillation/distiller.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/distiller.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/distiller.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/distiller.py,7,,not
transformers/examples/distillation/distiller.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/distiller.py,9,,not
transformers/examples/distillation/distiller.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/distiller.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/distiller.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/distiller.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/distiller.py,14,limitations under the License.,not
transformers/examples/distillation/distiller.py,219,"previously `dtype=torch.uint8`, cf pytorch 1.2.0 compatibility",not
transformers/examples/distillation/distiller.py,225,mask a number of words == 0 [8] (faster with fp16),not
transformers/examples/distillation/distiller.py,247,"previously `mlm_labels[1-pred_mask] = -1`, cf pytorch 1.2.0 compatibility",not
transformers/examples/distillation/distiller.py,249,sanity checks,not
transformers/examples/distillation/distiller.py,276,"previously `clm_labels[1-attn_mask] = -1`, cf pytorch 1.2.0 compatibility",not
transformers/examples/distillation/distiller.py,278,sanity checks,not
transformers/examples/distillation/distiller.py,301,number of sentences == 0 [8],not
transformers/examples/distillation/distiller.py,313,sequence length == 0 [8],not
transformers/examples/distillation/distiller.py,386,"(bs, seq_length, voc_size)",not
transformers/examples/distillation/distiller.py,390,"(bs, seq_length, voc_size)",not
transformers/examples/distillation/distiller.py,394,"(bs, seq_length, voc_size)",not
transformers/examples/distillation/distiller.py,398,"(bs, seq_length, voc_size)",not
transformers/examples/distillation/distiller.py,401,https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100,not
transformers/examples/distillation/distiller.py,402,https://github.com/peterliht/knowledge-distillation-pytorch/issues/2,not
transformers/examples/distillation/distiller.py,404,"(bs, seq_lenth, voc_size)",not
transformers/examples/distillation/distiller.py,406,"(bs, seq_lenth, voc_size)",not
transformers/examples/distillation/distiller.py,407,(bs * seq_length * voc_size) modulo the 1s in mask,not
transformers/examples/distillation/distiller.py,408,"(bs * seq_length, voc_size) modulo the 1s in mask",not
transformers/examples/distillation/distiller.py,409,(bs * seq_length * voc_size) modulo the 1s in mask,not
transformers/examples/distillation/distiller.py,410,"(bs * seq_length, voc_size) modulo the 1s in mask",not
transformers/examples/distillation/distiller.py,434,Reproducing batchmean reduction,not
transformers/examples/distillation/distiller.py,437,"(bs, seq_length, dim)",not
transformers/examples/distillation/distiller.py,438,"(bs, seq_length, dim)",not
transformers/examples/distillation/distiller.py,439,"(bs, seq_length, dim)",not
transformers/examples/distillation/distiller.py,443,(bs * seq_length * dim),not
transformers/examples/distillation/distiller.py,444,"(bs * seq_length, dim)",not
transformers/examples/distillation/distiller.py,445,(bs * seq_length * dim),not
transformers/examples/distillation/distiller.py,446,"(bs * seq_length, dim)",not
transformers/examples/distillation/distiller.py,448,"(bs * seq_length,)",not
transformers/examples/distillation/distiller.py,474,Check for NaN,not
transformers/examples/distillation/utils.py,1,coding=utf-8,not
transformers/examples/distillation/utils.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",not
transformers/examples/distillation/utils.py,3,,not
transformers/examples/distillation/utils.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/utils.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/utils.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/utils.py,7,,not
transformers/examples/distillation/utils.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/utils.py,9,,not
transformers/examples/distillation/utils.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/utils.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/utils.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/utils.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/utils.py,14,limitations under the License.,not
transformers/examples/distillation/utils.py,72,number of nodes / node ID,not
transformers/examples/distillation/utils.py,80,local job (single GPU),not
transformers/examples/distillation/utils.py,92,sanity checks,not
transformers/examples/distillation/utils.py,98,define whether this is the master process / if we are in multi-node distributed mode,not
transformers/examples/distillation/utils.py,102,summary,not
transformers/examples/distillation/utils.py,114,set GPU device,not
transformers/examples/distillation/utils.py,117,initialize multi-GPU,not
transformers/examples/distillation/run_squad_w_distillation.py,1,coding=utf-8,not
transformers/examples/distillation/run_squad_w_distillation.py,2,Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.,not
transformers/examples/distillation/run_squad_w_distillation.py,3,"Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.",not
transformers/examples/distillation/run_squad_w_distillation.py,4,,not
transformers/examples/distillation/run_squad_w_distillation.py,5,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/run_squad_w_distillation.py,6,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/run_squad_w_distillation.py,7,You may obtain a copy of the License at,not
transformers/examples/distillation/run_squad_w_distillation.py,8,,not
transformers/examples/distillation/run_squad_w_distillation.py,9,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/run_squad_w_distillation.py,10,,not
transformers/examples/distillation/run_squad_w_distillation.py,11,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/run_squad_w_distillation.py,12,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/run_squad_w_distillation.py,13,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/run_squad_w_distillation.py,14,See the License for the specific language governing permissions and,not
transformers/examples/distillation/run_squad_w_distillation.py,15,limitations under the License.,not
transformers/examples/distillation/run_squad_w_distillation.py,110,Prepare optimizer and schedule (linear warmup and decay),not
transformers/examples/distillation/run_squad_w_distillation.py,124,Check if saved optimizer or scheduler states exist,not
transformers/examples/distillation/run_squad_w_distillation.py,128,Load in optimizer and scheduler states,not
transformers/examples/distillation/run_squad_w_distillation.py,140,multi-gpu training (should be after apex fp16 initialization),not
transformers/examples/distillation/run_squad_w_distillation.py,144,Distributed training (should be after apex fp16 initialization),not
transformers/examples/distillation/run_squad_w_distillation.py,150,Train!,not
transformers/examples/distillation/run_squad_w_distillation.py,167,Check if continuing training from a checkpoint,not
transformers/examples/distillation/run_squad_w_distillation.py,170,set global_step to gobal_step of last saved checkpoint from model path,not
transformers/examples/distillation/run_squad_w_distillation.py,188,Added here for reproductibility,not
transformers/examples/distillation/run_squad_w_distillation.py,195,Skip past any already trained steps if resuming training,not
transformers/examples/distillation/run_squad_w_distillation.py,220,Distillation loss,not
transformers/examples/distillation/run_squad_w_distillation.py,247,mean() to average on multi-gpu parallel (not distributed) training,not
transformers/examples/distillation/run_squad_w_distillation.py,265,Update learning rate schedule,not
transformers/examples/distillation/run_squad_w_distillation.py,269,Log metrics,not
transformers/examples/distillation/run_squad_w_distillation.py,271,Only evaluate when single GPU otherwise metrics may not average well,not
transformers/examples/distillation/run_squad_w_distillation.py,281,Save model checkpoint,not
transformers/examples/distillation/run_squad_w_distillation.py,287,Take care of distributed/parallel training,SATD
transformers/examples/distillation/run_squad_w_distillation.py,319,Note that DistributedSampler samples randomly,not
transformers/examples/distillation/run_squad_w_distillation.py,323,multi-gpu evaluate,not
transformers/examples/distillation/run_squad_w_distillation.py,327,Eval!,not
transformers/examples/distillation/run_squad_w_distillation.py,342,XLM don't use segment_ids,not
transformers/examples/distillation/run_squad_w_distillation.py,355,"Some models (XLNet, XLM) use 5 arguments for their predictions, while the other ""simpler""",not
transformers/examples/distillation/run_squad_w_distillation.py,356,models only use two.,not
transformers/examples/distillation/run_squad_w_distillation.py,382,Compute predictions,not
transformers/examples/distillation/run_squad_w_distillation.py,392,XLNet uses a more complex post-processing procedure,not
transformers/examples/distillation/run_squad_w_distillation.py,425,Compute the F1 and exact scores.,not
transformers/examples/distillation/run_squad_w_distillation.py,432,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/distillation/run_squad_w_distillation.py,435,Load data features from cache or dataset file,not
transformers/examples/distillation/run_squad_w_distillation.py,484,"Make sure only the first process in distributed training process the dataset, and the others will use the cache",not
transformers/examples/distillation/run_squad_w_distillation.py,495,Required parameters,not
transformers/examples/distillation/run_squad_w_distillation.py,518,Distillation parameters (optional),not
transformers/examples/distillation/run_squad_w_distillation.py,541,Other parameters,not
transformers/examples/distillation/run_squad_w_distillation.py,711,Setup distant debugging if needed,not
transformers/examples/distillation/run_squad_w_distillation.py,713,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/distillation/run_squad_w_distillation.py,720,"Setup CUDA, GPU & distributed training",not
transformers/examples/distillation/run_squad_w_distillation.py,724,Initializes the distributed backend which will take care of sychronizing nodes/GPUs,not
transformers/examples/distillation/run_squad_w_distillation.py,731,Setup logging,not
transformers/examples/distillation/run_squad_w_distillation.py,746,Set seed,not
transformers/examples/distillation/run_squad_w_distillation.py,749,Load pretrained model and tokenizer,not
transformers/examples/distillation/run_squad_w_distillation.py,751,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/distillation/run_squad_w_distillation.py,789,Make sure only the first process in distributed training will download model & vocab,not
transformers/examples/distillation/run_squad_w_distillation.py,796,"Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.",not
transformers/examples/distillation/run_squad_w_distillation.py,797,"Otherwise it'll default to ""promote"" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=""O2""` will",not
transformers/examples/distillation/run_squad_w_distillation.py,798,"remove the need for this code, but it is still valid.",not
transformers/examples/distillation/run_squad_w_distillation.py,807,Training,not
transformers/examples/distillation/run_squad_w_distillation.py,813,Save the trained model and the tokenizer,not
transformers/examples/distillation/run_squad_w_distillation.py,815,Create output directory if needed,not
transformers/examples/distillation/run_squad_w_distillation.py,820,"Save a trained model, configuration and tokenizer using `save_pretrained()`.",not
transformers/examples/distillation/run_squad_w_distillation.py,821,They can then be reloaded using `from_pretrained()`,not
transformers/examples/distillation/run_squad_w_distillation.py,824,Take care of distributed/parallel training,SATD
transformers/examples/distillation/run_squad_w_distillation.py,828,Good practice: save your training arguments together with the trained model,not
transformers/examples/distillation/run_squad_w_distillation.py,831,Load a trained model and vocabulary that you have fine-tuned,not
transformers/examples/distillation/run_squad_w_distillation.py,836,Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory,not
transformers/examples/distillation/run_squad_w_distillation.py,846,Reduce model loading logs,not
transformers/examples/distillation/run_squad_w_distillation.py,851,Reload the model,not
transformers/examples/distillation/run_squad_w_distillation.py,856,Evaluate,not
transformers/examples/distillation/grouped_batch_sampler.py,1,coding=utf-8,not
transformers/examples/distillation/grouped_batch_sampler.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",not
transformers/examples/distillation/grouped_batch_sampler.py,3,,not
transformers/examples/distillation/grouped_batch_sampler.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/grouped_batch_sampler.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/grouped_batch_sampler.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/grouped_batch_sampler.py,7,,not
transformers/examples/distillation/grouped_batch_sampler.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/grouped_batch_sampler.py,9,,not
transformers/examples/distillation/grouped_batch_sampler.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/grouped_batch_sampler.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/grouped_batch_sampler.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/grouped_batch_sampler.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/grouped_batch_sampler.py,14,limitations under the License.,not
transformers/examples/distillation/grouped_batch_sampler.py,37,count number of elements per group,not
transformers/examples/distillation/grouped_batch_sampler.py,79,TODO,SATD
transformers/examples/distillation/grouped_batch_sampler.py,84,now we have run out of elements that satisfy,not
transformers/examples/distillation/grouped_batch_sampler.py,85,"the group criteria, let's return the remaining",not
transformers/examples/distillation/grouped_batch_sampler.py,86,elements so that the size of the sampler is,not
transformers/examples/distillation/grouped_batch_sampler.py,87,deterministic,not
transformers/examples/distillation/grouped_batch_sampler.py,91,"for the remaining batches, group the batches by similar lengths",not
transformers/examples/distillation/lm_seqs_dataset.py,1,coding=utf-8,not
transformers/examples/distillation/lm_seqs_dataset.py,2,"Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.",not
transformers/examples/distillation/lm_seqs_dataset.py,3,,not
transformers/examples/distillation/lm_seqs_dataset.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/lm_seqs_dataset.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/lm_seqs_dataset.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/lm_seqs_dataset.py,7,,not
transformers/examples/distillation/lm_seqs_dataset.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/lm_seqs_dataset.py,9,,not
transformers/examples/distillation/lm_seqs_dataset.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/lm_seqs_dataset.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/lm_seqs_dataset.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/lm_seqs_dataset.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/lm_seqs_dataset.py,14,limitations under the License.,not
transformers/examples/distillation/lm_seqs_dataset.py,136,data_len = sum(self.lengths),not
transformers/examples/distillation/lm_seqs_dataset.py,137,nb_unique_tokens = len(Counter(list(chain(*self.token_ids)))),not
transformers/examples/distillation/lm_seqs_dataset.py,138,logger.info(f'{data_len} tokens ({nb_unique_tokens} unique)'),not
transformers/examples/distillation/lm_seqs_dataset.py,140,unk_idx = self.params.special_tok_ids['unk_token'],not
transformers/examples/distillation/lm_seqs_dataset.py,141,nb_unkown = sum([(t==unk_idx).sum() for t in self.token_ids]),not
transformers/examples/distillation/lm_seqs_dataset.py,142,logger.info(f'{nb_unkown} unknown tokens (covering {100*nb_unkown/data_len:.2f}% of the data)'),not
transformers/examples/distillation/lm_seqs_dataset.py,152,Max for paddings,not
transformers/examples/distillation/lm_seqs_dataset.py,155,Pad token ids,not
transformers/examples/distillation/lm_seqs_dataset.py,164,"(bs, max_seq_len_)",not
transformers/examples/distillation/lm_seqs_dataset.py,165,(bs),not
transformers/examples/distillation/train.py,1,coding=utf-8,not
transformers/examples/distillation/train.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/examples/distillation/train.py,3,,not
transformers/examples/distillation/train.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/train.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/train.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/train.py,7,,not
transformers/examples/distillation/train.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/train.py,9,,not
transformers/examples/distillation/train.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/train.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/train.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/train.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/train.py,14,limitations under the License.,not
transformers/examples/distillation/train.py,222,ARGS,not
transformers/examples/distillation/train.py,239,SAVE PARAMS,not
transformers/examples/distillation/train.py,248,TOKENIZER,not
transformers/examples/distillation/train.py,258,DATA LOADER,not
transformers/examples/distillation/train.py,270,do not predict special tokens,not
transformers/examples/distillation/train.py,278,STUDENT,not
transformers/examples/distillation/train.py,293,TEACHER,not
transformers/examples/distillation/train.py,299,FREEZING,not
transformers/examples/distillation/train.py,305,SANITY CHECKS,not
transformers/examples/distillation/train.py,312,DISTILLER,not
transformers/examples/distillation/scripts/extract.py,1,coding=utf-8,not
transformers/examples/distillation/scripts/extract.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/examples/distillation/scripts/extract.py,3,,not
transformers/examples/distillation/scripts/extract.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/scripts/extract.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/scripts/extract.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/scripts/extract.py,7,,not
transformers/examples/distillation/scripts/extract.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/scripts/extract.py,9,,not
transformers/examples/distillation/scripts/extract.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/scripts/extract.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/scripts/extract.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/scripts/extract.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/scripts/extract.py,14,limitations under the License.,not
transformers/examples/distillation/scripts/extract.py,46,Embeddings,not
transformers/examples/distillation/scripts/extract.py,58,Transformer Blocks,not
transformers/examples/distillation/scripts/extract.py,85,Language Modeling Head ###s,not
transformers/examples/distillation/scripts/extract_distilbert.py,1,coding=utf-8,not
transformers/examples/distillation/scripts/extract_distilbert.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/examples/distillation/scripts/extract_distilbert.py,3,,not
transformers/examples/distillation/scripts/extract_distilbert.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/scripts/extract_distilbert.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/scripts/extract_distilbert.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/scripts/extract_distilbert.py,7,,not
transformers/examples/distillation/scripts/extract_distilbert.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/scripts/extract_distilbert.py,9,,not
transformers/examples/distillation/scripts/extract_distilbert.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/scripts/extract_distilbert.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/scripts/extract_distilbert.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/scripts/extract_distilbert.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/scripts/extract_distilbert.py,14,limitations under the License.,not
transformers/examples/distillation/scripts/token_counts.py,1,coding=utf-8,not
transformers/examples/distillation/scripts/token_counts.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/examples/distillation/scripts/token_counts.py,3,,not
transformers/examples/distillation/scripts/token_counts.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/scripts/token_counts.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/scripts/token_counts.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/scripts/token_counts.py,7,,not
transformers/examples/distillation/scripts/token_counts.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/scripts/token_counts.py,9,,not
transformers/examples/distillation/scripts/token_counts.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/scripts/token_counts.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/scripts/token_counts.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/scripts/token_counts.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/scripts/token_counts.py,14,limitations under the License.,not
transformers/examples/distillation/scripts/binarized_data.py,1,coding=utf-8,not
transformers/examples/distillation/scripts/binarized_data.py,2,"Copyright 2019-present, the HuggingFace Inc. team.",not
transformers/examples/distillation/scripts/binarized_data.py,3,,not
transformers/examples/distillation/scripts/binarized_data.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/distillation/scripts/binarized_data.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/distillation/scripts/binarized_data.py,6,You may obtain a copy of the License at,not
transformers/examples/distillation/scripts/binarized_data.py,7,,not
transformers/examples/distillation/scripts/binarized_data.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/distillation/scripts/binarized_data.py,9,,not
transformers/examples/distillation/scripts/binarized_data.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/distillation/scripts/binarized_data.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/distillation/scripts/binarized_data.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/distillation/scripts/binarized_data.py,13,See the License for the specific language governing permissions and,not
transformers/examples/distillation/scripts/binarized_data.py,14,limitations under the License.,not
transformers/examples/distillation/scripts/binarized_data.py,48,`[CLS]`,not
transformers/examples/distillation/scripts/binarized_data.py,49,`[SEP]`,not
transformers/examples/distillation/scripts/binarized_data.py,52,`<s>`,not
transformers/examples/distillation/scripts/binarized_data.py,53,`</s>`,not
transformers/examples/distillation/scripts/binarized_data.py,56,`<|endoftext|>`,not
transformers/examples/distillation/scripts/binarized_data.py,57,`<|endoftext|>`,not
transformers/examples/bertology/run_bertology.py,1,!/usr/bin/env python3,not
transformers/examples/bertology/run_bertology.py,2,Copyright 2018 CMU and The HuggingFace Inc. team.,not
transformers/examples/bertology/run_bertology.py,3,,not
transformers/examples/bertology/run_bertology.py,4,"Licensed under the Apache License, Version 2.0 (the ""License"");",not
transformers/examples/bertology/run_bertology.py,5,you may not use this file except in compliance with the License.,not
transformers/examples/bertology/run_bertology.py,6,You may obtain a copy of the License at,not
transformers/examples/bertology/run_bertology.py,7,,not
transformers/examples/bertology/run_bertology.py,8,http://www.apache.org/licenses/LICENSE-2.0,not
transformers/examples/bertology/run_bertology.py,9,,not
transformers/examples/bertology/run_bertology.py,10,"Unless required by applicable law or agreed to in writing, software",not
transformers/examples/bertology/run_bertology.py,11,"distributed under the License is distributed on an ""AS IS"" BASIS,",not
transformers/examples/bertology/run_bertology.py,12,"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",not
transformers/examples/bertology/run_bertology.py,13,See the License for the specific language governing permissions and,not
transformers/examples/bertology/run_bertology.py,14,limitations under the License.,not
transformers/examples/bertology/run_bertology.py,73,Prepare our tensors,not
transformers/examples/bertology/run_bertology.py,89,Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below),not
transformers/examples/bertology/run_bertology.py,95,"Loss and logits are the first, attention the last",not
transformers/examples/bertology/run_bertology.py,96,Backpropagate to populate the gradients in the head mask,not
transformers/examples/bertology/run_bertology.py,106,Also store our logits/labels if we want to compute metrics afterwards,not
transformers/examples/bertology/run_bertology.py,116,Normalize,not
transformers/examples/bertology/run_bertology.py,119,Layerwise importance normalization,not
transformers/examples/bertology/run_bertology.py,128,Print/save matrices,not
transformers/examples/bertology/run_bertology.py,161,save current head mask,not
transformers/examples/bertology/run_bertology.py,162,heads from least important to most - keep only not-masked heads,not
transformers/examples/bertology/run_bertology.py,169,mask heads,not
transformers/examples/bertology/run_bertology.py,177,Compute metric and head importance again,not
transformers/examples/bertology/run_bertology.py,201,Try pruning and test time speedup,not
transformers/examples/bertology/run_bertology.py,202,Pruning is like masking but we actually remove the masked weights,not
transformers/examples/bertology/run_bertology.py,237,Required parameters,not
transformers/examples/bertology/run_bertology.py,267,Other parameters,not
transformers/examples/bertology/run_bertology.py,336,Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script,not
transformers/examples/bertology/run_bertology.py,343,Setup devices and distributed training,not
transformers/examples/bertology/run_bertology.py,351,Initializes the distributed backend,not
transformers/examples/bertology/run_bertology.py,353,Setup logging,not
transformers/examples/bertology/run_bertology.py,357,Set seeds,not
transformers/examples/bertology/run_bertology.py,360,Prepare GLUE task,not
transformers/examples/bertology/run_bertology.py,369,Load pretrained model and tokenizer,not
transformers/examples/bertology/run_bertology.py,370,,not
transformers/examples/bertology/run_bertology.py,371,Distributed training:,not
transformers/examples/bertology/run_bertology.py,372,The .from_pretrained methods guarantee that only one local process can concurrently,not
transformers/examples/bertology/run_bertology.py,373,download model & vocab.,not
transformers/examples/bertology/run_bertology.py,392,Distributed and parallel training,not
transformers/examples/bertology/run_bertology.py,401,Print/save training arguments,not
transformers/examples/bertology/run_bertology.py,406,Prepare dataset for the GLUE task,not
transformers/examples/bertology/run_bertology.py,415,Compute head entropy and importance score,not
transformers/examples/bertology/run_bertology.py,418,Try head masking (set heads to zero until the score goes under a threshole),not
transformers/examples/bertology/run_bertology.py,419,and head pruning (remove masked heads and see the effect on the network),not
transformers/docs/source/conf.py,1,-*- coding: utf-8 -*-,not
transformers/docs/source/conf.py,2,,not
transformers/docs/source/conf.py,3,Configuration file for the Sphinx documentation builder.,not
transformers/docs/source/conf.py,4,,not
transformers/docs/source/conf.py,5,This file does only contain a selection of the most common options. For a,not
transformers/docs/source/conf.py,6,full list see the documentation:,not
transformers/docs/source/conf.py,7,http://www.sphinx-doc.org/en/master/config,not
transformers/docs/source/conf.py,9,-- Path setup --------------------------------------------------------------,not
transformers/docs/source/conf.py,11,"If extensions (or modules to document with autodoc) are in another directory,",not
transformers/docs/source/conf.py,12,add these directories to sys.path here. If the directory is relative to the,not
transformers/docs/source/conf.py,13,"documentation root, use os.path.abspath to make it absolute, like shown here.",not
transformers/docs/source/conf.py,14,,not
transformers/docs/source/conf.py,20,-- Project information -----------------------------------------------------,not
transformers/docs/source/conf.py,26,The short X.Y version,not
transformers/docs/source/conf.py,28,"The full version, including alpha/beta/rc tags",not
transformers/docs/source/conf.py,32,-- General configuration ---------------------------------------------------,not
transformers/docs/source/conf.py,34,"If your documentation needs a minimal Sphinx version, state it here.",not
transformers/docs/source/conf.py,35,,not
transformers/docs/source/conf.py,36,needs_sphinx = '1.0',not
transformers/docs/source/conf.py,38,"Add any Sphinx extension module names here, as strings. They can be",not
transformers/docs/source/conf.py,39,extensions coming with Sphinx (named 'sphinx.ext.*') or your custom,not
transformers/docs/source/conf.py,40,ones.,not
transformers/docs/source/conf.py,50,"Add any paths that contain templates here, relative to this directory.",not
transformers/docs/source/conf.py,53,The suffix(es) of source filenames.,not
transformers/docs/source/conf.py,54,You can specify multiple suffix as a list of string:,not
transformers/docs/source/conf.py,55,,not
transformers/docs/source/conf.py,57,source_suffix = '.rst',not
transformers/docs/source/conf.py,59,The master toctree document.,not
transformers/docs/source/conf.py,62,The language for content autogenerated by Sphinx. Refer to documentation,not
transformers/docs/source/conf.py,63,for a list of supported languages.,not
transformers/docs/source/conf.py,64,,not
transformers/docs/source/conf.py,65,This is also used if you do content translation via gettext catalogs.,not
transformers/docs/source/conf.py,66,"Usually you set ""language"" from the command line for these cases.",not
transformers/docs/source/conf.py,69,"List of patterns, relative to source directory, that match files and",not
transformers/docs/source/conf.py,70,directories to ignore when looking for source files.,not
transformers/docs/source/conf.py,71,This pattern also affects html_static_path and html_extra_path.,not
transformers/docs/source/conf.py,74,The name of the Pygments (syntax highlighting) style to use.,not
transformers/docs/source/conf.py,78,-- Options for HTML output -------------------------------------------------,not
transformers/docs/source/conf.py,80,The theme to use for HTML and HTML Help pages.  See the documentation for,not
transformers/docs/source/conf.py,81,a list of builtin themes.,not
transformers/docs/source/conf.py,82,,not
transformers/docs/source/conf.py,85,Theme options are theme-specific and customize the look and feel of a theme,not
transformers/docs/source/conf.py,86,"further.  For a list of options available for each theme, see the",not
transformers/docs/source/conf.py,87,documentation.,not
transformers/docs/source/conf.py,88,,not
transformers/docs/source/conf.py,93,"Add any paths that contain custom static files (such as style sheets) here,",not
transformers/docs/source/conf.py,94,"relative to this directory. They are copied after the builtin static files,",not
transformers/docs/source/conf.py,95,"so a file named ""default.css"" will overwrite the builtin ""default.css"".",not
transformers/docs/source/conf.py,98,"Custom sidebar templates, must be a dictionary that maps document names",not
transformers/docs/source/conf.py,99,to template names.,not
transformers/docs/source/conf.py,100,,not
transformers/docs/source/conf.py,101,The default sidebars (for documents that don't match any pattern) are,not
transformers/docs/source/conf.py,102,defined by theme itself.  Builtin themes are using these templates by,not
transformers/docs/source/conf.py,103,"default: ``['localtoc.html', 'relations.html', 'sourcelink.html',",not
transformers/docs/source/conf.py,104,'searchbox.html']``.,not
transformers/docs/source/conf.py,105,,not
transformers/docs/source/conf.py,106,html_sidebars = {},not
transformers/docs/source/conf.py,108,This must be the name of an image file (path relative to the configuration,not
transformers/docs/source/conf.py,109,directory) that is the favicon of the docs. Modern browsers use this as,not
transformers/docs/source/conf.py,110,"the icon for tabs, windows and bookmarks. It should be a Windows-style",not
transformers/docs/source/conf.py,111,icon file (.ico).,not
transformers/docs/source/conf.py,115,-- Options for HTMLHelp output ---------------------------------------------,not
transformers/docs/source/conf.py,117,Output file base name for HTML help builder.,not
transformers/docs/source/conf.py,121,-- Options for LaTeX output ------------------------------------------------,not
transformers/docs/source/conf.py,124,The paper size ('letterpaper' or 'a4paper').,not
transformers/docs/source/conf.py,125,,not
transformers/docs/source/conf.py,126,"'papersize': 'letterpaper',",not
transformers/docs/source/conf.py,128,"The font size ('10pt', '11pt' or '12pt').",not
transformers/docs/source/conf.py,129,,not
transformers/docs/source/conf.py,130,"'pointsize': '10pt',",not
transformers/docs/source/conf.py,132,Additional stuff for the LaTeX preamble.,not
transformers/docs/source/conf.py,133,,not
transformers/docs/source/conf.py,134,"'preamble': '',",not
transformers/docs/source/conf.py,136,Latex figure (float) alignment,not
transformers/docs/source/conf.py,137,,not
transformers/docs/source/conf.py,138,"'figure_align': 'htbp',",not
transformers/docs/source/conf.py,141,Grouping the document tree into LaTeX files. List of tuples,not
transformers/docs/source/conf.py,142,"(source start file, target name, title,",not
transformers/docs/source/conf.py,143,"author, documentclass [howto, manual, or own class]).",SATD
transformers/docs/source/conf.py,150,-- Options for manual page output ------------------------------------------,not
transformers/docs/source/conf.py,152,One entry per manual page. List of tuples,not
transformers/docs/source/conf.py,153,"(source start file, name, description, authors, manual section).",not
transformers/docs/source/conf.py,160,-- Options for Texinfo output ----------------------------------------------,not
transformers/docs/source/conf.py,162,Grouping the document tree into Texinfo files. List of tuples,not
transformers/docs/source/conf.py,163,"(source start file, target name, title, author,",not
transformers/docs/source/conf.py,164,"dir menu entry, description, category)",not
transformers/docs/source/conf.py,172,-- Options for Epub output -------------------------------------------------,not
transformers/docs/source/conf.py,174,Bibliographic Dublin Core info.,not
transformers/docs/source/conf.py,177,The unique identifier of the text. This can be a ISBN number,not
transformers/docs/source/conf.py,178,or the project homepage.,not
transformers/docs/source/conf.py,179,,not
transformers/docs/source/conf.py,180,epub_identifier = '',not
transformers/docs/source/conf.py,182,A unique identification for the text.,not
transformers/docs/source/conf.py,183,,not
transformers/docs/source/conf.py,184,epub_uid = '',not
transformers/docs/source/conf.py,186,A list of files that should not be packed into the epub file.,not
transformers/docs/source/conf.py,194,-- Extension configuration -------------------------------------------------,not
