file path,line #,comment,satd
scrapy/conftest.py,11,"not a test, but looks like a test",not
scrapy/conftest.py,13,contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess,not
scrapy/conftest.py,15,contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess,not
scrapy/conftest.py,17,Py36-only parts of respective tests,not
scrapy/conftest.py,34,Avoid executing tests when executing `--flake8` flag (pytest-flake8),not
scrapy/conftest.py,46,doctests,not
scrapy/extras/qps-bench-server.py,1,!/usr/bin/env python,not
scrapy/extras/qps-bench-server.py,28,reset stats on high iter-request times caused by client restarts,not
scrapy/extras/qps-bench-server.py,29,seconds,not
scrapy/extras/qpsclient.py,19,Max concurrency is limited by global CONCURRENT_REQUESTS setting,not
scrapy/extras/qpsclient.py,21,Requests per second goal,not
scrapy/extras/qpsclient.py,22,same as: 1 / download_delay,not
scrapy/extras/qpsclient.py,24,time in seconds to delay server responses,not
scrapy/extras/qpsclient.py,26,number of slots to create,not
scrapy/tests/test_utils_url.py,242,some corner cases (default to http://),not
scrapy/tests/test_utils_url.py,252,TODO: the following tests do not pass with current implementation,SATD
scrapy/tests/test_utils_url.py,316,"user: ""username@""",not
scrapy/tests/test_utils_url.py,317,password: none,not
scrapy/tests/test_utils_url.py,321,"user: ""username:pass""",not
scrapy/tests/test_utils_url.py,322,"password: """"",not
scrapy/tests/test_utils_url.py,326,"user: ""me""",not
scrapy/tests/test_utils_url.py,327,"password: ""user@domain.com""",not
scrapy/tests/test_utils_asyncio.py,12,the result should depend only on the pytest --reactor argument,not
scrapy/tests/test_utils_asyncio.py,16,this should do nothing,not
scrapy/tests/test_commands.py,155,only pass one argument. spider script shouldn't be created,not
scrapy/tests/test_commands.py,158,pass two arguments <name> <domain>. spider script should be created,not
scrapy/tests/test_commands.py,262,see https://github.com/scrapy/scrapy/issues/2811,SATD
scrapy/tests/test_commands.py,263,"The spider below should not be able to connect to localhost:12345,",not
scrapy/tests/test_commands.py,264,"which is intended,",not
scrapy/tests/test_commands.py,265,but this should not be because of DNS lookup error,not
scrapy/tests/test_commands.py,266,assumption: localhost will resolve in all cases (true?),not
scrapy/tests/test_downloadermiddleware_redirect.py,35,response without Location header but with status code is 3XX should be ignored,not
scrapy/tests/test_downloadermiddleware_redirect.py,61,Test that it redirects when dont_redirect is False,not
scrapy/tests/test_downloadermiddleware_redirect.py,87,response without Location header but with status code is 3XX should be ignored,not
scrapy/tests/test_downloadermiddleware_redirect.py,102,response without Location header but with status code is 3XX should be ignored,not
scrapy/tests/test_downloadermiddleware_redirect.py,118,response without Location header but with status code is 3XX should be ignored,not
scrapy/tests/test_downloadermiddleware_redirect.py,187,HTTP historically supports latin1,not
scrapy/tests/test_downloadermiddleware_redirect.py,195,header using UTF-8 encoding,not
scrapy/tests/test_downloadermiddleware_redirect.py,227,meta-refresh with high intervals don't trigger redirects,not
scrapy/tests/test_utils_python.py,115,no attributes given return False,not
scrapy/tests/test_utils_python.py,117,not existent attributes,not
scrapy/tests/test_utils_python.py,122,equal attribute,not
scrapy/tests/test_utils_python.py,126,obj1 has no attribute y,not
scrapy/tests/test_utils_python.py,130,equal attributes,not
scrapy/tests/test_utils_python.py,134,differente attributes,not
scrapy/tests/test_utils_python.py,137,test callable,not
scrapy/tests/test_utils_python.py,142,compare ['meta']['a'],not
scrapy/tests/test_utils_python.py,153,fail z equality,not
scrapy/tests/test_utils_python.py,210,TODO: how do we fix this to return the actual argument names?,SATD
scrapy/tests/test_spidermiddleware_referer.py,62,no credentials leak,not
scrapy/tests/test_spidermiddleware_referer.py,65,no referrer leak for local schemes,not
scrapy/tests/test_spidermiddleware_referer.py,69,no referrer leak for s3 origins,not
scrapy/tests/test_spidermiddleware_referer.py,87,TLS to TLS: send non-empty referrer,not
scrapy/tests/test_spidermiddleware_referer.py,94,TLS to non-TLS: do not send referrer,not
scrapy/tests/test_spidermiddleware_referer.py,99,non-TLS to TLS or non-TLS: send referrer,not
scrapy/tests/test_spidermiddleware_referer.py,109,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,116,"Same origin (protocol, host, port): send referrer",not
scrapy/tests/test_spidermiddleware_referer.py,124,Different host: do NOT send referrer,not
scrapy/tests/test_spidermiddleware_referer.py,129,Different port: do NOT send referrer,not
scrapy/tests/test_spidermiddleware_referer.py,134,Different protocols: do NOT send refferer,not
scrapy/tests/test_spidermiddleware_referer.py,141,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,149,"TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)",not
scrapy/tests/test_spidermiddleware_referer.py,155,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,162,TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades,not
scrapy/tests/test_spidermiddleware_referer.py,167,downgrade: send nothing,not
scrapy/tests/test_spidermiddleware_referer.py,170,upgrade: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,173,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,181,"Same origin (protocol, host, port): send referrer",not
scrapy/tests/test_spidermiddleware_referer.py,189,Different host: send origin as referrer,not
scrapy/tests/test_spidermiddleware_referer.py,193,exact match required,not
scrapy/tests/test_spidermiddleware_referer.py,196,Different port: send origin as referrer,not
scrapy/tests/test_spidermiddleware_referer.py,200,Different protocols: send origin as referrer,not
scrapy/tests/test_spidermiddleware_referer.py,207,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,209,TLS to non-TLS downgrade: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,216,"Same origin (protocol, host, port): send referrer",not
scrapy/tests/test_spidermiddleware_referer.py,224,Different host: send origin as referrer,not
scrapy/tests/test_spidermiddleware_referer.py,228,exact match required,not
scrapy/tests/test_spidermiddleware_referer.py,231,Different port: send origin as referrer,not
scrapy/tests/test_spidermiddleware_referer.py,235,downgrade,not
scrapy/tests/test_spidermiddleware_referer.py,239,non-TLS to non-TLS,not
scrapy/tests/test_spidermiddleware_referer.py,242,upgrade,not
scrapy/tests/test_spidermiddleware_referer.py,246,Different protocols: send origin as referrer,not
scrapy/tests/test_spidermiddleware_referer.py,250,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,253,TLS to non-TLS downgrade: send nothing,not
scrapy/tests/test_spidermiddleware_referer.py,260,TLS to TLS: send referrer,not
scrapy/tests/test_spidermiddleware_referer.py,268,"TLS to non-TLS: send referrer (yes, it's unsafe)",not
scrapy/tests/test_spidermiddleware_referer.py,273,"non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)",not
scrapy/tests/test_spidermiddleware_referer.py,283,test for user/password stripping,not
scrapy/tests/test_spidermiddleware_referer.py,293,--- Tests using settings to set policy using class path,not
scrapy/tests/test_spidermiddleware_referer.py,352,--- Tests using Request meta dict to set policy,not
scrapy/tests/test_spidermiddleware_referer.py,408,When an unknown policy is referenced in Request.meta,not
scrapy/tests/test_spidermiddleware_referer.py,409,"(here, a typo error),",not
scrapy/tests/test_spidermiddleware_referer.py,410,the policy defined in settings takes precedence,not
scrapy/tests/test_spidermiddleware_referer.py,418,same as above but with string value for settings policy,not
scrapy/tests/test_spidermiddleware_referer.py,426,"request meta references a wrong policy but it is set,",not
scrapy/tests/test_spidermiddleware_referer.py,427,"so the Referrer-Policy header in response is not used,",not
scrapy/tests/test_spidermiddleware_referer.py,428,and the settings' policy is applied,not
scrapy/tests/test_spidermiddleware_referer.py,436,"here, request meta does not set the policy",not
scrapy/tests/test_spidermiddleware_referer.py,437,so response headers take precedence,not
scrapy/tests/test_spidermiddleware_referer.py,445,"here, request meta does not set the policy,",not
scrapy/tests/test_spidermiddleware_referer.py,446,"but response headers also use an unknown policy,",not
scrapy/tests/test_spidermiddleware_referer.py,447,so the settings' policy is used,not
scrapy/tests/test_spidermiddleware_referer.py,545,parent,not
scrapy/tests/test_spidermiddleware_referer.py,546,target,not
scrapy/tests/test_spidermiddleware_referer.py,548,"redirections: code, URL",not
scrapy/tests/test_spidermiddleware_referer.py,552,expected initial referer,not
scrapy/tests/test_spidermiddleware_referer.py,553,expected referer for the redirection request,not
scrapy/tests/test_spidermiddleware_referer.py,559,redirecting to non-secure URL,not
scrapy/tests/test_spidermiddleware_referer.py,569,redirecting to non-secure URL: different origin,not
scrapy/tests/test_spidermiddleware_referer.py,609,parent,not
scrapy/tests/test_spidermiddleware_referer.py,610,target,not
scrapy/tests/test_spidermiddleware_referer.py,612,"redirections: code, URL",not
scrapy/tests/test_spidermiddleware_referer.py,616,"expected initial ""Referer""",not
scrapy/tests/test_spidermiddleware_referer.py,617,"expected ""Referer"" for the redirection request",not
scrapy/tests/test_spidermiddleware_referer.py,630,different origin,not
scrapy/tests/test_spidermiddleware_referer.py,651,origin,not
scrapy/tests/test_spidermiddleware_referer.py,652,target,not
scrapy/tests/test_spidermiddleware_referer.py,654,"redirections: code, URL",not
scrapy/tests/test_spidermiddleware_referer.py,658,"expected initial ""Referer""",not
scrapy/tests/test_spidermiddleware_referer.py,659,expected referer for the redirection request,not
scrapy/tests/test_spidermiddleware_referer.py,665,redirecting from secure to non-secure URL == different origin,not
scrapy/tests/test_spidermiddleware_referer.py,675,different domain == different origin,not
scrapy/tests/test_spidermiddleware_referer.py,702,send origin,not
scrapy/tests/test_spidermiddleware_referer.py,703,redirects to same origin: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,709,redirecting to non-secure URL: no referrer,not
scrapy/tests/test_spidermiddleware_referer.py,719,redirecting to non-secure URL (different domain): no referrer,not
scrapy/tests/test_spidermiddleware_referer.py,738,"HTTPS all along, so origin referrer is kept as-is",not
scrapy/tests/test_spidermiddleware_referer.py,747,TLS to non-TLS: no referrer,not
scrapy/tests/test_spidermiddleware_referer.py,749,TLS URL again: (still) no referrer,not
scrapy/tests/test_spidermiddleware_referer.py,769,origin,not
scrapy/tests/test_spidermiddleware_referer.py,770,target + redirection,not
scrapy/tests/test_spidermiddleware_referer.py,772,"redirections: code, URL",not
scrapy/tests/test_spidermiddleware_referer.py,776,expected initial referer,not
scrapy/tests/test_spidermiddleware_referer.py,777,expected referer for the redirection request,not
scrapy/tests/test_spidermiddleware_referer.py,783,redirecting to non-secure URL: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,793,redirecting to non-secure URL (different domain): send origin,not
scrapy/tests/test_spidermiddleware_referer.py,812,all different domains: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,821,TLS to non-TLS: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,823,TLS URL again: send origin (also),not
scrapy/tests/test_spidermiddleware_referer.py,845,origin,not
scrapy/tests/test_spidermiddleware_referer.py,846,target + redirection,not
scrapy/tests/test_spidermiddleware_referer.py,848,"redirections: code, URL",not
scrapy/tests/test_spidermiddleware_referer.py,852,expected initial referer,not
scrapy/tests/test_spidermiddleware_referer.py,853,expected referer for the redirection request,not
scrapy/tests/test_spidermiddleware_referer.py,859,"redirecting to non-secure URL: do not send the ""Referer"" header",not
scrapy/tests/test_spidermiddleware_referer.py,869,redirecting to non-secure URL (different domain): send origin,not
scrapy/tests/test_spidermiddleware_referer.py,888,all different domains: send origin,not
scrapy/tests/test_spidermiddleware_referer.py,897,"TLS to non-TLS: do not send ""Referer""",not
scrapy/tests/test_spidermiddleware_referer.py,899,TLS URL again: (still) send nothing,not
scrapy/tests/test_extension_telnet.py,17,This function has some side effects we don't need for this test,not
scrapy/tests/test_utils_request.py,19,make sure caching is working,not
scrapy/tests/test_utils_request.py,59,cached fingerprint must be cleared on request copy,not
scrapy/tests/test_utils_request.py,82,the representation is not important but it must not fail.,not
scrapy/tests/test_utils_log.py,69,disable it to avoid the extra warning,not
scrapy/tests/test_pipeline_crawl.py,63,prepare a directory for storing files,not
scrapy/tests/test_pipeline_crawl.py,90,check that logs show the expected number of successful file downloads,not
scrapy/tests/test_pipeline_crawl.py,94,check that the images/files checksums are what we know they should be,not
scrapy/tests/test_pipeline_crawl.py,103,check that the image files where actually written to the media store,not
scrapy/tests/test_pipeline_crawl.py,112,"check that the item does NOT have the ""images/files"" field populated",not
scrapy/tests/test_pipeline_crawl.py,117,check that there was 1 successful fetch and 3 other responses with non-200 code,not
scrapy/tests/test_pipeline_crawl.py,123,check that logs do show the failure on the file downloads,not
scrapy/tests/test_pipeline_crawl.py,127,check that no files were written to the media store,not
scrapy/tests/test_pipeline_crawl.py,180,somehow checksums for images are different for Python 3.3,not
scrapy/tests/test_scheduler.py,267,pylint: disable=protected-access,not
scrapy/tests/test_scheduler.py,274,pylint: disable=protected-access,not
scrapy/tests/test_utils_deprecate.py,109,ignore subclassing warnings,not
scrapy/tests/test_utils_deprecate.py,118,subclass instances don't warn,not
scrapy/tests/test_loader.py,12,test items,not
scrapy/tests/test_loader.py,31,test item loaders,not
scrapy/tests/test_loader.py,48,test processors,not
scrapy/tests/test_loader.py,75,Let's assume a SKU is only digits.,not
scrapy/tests/test_loader.py,80,take first which allows empty values,not
scrapy/tests/test_loader.py,89,"Should not return ""sku: None"".",not
scrapy/tests/test_loader.py,91,Should not ignore empty values.,not
scrapy/tests/test_loader.py,132,test add object value,not
scrapy/tests/test_loader.py,774,combining/accumulating CSS selectors and XPath expressions,not
scrapy/tests/test_loader.py,995,Functions as processors,not
scrapy/tests/test_utils_iterators.py,54,example taken from https://github.com/scrapy/scrapy/issues/1665,SATD
scrapy/tests/test_utils_iterators.py,92,with bytes,not
scrapy/tests/test_utils_iterators.py,94,Unicode body needs encoding information,not
scrapy/tests/test_utils_iterators.py,257,explicit type check cuz' we no like stinkin' autocasting! yarrr,not
scrapy/tests/test_downloadermiddleware_httpcompression.py,73,noqa: F401,not
scrapy/tests/test_downloadermiddleware_httpcompression.py,210,"build a gzipped file (here, a sitemap)",not
scrapy/tests/test_downloadermiddleware_httpcompression.py,231,build a gzipped response body containing this gzipped file,not
scrapy/tests/test_crawler.py,101,disable telnet if not available to avoid an extra warning,not
scrapy/tests/test_item.py,168,D class inverted,not
scrapy/tests/test_item.py,199,D class inverted,not
scrapy/tests/test_item.py,229,D class inverted,not
scrapy/tests/test_item.py,286,This attribute is an internal attribute in Python 3.6+,not
scrapy/tests/test_item.py,287,and must be propagated properly. See,not
scrapy/tests/test_item.py,288,https://docs.python.org/3.6/reference/datamodel.html#creating-the-class-object,not
scrapy/tests/test_item.py,289,"In <3.6, we add a dummy attribute just to ensure the",not
scrapy/tests/test_item.py,290,__new__ method propagates it correctly.,not
scrapy/tests/test_item.py,294,For rationale of this see:,not
scrapy/tests/test_item.py,295,https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222,not
scrapy/tests/test_item.py,296,noqa  https://github.com/scrapy/scrapy/issues/2836,SATD
scrapy/tests/test_item.py,313,This call to super() trigger the __classcell__ propagation,not
scrapy/tests/test_item.py,314,requirement. When not done properly raises an error:,not
scrapy/tests/test_item.py,315,TypeError: __class__ set to <class '__main__.MyItem'>,not
scrapy/tests/test_item.py,316,defining 'MyItem' as <class '__main__.MyItem'>,not
scrapy/tests/test_responsetypes.py,75,TODO: add more tests that check precedence between the different arguments,SATD
scrapy/tests/test_responsetypes.py,78,headers takes precedence over url,not
scrapy/tests/test_responsetypes.py,89,check that mime.types files shipped with scrapy are loaded,SATD
scrapy/tests/test_spider.py,434,.xml.gz but body decoded by HttpCompression middleware already,not
scrapy/tests/test_spider.py,599,spider without overridden make_requests_from_url method,not
scrapy/tests/test_spider.py,600,doesn't issue a warning,not
scrapy/tests/test_spider.py,605,spider without overridden make_requests_from_url method,not
scrapy/tests/test_spider.py,606,should issue a warning when called directly,not
scrapy/tests/test_spider.py,611,"spider with overridden make_requests_from_url issues a warning,",not
scrapy/tests/test_spider.py,612,but the method still works,not
scrapy/tests/test_robotstxt_interface.py,1,coding=utf-8,not
scrapy/tests/test_robotstxt_interface.py,6,check if reppy parser is installed,not
scrapy/tests/test_robotstxt_interface.py,8,noqa: F401,not
scrapy/tests/test_robotstxt_interface.py,15,check if robotexclusionrulesparser is installed,not
scrapy/tests/test_robotstxt_interface.py,17,noqa: F401,not
scrapy/tests/test_robotstxt_interface.py,24,check if protego parser is installed,not
scrapy/tests/test_robotstxt_interface.py,26,noqa: F401,not
scrapy/tests/test_mail.py,1,coding=utf-8,not
scrapy/tests/test_http_request.py,21,Request requires url in the __init__ method,not
scrapy/tests/test_http_request.py,24,url argument must be basestring,not
scrapy/tests/test_http_request.py,52,Different ways of setting headers attribute,not
scrapy/tests/test_http_request.py,62,headers must not be unicode,not
scrapy/tests/test_http_request.py,96,"encoding affects only query part of URI, not path",not
scrapy/tests/test_http_request.py,97,path part should always be UTF-8 encoded before percent-escaping,not
scrapy/tests/test_http_request.py,108,should be same as above,not
scrapy/tests/test_http_request.py,113,encoding is used for encoding query-string before percent-escaping;,not
scrapy/tests/test_http_request.py,114,path is still UTF-8 encoded before percent-escaping,not
scrapy/tests/test_http_request.py,119,percent-escaping sequences that do not match valid UTF-8 sequences,not
scrapy/tests/test_http_request.py,120,should be kept untouched (just upper-cased perhaps),not
scrapy/tests/test_http_request.py,121,,not
scrapy/tests/test_http_request.py,122,See https://tools.ietf.org/html/rfc3987#section-3.2,not
scrapy/tests/test_http_request.py,123,,not
scrapy/tests/test_http_request.py,124,"""Conversions from URIs to IRIs MUST NOT use any character encoding",not
scrapy/tests/test_http_request.py,125,"other than UTF-8 in steps 3 and 4, even if it might be possible to",not
scrapy/tests/test_http_request.py,126,guess from the context that another character encoding than UTF-8 was,not
scrapy/tests/test_http_request.py,127,"used in the URI.  For example, the URI",not
scrapy/tests/test_http_request.py,128,"""http://www.example.org/r%E9sum%E9.html"" might with some guessing be",not
scrapy/tests/test_http_request.py,129,interpreted to contain two e-acute characters encoded as iso-8859-1.,not
scrapy/tests/test_http_request.py,130,It must not be converted to an IRI containing these e-acute,not
scrapy/tests/test_http_request.py,131,"characters.  Otherwise, in the future the IRI will be mapped to",not
scrapy/tests/test_http_request.py,132,"""http://www.example.org/r%C3%A9sum%C3%A9.html"", which is a different",not
scrapy/tests/test_http_request.py,133,"URI from ""http://www.example.org/r%E9sum%E9.html"".",not
scrapy/tests/test_http_request.py,152,default encoding,not
scrapy/tests/test_http_request.py,163,ascii url,not
scrapy/tests/test_http_request.py,166,unicode url,not
scrapy/tests/test_http_request.py,182,make sure copy does not propagate callbacks,not
scrapy/tests/test_http_request.py,188,make sure flags list is shallow copied,not
scrapy/tests/test_http_request.py,192,make sure cb_kwargs dict is shallow copied,not
scrapy/tests/test_http_request.py,196,make sure meta dict is shallow copied,not
scrapy/tests/test_http_request.py,200,make sure headers attribute is shallow copied,not
scrapy/tests/test_http_request.py,206,Request.body can be identical since it's an immutable object (str),not
scrapy/tests/test_http_request.py,230,Empty attributes (which may fail if not compared properly),not
scrapy/tests/test_http_request.py,284,Note: more curated tests regarding curl conversion are in,not
scrapy/tests/test_http_request.py,285,`test_utils_curl.py`,not
scrapy/tests/test_http_request.py,341,By default: it works and ignores the unknown options: --foo and -z,not
scrapy/tests/test_http_request.py,342,avoid warning when executing tests,not
scrapy/tests/test_http_request.py,349,If `ignore_unknon_options` is set to `False` it raises an error with,not
scrapy/tests/test_http_request.py,350,the unknown options: --foo and -z,not
scrapy/tests/test_http_request.py,374,using default encoding (utf-8),not
scrapy/tests/test_http_request.py,383,using default encoding (utf-8),not
scrapy/tests/test_http_request.py,392,using default encoding (utf-8),not
scrapy/tests/test_http_request.py,415,using multiples values for a single key,not
scrapy/tests/test_http_request.py,617,xpath in _get_inputs(),not
scrapy/tests/test_http_request.py,618,xpath in _get_clickable(),not
scrapy/tests/test_http_request.py,1287,empty data,not
scrapy/tests/test_http_request.py,1292,data is not passed,not
scrapy/tests/test_http_request.py,1306,method passed explicitly,not
scrapy/tests/test_downloadermiddleware_useragent.py,23,settings UESR_AGENT to None should remove the user agent,not
scrapy/tests/test_webclient.py,15,deprecated in Twisted 19.7.0,not
scrapy/tests/test_webclient.py,16,(remove once we bump our requirement past that version),not
scrapy/tests/test_webclient.py,97,basic test stolen from twisted HTTPageGetter,not
scrapy/tests/test_webclient.py,119,test minimal sent headers,not
scrapy/tests/test_webclient.py,126,test a simple POST with body and content-type,not
scrapy/tests/test_webclient.py,142,test a POST method with no body provided,not
scrapy/tests/test_webclient.py,154,test with single and multivalued headers,not
scrapy/tests/test_webclient.py,171,same test with single and multivalued headers but using Headers class,not
scrapy/tests/test_webclient.py,199,regression test for: http://dev.scrapy.org/ticket/258,SATD
scrapy/tests/test_webclient.py,258,"if we pass Host header explicitly, it should be used, otherwise",not
scrapy/tests/test_webclient.py,259,it should extract from url,not
scrapy/tests/test_webclient.py,309,Clean up the server which is hanging around not doing,SATD
scrapy/tests/test_webclient.py,310,anything.,not
scrapy/tests/test_webclient.py,312,There might be nothing here if the server managed to already see,not
scrapy/tests/test_webclient.py,313,that the connection was lost.,not
scrapy/tests/test_webclient.py,398,we try to use a cipher that is not enabled by default in OpenSSL,not
scrapy/tests/test_utils_defer.py,18,it is [1] with maybeDeferred,not
scrapy/tests/test_utils_defer.py,19,"add another value, that should be catched by assertEqual",not
scrapy/tests/test_utils_defer.py,32,it is [1] with maybeDeferred,not
scrapy/tests/test_utils_defer.py,33,"add another value, that should be catched by assertEqual",not
scrapy/tests/test_squeues.py,33,Selectors should fail (lxml.html.HtmlElement objects can't be pickled),not
scrapy/tests/test_downloadermiddleware_httpproxy.py,67,proxy from request.meta,not
scrapy/tests/test_downloadermiddleware_httpproxy.py,80,proxy from request.meta,not
scrapy/tests/test_downloadermiddleware_httpproxy.py,87,utf-8 encoding,not
scrapy/tests/test_downloadermiddleware_httpproxy.py,95,proxy from request.meta,not
scrapy/tests/test_downloadermiddleware_httpproxy.py,101,default latin-1 encoding,not
scrapy/tests/test_downloadermiddleware_httpproxy.py,108,"proxy from request.meta, latin-1 encoding",not
scrapy/tests/test_downloadermiddleware_httpproxy.py,140,proxy from meta['proxy'] takes precedence,not
scrapy/tests/mockserver.py,41,silence CancelledError,not
scrapy/tests/mockserver.py,60,"order == ""desc""",not
scrapy/tests/mockserver.py,85,send headers now and delay body,not
scrapy/tests/mockserver.py,136,"we force the body content, otherwise Twisted redirectTo()",not
scrapy/tests/mockserver.py,137,"returns HTML with <meta http-equiv=""refresh""",not
scrapy/tests/mockserver.py,268,disabling TLS1.2+ because it unconditionally enables some strong ciphers,not
scrapy/tests/test_utils_template.py,30,Failure of test itself,not
scrapy/tests/test_utils_template.py,39,Failure of test iself,not
scrapy/tests/test_logformatter.py,67,"In practice, the complete traceback is shown by passing the",not
scrapy/tests/test_logformatter.py,68,'exc_info' argument to the logging function,not
scrapy/tests/test_logformatter.py,77,"In practice, the complete traceback is shown by passing the",not
scrapy/tests/test_logformatter.py,78,'exc_info' argument to the logging function,not
scrapy/tests/test_logformatter.py,90,"In practice, the complete traceback is shown by passing the",not
scrapy/tests/test_logformatter.py,91,'exc_info' argument to the logging function,not
scrapy/tests/test_logformatter.py,99,"In practice, the complete traceback is shown by passing the",not
scrapy/tests/test_logformatter.py,100,'exc_info' argument to the logging function,not
scrapy/tests/test_downloadermiddleware_robotstxt.py,90,"garbage response should be discarded, equal 'allow all'",not
scrapy/tests/test_downloadermiddleware_robotstxt.py,113,empty response should equal 'allow all',not
scrapy/tests/test_downloadermiddleware_robotstxt.py,176,not actually used,not
scrapy/tests/test_downloadermiddleware_robotstxt.py,182,not actually used,not
scrapy/tests/test_proxy_connect.py,97,The proxy returns a 407 error code but it does not reach the client;,not
scrapy/tests/test_proxy_connect.py,98,he just sees a TunnelError.,not
scrapy/tests/test_pipeline_media.py,62,Check that failures are logged by default,not
scrapy/tests/test_pipeline_media.py,75,disable failure logging and check again,not
scrapy/tests/test_pipeline_media.py,120,Create sample pair of Request and Response objects,not
scrapy/tests/test_pipeline_media.py,124,Simulate the Media Pipeline behavior to produce a Twisted Failure,not
scrapy/tests/test_pipeline_media.py,126,Simulate a Twisted inline callback returning a Response,not
scrapy/tests/test_pipeline_media.py,131,Simulate the media_downloaded callback raising a FileException,not
scrapy/tests/test_pipeline_media.py,132,This usually happens when the status code is not 200 OK,not
scrapy/tests/test_pipeline_media.py,136,Simulate Twisted capturing the FileException,not
scrapy/tests/test_pipeline_media.py,137,It encapsulates the exception inside a Twisted Failure,not
scrapy/tests/test_pipeline_media.py,140,The Failure should encapsulate a FileException ...,not
scrapy/tests/test_pipeline_media.py,142,... and it should have the StopIteration exception set as its context,not
scrapy/tests/test_pipeline_media.py,145,Let's calculate the request fingerprint and fake some runtime data...,not
scrapy/tests/test_pipeline_media.py,151,When calling the method that caches the Request's result ...,not
scrapy/tests/test_pipeline_media.py,153,... it should store the Twisted Failure ...,not
scrapy/tests/test_pipeline_media.py,155,... encapsulating the original FileException ...,not
scrapy/tests/test_pipeline_media.py,157,... but it should not store the StopIteration exception on its context,not
scrapy/tests/test_pipeline_media.py,245,only once,not
scrapy/tests/test_pipeline_media.py,246,first hook called,not
scrapy/tests/test_pipeline_media.py,249,last hook called,not
scrapy/tests/test_pipeline_media.py,250,"twice, one per request",not
scrapy/tests/test_pipeline_media.py,252,one to handle success and other for failure,not
scrapy/tests/test_pipeline_media.py,258,returns single Request (without callback),not
scrapy/tests/test_pipeline_media.py,260,pass a single item,not
scrapy/tests/test_pipeline_media.py,265,returns iterable of Requests,not
scrapy/tests/test_pipeline_media.py,283,"rsp2 is ignored, rsp1 must be in results because request fingerprints are the same",not
scrapy/tests/test_pipeline_media.py,349,These are the status codes we want,not
scrapy/tests/test_pipeline_media.py,350,the downloader to handle itself,not
scrapy/tests/test_pipeline_media.py,357,we still want to get 4xx and 5xx,not
scrapy/tests/test_utils_trackref.py,22,NOQA,not
scrapy/tests/test_utils_trackref.py,23,NOQA,not
scrapy/tests/test_utils_trackref.py,24,NOQA,not
scrapy/tests/test_utils_trackref.py,49,NOQA,not
scrapy/tests/test_utils_trackref.py,57,NOQA,not
scrapy/tests/test_utils_trackref.py,58,NOQA,not
scrapy/tests/test_utils_trackref.py,59,NOQA,not
scrapy/tests/test_utils_trackref.py,65,NOQA,not
scrapy/tests/test_utils_trackref.py,66,NOQA,not
scrapy/tests/test_utils_trackref.py,67,NOQA,not
scrapy/tests/test_request_cb_kwargs.py,154,check exceptions for argument mismatch,not
scrapy/tests/test_utils_conf.py,57,Higher priority takes precedence,not
scrapy/tests/test_utils_conf.py,67,Same priority raises ValueError,not
scrapy/tests/test_utils_conf.py,73,work well with None and numeric values,not
scrapy/tests/test_utils_conf.py,80,raise exception for invalid values,not
scrapy/tests/test_utils_console.py,39,default shell should be 'ipython',not
scrapy/tests/test_downloadermiddleware.py,26,some mw depends on stats collector,not
scrapy/tests/test_downloadermiddleware.py,46,catch deferred result and return the value,not
scrapy/tests/test_pipeline_files.py,245,Values from settings for custom pipeline should be set on pipeline instance.,not
scrapy/tests/test_pipeline_files.py,261,Values from settings for custom pipeline should be set on pipeline instance.,not
scrapy/tests/test_contracts.py,232,extract contracts correctly,not
scrapy/tests/test_contracts.py,238,returns request for valid method,not
scrapy/tests/test_contracts.py,242,no request for missing url,not
scrapy/tests/test_contracts.py,250,extract contracts correctly,not
scrapy/tests/test_contracts.py,271,returns_request,not
scrapy/tests/test_contracts.py,276,returns_item,not
scrapy/tests/test_contracts.py,281,"returns_item (error, callback doesn't take keyword arguments)",not
scrapy/tests/test_contracts.py,286,"returns_item (error, contract doesn't provide keyword arguments)",not
scrapy/tests/test_contracts.py,295,returns_item,not
scrapy/tests/test_contracts.py,300,returns_dict_item,not
scrapy/tests/test_contracts.py,305,returns_request,not
scrapy/tests/test_contracts.py,310,returns_fail,not
scrapy/tests/test_contracts.py,315,returns_dict_fail,not
scrapy/tests/test_contracts.py,324,scrapes_item_ok,SATD
scrapy/tests/test_contracts.py,329,scrapes_dict_item_ok,SATD
scrapy/tests/test_contracts.py,334,scrapes_item_fail,SATD
scrapy/tests/test_contracts.py,339,scrapes_dict_item_fail,SATD
scrapy/tests/test_contracts.py,344,scrapes_multiple_missing_fields,SATD
scrapy/tests/spiders.py,243,no need to retry,not
scrapy/tests/test_utils_datatypes.py,227,"supplied sequence is a set, so checking for list (non)inclusion fails",not
scrapy/tests/test_utils_datatypes.py,278,PyPy takes longer to collect dead references,not
scrapy/tests/test_utils_datatypes.py,307,delete reference to the last object in the list,not
scrapy/tests/test_utils_datatypes.py,309,"delete half of the objects, make sure that is reflected in the cache",not
scrapy/tests/test_utils_datatypes.py,313,PyPy takes longer to collect dead references,not
scrapy/tests/test_downloadermiddleware_httpcache.py,112,content-type header,not
scrapy/tests/test_downloadermiddleware_httpcache.py,115,wait for cache to expire,not
scrapy/tests/test_downloadermiddleware_httpcache.py,122,give the chance to expire,not
scrapy/tests/test_downloadermiddleware_httpcache.py,140,make sure our dbm module has been loaded,not
scrapy/tests/test_downloadermiddleware_httpcache.py,191,http responses are cached by default,not
scrapy/tests/test_downloadermiddleware_httpcache.py,202,file response is not cached by default,not
scrapy/tests/test_downloadermiddleware_httpcache.py,211,s3 scheme response is cached by default,not
scrapy/tests/test_downloadermiddleware_httpcache.py,222,ignore s3 scheme,not
scrapy/tests/test_downloadermiddleware_httpcache.py,232,test response is not cached,not
scrapy/tests/test_downloadermiddleware_httpcache.py,240,test response is cached,not
scrapy/tests/test_downloadermiddleware_httpcache.py,277,response for a request with no-store must not be cached,not
scrapy/tests/test_downloadermiddleware_httpcache.py,281,Re-do request without no-store and expect it to be cached,not
scrapy/tests/test_downloadermiddleware_httpcache.py,287,request with no-cache directive must not return cached response,not
scrapy/tests/test_downloadermiddleware_httpcache.py,288,but it allows new response to be stored,not
scrapy/tests/test_downloadermiddleware_httpcache.py,299,304 is not cacheable no matter what servers sends,not
scrapy/tests/test_downloadermiddleware_httpcache.py,305,Always obey no-store cache control,not
scrapy/tests/test_downloadermiddleware_httpcache.py,307,invalid,not
scrapy/tests/test_downloadermiddleware_httpcache.py,308,invalid,not
scrapy/tests/test_downloadermiddleware_httpcache.py,309,Ignore responses missing expiration and/or validation headers,not
scrapy/tests/test_downloadermiddleware_httpcache.py,314,Cache responses with expiration and/or validation headers,not
scrapy/tests/test_downloadermiddleware_httpcache.py,342,cache unconditionally unless response contains no-store or is a 304,not
scrapy/tests/test_downloadermiddleware_httpcache.py,366,Obey max-age if present over any others,not
scrapy/tests/test_downloadermiddleware_httpcache.py,373,obey Expires if max-age is not present,not
scrapy/tests/test_downloadermiddleware_httpcache.py,380,Default missing Date header to right now,not
scrapy/tests/test_downloadermiddleware_httpcache.py,382,Firefox - Expires if age is greater than 10% of (Date - Last-Modified),not
scrapy/tests/test_downloadermiddleware_httpcache.py,384,Firefox - Set one year maxage to permanent redirects missing expiration info,not
scrapy/tests/test_downloadermiddleware_httpcache.py,391,cache fresh response,not
scrapy/tests/test_downloadermiddleware_httpcache.py,395,return fresh cached response without network interaction,not
scrapy/tests/test_downloadermiddleware_httpcache.py,399,validate cached response if request max-age set as 0,not
scrapy/tests/test_downloadermiddleware_httpcache.py,416,"no-cache forces expiration, also revalidation if validators exists",not
scrapy/tests/test_downloadermiddleware_httpcache.py,428,cache expired response,not
scrapy/tests/test_downloadermiddleware_httpcache.py,432,Same request but as cached response is stale a new response must,not
scrapy/tests/test_downloadermiddleware_httpcache.py,433,be returned,not
scrapy/tests/test_downloadermiddleware_httpcache.py,439,"Previous response expired too, subsequent request to same",not
scrapy/tests/test_downloadermiddleware_httpcache.py,440,resource must revalidate and succeed on 304 if validators,not
scrapy/tests/test_downloadermiddleware_httpcache.py,441,are present,not
scrapy/tests/test_downloadermiddleware_httpcache.py,447,get cached response on server errors unless must-revalidate,not
scrapy/tests/test_downloadermiddleware_httpcache.py,448,in cached response,not
scrapy/tests/test_downloadermiddleware_httpcache.py,457,Requests with max-stale can fetch expired cached responses,not
scrapy/tests/test_downloadermiddleware_httpcache.py,458,unless cached response has must-revalidate,not
scrapy/tests/test_downloadermiddleware_httpcache.py,473,Simulate encountering an error on download attempts,not
scrapy/tests/test_downloadermiddleware_httpcache.py,476,Use cached response as recovery,not
scrapy/tests/test_downloadermiddleware_httpcache.py,479,Do not use cached response for unhandled exceptions,not
scrapy/tests/test_downloadermiddleware_httpcache.py,495,cache fresh response,not
scrapy/tests/test_downloadermiddleware_httpcache.py,499,return fresh cached response without network interaction,not
scrapy/tests/test_crawl.py,39,10 + start_url,not
scrapy/tests/test_crawl.py,68,Ensure that the same test parameters would cause a failure if no,not
scrapy/tests/test_crawl.py,69,"download delay is set. Otherwise, it means we are using a combination",not
scrapy/tests/test_crawl.py,70,of ``total`` and ``delay`` values that are too small for the test,not
scrapy/tests/test_crawl.py,71,code above to have any meaning.,not
scrapy/tests/test_crawl.py,97,server hangs after receiving response headers,not
scrapy/tests/test_crawl.py,122,try to fetch the homepage of a non-existent domain,not
scrapy/tests/test_crawl.py,169,Completeness of responses without Content-Length or Transfer-Encoding,not
scrapy/tests/test_crawl.py,170,"can not be determined, we treat them as valid but flagged as ""partial""",not
scrapy/tests/test_crawl.py,196,connection lost after receiving data,not
scrapy/tests/test_crawl.py,204,connection lost before receiving data,not
scrapy/tests/test_crawl.py,226,basic asserts in case of weird communication errors,not
scrapy/tests/test_crawl.py,229,start requests doesn't set Referer header,not
scrapy/tests/test_crawl.py,232,following request sets Referer to start request url,not
scrapy/tests/test_crawl.py,235,next request avoids Referer header,not
scrapy/tests/test_crawl.py,238,last request explicitly sets a Referer header,not
scrapy/tests/test_crawl.py,400,some random items,not
scrapy/tests/test_engine.py,73,no dont_filter=True,not
scrapy/tests/test_engine.py,125,duplicate,not
scrapy/tests/test_engine.py,245,response tests,not
scrapy/tests/test_engine.py,316,signal was fired multiple times,not
scrapy/tests/test_engine.py,318,bytes were received in order,not
scrapy/tests/test_pipeline_images.py,79,straigh forward case: RGB and JPEG,not
scrapy/tests/test_pipeline_images.py,86,check that thumbnail keep image ratio,not
scrapy/tests/test_pipeline_images.py,91,transparency case: RGBA and PNG,not
scrapy/tests/test_pipeline_images.py,98,transparency case with palette: P and PNG,not
scrapy/tests/test_pipeline_images.py,161,Pipeline attribute names with corresponding setting names.,not
scrapy/tests/test_pipeline_images.py,170,This should match what is defined in ImagesPipeline.,not
scrapy/tests/test_pipeline_images.py,217,Values should be in different range than fake_settings.,not
scrapy/tests/test_pipeline_images.py,254,Instance attribute (lowercase) must be equal to class attribute (uppercase).,not
scrapy/tests/test_pipeline_images.py,268,Instance attribute (lowercase) must be equal to,not
scrapy/tests/test_pipeline_images.py,269,value defined in settings.,not
scrapy/tests/test_pipeline_images.py,285,Values from settings for custom pipeline should be set on pipeline instance.,not
scrapy/tests/test_pipeline_images.py,301,Values from settings for custom pipeline should be set on pipeline instance.,not
scrapy/tests/test_selector.py,55,u'\xa3'     = pound symbol in unicode,not
scrapy/tests/test_selector.py,56,u'\xc2\xa3' = pound symbol in utf-8,not
scrapy/tests/test_selector.py,57,u'\xa3'     = pound symbol in latin-1 (iso-8859-1),not
scrapy/tests/test_selector.py,73,\xe9 alone isn't valid utf8 sequence,not
scrapy/tests/test_utils_curl.py,186,case 1: ignore_unknown_options=True:,not
scrapy/tests/test_utils_curl.py,187,avoid warning when executing tests,not
scrapy/tests/test_utils_curl.py,194,case 2: ignore_unknown_options=False (raise exception):,not
scrapy/tests/test_feedexport.py,104,RFC3986: 3.2.1. User Information,not
scrapy/tests/test_feedexport.py,119,"again, to check s3 objects are overwritten",not
scrapy/tests/test_feedexport.py,170,noqa: F401,not
scrapy/tests/test_feedexport.py,176,Instantiate with crawler,not
scrapy/tests/test_feedexport.py,181,Instantiate directly,not
scrapy/tests/test_feedexport.py,187,URI priority > settings priority,not
scrapy/tests/test_feedexport.py,193,Backward compatibility for initialising without settings,not
scrapy/tests/test_feedexport.py,271,noqa: F401,not
scrapy/tests/test_feedexport.py,291,noqa: F401,not
scrapy/tests/test_feedexport.py,524,XML,not
scrapy/tests/test_feedexport.py,528,JSON,not
scrapy/tests/test_feedexport.py,583,feed exporters use field names from Item,not
scrapy/tests/test_feedexport.py,640,"by default, Scrapy uses fields of the first Item for CSV and",SATD
scrapy/tests/test_feedexport.py,641,all fields for JSON Lines,not
scrapy/tests/test_feedexport.py,653,edge case: FEED_EXPORT_FIELDS==[] means the same as default None,not
scrapy/tests/test_feedexport.py,658,it is possible to override fields using FEED_EXPORT_FIELDS,not
scrapy/tests/test_feedexport.py,672,"When dicts are used, only keys from the first row are used as",not
scrapy/tests/test_feedexport.py,673,"a header for CSV, and all fields are used for JSON Lines.",not
scrapy/tests/test_feedexport.py,688,FEED_EXPORT_FIELDS option allows to order export fields,not
scrapy/tests/test_feedexport.py,689,"and to select a subset of fields to export, both for Items and dicts.",not
scrapy/tests/test_feedexport.py,697,export all columns,not
scrapy/tests/test_feedexport.py,706,export a subset of columns,not
scrapy/tests/test_feedexport.py,800,JSON,not
scrapy/tests/test_feedexport.py,865,XML,not
scrapy/tests/test_downloadermiddleware_retry.py,37,dont retry 404s,not
scrapy/tests/test_downloadermiddleware_retry.py,44,first retry,not
scrapy/tests/test_downloadermiddleware_retry.py,48,Test retry when dont_retry set to False,not
scrapy/tests/test_downloadermiddleware_retry.py,52,first retry,not
scrapy/tests/test_downloadermiddleware_retry.py,66,first retry,not
scrapy/tests/test_downloadermiddleware_retry.py,71,second retry,not
scrapy/tests/test_downloadermiddleware_retry.py,76,discard it,not
scrapy/tests/test_downloadermiddleware_retry.py,98,first retry,not
scrapy/tests/test_downloadermiddleware_retry.py,103,second retry,not
scrapy/tests/test_downloadermiddleware_retry.py,108,discard it,not
scrapy/tests/test_downloadermiddleware_retry.py,123,SETTINGS: RETRY_TIMES = 0,not
scrapy/tests/test_downloadermiddleware_retry.py,131,SETTINGS: meta(max_retry_times) = 0,not
scrapy/tests/test_downloadermiddleware_retry.py,139,SETTINGS: RETRY_TIMES is NON-ZERO,not
scrapy/tests/test_downloadermiddleware_retry.py,147,SETINGS: RETRY_TIMES < meta(max_retry_times),not
scrapy/tests/test_downloadermiddleware_retry.py,159,SETINGS: RETRY_TIMES > meta(max_retry_times),not
scrapy/tests/test_downloadermiddleware_retry.py,171,SETTINGS: meta(max_retry_times) = 4,not
scrapy/tests/test_downloadermiddleware_retry.py,186,discard it,not
scrapy/tests/__init__.py,9,ignore system-wide proxies for tests,not
scrapy/tests/__init__.py,10,which would send requests to a totally unsuspecting server,not
scrapy/tests/__init__.py,11,(e.g. because urllib does not fully understand the proxy spec),not
scrapy/tests/__init__.py,16,Absolutize paths to coverage config and output file because tests that,not
scrapy/tests/__init__.py,17,spawn subprocesses also changes current working directory.,not
scrapy/tests/test_spidermiddleware_output_chain.py,17,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,18,(0) recover from an exception on a spider callback,not
scrapy/tests/test_spidermiddleware_output_chain.py,46,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,47,(1) exceptions from a spider middleware's process_spider_input method,not
scrapy/tests/test_spidermiddleware_output_chain.py,58,spider,not
scrapy/tests/test_spidermiddleware_output_chain.py,62,engine,not
scrapy/tests/test_spidermiddleware_output_chain.py,84,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,85,(2) exceptions from a spider callback (generator),not
scrapy/tests/test_spidermiddleware_output_chain.py,103,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,104,"(2.1) exceptions from a spider callback (generator, middleware right after callback)",not
scrapy/tests/test_spidermiddleware_output_chain.py,114,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,115,(3) exceptions from a spider callback (not a generator),not
scrapy/tests/test_spidermiddleware_output_chain.py,131,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,132,"(3.1) exceptions from a spider callback (not a generator, middleware right after callback)",not
scrapy/tests/test_spidermiddleware_output_chain.py,142,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,143,(4) exceptions from a middleware process_spider_output method (generator),not
scrapy/tests/test_spidermiddleware_output_chain.py,208,================================================================================,not
scrapy/tests/test_spidermiddleware_output_chain.py,209,(5) exceptions from a middleware process_spider_output method (not generator),not
scrapy/tests/test_spidermiddleware_output_chain.py,279,================================================================================,not
scrapy/tests/test_downloader_handlers.py,46,Default is lazy for backward compatibility,not
scrapy/tests/test_downloader_handlers.py,84,force load handlers,not
scrapy/tests/test_downloader_handlers.py,95,force load lazy handler,not
scrapy/tests/test_downloader_handlers.py,155,Disable terminating chunk on finish.,not
scrapy/tests/test_downloader_handlers.py,175,We have to force a disconnection for HTTP/1.1 clients. Otherwise,not
scrapy/tests/test_downloader_handlers.py,176,client keeps the connection open waiting for more data.,not
scrapy/tests/test_downloader_handlers.py,177,twisted >=16.3.0,not
scrapy/tests/test_downloader_handlers.py,209,only used for HTTPS tests,not
scrapy/tests/test_downloader_handlers.py,284,client connects but no data is received,not
scrapy/tests/test_downloader_handlers.py,293,"client connects, server send headers and some body bytes but hangs",not
scrapy/tests/test_downloader_handlers.py,351,PayloadResource requires body length to be 100,not
scrapy/tests/test_downloader_handlers.py,397,10 is minimal size for this request and the limit is only counted on,not
scrapy/tests/test_downloader_handlers.py,398,response body. (regardless of headers),not
scrapy/tests/test_downloader_handlers.py,418,"As the error message is logged in the dataReceived callback, we",not
scrapy/tests/test_downloader_handlers.py,419,have to give a bit of time to the reactor to process the queue,not
scrapy/tests/test_downloader_handlers.py,420,after closing the connection.,not
scrapy/tests/test_downloader_handlers.py,517,"above tests use a server certificate for ""localhost"",",not
scrapy/tests/test_downloader_handlers.py,518,"client connection to ""localhost"" too.",not
scrapy/tests/test_downloader_handlers.py,519,"here we test that even if the server certificate is for another domain,",not
scrapy/tests/test_downloader_handlers.py,520,"""www.example.com"" in this case,",not
scrapy/tests/test_downloader_handlers.py,521,the tests still pass,not
scrapy/tests/test_downloader_handlers.py,542,noqa: F401,not
scrapy/tests/test_downloader_handlers.py,603,"http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid",not
scrapy/tests/test_downloader_handlers.py,604,download it,not
scrapy/tests/test_downloader_handlers.py,621,PayloadResource requires body length to be 100,not
scrapy/tests/test_downloader_handlers.py,626,"download_maxsize < 100, hence the CancelledError",not
scrapy/tests/test_downloader_handlers.py,629,See issue https://twistedmatrix.com/trac/ticket/8175,not
scrapy/tests/test_downloader_handlers.py,634,download_maxsize = 50 is enough for the gzipped response,not
scrapy/tests/test_downloader_handlers.py,648,Note: this is an ugly hack for CONNECT request timeout test.,SATD
scrapy/tests/test_downloader_handlers.py,649,Returning some data here fail SSL/TLS handshake,not
scrapy/tests/test_downloader_handlers.py,650,"ToDo: implement proper HTTPS proxy tests, not faking them.",SATD
scrapy/tests/test_downloader_handlers.py,750,"anon=True, # implicit",not
scrapy/tests/test_downloader_handlers.py,767,test use same example keys than amazon developer guide,not
scrapy/tests/test_downloader_handlers.py,768,http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf,not
scrapy/tests/test_downloader_handlers.py,769,and the tests described here are the examples from that manual,not
scrapy/tests/test_downloader_handlers.py,791,noqa: F401,not
scrapy/tests/test_downloader_handlers.py,795,"We need to mock botocore.auth.formatdate, because otherwise",not
scrapy/tests/test_downloader_handlers.py,796,botocore overrides Date header with current date and time,not
scrapy/tests/test_downloader_handlers.py,797,and Authorization header is different each time,not
scrapy/tests/test_downloader_handlers.py,817,gets an object from the johnsmith bucket.,not
scrapy/tests/test_downloader_handlers.py,826,puts an object into the johnsmith bucket.,not
scrapy/tests/test_downloader_handlers.py,843,lists the content of the johnsmith bucket.,not
scrapy/tests/test_downloader_handlers.py,857,fetches the access control policy sub-resource for the 'johnsmith' bucket.,not
scrapy/tests/test_downloader_handlers.py,868,noqa: F401,not
scrapy/tests/test_downloader_handlers.py,874,deletes an object from the 'johnsmith' bucket using the,not
scrapy/tests/test_downloader_handlers.py,875,path-style and Date alternative.,not
scrapy/tests/test_downloader_handlers.py,884,botocore does not override Date with x-amz-date,not
scrapy/tests/test_downloader_handlers.py,889,uploads an object to a CNAME style virtual hosted bucket with metadata.,not
scrapy/tests/test_downloader_handlers.py,913,ensure that spaces are quoted properly before signing,not
scrapy/tests/test_downloader_handlers.py,937,setup dirs and test file,not
scrapy/tests/test_downloader_handlers.py,946,setup server,not
scrapy/tests/test_downloader_handlers.py,1052,setup dir and test file,not
scrapy/tests/test_downloader_handlers.py,1060,setup server for anonymous access,not
scrapy/tests/test_http_cookies.py,71,get_all result must be native string,not
scrapy/tests/test_downloadermiddleware_cookies.py,115,merge some cookies into jar,not
scrapy/tests/test_downloadermiddleware_cookies.py,121,test Cookie header is not seted to request,not
scrapy/tests/test_downloadermiddleware_cookies.py,126,check that returned cookies are not merged back to jar,not
scrapy/tests/test_downloadermiddleware_cookies.py,130,check that cookies are merged back,not
scrapy/tests/test_downloadermiddleware_cookies.py,135,check that cookies are merged when dont_merge_cookies is passed as 0,not
scrapy/tests/test_downloadermiddleware_cookies.py,141,merge some cookies into jar,not
scrapy/tests/test_downloadermiddleware_cookies.py,152,embed C1 and C3 for scrapytest.org/foo,SATD
scrapy/tests/test_downloadermiddleware_cookies.py,157,embed C2 for scrapytest.org/bar,SATD
scrapy/tests/test_downloadermiddleware_cookies.py,162,embed nothing for scrapytest.org/baz,SATD
scrapy/tests/test_downloadermiddleware_cookies.py,206,cookies from hosts with port,not
scrapy/tests/test_downloadermiddleware_cookies.py,222,skip cookie retrieval for not http request,not
scrapy/tests/test_linkextractors.py,14,a hack to skip base class tests in pytest,SATD
scrapy/tests/test_linkextractors.py,266,jpg is ignored by default,not
scrapy/tests/test_linkextractors.py,274,override denied extensions,not
scrapy/tests/test_linkextractors.py,494,Simple text inclusion test,not
scrapy/tests/test_linkextractors.py,499,Unique regex test,not
scrapy/tests/test_linkextractors.py,504,Multiple regex test,not
scrapy/tests/test_http_response.py,21,Response requires url in the consturctor,not
scrapy/tests/test_http_response.py,25,body can be str or None,not
scrapy/tests/test_http_response.py,28,test presence of all optional parameters,not
scrapy/tests/test_http_response.py,62,make sure flags list is shallow copied,not
scrapy/tests/test_http_response.py,66,make sure headers attribute is shallow copied,not
scrapy/tests/test_http_response.py,114,Empty attributes (which may fail if not compared properly),not
scrapy/tests/test_http_response.py,161,Response.follow,not
scrapy/tests/test_http_response.py,192,Response.follow_all,not
scrapy/tests/test_http_response.py,315,instantiate with unicode url without encoding (should set default encoding),not
scrapy/tests/test_http_response.py,319,make sure urls are converted to str,not
scrapy/tests/test_http_response.py,339,check body_as_unicode,not
scrapy/tests/test_http_response.py,343,check response.text,not
scrapy/tests/test_http_response.py,372,TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies,not
scrapy/tests/test_http_response.py,396,w3lib < 1.19.0,not
scrapy/tests/test_http_response.py,397,w3lib >= 1.19.0,not
scrapy/tests/test_http_response.py,401,"Inferring encoding from body also cache decoded body as sideeffect,",not
scrapy/tests/test_http_response.py,402,this test tries to ensure that calling response.encoding and,not
scrapy/tests/test_http_response.py,403,response.text in indistint order doesn't affect final,not
scrapy/tests/test_http_response.py,404,values for encoding and decoded body.,not
scrapy/tests/test_http_response.py,409,Test response without content-type and BOM encoding,not
scrapy/tests/test_http_response.py,417,Body caching sideeffect isn't triggered when encoding is declared in,not
scrapy/tests/test_http_response.py,418,content-type header but BOM still need to be removed from decoded,not
scrapy/tests/test_http_response.py,419,body,not
scrapy/tests/test_http_response.py,430,XXX: Policy for replacing invalid chars may suffer minor variations,SATD
scrapy/tests/test_http_response.py,431,but it should always contain the unicode replacement char (u'\ufffd'),not
scrapy/tests/test_http_response.py,436,Do not destroy html tags due to encoding bugs,not
scrapy/tests/test_http_response.py,441,FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse,SATD
scrapy/tests/test_http_response.py,442,"r = self.response_class(""http://www.example.com"", body=b'PREFIX\xe3\xabSUFFIX')",not
scrapy/tests/test_http_response.py,443,"assert u'\ufffd' in r.text, repr(r.text)",not
scrapy/tests/test_http_response.py,451,property is cached,not
scrapy/tests/test_http_response.py,522,select <a> elements,not
scrapy/tests/test_http_response.py,527,select <link> elements,not
scrapy/tests/test_http_response.py,534,href attributes should work,not
scrapy/tests/test_http_response.py,539,non-a elements are not supported,not
scrapy/tests/test_http_response.py,692,for conflicting declarations headers must take precedence,not
scrapy/tests/test_http_response.py,699,make sure replace() preserves the encoding of the original response,not
scrapy/tests/test_http_response.py,723,make sure replace() preserves the explicit encoding passed in the __init__ method,not
scrapy/tests/test_http_response.py,731,make sure replace() keeps the previous encoding unless overridden explicitly,not
scrapy/tests/test_http_response.py,747,property is cached,not
scrapy/tests/test_spidermiddleware_httperror.py,47,it assumes there is a response attached to failure,not
scrapy/tests/test_spidermiddleware_httperror.py,198,HttpError logs ignored responses with level INFO,not
scrapy/tests/test_spidermiddleware_httperror.py,210,"with level WARNING, we shouldn't capture anything from HttpError",not
scrapy/tests/test_utils_project.py,21,create an empty scrapy.cfg,SATD
scrapy/tests/test_spidermiddleware.py,28,catch deferred result and return the value,not
scrapy/tests/test_command_fetch.py,32,required on win32,not
scrapy/tests/test_spiderstate.py,36,"state attribute must be present if jobdir is not set, to provide a",not
scrapy/tests/test_spiderstate.py,37,consistent interface,not
scrapy/tests/test_exporters.py,203,datetime is not marshallable,not
scrapy/tests/mocks/dummydbm.py,22,return same instance for same file argument,not
scrapy/tests/test_spiderloader/__init__.py,10,ugly hack to avoid cyclic imports of scrapy.spiders when running this test,SATD
scrapy/tests/test_spiderloader/__init__.py,11,alone,not
scrapy/tests/test_spiderloader/__init__.py,129,copy 1 spider module so as to have duplicate spider name,not
scrapy/tests/test_spiderloader/__init__.py,150,copy 2 spider modules so as to have duplicate spider name,not
scrapy/tests/test_spiderloader/__init__.py,151,"This should issue 2 warning, 1 for each duplicate spider name",SATD
scrapy/tests/test_utils_misc/test_return_with_argument_inside_generator.py,37,not recursive,not
scrapy/tests/test_utils_misc/__init__.py,112,Check usage of correct constructor using four mocks:,not
scrapy/tests/test_utils_misc/__init__.py,113,1. with no alternative constructors,not
scrapy/tests/test_utils_misc/__init__.py,114,2. with from_settings() constructor,not
scrapy/tests/test_utils_misc/__init__.py,115,3. with from_crawler() constructor,not
scrapy/tests/test_utils_misc/__init__.py,116,4. with from_settings() and from_crawler() constructor,not
scrapy/tests/test_utils_misc/__init__.py,129,Check adoption of crawler settings,not
scrapy/tests/test_settings/__init__.py,49,Insufficient priority,not
scrapy/tests/test_settings/__init__.py,74,Note priority 30,not
scrapy/tests/test_settings/__init__.py,283,Empty settings should return 'default',not
scrapy/tests/test_cmdline/__init__.py,60,XXX: There's gotta be a smarter way to do this...,SATD
scrapy/docs/conf.py,1,"Scrapy documentation build configuration file, created by",SATD
scrapy/docs/conf.py,2,sphinx-quickstart on Mon Nov 24 12:02:52 2008.,not
scrapy/docs/conf.py,3,,not
scrapy/docs/conf.py,4,This file is execfile()d with the current directory set to its containing dir.,not
scrapy/docs/conf.py,5,,not
scrapy/docs/conf.py,6,"The contents of this file are pickled, so don't put values in the namespace",not
scrapy/docs/conf.py,7,"that aren't pickleable (module imports are okay, they're removed automatically).",not
scrapy/docs/conf.py,8,,not
scrapy/docs/conf.py,9,All configuration values have a default; values that are commented out,not
scrapy/docs/conf.py,10,serve to show the default.,not
scrapy/docs/conf.py,16,"If your extensions are in another directory, add it here. If the directory",not
scrapy/docs/conf.py,17,"is relative to the documentation root, use os.path.abspath to make it",not
scrapy/docs/conf.py,18,"absolute, like shown here.",not
scrapy/docs/conf.py,23,General configuration,not
scrapy/docs/conf.py,24,---------------------,not
scrapy/docs/conf.py,26,"Add any Sphinx extension module names here, as strings. They can be extensions",not
scrapy/docs/conf.py,27,coming with Sphinx (named 'sphinx.ext.*') or your custom ones.,not
scrapy/docs/conf.py,38,"Add any paths that contain templates here, relative to this directory.",not
scrapy/docs/conf.py,41,The suffix of source filenames.,not
scrapy/docs/conf.py,44,The encoding of source files.,not
scrapy/docs/conf.py,45,source_encoding = 'utf-8',not
scrapy/docs/conf.py,47,The master toctree document.,not
scrapy/docs/conf.py,50,General information about the project.,not
scrapy/docs/conf.py,54,"The version info for the project you're documenting, acts as replacement for",not
scrapy/docs/conf.py,55,"|version| and |release|, also used in various other places throughout the",not
scrapy/docs/conf.py,56,built documents.,not
scrapy/docs/conf.py,57,,not
scrapy/docs/conf.py,58,The short X.Y version.,not
scrapy/docs/conf.py,67,The language for content autogenerated by Sphinx. Refer to documentation,not
scrapy/docs/conf.py,68,for a list of supported languages.,not
scrapy/docs/conf.py,71,"There are two options for replacing |today|: either, you set today to some",not
scrapy/docs/conf.py,72,"non-false value, then it is used:",not
scrapy/docs/conf.py,73,today = '',not
scrapy/docs/conf.py,74,"Else, today_fmt is used as the format for a strftime call.",not
scrapy/docs/conf.py,75,"today_fmt = '%B %d, %Y'",not
scrapy/docs/conf.py,77,List of documents that shouldn't be included in the build.,not
scrapy/docs/conf.py,78,unused_docs = [],not
scrapy/docs/conf.py,82,"List of directories, relative to source directory, that shouldn't be searched",not
scrapy/docs/conf.py,83,for source files.,not
scrapy/docs/conf.py,86,The reST default role (used for this markup: `text`) to use for all documents.,not
scrapy/docs/conf.py,87,default_role = None,not
scrapy/docs/conf.py,89,"If true, '()' will be appended to :func: etc. cross-reference text.",not
scrapy/docs/conf.py,90,add_function_parentheses = True,not
scrapy/docs/conf.py,92,"If true, the current module name will be prepended to all description",not
scrapy/docs/conf.py,93,unit titles (such as .. function::).,not
scrapy/docs/conf.py,94,add_module_names = True,not
scrapy/docs/conf.py,96,"If true, sectionauthor and moduleauthor directives will be shown in the",not
scrapy/docs/conf.py,97,output. They are ignored by default.,not
scrapy/docs/conf.py,98,show_authors = False,not
scrapy/docs/conf.py,100,The name of the Pygments (syntax highlighting) style to use.,not
scrapy/docs/conf.py,104,Options for HTML output,not
scrapy/docs/conf.py,105,-----------------------,not
scrapy/docs/conf.py,107,The theme to use for HTML and HTML Help pages.  See the documentation for,not
scrapy/docs/conf.py,108,a list of builtin themes.,not
scrapy/docs/conf.py,111,Theme options are theme-specific and customize the look and feel of a theme,not
scrapy/docs/conf.py,112,"further.  For a list of options available for each theme, see the",not
scrapy/docs/conf.py,113,documentation.,not
scrapy/docs/conf.py,114,html_theme_options = {},not
scrapy/docs/conf.py,116,"Add any paths that contain custom themes here, relative to this directory.",not
scrapy/docs/conf.py,117,Add path to the RTD explicitly to robustify builds (otherwise might,not
scrapy/docs/conf.py,118,fail in a clean Debian build env),not
scrapy/docs/conf.py,123,The style sheet to use for HTML and HTML Help pages. A file of that name,not
scrapy/docs/conf.py,124,"must exist either in Sphinx' static/ path, or in one of the custom paths",not
scrapy/docs/conf.py,125,given in html_static_path.,not
scrapy/docs/conf.py,126,html_style = 'scrapydoc.css',SATD
scrapy/docs/conf.py,128,"The name for this set of Sphinx documents.  If None, it defaults to",not
scrapy/docs/conf.py,129,"""<project> v<release> documentation"".",not
scrapy/docs/conf.py,130,html_title = None,not
scrapy/docs/conf.py,132,A shorter title for the navigation bar.  Default is the same as html_title.,not
scrapy/docs/conf.py,133,html_short_title = None,not
scrapy/docs/conf.py,135,The name of an image file (relative to this directory) to place at the top,not
scrapy/docs/conf.py,136,of the sidebar.,not
scrapy/docs/conf.py,137,html_logo = None,not
scrapy/docs/conf.py,139,The name of an image file (within the static path) to use as favicon of the,not
scrapy/docs/conf.py,140,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32,not
scrapy/docs/conf.py,141,pixels large.,not
scrapy/docs/conf.py,142,html_favicon = None,not
scrapy/docs/conf.py,144,"Add any paths that contain custom static files (such as style sheets) here,",not
scrapy/docs/conf.py,145,"relative to this directory. They are copied after the builtin static files,",not
scrapy/docs/conf.py,146,"so a file named ""default.css"" will overwrite the builtin ""default.css"".",not
scrapy/docs/conf.py,149,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",not
scrapy/docs/conf.py,150,using the given strftime format.,not
scrapy/docs/conf.py,153,"Custom sidebar templates, maps document names to template names.",not
scrapy/docs/conf.py,154,html_sidebars = {},not
scrapy/docs/conf.py,156,"Additional templates that should be rendered to pages, maps page names to",not
scrapy/docs/conf.py,157,template names.,not
scrapy/docs/conf.py,158,html_additional_pages = {},not
scrapy/docs/conf.py,160,"If false, no module index is generated.",not
scrapy/docs/conf.py,161,html_use_modindex = True,not
scrapy/docs/conf.py,163,"If false, no index is generated.",not
scrapy/docs/conf.py,164,html_use_index = True,not
scrapy/docs/conf.py,166,"If true, the index is split into individual pages for each letter.",not
scrapy/docs/conf.py,167,html_split_index = False,not
scrapy/docs/conf.py,169,"If true, the reST sources are included in the HTML build as _sources/<name>.",not
scrapy/docs/conf.py,172,"If true, an OpenSearch description file will be output, and all pages will",not
scrapy/docs/conf.py,173,contain a <link> tag referring to it.  The value of this option must be the,not
scrapy/docs/conf.py,174,base URL from which the finished HTML is served.,not
scrapy/docs/conf.py,175,html_use_opensearch = '',not
scrapy/docs/conf.py,177,"If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").",not
scrapy/docs/conf.py,178,html_file_suffix = '',not
scrapy/docs/conf.py,180,Output file base name for HTML help builder.,not
scrapy/docs/conf.py,184,Options for LaTeX output,not
scrapy/docs/conf.py,185,------------------------,not
scrapy/docs/conf.py,187,The paper size ('letter' or 'a4').,not
scrapy/docs/conf.py,188,latex_paper_size = 'letter',not
scrapy/docs/conf.py,190,"The font size ('10pt', '11pt' or '12pt').",not
scrapy/docs/conf.py,191,latex_font_size = '10pt',not
scrapy/docs/conf.py,193,Grouping the document tree into LaTeX files. List of tuples,not
scrapy/docs/conf.py,194,"(source start file, target name, title, author, document class [howto/manual]).",not
scrapy/docs/conf.py,200,The name of an image file (relative to this directory) to place at the top of,not
scrapy/docs/conf.py,201,the title page.,not
scrapy/docs/conf.py,202,latex_logo = None,not
scrapy/docs/conf.py,204,"For ""manual"" documents, if this is true, then toplevel headings are parts,",not
scrapy/docs/conf.py,205,not chapters.,not
scrapy/docs/conf.py,206,latex_use_parts = False,not
scrapy/docs/conf.py,208,Additional stuff for the LaTeX preamble.,not
scrapy/docs/conf.py,209,latex_preamble = '',not
scrapy/docs/conf.py,211,Documents to append as an appendix to all manuals.,not
scrapy/docs/conf.py,212,latex_appendices = [],not
scrapy/docs/conf.py,214,"If false, no module index is generated.",not
scrapy/docs/conf.py,215,latex_use_modindex = True,not
scrapy/docs/conf.py,218,Options for the linkcheck builder,not
scrapy/docs/conf.py,219,---------------------------------,not
scrapy/docs/conf.py,221,A list of regular expressions that match URIs that should not be checked when,not
scrapy/docs/conf.py,222,doing a linkcheck build.,not
scrapy/docs/conf.py,229,Options for the Coverage extension,not
scrapy/docs/conf.py,230,----------------------------------,not
scrapy/docs/conf.py,232,Contracts add_pre_hook and add_post_hook are not documented because,not
scrapy/docs/conf.py,233,"they should be transparent to contract developers, for whom pre_hook and",not
scrapy/docs/conf.py,234,post_hook should be the actual concern.,not
scrapy/docs/conf.py,237,"ContractsManager is an internal class, developers are not expected to",not
scrapy/docs/conf.py,238,interact with it directly in any way.,not
scrapy/docs/conf.py,241,For default contracts we only want to document their general purpose in,not
scrapy/docs/conf.py,242,"their __init__ method, the methods they reimplement to achieve that purpose",not
scrapy/docs/conf.py,243,should be irrelevant to developers using those contracts.,not
scrapy/docs/conf.py,246,"Methods of downloader middlewares are not documented, only the classes",not
scrapy/docs/conf.py,247,"themselves, since downloader middlewares are controlled through Scrapy",not
scrapy/docs/conf.py,248,settings.,not
scrapy/docs/conf.py,251,Base classes of downloader middlewares are implementation details that,not
scrapy/docs/conf.py,252,are not meant for users.,not
scrapy/docs/conf.py,255,Private exception used by the command-line interface implementation.,not
scrapy/docs/conf.py,258,Methods of BaseItemExporter subclasses are only documented in,not
scrapy/docs/conf.py,259,BaseItemExporter.,not
scrapy/docs/conf.py,262,Extension behavior is only modified through settings. Methods of,not
scrapy/docs/conf.py,263,"extension classes, as well as helper functions, are implementation",not
scrapy/docs/conf.py,264,details that are not documented.,not
scrapy/docs/conf.py,265,methods,not
scrapy/docs/conf.py,266,helper functions,not
scrapy/docs/conf.py,268,"Never documented before, and deprecated now.",not
scrapy/docs/conf.py,272,Implementation detail of LxmlLinkExtractor,not
scrapy/docs/conf.py,277,Options for the InterSphinx extension,not
scrapy/docs/conf.py,278,-------------------------------------,not
scrapy/docs/conf.py,292,Options for sphinx-hoverxref options,not
scrapy/docs/conf.py,293,------------------------------------,not
scrapy/docs/utils/linkfix.py,1,!/usr/bin/python,not
scrapy/docs/utils/linkfix.py,20,Used for remembering the file (and its contents),not
scrapy/docs/utils/linkfix.py,21,so we don't have to open the same file again.,not
scrapy/docs/utils/linkfix.py,25,A regex that matches standard linkcheck output lines,not
scrapy/docs/utils/linkfix.py,28,Read lines from the linkcheck output file,not
scrapy/docs/utils/linkfix.py,36,"For every line, fix the respective file",not
scrapy/docs/utils/linkfix.py,44,Broken links can't be fixed and,not
scrapy/docs/utils/linkfix.py,45,I am not sure what do with the local ones.,not
scrapy/docs/utils/linkfix.py,49,If this is a new file,not
scrapy/docs/utils/linkfix.py,52,Update the previous file,not
scrapy/docs/utils/linkfix.py,59,Read the new file to memory,not
scrapy/docs/utils/linkfix.py,65,We don't understand what the current line means!,not
scrapy/docs/_ext/scrapydocs.py,19,index entries for setting directives look like:,not
scrapy/docs/_ext/scrapydocs.py,20,"[(u'pair', u'SETTING_NAME; setting', u'std:setting-SETTING_NAME', '')]",not
scrapy/docs/_ext/scrapydocs.py,27,target nodes are placed next to the node in the doc tree,not
scrapy/scrapy/cmdline.py,18,TODO: add `name` attribute to commands and and merge this function with,SATD
scrapy/scrapy/cmdline.py,19,scrapy.utils.spider.iter_spider_classes,SATD
scrapy/scrapy/cmdline.py,113,set EDITOR from environment if available,not
scrapy/scrapy/cmdline.py,168,"Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect()",not
scrapy/scrapy/cmdline.py,169,on exit: http://doc.pypy.org/en/latest/cpython_differences.html?highlight=gc.collect#differences-related-to-garbage-collection-strategies,not
scrapy/scrapy/robotstxt.py,18,"If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.",not
scrapy/scrapy/robotstxt.py,19,Switch to 'allow all' state.,not
scrapy/scrapy/logformatter.py,68,backward compatibility with Scrapy logformatter below 1.4 version,SATD
scrapy/scrapy/signals.py,25,for backward compatibility,not
scrapy/scrapy/squeues.py,85,Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s),not
scrapy/scrapy/squeues.py,86,TypeError is raised from parsel.Selector,not
scrapy/scrapy/resolver.py,9,TODO: cache misses,SATD
scrapy/scrapy/resolver.py,38,"in Twisted<=16.6, getHostByName() is always called with",not
scrapy/scrapy/resolver.py,39,"a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),",not
scrapy/scrapy/resolver.py,40,so the input argument above is simply overridden,not
scrapy/scrapy/resolver.py,41,to enforce Scrapy's DNS_TIMEOUT setting's value,SATD
scrapy/scrapy/mail.py,113,Import twisted.mail here because it is not available in python3,not
scrapy/scrapy/shell.py,42,disable accidental Ctrl-C key press from shutting down the engine,not
scrapy/scrapy/shell.py,73,try all by default,not
scrapy/scrapy/shell.py,75,always add standard shell as fallback,not
scrapy/scrapy/exceptions.py,8,Internal,not
scrapy/scrapy/exceptions.py,24,HTTP and crawling,not
scrapy/scrapy/exceptions.py,44,Items,not
scrapy/scrapy/exceptions.py,57,Commands,not
scrapy/scrapy/dupefilters.py,17,can return deferred,not
scrapy/scrapy/dupefilters.py,20,can return a deferred,not
scrapy/scrapy/dupefilters.py,23,log that a request has been filtered,not
scrapy/scrapy/__init__.py,8,Scrapy version,SATD
scrapy/scrapy/__init__.py,15,Check minimum required Python version,not
scrapy/scrapy/__init__.py,21,Ignore noisy twisted deprecation warnings,not
scrapy/scrapy/__init__.py,26,Apply monkey patches to fix issues in external libraries,not
scrapy/scrapy/__init__.py,33,Declare top-level shortcuts,not
scrapy/scrapy/exporters.py,101,there is a small difference between the behaviour or JsonItemExporter.indent,not
scrapy/scrapy/exporters.py,102,and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent,SATD
scrapy/scrapy/exporters.py,103,the addition of newlines everywhere,not
scrapy/scrapy/exporters.py,203,Windows needs this https://github.com/scrapy/scrapy/issues/3034,SATD
scrapy/scrapy/exporters.py,217,list in value may not contain strings,not
scrapy/scrapy/exporters.py,242,for dicts try using fields of the first item,not
scrapy/scrapy/exporters.py,245,use fields declared in Item,not
scrapy/scrapy/_monkeypatches.py,4,Undo what Twisted's perspective broker adds to pickle register,not
scrapy/scrapy/_monkeypatches.py,5,to prevent bugs like Twisted#7989 while serializing requests,not
scrapy/scrapy/_monkeypatches.py,6,NOQA,not
scrapy/scrapy/_monkeypatches.py,7,Remove only entries with twisted serializers for non-twisted types.,not
scrapy/scrapy/item.py,78,avoid creating dict for most common case,not
scrapy/scrapy/pqueues.py,22,as we replace some letters we can get collision for different slots,not
scrapy/scrapy/pqueues.py,23,add we add unique part,not
scrapy/scrapy/pqueues.py,86,this may fail (eg. serialization error),not
scrapy/scrapy/pqueues.py,161,slot -> priority queue,not
scrapy/scrapy/crawler.py,10,zope >= 5.0 only supports MultipleInvalid,not
scrapy/scrapy/crawler.py,63,scrapy root handler already installed: update it with new settings,SATD
scrapy/scrapy/crawler.py,65,lambda is assigned to Crawler attribute because this way it is not,not
scrapy/scrapy/crawler.py,66,garbage collected after leaving __init__ scope,not
scrapy/scrapy/crawler.py,316,Don't start the reactor if the deferreds are already fired,not
scrapy/scrapy/crawler.py,327,blocking call,not
scrapy/scrapy/crawler.py,338,raised if already stopped or in shutdown stage,not
scrapy/scrapy/http/cookies.py,25,the cookiejar implementation iterates through all domains,not
scrapy/scrapy/http/cookies.py,26,instead we restrict to potential matches on the domain,not
scrapy/scrapy/http/cookies.py,50,This is still quite inefficient for large number of cookies,not
scrapy/scrapy/http/cookies.py,142,python3 uses attributes instead of methods,not
scrapy/scrapy/http/cookies.py,189,python3 cookiejars calls get_all,not
scrapy/scrapy/http/cookies.py,193,python2 cookiejars calls getheaders,not
scrapy/scrapy/http/response/__init__.py,125,type: (...) -> Request,not
scrapy/scrapy/http/response/__init__.py,164,"type: (...) -> Generator[Request, None, None]",not
scrapy/scrapy/http/response/text.py,43,used by encoding detection,not
scrapy/scrapy/http/response/text.py,74,access self.encoding before _cached_ubody to make sure,not
scrapy/scrapy/http/response/text.py,75,_body_inferred_encoding is called,not
scrapy/scrapy/http/response/text.py,130,type: (...) -> Request,not
scrapy/scrapy/http/response/text.py,172,"type: (...) -> Generator[Request, None, None]",not
scrapy/scrapy/http/response/text.py,236,type: (parsel.Selector) -> str,not
scrapy/scrapy/http/response/text.py,238,e.g. ::attr(href) result,not
scrapy/scrapy/http/request/rpc.py,24,spec defines that requests must use POST method,not
scrapy/scrapy/http/request/rpc.py,27,xmlrpc query multiples times over the same url,not
scrapy/scrapy/http/request/rpc.py,30,restore encoding,not
scrapy/scrapy/http/request/form.py,95,"Get form element from xpath, if not found, go up",not
scrapy/scrapy/http/request/form.py,108,"If we get here, it means that either formname was None",not
scrapy/scrapy/http/request/form.py,109,or invalid,not
scrapy/scrapy/http/request/form.py,163,Match browser behaviour on simple select tag without options selected,not
scrapy/scrapy/http/request/form.py,164,And for select tags wihout options,not
scrapy/scrapy/http/request/form.py,168,This is a workround to bug in lxml fixed 2.3.1,SATD
scrapy/scrapy/http/request/form.py,169,fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139,not
scrapy/scrapy/http/request/form.py,189,"If we don't have clickdata, we just use the first clickable element",not
scrapy/scrapy/http/request/form.py,194,"If clickdata is given, we compare it to the clickable elements to find a",not
scrapy/scrapy/http/request/form.py,195,"match. We first look to see if the number is specified in clickdata,",not
scrapy/scrapy/http/request/form.py,196,because that uniquely identifies the element,not
scrapy/scrapy/http/request/form.py,206,"We didn't find it, so now we build an XPath expression out of the other",not
scrapy/scrapy/http/request/form.py,207,"arguments, because they can be used as such",not
scrapy/scrapy/http/request/__init__.py,23,this one has to be set first,not
scrapy/scrapy/contracts/default.py,10,contracts,not
scrapy/scrapy/contracts/__init__.py,61,calculate request args,not
scrapy/scrapy/contracts/__init__.py,64,Don't filter requests to allow,not
scrapy/scrapy/contracts/__init__.py,65,testing different callbacks on the same URL.,not
scrapy/scrapy/contracts/__init__.py,74,check if all positional arguments are defined in kwargs,not
scrapy/scrapy/contracts/__init__.py,78,execute pre and post hooks in order,not
scrapy/scrapy/templates/project/module/spiders/__init__.py,1,This package will contain the spiders of your Scrapy project,not
scrapy/scrapy/templates/project/module/spiders/__init__.py,2,,not
scrapy/scrapy/templates/project/module/spiders/__init__.py,3,Please refer to the documentation for information on how to create and manage,not
scrapy/scrapy/templates/project/module/spiders/__init__.py,4,your spiders.,not
scrapy/scrapy/utils/curl.py,27,"`--compressed` argument is not safe to ignore, but it's included here",not
scrapy/scrapy/utils/curl.py,28,because the `HttpCompressionMiddleware` is enabled by default,not
scrapy/scrapy/utils/curl.py,63,"curl automatically prepends 'http' if the scheme is missing, but Request",not
scrapy/scrapy/utils/curl.py,64,needs the scheme to work,not
scrapy/scrapy/utils/ssl.py,7,The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.,not
scrapy/scrapy/utils/ssl.py,8,"Using the binding directly, as this code does, requires cryptography 2.4.",not
scrapy/scrapy/utils/ssl.py,17,from OpenSSL.crypto.X509Name.__repr__,not
scrapy/scrapy/utils/ssl.py,25,requires OpenSSL 1.0.2,not
scrapy/scrapy/utils/ssl.py,28,adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key(),not
scrapy/scrapy/utils/console.py,17,Always use .instace() to ensure _instance propagation to all parents,not
scrapy/scrapy/utils/console.py,18,this is needed for <TAB> completion works well for new imports,not
scrapy/scrapy/utils/console.py,19,and clear the instance to always have the fresh env,not
scrapy/scrapy/utils/console.py,20,on repeated breaks like with inspect_response(),not
scrapy/scrapy/utils/console.py,50,readline module is only available on unix systems,not
scrapy/scrapy/utils/console.py,55,noqa: F401,not
scrapy/scrapy/utils/console.py,76,"list, preference order of shells",not
scrapy/scrapy/utils/console.py,78,available embeddable shells,not
scrapy/scrapy/utils/console.py,83,"function test: run all setup code (imports),",not
scrapy/scrapy/utils/console.py,84,but dont fall into the shell,not
scrapy/scrapy/utils/console.py,101,raised when using exit() in python code.interact,not
scrapy/scrapy/utils/spider.py,28,this needs to be imported here until get rid of the spider manager,not
scrapy/scrapy/utils/spider.py,29,singleton in scrapy.spider.spiders,SATD
scrapy/scrapy/utils/conf.py,46,"BEGIN Backward compatibility for old (base, custom) call signature",not
scrapy/scrapy/utils/conf.py,53,END Backward compatibility,not
scrapy/scrapy/utils/conf.py,169,FEEDS setting should take precedence over the -o and -t CLI options,not
scrapy/scrapy/utils/test.py,50,assuming boto=2.2.2,not
scrapy/scrapy/utils/test.py,64,loads acl before it will be deleted,not
scrapy/scrapy/utils/markup.py,9,noqa: F401,not
scrapy/scrapy/utils/multipart.py,9,noqa: F401,not
scrapy/scrapy/utils/reqser.py,24,urls should be safe (safe_string_url),not
scrapy/scrapy/utils/reqser.py,77,func has no __self__,not
scrapy/scrapy/utils/reqser.py,83,We need to use __func__ to access the original,not
scrapy/scrapy/utils/reqser.py,84,function object because instance method objects,not
scrapy/scrapy/utils/reqser.py,85,are generated each time attribute is retrieved from,not
scrapy/scrapy/utils/reqser.py,86,instance.,not
scrapy/scrapy/utils/reqser.py,87,,not
scrapy/scrapy/utils/reqser.py,88,Reference: The standard type hierarchy,not
scrapy/scrapy/utils/reqser.py,89,https://docs.python.org/3/reference/datamodel.html,not
scrapy/scrapy/utils/log.py,84,Route warnings through python logging,not
scrapy/scrapy/utils/log.py,202,"NOTE: This also handles 'args' being an empty dict, that case doesn't",not
scrapy/scrapy/utils/log.py,203,play well in logger.log calls,not
scrapy/scrapy/utils/boto.py,8,noqa: F401,not
scrapy/scrapy/utils/response.py,65,XXX: this implementation is a bit dirty and could be improved,SATD
scrapy/scrapy/utils/engine.py,3,used in global tests code,not
scrapy/scrapy/utils/engine.py,4,noqa: F401,not
scrapy/scrapy/utils/python.py,259,not attributes given return False by default,not
scrapy/scrapy/utils/python.py,265,support callables like itemgetter,not
scrapy/scrapy/utils/python.py,271,all attributes equal,not
scrapy/scrapy/utils/python.py,322,Collecting weakreferences can take two collections on PyPy.,not
scrapy/scrapy/utils/misc.py,94,named group,not
scrapy/scrapy/utils/misc.py,96,full regex or numbered groups,not
scrapy/scrapy/utils/ossignal.py,24,Catch Ctrl-Break in windows,not
scrapy/scrapy/utils/defer.py,56,FIXME: Hack to avoid introspecting tracebacks. This to speed up,SATD
scrapy/scrapy/utils/defer.py,57,"processing of IgnoreRequest errors which are, by far, the most common",not
scrapy/scrapy/utils/defer.py,58,exception in Scrapy - see #125,SATD
scrapy/scrapy/utils/defer.py,125,workaround for Python before 3.5.3 not having asyncio.isfuture,SATD
scrapy/scrapy/utils/defer.py,137,"wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines",not
scrapy/scrapy/utils/defer.py,138,"that use asyncio, e.g. ""await asyncio.sleep(1)""",not
scrapy/scrapy/utils/defer.py,141,"wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor",not
scrapy/scrapy/utils/defer.py,162,noqa: E722,not
scrapy/scrapy/utils/http.py,11,noqa: F401,not
scrapy/scrapy/utils/url.py,12,scrapy.utils.url was moved to w3lib.url and import * ensures this,SATD
scrapy/scrapy/utils/url.py,13,move doesn't break old code,not
scrapy/scrapy/utils/url.py,15,noqa: F401,not
scrapy/scrapy/utils/url.py,92,Note: this does not match Windows filepath,not
scrapy/scrapy/utils/deprecate.py,78,see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass,not
scrapy/scrapy/utils/deprecate.py,79,and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks,not
scrapy/scrapy/utils/deprecate.py,80,for implementation details,not
scrapy/scrapy/utils/deprecate.py,87,we should do the magic only if second `issubclass` argument,not
scrapy/scrapy/utils/deprecate.py,88,is the deprecated class itself - subclasses of the,not
scrapy/scrapy/utils/deprecate.py,89,deprecated class should not use custom `__subclasscheck__`,not
scrapy/scrapy/utils/deprecate.py,90,method.,not
scrapy/scrapy/utils/deprecate.py,115,Sometimes inspect.stack() fails (e.g. when the first import of,not
scrapy/scrapy/utils/deprecate.py,116,deprecated class is in jinja2 template). __module__ attribute is not,not
scrapy/scrapy/utils/deprecate.py,117,important enough to raise an exception as users may be unable,not
scrapy/scrapy/utils/deprecate.py,118,to fix inspect.stack() errors.,not
scrapy/scrapy/utils/datatypes.py,103,"key is not weak-referenceable, skip caching",not
scrapy/scrapy/utils/datatypes.py,109,"key is not weak-referenceable, it's not cached",not
scrapy/scrapy/utils/gz.py,9,- Python>=3.5 GzipFile's read() has issues returning leftover,not
scrapy/scrapy/utils/gz.py,10,uncompressed data when input is corrupted,not
scrapy/scrapy/utils/gz.py,11,(regression or bug-fix compared to Python 3.4),not
scrapy/scrapy/utils/gz.py,12,"- read1(), which fetches data before raising EOFError on next call",not
scrapy/scrapy/utils/gz.py,13,works here but is only available from Python>=3.3,not
scrapy/scrapy/utils/gz.py,32,"complete only if there is some data, otherwise re-raise",not
scrapy/scrapy/utils/gz.py,33,see issue 87 about catching struct.error,not
scrapy/scrapy/utils/gz.py,34,some pages are quite small so output_list is empty and f.extrabuf,not
scrapy/scrapy/utils/gz.py,35,contains the whole page content,not
scrapy/scrapy/utils/testproc.py,11,trial chdirs to temp dir,not
scrapy/scrapy/core/scheduler.py,122,non serializable request,not
scrapy/scrapy/core/engine.py,28,requests in progress,not
scrapy/scrapy/core/engine.py,99,Will also close spiders and downloader,not
scrapy/scrapy/core/engine.py,102,Will also close downloader,not
scrapy/scrapy/core/engine.py,175,"downloader middleware can return requests (for example, redirects)",not
scrapy/scrapy/core/engine.py,179,response is a Response or Failure,not
scrapy/scrapy/core/engine.py,188,scraper is not idle,SATD
scrapy/scrapy/core/engine.py,192,downloader has pending requests,not
scrapy/scrapy/core/engine.py,196,not all start requests are handled,not
scrapy/scrapy/core/engine.py,200,scheduler has pending requests,not
scrapy/scrapy/core/engine.py,246,tie request to response received,not
scrapy/scrapy/core/scraper.py,132,returns spider's processed output,not
scrapy/scrapy/core/spidermw.py,74,don't handle _InvalidOutput exception,not
scrapy/scrapy/core/spidermw.py,83,stop exception handling by handing control over to the,not
scrapy/scrapy/core/spidermw.py,84,process_spider_output chain if an iterable has been returned,not
scrapy/scrapy/core/spidermw.py,94,items in this iterable do not need to go through the process_spider_output,not
scrapy/scrapy/core/spidermw.py,95,"chain, they went through it already from the process_spider_exception method",not
scrapy/scrapy/core/spidermw.py,103,might fail directly if the output value is not a generator,not
scrapy/scrapy/core/downloader/__init__.py,140,Delay queue processing if a download_delay is configured,not
scrapy/scrapy/core/downloader/__init__.py,149,Process enqueued requests if there are free slots to transfer for this slot,not
scrapy/scrapy/core/downloader/__init__.py,155,prevent burst if inter-request delays were configured,not
scrapy/scrapy/core/downloader/__init__.py,161,The order is very important for the following deferreds. Do not change!,not
scrapy/scrapy/core/downloader/__init__.py,163,1. Create the download deferred,not
scrapy/scrapy/core/downloader/__init__.py,166,2. Notify response_downloaded listeners about the recent download,not
scrapy/scrapy/core/downloader/__init__.py,167,before querying queue for next request,not
scrapy/scrapy/core/downloader/__init__.py,176,"3. After response arrives, remove the request from transferring",not
scrapy/scrapy/core/downloader/__init__.py,177,state to free up the transferring slot so it can be used by the,not
scrapy/scrapy/core/downloader/__init__.py,178,following requests (perhaps those which came from the downloader,not
scrapy/scrapy/core/downloader/__init__.py,179,middleware itself),not
scrapy/scrapy/core/downloader/contextfactory.py,38,setting verify=True will require you to provide CAs,not
scrapy/scrapy/core/downloader/contextfactory.py,39,to verify against; in other words: it's not that simple,not
scrapy/scrapy/core/downloader/contextfactory.py,41,backward-compatible SSL/TLS method:,not
scrapy/scrapy/core/downloader/contextfactory.py,42,,not
scrapy/scrapy/core/downloader/contextfactory.py,43,* this will respect `method` attribute in often recommended,not
scrapy/scrapy/core/downloader/contextfactory.py,44,`ScrapyClientContextFactory` subclass,SATD
scrapy/scrapy/core/downloader/contextfactory.py,45,(https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133),SATD
scrapy/scrapy/core/downloader/contextfactory.py,46,,not
scrapy/scrapy/core/downloader/contextfactory.py,47,* getattr() for `_ssl_method` attribute for context factories,not
scrapy/scrapy/core/downloader/contextfactory.py,48,"not calling super(..., self).__init__",not
scrapy/scrapy/core/downloader/contextfactory.py,55,"kept for old-style HTTP/1.0 downloader context twisted calls,",not
scrapy/scrapy/core/downloader/contextfactory.py,56,e.g. connectSSL(),not
scrapy/scrapy/core/downloader/contextfactory.py,85,trustRoot set to platformTrust() will use the platform's root CAs.,not
scrapy/scrapy/core/downloader/contextfactory.py,86,,not
scrapy/scrapy/core/downloader/contextfactory.py,87,This means that a website like https://www.cacert.org will be rejected,not
scrapy/scrapy/core/downloader/contextfactory.py,88,"by default, since CAcert.org CA certificate is seldom shipped.",not
scrapy/scrapy/core/downloader/tls.py,23,protocol negotiation (recommended),not
scrapy/scrapy/core/downloader/tls.py,24,SSL 3 (NOT recommended),not
scrapy/scrapy/core/downloader/tls.py,25,TLS 1.0 only,not
scrapy/scrapy/core/downloader/tls.py,26,TLS 1.1 only,not
scrapy/scrapy/core/downloader/tls.py,27,TLS 1.2 only,not
scrapy/scrapy/core/downloader/tls.py,58,requires pyOPenSSL 0.15,not
scrapy/scrapy/core/downloader/tls.py,59,requires pyOPenSSL 16.0.0,not
scrapy/scrapy/core/downloader/webclient.py,15,"Assume parsed is urlparse-d from Request.url,",not
scrapy/scrapy/core/downloader/webclient.py,16,which was passed via safe_url_string and is ascii-only.,not
scrapy/scrapy/core/downloader/webclient.py,44,bucket for response headers,not
scrapy/scrapy/core/downloader/webclient.py,46,Method command,not
scrapy/scrapy/core/downloader/webclient.py,48,Headers,not
scrapy/scrapy/core/downloader/webclient.py,53,Body,not
scrapy/scrapy/core/downloader/webclient.py,86,transport cleanup needed for HTTPS connections,not
scrapy/scrapy/core/downloader/webclient.py,109,converting to bytes to comply to Twisted interface,not
scrapy/scrapy/core/downloader/webclient.py,119,Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected,not
scrapy/scrapy/core/downloader/webclient.py,120,to have _disconnectedDeferred. See Twisted r32329.,not
scrapy/scrapy/core/downloader/webclient.py,121,As Scrapy implements it's own logic to handle redirects is not,SATD
scrapy/scrapy/core/downloader/webclient.py,122,needed to add the callback _waitForDisconnect.,not
scrapy/scrapy/core/downloader/webclient.py,123,Specifically this avoids the AttributeError exception when,not
scrapy/scrapy/core/downloader/webclient.py,124,clientConnectionFailed method is called.,not
scrapy/scrapy/core/downloader/webclient.py,129,set Host header based on url,not
scrapy/scrapy/core/downloader/webclient.py,132,set Content-Length based len of body,not
scrapy/scrapy/core/downloader/webclient.py,135,just in case a broken http/1.1 decides to keep connection alive,not
scrapy/scrapy/core/downloader/webclient.py,137,Content-Length must be specified in POST method even with no body,not
scrapy/scrapy/core/downloader/handlers/http11.py,47,try method-aware context factory,not
scrapy/scrapy/core/downloader/handlers/http11.py,56,use context factory defaults,not
scrapy/scrapy/core/downloader/handlers/http11.py,92,"closeCachedConnections will hang on network or server issues, so",not
scrapy/scrapy/core/downloader/handlers/http11.py,93,we'll manually timeout the deferred.,not
scrapy/scrapy/core/downloader/handlers/http11.py,94,,not
scrapy/scrapy/core/downloader/handlers/http11.py,95,Twisted issue addressing this problem can be found here:,not
scrapy/scrapy/core/downloader/handlers/http11.py,96,https://twistedmatrix.com/trac/ticket/7738.,not
scrapy/scrapy/core/downloader/handlers/http11.py,97,,not
scrapy/scrapy/core/downloader/handlers/http11.py,98,"closeCachedConnections doesn't handle external errbacks, so we'll",not
scrapy/scrapy/core/downloader/handlers/http11.py,99,issue a callback after `_disconnect_timeout` seconds.,not
scrapy/scrapy/core/downloader/handlers/http11.py,150,make sure that enough (all) bytes are consumed,not
scrapy/scrapy/core/downloader/handlers/http11.py,151,and that we've got all HTTP headers (ending with a blank line),not
scrapy/scrapy/core/downloader/handlers/http11.py,152,from the proxy so that we don't send those bytes to the TLS layer,not
scrapy/scrapy/core/downloader/handlers/http11.py,153,,not
scrapy/scrapy/core/downloader/handlers/http11.py,154,see https://github.com/scrapy/scrapy/issues/2491,SATD
scrapy/scrapy/core/downloader/handlers/http11.py,160,set proper Server Name Indication extension,not
scrapy/scrapy/core/downloader/handlers/http11.py,233,proxy host and port are required for HTTP pool `key`,not
scrapy/scrapy/core/downloader/handlers/http11.py,234,"otherwise, same remote host connection request could reuse",not
scrapy/scrapy/core/downloader/handlers/http11.py,235,a cached tunneled connection to a different proxy,not
scrapy/scrapy/core/downloader/handlers/http11.py,263,"Cache *all* connections under the same key, since we are only",not
scrapy/scrapy/core/downloader/handlers/http11.py,264,"connecting to a single destination, the proxy:",not
scrapy/scrapy/core/downloader/handlers/http11.py,342,request details,not
scrapy/scrapy/core/downloader/handlers/http11.py,354,set download latency,not
scrapy/scrapy/core/downloader/handlers/http11.py,356,response body is ready to be consumed,not
scrapy/scrapy/core/downloader/handlers/http11.py,359,check download timeout,not
scrapy/scrapy/core/downloader/handlers/http11.py,368,"needed for HTTPS requests, otherwise _ResponseReader doesn't",not
scrapy/scrapy/core/downloader/handlers/http11.py,369,receive connectionLost(),not
scrapy/scrapy/core/downloader/handlers/http11.py,380,deliverBody hangs for responses without body,not
scrapy/scrapy/core/downloader/handlers/http11.py,410,Abort connection immediately.,not
scrapy/scrapy/core/downloader/handlers/http11.py,426,save response for timeouts,not
scrapy/scrapy/core/downloader/handlers/http11.py,489,This maybe called several times after cancel was called with buffered data.,not
scrapy/scrapy/core/downloader/handlers/http11.py,509,Clear buffer earlier to avoid keeping data in memory for a long time.,not
scrapy/scrapy/core/downloader/handlers/s3.py,25,noqa: F401,not
scrapy/scrapy/core/downloader/handlers/s3.py,45,"If no credentials could be found anywhere,",not
scrapy/scrapy/core/downloader/handlers/s3.py,46,consider this an anonymous connection request by default;,not
scrapy/scrapy/core/downloader/handlers/s3.py,47,unless 'anon' was set explicitly (True/False).,not
scrapy/scrapy/core/downloader/handlers/__init__.py,21,stores acceptable schemes on instancing,not
scrapy/scrapy/core/downloader/handlers/__init__.py,22,stores instanced handlers for schemes,not
scrapy/scrapy/core/downloader/handlers/__init__.py,23,remembers failed handlers,not
scrapy/scrapy/loader/__init__.py,46,values from initial item,not
scrapy/scrapy/settings/__init__.py,440,Do not pass kwarg values here. We don't want to promote user-defined,not
scrapy/scrapy/settings/__init__.py,441,"dicts, and we want to update, not replace, default dicts with the",not
scrapy/scrapy/settings/__init__.py,442,values given by the user,not
scrapy/scrapy/settings/__init__.py,445,Promote default dictionaries to BaseSettings instances for per-key,not
scrapy/scrapy/settings/__init__.py,446,priorities,not
scrapy/scrapy/settings/default_settings.py,76,3mins,not
scrapy/scrapy/settings/default_settings.py,78,1024m,not
scrapy/scrapy/settings/default_settings.py,79,32m,not
scrapy/scrapy/settings/default_settings.py,88,"Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:",not
scrapy/scrapy/settings/default_settings.py,95,Engine side,not
scrapy/scrapy/settings/default_settings.py,110,Downloader side,not
scrapy/scrapy/settings/default_settings.py,137,a function to extend uri arguments,not
scrapy/scrapy/settings/default_settings.py,215,enable memory debugging,not
scrapy/scrapy/settings/default_settings.py,216,send memory debugging report by mail at engine shutdown,not
scrapy/scrapy/settings/default_settings.py,235,uses Firefox default setting,not
scrapy/scrapy/settings/default_settings.py,242,initial response + 2 retries = 3 requests,not
scrapy/scrapy/settings/default_settings.py,263,Engine side,not
scrapy/scrapy/settings/default_settings.py,269,Spider side,not
scrapy/scrapy/spiders/sitemap.py,74,actual gzipped sitemap files are decompressed above ;,not
scrapy/scrapy/spiders/sitemap.py,75,if we are here (response body is not gzipped),not
scrapy/scrapy/spiders/sitemap.py,76,"and have a response for .xml.gz,",not
scrapy/scrapy/spiders/sitemap.py,77,it usually means that it was already gunzipped,not
scrapy/scrapy/spiders/sitemap.py,78,"by HttpCompression middleware,",not
scrapy/scrapy/spiders/sitemap.py,79,"the HTTP response being sent with ""Content-Encoding: gzip""",not
scrapy/scrapy/spiders/sitemap.py,80,"without actually being a .xml.gz file in the first place,",not
scrapy/scrapy/spiders/sitemap.py,81,"merely XML gzip-compressed on the fly,",not
scrapy/scrapy/spiders/sitemap.py,82,"in other word, here, we have plain XML",not
scrapy/scrapy/spiders/sitemap.py,97,"Also consider alternate URLs (xhtml:link rel=""alternate"")",not
scrapy/scrapy/spiders/feed.py,47,backward compatibility,not
scrapy/scrapy/spiders/feed.py,103,"When this is None, python's csv module's default delimiter is used",not
scrapy/scrapy/spiders/feed.py,104,"When this is None, python's csv module's default quotechar is used",not
scrapy/scrapy/spiders/__init__.py,112,Top-level imports,not
scrapy/scrapy/spiders/__init__.py,113,noqa: F401,not
scrapy/scrapy/spiders/__init__.py,114,noqa: F401,not
scrapy/scrapy/spiders/__init__.py,115,noqa: F401,not
scrapy/scrapy/linkextractors/htmlparser.py,57,wrapper needed to allow to work directly with text,not
scrapy/scrapy/linkextractors/sgml.py,71,wrapper needed to allow to work directly with text,not
scrapy/scrapy/linkextractors/lxmlhtml.py,19,from lxml/src/lxml/html/__init__.py,not
scrapy/scrapy/linkextractors/lxmlhtml.py,63,hacky way to get the underlying lxml parsed document,SATD
scrapy/scrapy/linkextractors/lxmlhtml.py,65,pseudo lxml.html.HtmlElement.make_links_absolute(base_url),not
scrapy/scrapy/linkextractors/lxmlhtml.py,71,skipping bogus links,not
scrapy/scrapy/linkextractors/lxmlhtml.py,77,to fix relative links after process_value,not
scrapy/scrapy/linkextractors/__init__.py,22,common file extensions that are not followed if they occur in links,not
scrapy/scrapy/linkextractors/__init__.py,24,archives,not
scrapy/scrapy/linkextractors/__init__.py,27,images,not
scrapy/scrapy/linkextractors/__init__.py,31,audio,not
scrapy/scrapy/linkextractors/__init__.py,34,video,not
scrapy/scrapy/linkextractors/__init__.py,38,office suites,not
scrapy/scrapy/linkextractors/__init__.py,42,other,not
scrapy/scrapy/linkextractors/__init__.py,135,Top-level imports,not
scrapy/scrapy/linkextractors/__init__.py,136,noqa: F401,not
scrapy/scrapy/extensions/feedexport.py,97,BEGIN Backward compatibility for initialising without keys (and,not
scrapy/scrapy/extensions/feedexport.py,98,without using from_crawler),not
scrapy/scrapy/extensions/feedexport.py,113,END Backward compatibility,not
scrapy/scrapy/extensions/feedexport.py,119,"remove first ""/""",not
scrapy/scrapy/extensions/feedexport.py,187,feed params,not
scrapy/scrapy/extensions/feedexport.py,191,flags,not
scrapy/scrapy/extensions/feedexport.py,225,Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings,not
scrapy/scrapy/extensions/feedexport.py,232,handle pathlib.Path objects,not
scrapy/scrapy/extensions/feedexport.py,235,End: Backward compatibility for FEED_URI and FEED_FORMAT settings,not
scrapy/scrapy/extensions/feedexport.py,237,'FEEDS' setting takes precedence over 'FEED_URI',not
scrapy/scrapy/extensions/feedexport.py,239,handle pathlib.Path objects,not
scrapy/scrapy/extensions/feedexport.py,271,We need to call slot.storage.store nonetheless to get the file,not
scrapy/scrapy/extensions/feedexport.py,272,properly closed.,not
scrapy/scrapy/extensions/memusage.py,28,stdlib's resource module is only available on unix platforms.,not
scrapy/scrapy/extensions/memusage.py,50,"on macOS ru_maxrss is in bytes, on Linux it is in KB",not
scrapy/scrapy/extensions/memusage.py,97,warn only once,not
scrapy/scrapy/extensions/httpcache.py,44,one year,not
scrapy/scrapy/extensions/httpcache.py,67,"obey user-agent directive ""Cache-Control: no-store""",not
scrapy/scrapy/extensions/httpcache.py,70,Any other is eligible for caching,not
scrapy/scrapy/extensions/httpcache.py,74,What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1,not
scrapy/scrapy/extensions/httpcache.py,75,Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4,not
scrapy/scrapy/extensions/httpcache.py,76,Status code 206 is not included because cache can not deal with partial contents,not
scrapy/scrapy/extensions/httpcache.py,78,"obey directive ""Cache-Control: no-store""",not
scrapy/scrapy/extensions/httpcache.py,81,Never cache 304 (Not Modified) responses,not
scrapy/scrapy/extensions/httpcache.py,84,Cache unconditionally if configured to do so,not
scrapy/scrapy/extensions/httpcache.py,87,Any hint on response expiration is good,not
scrapy/scrapy/extensions/httpcache.py,90,Firefox fallbacks this statuses to one year expiration if none is set,not
scrapy/scrapy/extensions/httpcache.py,93,Other statuses without expiration requires at least one validator,not
scrapy/scrapy/extensions/httpcache.py,96,Any other is probably not eligible for caching,not
scrapy/scrapy/extensions/httpcache.py,97,Makes no sense to cache responses that does not contain expiration,not
scrapy/scrapy/extensions/httpcache.py,98,info and can not be revalidated,not
scrapy/scrapy/extensions/httpcache.py,120,"From RFC2616: ""Indicates that the client is willing to",not
scrapy/scrapy/extensions/httpcache.py,121,accept a response that has exceeded its expiration time.,not
scrapy/scrapy/extensions/httpcache.py,122,"If max-stale is assigned a value, then the client is",not
scrapy/scrapy/extensions/httpcache.py,123,willing to accept a response that has exceeded its,not
scrapy/scrapy/extensions/httpcache.py,124,expiration time by no more than the specified number of,not
scrapy/scrapy/extensions/httpcache.py,125,"seconds. If no value is assigned to max-stale, then the",not
scrapy/scrapy/extensions/httpcache.py,126,"client is willing to accept a stale response of any age.""",not
scrapy/scrapy/extensions/httpcache.py,137,"Cached response is stale, try to set validators if any",not
scrapy/scrapy/extensions/httpcache.py,142,"Use the cached response if the new response is a server error,",not
scrapy/scrapy/extensions/httpcache.py,143,as long as the old response didn't specify must-revalidate.,not
scrapy/scrapy/extensions/httpcache.py,149,Use the cached response if the server says it hasn't changed.,not
scrapy/scrapy/extensions/httpcache.py,166,Reference nsHttpResponseHead::ComputeFreshnessLifetime,not
scrapy/scrapy/extensions/httpcache.py,167,https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706,not
scrapy/scrapy/extensions/httpcache.py,173,Parse date header or synthesize it if none exists,not
scrapy/scrapy/extensions/httpcache.py,176,Try HTTP/1.0 Expires header,not
scrapy/scrapy/extensions/httpcache.py,179,When parsing Expires header fails RFC 2616 section 14.21 says we,not
scrapy/scrapy/extensions/httpcache.py,180,should treat this as an expiration time in the past.,not
scrapy/scrapy/extensions/httpcache.py,183,Fallback to heuristic using last-modified header,not
scrapy/scrapy/extensions/httpcache.py,184,This is not in RFC but on Firefox caching implementation,not
scrapy/scrapy/extensions/httpcache.py,189,This request can be cached indefinitely,not
scrapy/scrapy/extensions/httpcache.py,193,Insufficient information to compute fresshness lifetime,not
scrapy/scrapy/extensions/httpcache.py,197,Reference nsHttpResponseHead::ComputeCurrentAge,not
scrapy/scrapy/extensions/httpcache.py,198,https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658,not
scrapy/scrapy/extensions/httpcache.py,200,"If Date header is not set we assume it is a fast connection, and",not
scrapy/scrapy/extensions/httpcache.py,201,clock is in sync with the server,not
scrapy/scrapy/extensions/httpcache.py,236,not cached,not
scrapy/scrapy/extensions/httpcache.py,261,not found,not
scrapy/scrapy/extensions/httpcache.py,265,expired,not
scrapy/scrapy/extensions/httpcache.py,292,not cached,not
scrapy/scrapy/extensions/httpcache.py,338,not found,not
scrapy/scrapy/extensions/httpcache.py,341,expired,not
scrapy/scrapy/extensions/telnet.py,32,signal to update telnet variables,not
scrapy/scrapy/extensions/telnet.py,33,args: telnet_vars,not
scrapy/scrapy/extensions/telnet.py,98,Note: if you add entries here also update topics/telnetconsole.rst,not
scrapy/scrapy/extensions/throttle.py,71,If a server needs `latency` seconds to respond then,not
scrapy/scrapy/extensions/throttle.py,72,we should send a request each `latency/N` seconds,not
scrapy/scrapy/extensions/throttle.py,73,to have N requests processed in parallel,not
scrapy/scrapy/extensions/throttle.py,76,Adjust the delay to make it closer to target_delay,not
scrapy/scrapy/extensions/throttle.py,79,"If target delay is bigger than old delay, then use it instead of mean.",not
scrapy/scrapy/extensions/throttle.py,80,It works better with problematic sites.,SATD
scrapy/scrapy/extensions/throttle.py,83,Make sure self.mindelay <= new_delay <= self.max_delay,not
scrapy/scrapy/extensions/throttle.py,86,Dont adjust delay if response status != 200 and new delay is smaller,not
scrapy/scrapy/extensions/throttle.py,87,"than old one, as error pages (and redirections) are usually small and",not
scrapy/scrapy/extensions/throttle.py,88,"so tend to reduce latency, thus provoking a positive feedback by",not
scrapy/scrapy/extensions/throttle.py,89,reducing delay instead of increase.,not
scrapy/scrapy/extensions/debug.py,28,win32 platforms don't support SIGUSR signals,not
scrapy/scrapy/extensions/debug.py,60,win32 platforms don't support SIGUSR signals,not
scrapy/scrapy/selector/__init__.py,4,noqa: F401,not
scrapy/scrapy/commands/fetch.py,54,"by default, let the framework handle redirects,",not
scrapy/scrapy/commands/fetch.py,55,i.e. command handles all codes expect 3xx,not
scrapy/scrapy/commands/check.py,58,load contracts,not
scrapy/scrapy/commands/check.py,64,contract requests,not
scrapy/scrapy/commands/check.py,81,start checks,not
scrapy/scrapy/commands/parse.py,164,memorize first request,not
scrapy/scrapy/commands/parse.py,168,determine real callback,not
scrapy/scrapy/commands/parse.py,192,parse items and requests,not
scrapy/scrapy/commands/parse.py,210,update request meta if any extra meta was passed through the --meta/-m opts.,not
scrapy/scrapy/commands/parse.py,214,update cb_kwargs if any extra values were was passed through the --cbkwargs option.,not
scrapy/scrapy/commands/parse.py,253,parse arguments,not
scrapy/scrapy/commands/parse.py,259,prepare spidercls,not
scrapy/scrapy/commands/shell.py,52,first argument may be a local file,not
scrapy/scrapy/commands/shell.py,64,The crawler is created this way since the Shell manually handles the,not
scrapy/scrapy/commands/shell.py,65,"crawling engine, so the set up in the crawl method won't work",not
scrapy/scrapy/commands/shell.py,67,The Shell class needs a persistent engine in the crawler,not
scrapy/scrapy/commands/genspider.py,74,if spider already exists and not --force then halt,not
scrapy/scrapy/commands/__init__.py,17,default settings to be used for this command instead of global defaults,not
scrapy/scrapy/commands/__init__.py,23,set in scrapy.cmdline,SATD
scrapy/scrapy/pipelines/files.py,86,Overriden from settings.FILES_STORE_S3_ACL in FilesPipeline.from_settings,not
scrapy/scrapy/pipelines/files.py,128,disable ssl (is_secure=False) because of this python bug:,not
scrapy/scrapy/pipelines/files.py,129,https://bugs.python.org/issue5103,not
scrapy/scrapy/pipelines/files.py,176,This is required while we need to support both boto and botocore.,not
scrapy/scrapy/pipelines/files.py,222,The bucket's default object ACL will be applied to the object.,not
scrapy/scrapy/pipelines/files.py,223,Overriden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.,not
scrapy/scrapy/pipelines/files.py,314,The file doesn't exist,not
scrapy/scrapy/pipelines/files.py,403,to support win32 paths like: C:\\some\dir,not
scrapy/scrapy/pipelines/files.py,413,returning None force download,not
scrapy/scrapy/pipelines/files.py,417,returning None force download,not
scrapy/scrapy/pipelines/files.py,422,returning None force download,not
scrapy/scrapy/pipelines/files.py,518,Overridable Interface,not
scrapy/scrapy/pipelines/files.py,538,Handles empty and wild extensions by trying to guess the,not
scrapy/scrapy/pipelines/files.py,539,mime type then extension or default to empty string otherwise,not
scrapy/scrapy/pipelines/media.py,91,Return cached result if request was already seen,not
scrapy/scrapy/pipelines/media.py,95,"Otherwise, wait for result",not
scrapy/scrapy/pipelines/media.py,99,Check if request is downloading right now to avoid doing it twice,not
scrapy/scrapy/pipelines/media.py,103,Download request checking media_to_download hook output first,not
scrapy/scrapy/pipelines/media.py,111,it must return wad at last,not
scrapy/scrapy/pipelines/media.py,123,this ugly code was left only to support tests. TODO: remove,SATD
scrapy/scrapy/pipelines/media.py,138,minimize cached information for failure,not
scrapy/scrapy/pipelines/media.py,143,This code fixes a memory leak by avoiding to keep references to,not
scrapy/scrapy/pipelines/media.py,144,the Request and Response objects on the Media Pipeline cache.,not
scrapy/scrapy/pipelines/media.py,145,,not
scrapy/scrapy/pipelines/media.py,146,What happens when the media_downloaded callback raises an,not
scrapy/scrapy/pipelines/media.py,147,"exception, for example a FileException('download-error') when",not
scrapy/scrapy/pipelines/media.py,148,"the Response status code is not 200 OK, is that the original",not
scrapy/scrapy/pipelines/media.py,149,StopIteration exception (which in turn contains the failed,not
scrapy/scrapy/pipelines/media.py,150,"Response and by extension, the original Request) gets encapsulated",not
scrapy/scrapy/pipelines/media.py,151,within the FileException context.,not
scrapy/scrapy/pipelines/media.py,152,,not
scrapy/scrapy/pipelines/media.py,153,"Originally, Scrapy was using twisted.internet.defer.returnValue",SATD
scrapy/scrapy/pipelines/media.py,154,"inside functions decorated with twisted.internet.defer.inlineCallbacks,",not
scrapy/scrapy/pipelines/media.py,155,encapsulating the returned Response in a _DefGen_Return exception,not
scrapy/scrapy/pipelines/media.py,156,instead of a StopIteration.,not
scrapy/scrapy/pipelines/media.py,157,,not
scrapy/scrapy/pipelines/media.py,158,To avoid keeping references to the Response and therefore Request,not
scrapy/scrapy/pipelines/media.py,159,"objects on the Media Pipeline cache, we should wipe the context of",not
scrapy/scrapy/pipelines/media.py,160,the encapsulated exception when it is a StopIteration instance,not
scrapy/scrapy/pipelines/media.py,161,,not
scrapy/scrapy/pipelines/media.py,162,This problem does not occur in Python 2.7 since we don't have,not
scrapy/scrapy/pipelines/media.py,163,Exception Chaining (https://www.python.org/dev/peps/pep-3134/).,not
scrapy/scrapy/pipelines/media.py,169,cache result,not
scrapy/scrapy/pipelines/media.py,173,Overridable Interface,not
scrapy/scrapy/pipelines/images.py,17,TODO: from scrapy.pipelines.media import MediaPipeline,SATD
scrapy/scrapy/pipelines/images.py,36,Uppercase attributes kept for backward compatibility with code that subclasses,not
scrapy/scrapy/pipelines/images.py,37,ImagesPipeline. They may be overridden by settings.,not
scrapy/scrapy/spidermiddlewares/offsite.py,47,hostname can be None for wrong urls (like javascript links),not
scrapy/scrapy/spidermiddlewares/offsite.py,55,allow all by default,not
scrapy/scrapy/spidermiddlewares/referer.py,73,Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy,not
scrapy/scrapy/spidermiddlewares/referer.py,265,Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string,not
scrapy/scrapy/spidermiddlewares/referer.py,303,Note: this hook is a bit of a hack to intercept redirections,SATD
scrapy/scrapy/spidermiddlewares/referer.py,343,"check redirected request to patch ""Referer"" header if necessary",not
scrapy/scrapy/spidermiddlewares/referer.py,347,we don't patch the referrer value if there is none,not
scrapy/scrapy/spidermiddlewares/referer.py,349,the request's referrer header value acts as a surrogate,not
scrapy/scrapy/spidermiddlewares/referer.py,350,for the parent response URL,not
scrapy/scrapy/spidermiddlewares/referer.py,351,,not
scrapy/scrapy/spidermiddlewares/referer.py,352,"Note: if the 3xx response contained a Referrer-Policy header,",not
scrapy/scrapy/spidermiddlewares/referer.py,353,the information is not available using this hook,not
scrapy/scrapy/spidermiddlewares/httperror.py,32,common case,not
scrapy/scrapy/spidermiddlewares/depth.py,52,base case (depth=0),not
scrapy/scrapy/downloadermiddlewares/robotstxt.py,31,"check if parser dependencies are met, this should throw an error otherwise.",not
scrapy/scrapy/downloadermiddlewares/cookies.py,36,set Cookie header,not
scrapy/scrapy/downloadermiddlewares/cookies.py,45,extract cookies from Set-Cookie and drop invalid/expired cookies,not
scrapy/scrapy/downloadermiddlewares/cookies.py,72,build cookie string,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,52,Skip uncacheable requests,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,54,flag as uncacheable,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,57,Look for cached response and check if expired,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,64,first time request,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,66,Return cached response only if not expired,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,72,Keep a reference to cached response to avoid a second cache lookup on,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,73,process_response hook,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,80,Skip cached responses and uncacheable requests,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,85,"RFC2616 requires origin server to set Date header,",not
scrapy/scrapy/downloadermiddlewares/httpcache.py,86,https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18,not
scrapy/scrapy/downloadermiddlewares/httpcache.py,90,Do not validate first-hand responses,not
scrapy/scrapy/downloadermiddlewares/retry.py,36,IOError is raised by the HttpCompression middleware when trying to,not
scrapy/scrapy/downloadermiddlewares/retry.py,37,decompress an empty response,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,23,XXX: Google parses at least first 100k bytes; scrapy's redirect,SATD
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,24,middleware parses first 4k. 4k turns out to be insufficient,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,25,"for this middleware, and parsing 100k could be slow.",not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,26,We use something in between (32K) by default.,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,39,other HTTP methods are either not safe or don't have a body,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,42,prevent loops,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,48,scrapy already handles #! links properly,SATD
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,66,XXX: move it to w3lib?,SATD
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,82,Stripping scripts and comments is slow (about 20x slower than,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,83,just checking if a string is in text); this is a quick fail-fast,not
scrapy/scrapy/downloadermiddlewares/ajaxcrawl.py,84,path that should work for most pages.,not
scrapy/scrapy/downloadermiddlewares/httpcompression.py,45,force recalculating the encoding until we make sure the,not
scrapy/scrapy/downloadermiddlewares/httpcompression.py,46,responsetypes guessing is reliable,not
scrapy/scrapy/downloadermiddlewares/httpcompression.py,62,ugly hack to work with raw deflate content that may,SATD
scrapy/scrapy/downloadermiddlewares/httpcompression.py,63,"be sent by microsoft servers. For more information, see:",not
scrapy/scrapy/downloadermiddlewares/httpcompression.py,64,http://carsten.codimi.de/gzip.yaws/,not
scrapy/scrapy/downloadermiddlewares/httpcompression.py,65,http://www.port80software.com/200ok/archive/2005/10/31/868.aspx,not
scrapy/scrapy/downloadermiddlewares/httpcompression.py,66,http://www.gzip.org/zlib/zlib_faq.html#faq38,not
scrapy/scrapy/downloadermiddlewares/httpproxy.py,43,ignore if proxy is already set,not
scrapy/scrapy/downloadermiddlewares/httpproxy.py,47,extract credentials if present,not
scrapy/scrapy/downloadermiddlewares/httpproxy.py,59,'no_proxy' is only supported by http schemes,not
