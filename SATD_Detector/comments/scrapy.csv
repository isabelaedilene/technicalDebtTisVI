file path,line #,comment,satd
scrapy/conftest.py,11,"not a test, but looks like a test",
scrapy/conftest.py,13,contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess,
scrapy/conftest.py,15,Py36-only parts of respective tests,
scrapy/conftest.py,32,Avoid executing tests when executing `--flake8` flag (pytest-flake8),
scrapy/conftest.py,44,doctests,
scrapy/extras/qps-bench-server.py,1,!/usr/bin/env python,
scrapy/extras/qps-bench-server.py,28,reset stats on high iter-request times caused by client restarts,
scrapy/extras/qps-bench-server.py,29,seconds,
scrapy/extras/qpsclient.py,19,Max concurrency is limited by global CONCURRENT_REQUESTS setting,
scrapy/extras/qpsclient.py,21,Requests per second goal,
scrapy/extras/qpsclient.py,22,same as: 1 / download_delay,
scrapy/extras/qpsclient.py,24,time in seconds to delay server responses,
scrapy/extras/qpsclient.py,26,number of slots to create,
scrapy/tests/test_utils_url.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_utils_url.py,226,some corner cases (default to http://),
scrapy/tests/test_utils_url.py,235,TODO: the following tests do not pass with current implementation,
scrapy/tests/test_utils_url.py,293,"user: ""username@""",
scrapy/tests/test_utils_url.py,294,password: none,
scrapy/tests/test_utils_url.py,298,"user: ""username:pass""",
scrapy/tests/test_utils_url.py,299,"password: """"",
scrapy/tests/test_utils_url.py,303,"user: ""me""",
scrapy/tests/test_utils_url.py,304,"password: ""user@domain.com""",
scrapy/tests/test_utils_asyncio.py,12,the result should depend only on the pytest --reactor argument,
scrapy/tests/test_utils_asyncio.py,16,this should do nothing,
scrapy/tests/test_commands.py,155,only pass one argument. spider script shouldn't be created,
scrapy/tests/test_commands.py,158,pass two arguments <name> <domain>. spider script should be created,
scrapy/tests/test_commands.py,262,see https://github.com/scrapy/scrapy/issues/2811,
scrapy/tests/test_commands.py,263,"The spider below should not be able to connect to localhost:12345,",
scrapy/tests/test_commands.py,264,"which is intended,",
scrapy/tests/test_commands.py,265,but this should not be because of DNS lookup error,
scrapy/tests/test_commands.py,266,assumption: localhost will resolve in all cases (true?),
scrapy/tests/test_downloadermiddleware_redirect.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_downloadermiddleware_redirect.py,37,response without Location header but with status code is 3XX should be ignored,
scrapy/tests/test_downloadermiddleware_redirect.py,63,Test that it redirects when dont_redirect is False,
scrapy/tests/test_downloadermiddleware_redirect.py,89,response without Location header but with status code is 3XX should be ignored,
scrapy/tests/test_downloadermiddleware_redirect.py,104,response without Location header but with status code is 3XX should be ignored,
scrapy/tests/test_downloadermiddleware_redirect.py,120,response without Location header but with status code is 3XX should be ignored,
scrapy/tests/test_downloadermiddleware_redirect.py,190,HTTP historically supports latin1,
scrapy/tests/test_downloadermiddleware_redirect.py,198,header using UTF-8 encoding,
scrapy/tests/test_downloadermiddleware_redirect.py,230,meta-refresh with high intervals don't trigger redirects,
scrapy/tests/test_utils_python.py,115,no attributes given return False,
scrapy/tests/test_utils_python.py,117,not existent attributes,
scrapy/tests/test_utils_python.py,122,equal attribute,
scrapy/tests/test_utils_python.py,126,obj1 has no attribute y,
scrapy/tests/test_utils_python.py,130,equal attributes,
scrapy/tests/test_utils_python.py,134,differente attributes,
scrapy/tests/test_utils_python.py,137,test callable,
scrapy/tests/test_utils_python.py,142,compare ['meta']['a'],
scrapy/tests/test_utils_python.py,151,fail z equality,
scrapy/tests/test_utils_python.py,208,TODO: how do we fix this to return the actual argument names?,
scrapy/tests/test_spidermiddleware_referer.py,62,no credentials leak,
scrapy/tests/test_spidermiddleware_referer.py,65,no referrer leak for local schemes,
scrapy/tests/test_spidermiddleware_referer.py,69,no referrer leak for s3 origins,
scrapy/tests/test_spidermiddleware_referer.py,87,TLS to TLS: send non-empty referrer,
scrapy/tests/test_spidermiddleware_referer.py,94,TLS to non-TLS: do not send referrer,
scrapy/tests/test_spidermiddleware_referer.py,99,non-TLS to TLS or non-TLS: send referrer,
scrapy/tests/test_spidermiddleware_referer.py,109,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,116,"Same origin (protocol, host, port): send referrer",
scrapy/tests/test_spidermiddleware_referer.py,124,Different host: do NOT send referrer,
scrapy/tests/test_spidermiddleware_referer.py,129,Different port: do NOT send referrer,
scrapy/tests/test_spidermiddleware_referer.py,134,Different protocols: do NOT send refferer,
scrapy/tests/test_spidermiddleware_referer.py,141,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,149,"TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)",
scrapy/tests/test_spidermiddleware_referer.py,155,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,162,TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades,
scrapy/tests/test_spidermiddleware_referer.py,167,downgrade: send nothing,
scrapy/tests/test_spidermiddleware_referer.py,170,upgrade: send origin,
scrapy/tests/test_spidermiddleware_referer.py,173,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,181,"Same origin (protocol, host, port): send referrer",
scrapy/tests/test_spidermiddleware_referer.py,189,Different host: send origin as referrer,
scrapy/tests/test_spidermiddleware_referer.py,193,exact match required,
scrapy/tests/test_spidermiddleware_referer.py,196,Different port: send origin as referrer,
scrapy/tests/test_spidermiddleware_referer.py,200,Different protocols: send origin as referrer,
scrapy/tests/test_spidermiddleware_referer.py,207,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,209,TLS to non-TLS downgrade: send origin,
scrapy/tests/test_spidermiddleware_referer.py,216,"Same origin (protocol, host, port): send referrer",
scrapy/tests/test_spidermiddleware_referer.py,224,Different host: send origin as referrer,
scrapy/tests/test_spidermiddleware_referer.py,228,exact match required,
scrapy/tests/test_spidermiddleware_referer.py,231,Different port: send origin as referrer,
scrapy/tests/test_spidermiddleware_referer.py,235,downgrade,
scrapy/tests/test_spidermiddleware_referer.py,239,non-TLS to non-TLS,
scrapy/tests/test_spidermiddleware_referer.py,242,upgrade,
scrapy/tests/test_spidermiddleware_referer.py,246,Different protocols: send origin as referrer,
scrapy/tests/test_spidermiddleware_referer.py,250,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,253,TLS to non-TLS downgrade: send nothing,
scrapy/tests/test_spidermiddleware_referer.py,260,TLS to TLS: send referrer,
scrapy/tests/test_spidermiddleware_referer.py,268,"TLS to non-TLS: send referrer (yes, it's unsafe)",
scrapy/tests/test_spidermiddleware_referer.py,273,"non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)",
scrapy/tests/test_spidermiddleware_referer.py,283,test for user/password stripping,
scrapy/tests/test_spidermiddleware_referer.py,293,--- Tests using settings to set policy using class path,
scrapy/tests/test_spidermiddleware_referer.py,352,--- Tests using Request meta dict to set policy,
scrapy/tests/test_spidermiddleware_referer.py,408,When an unknown policy is referenced in Request.meta,
scrapy/tests/test_spidermiddleware_referer.py,409,"(here, a typo error),",
scrapy/tests/test_spidermiddleware_referer.py,410,the policy defined in settings takes precedence,
scrapy/tests/test_spidermiddleware_referer.py,418,same as above but with string value for settings policy,
scrapy/tests/test_spidermiddleware_referer.py,426,"request meta references a wrong policy but it is set,",
scrapy/tests/test_spidermiddleware_referer.py,427,"so the Referrer-Policy header in response is not used,",
scrapy/tests/test_spidermiddleware_referer.py,428,and the settings' policy is applied,
scrapy/tests/test_spidermiddleware_referer.py,436,"here, request meta does not set the policy",
scrapy/tests/test_spidermiddleware_referer.py,437,so response headers take precedence,
scrapy/tests/test_spidermiddleware_referer.py,445,"here, request meta does not set the policy,",
scrapy/tests/test_spidermiddleware_referer.py,446,"but response headers also use an unknown policy,",
scrapy/tests/test_spidermiddleware_referer.py,447,so the settings' policy is used,
scrapy/tests/test_spidermiddleware_referer.py,544,parent,
scrapy/tests/test_spidermiddleware_referer.py,545,target,
scrapy/tests/test_spidermiddleware_referer.py,547,"redirections: code, URL",
scrapy/tests/test_spidermiddleware_referer.py,551,expected initial referer,
scrapy/tests/test_spidermiddleware_referer.py,552,expected referer for the redirection request,
scrapy/tests/test_spidermiddleware_referer.py,557,redirecting to non-secure URL,
scrapy/tests/test_spidermiddleware_referer.py,566,redirecting to non-secure URL: different origin,
scrapy/tests/test_spidermiddleware_referer.py,605,parent,
scrapy/tests/test_spidermiddleware_referer.py,606,target,
scrapy/tests/test_spidermiddleware_referer.py,608,"redirections: code, URL",
scrapy/tests/test_spidermiddleware_referer.py,612,"expected initial ""Referer""",
scrapy/tests/test_spidermiddleware_referer.py,613,"expected ""Referer"" for the redirection request",
scrapy/tests/test_spidermiddleware_referer.py,624,different origin,
scrapy/tests/test_spidermiddleware_referer.py,644,origin,
scrapy/tests/test_spidermiddleware_referer.py,645,target,
scrapy/tests/test_spidermiddleware_referer.py,647,"redirections: code, URL",
scrapy/tests/test_spidermiddleware_referer.py,651,"expected initial ""Referer""",
scrapy/tests/test_spidermiddleware_referer.py,652,expected referer for the redirection request,
scrapy/tests/test_spidermiddleware_referer.py,657,redirecting from secure to non-secure URL == different origin,
scrapy/tests/test_spidermiddleware_referer.py,666,different domain == different origin,
scrapy/tests/test_spidermiddleware_referer.py,692,send origin,
scrapy/tests/test_spidermiddleware_referer.py,693,redirects to same origin: send origin,
scrapy/tests/test_spidermiddleware_referer.py,698,redirecting to non-secure URL: no referrer,
scrapy/tests/test_spidermiddleware_referer.py,707,redirecting to non-secure URL (different domain): no referrer,
scrapy/tests/test_spidermiddleware_referer.py,724,"HTTPS all along, so origin referrer is kept as-is",
scrapy/tests/test_spidermiddleware_referer.py,732,TLS to non-TLS: no referrer,
scrapy/tests/test_spidermiddleware_referer.py,734,TLS URL again: (still) no referrer,
scrapy/tests/test_spidermiddleware_referer.py,753,origin,
scrapy/tests/test_spidermiddleware_referer.py,754,target + redirection,
scrapy/tests/test_spidermiddleware_referer.py,756,"redirections: code, URL",
scrapy/tests/test_spidermiddleware_referer.py,760,expected initial referer,
scrapy/tests/test_spidermiddleware_referer.py,761,expected referer for the redirection request,
scrapy/tests/test_spidermiddleware_referer.py,766,redirecting to non-secure URL: send origin,
scrapy/tests/test_spidermiddleware_referer.py,775,redirecting to non-secure URL (different domain): send origin,
scrapy/tests/test_spidermiddleware_referer.py,792,all different domains: send origin,
scrapy/tests/test_spidermiddleware_referer.py,800,TLS to non-TLS: send origin,
scrapy/tests/test_spidermiddleware_referer.py,802,TLS URL again: send origin (also),
scrapy/tests/test_spidermiddleware_referer.py,823,origin,
scrapy/tests/test_spidermiddleware_referer.py,824,target + redirection,
scrapy/tests/test_spidermiddleware_referer.py,826,"redirections: code, URL",
scrapy/tests/test_spidermiddleware_referer.py,830,expected initial referer,
scrapy/tests/test_spidermiddleware_referer.py,831,expected referer for the redirection request,
scrapy/tests/test_spidermiddleware_referer.py,836,"redirecting to non-secure URL: do not send the ""Referer"" header",
scrapy/tests/test_spidermiddleware_referer.py,845,redirecting to non-secure URL (different domain): send origin,
scrapy/tests/test_spidermiddleware_referer.py,862,all different domains: send origin,
scrapy/tests/test_spidermiddleware_referer.py,870,"TLS to non-TLS: do not send ""Referer""",
scrapy/tests/test_spidermiddleware_referer.py,872,TLS URL again: (still) send nothing,
scrapy/tests/test_extension_telnet.py,17,This function has some side effects we don't need for this test,
scrapy/tests/test_utils_request.py,19,make sure caching is working,
scrapy/tests/test_utils_request.py,59,cached fingerprint must be cleared on request copy,
scrapy/tests/test_utils_request.py,82,the representation is not important but it must not fail.,
scrapy/tests/test_utils_log.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_utils_log.py,70,disable it to avoid the extra warning,
scrapy/tests/test_pipeline_crawl.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_pipeline_crawl.py,66,prepare a directory for storing files,
scrapy/tests/test_pipeline_crawl.py,93,check that logs show the expected number of successful file downloads,
scrapy/tests/test_pipeline_crawl.py,97,check that the images/files checksums are what we know they should be,
scrapy/tests/test_pipeline_crawl.py,106,check that the image files where actually written to the media store,
scrapy/tests/test_pipeline_crawl.py,115,"check that the item does NOT have the ""images/files"" field populated",
scrapy/tests/test_pipeline_crawl.py,120,check that there was 1 successful fetch and 3 other responses with non-200 code,
scrapy/tests/test_pipeline_crawl.py,126,check that logs do show the failure on the file downloads,
scrapy/tests/test_pipeline_crawl.py,130,check that no files were written to the media store,
scrapy/tests/test_pipeline_crawl.py,183,somehow checksums for images are different for Python 3.3,
scrapy/tests/test_scheduler.py,267,pylint: disable=protected-access,
scrapy/tests/test_scheduler.py,274,pylint: disable=protected-access,
scrapy/tests/test_utils_deprecate.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_utils_deprecate.py,110,ignore subclassing warnings,
scrapy/tests/test_utils_deprecate.py,119,subclass instances don't warn,
scrapy/tests/test_loader.py,12,test items,
scrapy/tests/test_loader.py,31,test item loaders,
scrapy/tests/test_loader.py,48,test processors,
scrapy/tests/test_loader.py,75,Let's assume a SKU is only digits.,
scrapy/tests/test_loader.py,80,take first which allows empty values,
scrapy/tests/test_loader.py,89,"Should not return ""sku: None"".",
scrapy/tests/test_loader.py,91,Should not ignore empty values.,
scrapy/tests/test_loader.py,132,test add object value,
scrapy/tests/test_loader.py,774,combining/accumulating CSS selectors and XPath expressions,
scrapy/tests/test_loader.py,995,Functions as processors,
scrapy/tests/test_utils_iterators.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_utils_iterators.py,55,example taken from https://github.com/scrapy/scrapy/issues/1665,
scrapy/tests/test_utils_iterators.py,93,with bytes,
scrapy/tests/test_utils_iterators.py,95,Unicode body needs encoding information,
scrapy/tests/test_utils_iterators.py,258,explicit type check cuz' we no like stinkin' autocasting! yarrr,
scrapy/tests/test_downloadermiddleware_httpcompression.py,73,noqa: F401,
scrapy/tests/test_downloadermiddleware_httpcompression.py,210,"build a gzipped file (here, a sitemap)",
scrapy/tests/test_downloadermiddleware_httpcompression.py,231,build a gzipped response body containing this gzipped file,
scrapy/tests/test_crawler.py,101,disable telnet if not available to avoid an extra warning,
scrapy/tests/test_item.py,168,D class inverted,
scrapy/tests/test_item.py,199,D class inverted,
scrapy/tests/test_item.py,229,D class inverted,
scrapy/tests/test_item.py,286,This attribute is an internal attribute in Python 3.6+,
scrapy/tests/test_item.py,287,and must be propagated properly. See,
scrapy/tests/test_item.py,288,https://docs.python.org/3.6/reference/datamodel.html#creating-the-class-object,
scrapy/tests/test_item.py,289,"In <3.6, we add a dummy attribute just to ensure the",
scrapy/tests/test_item.py,290,__new__ method propagates it correctly.,
scrapy/tests/test_item.py,294,For rationale of this see:,
scrapy/tests/test_item.py,295,https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222,
scrapy/tests/test_item.py,296,noqa  https://github.com/scrapy/scrapy/issues/2836,
scrapy/tests/test_item.py,313,This call to super() trigger the __classcell__ propagation,
scrapy/tests/test_item.py,314,requirement. When not done properly raises an error:,
scrapy/tests/test_item.py,315,TypeError: __class__ set to <class '__main__.MyItem'>,
scrapy/tests/test_item.py,316,defining 'MyItem' as <class '__main__.MyItem'>,
scrapy/tests/test_responsetypes.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_responsetypes.py,76,TODO: add more tests that check precedence between the different arguments,
scrapy/tests/test_responsetypes.py,79,headers takes precedence over url,
scrapy/tests/test_responsetypes.py,90,check that mime.types files shipped with scrapy are loaded,
scrapy/tests/test_spider.py,434,.xml.gz but body decoded by HttpCompression middleware already,
scrapy/tests/test_spider.py,599,spider without overridden make_requests_from_url method,
scrapy/tests/test_spider.py,600,doesn't issue a warning,
scrapy/tests/test_spider.py,605,spider without overridden make_requests_from_url method,
scrapy/tests/test_spider.py,606,should issue a warning when called directly,
scrapy/tests/test_spider.py,611,"spider with overridden make_requests_from_url issues a warning,",
scrapy/tests/test_spider.py,612,but the method still works,
scrapy/tests/test_robotstxt_interface.py,1,coding=utf-8,
scrapy/tests/test_robotstxt_interface.py,6,check if reppy parser is installed,
scrapy/tests/test_robotstxt_interface.py,8,noqa: F401,
scrapy/tests/test_robotstxt_interface.py,15,check if robotexclusionrulesparser is installed,
scrapy/tests/test_robotstxt_interface.py,17,noqa: F401,
scrapy/tests/test_robotstxt_interface.py,24,check if protego parser is installed,
scrapy/tests/test_robotstxt_interface.py,26,noqa: F401,
scrapy/tests/test_mail.py,1,coding=utf-8,
scrapy/tests/test_http_request.py,21,Request requires url in the __init__ method,
scrapy/tests/test_http_request.py,24,url argument must be basestring,
scrapy/tests/test_http_request.py,52,Different ways of setting headers attribute,
scrapy/tests/test_http_request.py,62,headers must not be unicode,
scrapy/tests/test_http_request.py,96,"encoding affects only query part of URI, not path",
scrapy/tests/test_http_request.py,97,path part should always be UTF-8 encoded before percent-escaping,
scrapy/tests/test_http_request.py,108,should be same as above,
scrapy/tests/test_http_request.py,113,encoding is used for encoding query-string before percent-escaping;,
scrapy/tests/test_http_request.py,114,path is still UTF-8 encoded before percent-escaping,
scrapy/tests/test_http_request.py,119,percent-escaping sequences that do not match valid UTF-8 sequences,
scrapy/tests/test_http_request.py,120,should be kept untouched (just upper-cased perhaps),
scrapy/tests/test_http_request.py,121,,
scrapy/tests/test_http_request.py,122,See https://tools.ietf.org/html/rfc3987#section-3.2,
scrapy/tests/test_http_request.py,123,,
scrapy/tests/test_http_request.py,124,"""Conversions from URIs to IRIs MUST NOT use any character encoding",
scrapy/tests/test_http_request.py,125,"other than UTF-8 in steps 3 and 4, even if it might be possible to",
scrapy/tests/test_http_request.py,126,guess from the context that another character encoding than UTF-8 was,
scrapy/tests/test_http_request.py,127,"used in the URI.  For example, the URI",
scrapy/tests/test_http_request.py,128,"""http://www.example.org/r%E9sum%E9.html"" might with some guessing be",
scrapy/tests/test_http_request.py,129,interpreted to contain two e-acute characters encoded as iso-8859-1.,
scrapy/tests/test_http_request.py,130,It must not be converted to an IRI containing these e-acute,
scrapy/tests/test_http_request.py,131,"characters.  Otherwise, in the future the IRI will be mapped to",
scrapy/tests/test_http_request.py,132,"""http://www.example.org/r%C3%A9sum%C3%A9.html"", which is a different",
scrapy/tests/test_http_request.py,133,"URI from ""http://www.example.org/r%E9sum%E9.html"".",
scrapy/tests/test_http_request.py,152,default encoding,
scrapy/tests/test_http_request.py,163,ascii url,
scrapy/tests/test_http_request.py,166,unicode url,
scrapy/tests/test_http_request.py,182,make sure copy does not propagate callbacks,
scrapy/tests/test_http_request.py,188,make sure flags list is shallow copied,
scrapy/tests/test_http_request.py,192,make sure cb_kwargs dict is shallow copied,
scrapy/tests/test_http_request.py,196,make sure meta dict is shallow copied,
scrapy/tests/test_http_request.py,200,make sure headers attribute is shallow copied,
scrapy/tests/test_http_request.py,206,Request.body can be identical since it's an immutable object (str),
scrapy/tests/test_http_request.py,230,Empty attributes (which may fail if not compared properly),
scrapy/tests/test_http_request.py,284,Note: more curated tests regarding curl conversion are in,
scrapy/tests/test_http_request.py,285,`test_utils_curl.py`,
scrapy/tests/test_http_request.py,341,By default: it works and ignores the unknown options: --foo and -z,
scrapy/tests/test_http_request.py,342,avoid warning when executing tests,
scrapy/tests/test_http_request.py,349,If `ignore_unknon_options` is set to `False` it raises an error with,
scrapy/tests/test_http_request.py,350,the unknown options: --foo and -z,
scrapy/tests/test_http_request.py,374,using default encoding (utf-8),
scrapy/tests/test_http_request.py,383,using default encoding (utf-8),
scrapy/tests/test_http_request.py,392,using default encoding (utf-8),
scrapy/tests/test_http_request.py,417,using multiples values for a single key,
scrapy/tests/test_http_request.py,615,xpath in _get_inputs(),
scrapy/tests/test_http_request.py,616,xpath in _get_clickable(),
scrapy/tests/test_http_request.py,1285,empty data,
scrapy/tests/test_http_request.py,1290,data is not passed,
scrapy/tests/test_http_request.py,1304,method passed explicitly,
scrapy/tests/test_downloadermiddleware_useragent.py,23,settings UESR_AGENT to None should remove the user agent,
scrapy/tests/test_webclient.py,15,deprecated in Twisted 19.7.0,
scrapy/tests/test_webclient.py,16,(remove once we bump our requirement past that version),
scrapy/tests/test_webclient.py,89,basic test stolen from twisted HTTPageGetter,
scrapy/tests/test_webclient.py,111,test minimal sent headers,
scrapy/tests/test_webclient.py,118,test a simple POST with body and content-type,
scrapy/tests/test_webclient.py,134,test a POST method with no body provided,
scrapy/tests/test_webclient.py,146,test with single and multivalued headers,
scrapy/tests/test_webclient.py,162,same test with single and multivalued headers but using Headers class,
scrapy/tests/test_webclient.py,189,regression test for: http://dev.scrapy.org/ticket/258,
scrapy/tests/test_webclient.py,253,"if we pass Host header explicitly, it should be used, otherwise",
scrapy/tests/test_webclient.py,254,it should extract from url,
scrapy/tests/test_webclient.py,304,Clean up the server which is hanging around not doing,
scrapy/tests/test_webclient.py,305,anything.,
scrapy/tests/test_webclient.py,307,There might be nothing here if the server managed to already see,
scrapy/tests/test_webclient.py,308,that the connection was lost.,
scrapy/tests/test_webclient.py,393,we try to use a cipher that is not enabled by default in OpenSSL,
scrapy/tests/test_utils_defer.py,18,it is [1] with maybeDeferred,
scrapy/tests/test_utils_defer.py,19,"add another value, that should be catched by assertEqual",
scrapy/tests/test_utils_defer.py,32,it is [1] with maybeDeferred,
scrapy/tests/test_utils_defer.py,33,"add another value, that should be catched by assertEqual",
scrapy/tests/test_squeues.py,34,Trigger Twisted bug #7989,
scrapy/tests/test_squeues.py,35,NOQA,
scrapy/tests/test_squeues.py,38,Use a different unpickleable object,
scrapy/tests/test_squeues.py,45,Selectors should fail (lxml.html.HtmlElement objects can't be pickled),
scrapy/tests/test_downloadermiddleware_httpproxy.py,67,proxy from request.meta,
scrapy/tests/test_downloadermiddleware_httpproxy.py,80,proxy from request.meta,
scrapy/tests/test_downloadermiddleware_httpproxy.py,87,utf-8 encoding,
scrapy/tests/test_downloadermiddleware_httpproxy.py,95,proxy from request.meta,
scrapy/tests/test_downloadermiddleware_httpproxy.py,101,default latin-1 encoding,
scrapy/tests/test_downloadermiddleware_httpproxy.py,108,"proxy from request.meta, latin-1 encoding",
scrapy/tests/test_downloadermiddleware_httpproxy.py,140,proxy from meta['proxy'] takes precedence,
scrapy/tests/mockserver.py,39,silence CancelledError,
scrapy/tests/mockserver.py,58,"order == ""desc""",
scrapy/tests/mockserver.py,83,send headers now and delay body,
scrapy/tests/mockserver.py,134,"we force the body content, otherwise Twisted redirectTo()",
scrapy/tests/mockserver.py,135,"returns HTML with <meta http-equiv=""refresh""",
scrapy/tests/mockserver.py,234,disabling TLS1.2+ because it unconditionally enables some strong ciphers,
scrapy/tests/test_utils_template.py,30,Failure of test itself,
scrapy/tests/test_utils_template.py,39,Failure of test iself,
scrapy/tests/test_logformatter.py,67,"In practice, the complete traceback is shown by passing the",
scrapy/tests/test_logformatter.py,68,'exc_info' argument to the logging function,
scrapy/tests/test_logformatter.py,77,"In practice, the complete traceback is shown by passing the",
scrapy/tests/test_logformatter.py,78,'exc_info' argument to the logging function,
scrapy/tests/test_logformatter.py,90,"In practice, the complete traceback is shown by passing the",
scrapy/tests/test_logformatter.py,91,'exc_info' argument to the logging function,
scrapy/tests/test_logformatter.py,99,"In practice, the complete traceback is shown by passing the",
scrapy/tests/test_logformatter.py,100,'exc_info' argument to the logging function,
scrapy/tests/test_downloadermiddleware_robotstxt.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_downloadermiddleware_robotstxt.py,91,"garbage response should be discarded, equal 'allow all'",
scrapy/tests/test_downloadermiddleware_robotstxt.py,114,empty response should equal 'allow all',
scrapy/tests/test_downloadermiddleware_robotstxt.py,177,not actually used,
scrapy/tests/test_downloadermiddleware_robotstxt.py,183,not actually used,
scrapy/tests/test_proxy_connect.py,97,The proxy returns a 407 error code but it does not reach the client;,
scrapy/tests/test_proxy_connect.py,98,he just sees a TunnelError.,
scrapy/tests/test_pipeline_media.py,62,Check that failures are logged by default,
scrapy/tests/test_pipeline_media.py,75,disable failure logging and check again,
scrapy/tests/test_pipeline_media.py,120,Create sample pair of Request and Response objects,
scrapy/tests/test_pipeline_media.py,124,Simulate the Media Pipeline behavior to produce a Twisted Failure,
scrapy/tests/test_pipeline_media.py,126,Simulate a Twisted inline callback returning a Response,
scrapy/tests/test_pipeline_media.py,127,The returnValue method raises an exception encapsulating the value,
scrapy/tests/test_pipeline_media.py,132,Simulate the media_downloaded callback raising a FileException,
scrapy/tests/test_pipeline_media.py,133,This usually happens when the status code is not 200 OK,
scrapy/tests/test_pipeline_media.py,137,Simulate Twisted capturing the FileException,
scrapy/tests/test_pipeline_media.py,138,It encapsulates the exception inside a Twisted Failure,
scrapy/tests/test_pipeline_media.py,141,The Failure should encapsulate a FileException ...,
scrapy/tests/test_pipeline_media.py,143,... and it should have the returnValue exception set as its context,
scrapy/tests/test_pipeline_media.py,146,Let's calculate the request fingerprint and fake some runtime data...,
scrapy/tests/test_pipeline_media.py,152,When calling the method that caches the Request's result ...,
scrapy/tests/test_pipeline_media.py,154,... it should store the Twisted Failure ...,
scrapy/tests/test_pipeline_media.py,156,... encapsulating the original FileException ...,
scrapy/tests/test_pipeline_media.py,158,... but it should not store the returnValue exception on its context,
scrapy/tests/test_pipeline_media.py,240,only once,
scrapy/tests/test_pipeline_media.py,241,first hook called,
scrapy/tests/test_pipeline_media.py,244,last hook called,
scrapy/tests/test_pipeline_media.py,245,"twice, one per request",
scrapy/tests/test_pipeline_media.py,247,one to handle success and other for failure,
scrapy/tests/test_pipeline_media.py,253,returns single Request (without callback),
scrapy/tests/test_pipeline_media.py,255,pass a single item,
scrapy/tests/test_pipeline_media.py,260,returns iterable of Requests,
scrapy/tests/test_pipeline_media.py,278,"rsp2 is ignored, rsp1 must be in results because request fingerprints are the same",
scrapy/tests/test_pipeline_media.py,344,These are the status codes we want,
scrapy/tests/test_pipeline_media.py,345,the downloader to handle itself,
scrapy/tests/test_pipeline_media.py,352,we still want to get 4xx and 5xx,
scrapy/tests/test_utils_trackref.py,22,NOQA,
scrapy/tests/test_utils_trackref.py,23,NOQA,
scrapy/tests/test_utils_trackref.py,24,NOQA,
scrapy/tests/test_utils_trackref.py,49,NOQA,
scrapy/tests/test_utils_trackref.py,57,NOQA,
scrapy/tests/test_utils_trackref.py,58,NOQA,
scrapy/tests/test_utils_trackref.py,59,NOQA,
scrapy/tests/test_utils_trackref.py,65,NOQA,
scrapy/tests/test_utils_trackref.py,66,NOQA,
scrapy/tests/test_utils_trackref.py,67,NOQA,
scrapy/tests/test_request_cb_kwargs.py,154,check exceptions for argument mismatch,
scrapy/tests/test_utils_conf.py,57,Higher priority takes precedence,
scrapy/tests/test_utils_conf.py,67,Same priority raises ValueError,
scrapy/tests/test_utils_conf.py,73,work well with None and numeric values,
scrapy/tests/test_utils_conf.py,80,raise exception for invalid values,
scrapy/tests/test_utils_console.py,39,default shell should be 'ipython',
scrapy/tests/test_downloadermiddleware.py,26,some mw depends on stats collector,
scrapy/tests/test_downloadermiddleware.py,46,catch deferred result and return the value,
scrapy/tests/test_pipeline_files.py,245,Values from settings for custom pipeline should be set on pipeline instance.,
scrapy/tests/test_pipeline_files.py,261,Values from settings for custom pipeline should be set on pipeline instance.,
scrapy/tests/test_contracts.py,232,extract contracts correctly,
scrapy/tests/test_contracts.py,238,returns request for valid method,
scrapy/tests/test_contracts.py,242,no request for missing url,
scrapy/tests/test_contracts.py,250,extract contracts correctly,
scrapy/tests/test_contracts.py,271,returns_request,
scrapy/tests/test_contracts.py,276,returns_item,
scrapy/tests/test_contracts.py,281,"returns_item (error, callback doesn't take keyword arguments)",
scrapy/tests/test_contracts.py,286,"returns_item (error, contract doesn't provide keyword arguments)",
scrapy/tests/test_contracts.py,295,returns_item,
scrapy/tests/test_contracts.py,300,returns_dict_item,
scrapy/tests/test_contracts.py,305,returns_request,
scrapy/tests/test_contracts.py,310,returns_fail,
scrapy/tests/test_contracts.py,315,returns_dict_fail,
scrapy/tests/test_contracts.py,324,scrapes_item_ok,
scrapy/tests/test_contracts.py,329,scrapes_dict_item_ok,
scrapy/tests/test_contracts.py,334,scrapes_item_fail,
scrapy/tests/test_contracts.py,339,scrapes_dict_item_fail,
scrapy/tests/test_contracts.py,344,scrapes_multiple_missing_fields,
scrapy/tests/spiders.py,244,no need to retry,
scrapy/tests/test_utils_datatypes.py,227,"supplied sequence is a set, so checking for list (non)inclusion fails",
scrapy/tests/test_utils_datatypes.py,278,PyPy takes longer to collect dead references,
scrapy/tests/test_utils_datatypes.py,307,delete reference to the last object in the list,
scrapy/tests/test_utils_datatypes.py,309,"delete half of the objects, make sure that is reflected in the cache",
scrapy/tests/test_utils_datatypes.py,313,PyPy takes longer to collect dead references,
scrapy/tests/test_downloadermiddleware_httpcache.py,112,content-type header,
scrapy/tests/test_downloadermiddleware_httpcache.py,115,wait for cache to expire,
scrapy/tests/test_downloadermiddleware_httpcache.py,122,give the chance to expire,
scrapy/tests/test_downloadermiddleware_httpcache.py,140,make sure our dbm module has been loaded,
scrapy/tests/test_downloadermiddleware_httpcache.py,191,http responses are cached by default,
scrapy/tests/test_downloadermiddleware_httpcache.py,202,file response is not cached by default,
scrapy/tests/test_downloadermiddleware_httpcache.py,211,s3 scheme response is cached by default,
scrapy/tests/test_downloadermiddleware_httpcache.py,222,ignore s3 scheme,
scrapy/tests/test_downloadermiddleware_httpcache.py,232,test response is not cached,
scrapy/tests/test_downloadermiddleware_httpcache.py,240,test response is cached,
scrapy/tests/test_downloadermiddleware_httpcache.py,277,response for a request with no-store must not be cached,
scrapy/tests/test_downloadermiddleware_httpcache.py,281,Re-do request without no-store and expect it to be cached,
scrapy/tests/test_downloadermiddleware_httpcache.py,287,request with no-cache directive must not return cached response,
scrapy/tests/test_downloadermiddleware_httpcache.py,288,but it allows new response to be stored,
scrapy/tests/test_downloadermiddleware_httpcache.py,299,304 is not cacheable no matter what servers sends,
scrapy/tests/test_downloadermiddleware_httpcache.py,305,Always obey no-store cache control,
scrapy/tests/test_downloadermiddleware_httpcache.py,307,invalid,
scrapy/tests/test_downloadermiddleware_httpcache.py,308,invalid,
scrapy/tests/test_downloadermiddleware_httpcache.py,309,Ignore responses missing expiration and/or validation headers,
scrapy/tests/test_downloadermiddleware_httpcache.py,314,Cache responses with expiration and/or validation headers,
scrapy/tests/test_downloadermiddleware_httpcache.py,342,cache unconditionally unless response contains no-store or is a 304,
scrapy/tests/test_downloadermiddleware_httpcache.py,366,Obey max-age if present over any others,
scrapy/tests/test_downloadermiddleware_httpcache.py,373,obey Expires if max-age is not present,
scrapy/tests/test_downloadermiddleware_httpcache.py,380,Default missing Date header to right now,
scrapy/tests/test_downloadermiddleware_httpcache.py,382,Firefox - Expires if age is greater than 10% of (Date - Last-Modified),
scrapy/tests/test_downloadermiddleware_httpcache.py,384,Firefox - Set one year maxage to permanent redirects missing expiration info,
scrapy/tests/test_downloadermiddleware_httpcache.py,391,cache fresh response,
scrapy/tests/test_downloadermiddleware_httpcache.py,395,return fresh cached response without network interaction,
scrapy/tests/test_downloadermiddleware_httpcache.py,399,validate cached response if request max-age set as 0,
scrapy/tests/test_downloadermiddleware_httpcache.py,416,"no-cache forces expiration, also revalidation if validators exists",
scrapy/tests/test_downloadermiddleware_httpcache.py,428,cache expired response,
scrapy/tests/test_downloadermiddleware_httpcache.py,432,Same request but as cached response is stale a new response must,
scrapy/tests/test_downloadermiddleware_httpcache.py,433,be returned,
scrapy/tests/test_downloadermiddleware_httpcache.py,439,"Previous response expired too, subsequent request to same",
scrapy/tests/test_downloadermiddleware_httpcache.py,440,resource must revalidate and succeed on 304 if validators,
scrapy/tests/test_downloadermiddleware_httpcache.py,441,are present,
scrapy/tests/test_downloadermiddleware_httpcache.py,447,get cached response on server errors unless must-revalidate,
scrapy/tests/test_downloadermiddleware_httpcache.py,448,in cached response,
scrapy/tests/test_downloadermiddleware_httpcache.py,457,Requests with max-stale can fetch expired cached responses,
scrapy/tests/test_downloadermiddleware_httpcache.py,458,unless cached response has must-revalidate,
scrapy/tests/test_downloadermiddleware_httpcache.py,473,Simulate encountering an error on download attempts,
scrapy/tests/test_downloadermiddleware_httpcache.py,476,Use cached response as recovery,
scrapy/tests/test_downloadermiddleware_httpcache.py,479,Do not use cached response for unhandled exceptions,
scrapy/tests/test_downloadermiddleware_httpcache.py,495,cache fresh response,
scrapy/tests/test_downloadermiddleware_httpcache.py,499,return fresh cached response without network interaction,
scrapy/tests/test_crawl.py,36,10 + start_url,
scrapy/tests/test_crawl.py,65,Ensure that the same test parameters would cause a failure if no,
scrapy/tests/test_crawl.py,66,"download delay is set. Otherwise, it means we are using a combination",
scrapy/tests/test_crawl.py,67,of ``total`` and ``delay`` values that are too small for the test,
scrapy/tests/test_crawl.py,68,code above to have any meaning.,
scrapy/tests/test_crawl.py,94,server hangs after receiving response headers,
scrapy/tests/test_crawl.py,119,try to fetch the homepage of a non-existent domain,
scrapy/tests/test_crawl.py,150,"self.assertTrue(False, crawler.spider.seedsseen)",
scrapy/tests/test_crawl.py,151,"self.assertTrue(crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),",
scrapy/tests/test_crawl.py,152,crawler.spider.seedsseen),
scrapy/tests/test_crawl.py,166,Completeness of responses without Content-Length or Transfer-Encoding,
scrapy/tests/test_crawl.py,167,"can not be determined, we treat them as valid but flagged as ""partial""",
scrapy/tests/test_crawl.py,193,connection lost after receiving data,
scrapy/tests/test_crawl.py,201,connection lost before receiving data,
scrapy/tests/test_crawl.py,223,basic asserts in case of weird communication errors,
scrapy/tests/test_crawl.py,226,start requests doesn't set Referer header,
scrapy/tests/test_crawl.py,229,following request sets Referer to start request url,
scrapy/tests/test_crawl.py,232,next request avoids Referer header,
scrapy/tests/test_crawl.py,235,last request explicitly sets a Referer header,
scrapy/tests/test_crawl.py,397,some random items,
scrapy/tests/test_engine.py,71,no dont_filter=True,
scrapy/tests/test_engine.py,118,a duplicate,
scrapy/tests/test_engine.py,231,response tests,
scrapy/tests/test_pipeline_images.py,79,straigh forward case: RGB and JPEG,
scrapy/tests/test_pipeline_images.py,86,check that thumbnail keep image ratio,
scrapy/tests/test_pipeline_images.py,91,transparency case: RGBA and PNG,
scrapy/tests/test_pipeline_images.py,98,transparency case with palette: P and PNG,
scrapy/tests/test_pipeline_images.py,161,Pipeline attribute names with corresponding setting names.,
scrapy/tests/test_pipeline_images.py,170,This should match what is defined in ImagesPipeline.,
scrapy/tests/test_pipeline_images.py,217,Values should be in different range than fake_settings.,
scrapy/tests/test_pipeline_images.py,254,Instance attribute (lowercase) must be equal to class attribute (uppercase).,
scrapy/tests/test_pipeline_images.py,268,Instance attribute (lowercase) must be equal to,
scrapy/tests/test_pipeline_images.py,269,value defined in settings.,
scrapy/tests/test_pipeline_images.py,285,Values from settings for custom pipeline should be set on pipeline instance.,
scrapy/tests/test_pipeline_images.py,301,Values from settings for custom pipeline should be set on pipeline instance.,
scrapy/tests/test_selector.py,55,u'\xa3'     = pound symbol in unicode,
scrapy/tests/test_selector.py,56,u'\xc2\xa3' = pound symbol in utf-8,
scrapy/tests/test_selector.py,57,u'\xa3'     = pound symbol in latin-1 (iso-8859-1),
scrapy/tests/test_selector.py,74,\xe9 alone isn't valid utf8 sequence,
scrapy/tests/test_utils_curl.py,186,case 1: ignore_unknown_options=True:,
scrapy/tests/test_utils_curl.py,187,avoid warning when executing tests,
scrapy/tests/test_utils_curl.py,194,case 2: ignore_unknown_options=False (raise exception):,
scrapy/tests/test_feedexport.py,104,RFC3986: 3.2.1. User Information,
scrapy/tests/test_feedexport.py,119,"again, to check s3 objects are overwritten",
scrapy/tests/test_feedexport.py,170,noqa: F401,
scrapy/tests/test_feedexport.py,176,Instantiate with crawler,
scrapy/tests/test_feedexport.py,181,Instantiate directly,
scrapy/tests/test_feedexport.py,187,URI priority > settings priority,
scrapy/tests/test_feedexport.py,193,Backward compatibility for initialising without settings,
scrapy/tests/test_feedexport.py,271,noqa: F401,
scrapy/tests/test_feedexport.py,291,noqa: F401,
scrapy/tests/test_feedexport.py,524,XML,
scrapy/tests/test_feedexport.py,528,JSON,
scrapy/tests/test_feedexport.py,583,feed exporters use field names from Item,
scrapy/tests/test_feedexport.py,640,"by default, Scrapy uses fields of the first Item for CSV and",
scrapy/tests/test_feedexport.py,641,all fields for JSON Lines,
scrapy/tests/test_feedexport.py,653,edge case: FEED_EXPORT_FIELDS==[] means the same as default None,
scrapy/tests/test_feedexport.py,658,it is possible to override fields using FEED_EXPORT_FIELDS,
scrapy/tests/test_feedexport.py,672,"When dicts are used, only keys from the first row are used as",
scrapy/tests/test_feedexport.py,673,"a header for CSV, and all fields are used for JSON Lines.",
scrapy/tests/test_feedexport.py,688,FEED_EXPORT_FIELDS option allows to order export fields,
scrapy/tests/test_feedexport.py,689,"and to select a subset of fields to export, both for Items and dicts.",
scrapy/tests/test_feedexport.py,697,export all columns,
scrapy/tests/test_feedexport.py,706,export a subset of columns,
scrapy/tests/test_feedexport.py,800,JSON,
scrapy/tests/test_feedexport.py,865,XML,
scrapy/tests/test_downloadermiddleware_retry.py,31,dont retry 404s,
scrapy/tests/test_downloadermiddleware_retry.py,38,first retry,
scrapy/tests/test_downloadermiddleware_retry.py,42,Test retry when dont_retry set to False,
scrapy/tests/test_downloadermiddleware_retry.py,46,first retry,
scrapy/tests/test_downloadermiddleware_retry.py,60,first retry,
scrapy/tests/test_downloadermiddleware_retry.py,65,second retry,
scrapy/tests/test_downloadermiddleware_retry.py,70,discard it,
scrapy/tests/test_downloadermiddleware_retry.py,92,first retry,
scrapy/tests/test_downloadermiddleware_retry.py,97,second retry,
scrapy/tests/test_downloadermiddleware_retry.py,102,discard it,
scrapy/tests/test_downloadermiddleware_retry.py,117,SETTINGS: RETRY_TIMES = 0,
scrapy/tests/test_downloadermiddleware_retry.py,125,SETTINGS: meta(max_retry_times) = 0,
scrapy/tests/test_downloadermiddleware_retry.py,133,SETTINGS: RETRY_TIMES is NON-ZERO,
scrapy/tests/test_downloadermiddleware_retry.py,141,SETINGS: RETRY_TIMES < meta(max_retry_times),
scrapy/tests/test_downloadermiddleware_retry.py,153,SETINGS: RETRY_TIMES > meta(max_retry_times),
scrapy/tests/test_downloadermiddleware_retry.py,165,SETTINGS: meta(max_retry_times) = 4,
scrapy/tests/test_downloadermiddleware_retry.py,180,discard it,
scrapy/tests/__init__.py,9,ignore system-wide proxies for tests,
scrapy/tests/__init__.py,10,which would send requests to a totally unsuspecting server,
scrapy/tests/__init__.py,11,(e.g. because urllib does not fully understand the proxy spec),
scrapy/tests/__init__.py,16,Absolutize paths to coverage config and output file because tests that,
scrapy/tests/__init__.py,17,spawn subprocesses also changes current working directory.,
scrapy/tests/test_spidermiddleware_output_chain.py,17,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,18,(0) recover from an exception on a spider callback,
scrapy/tests/test_spidermiddleware_output_chain.py,46,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,47,(1) exceptions from a spider middleware's process_spider_input method,
scrapy/tests/test_spidermiddleware_output_chain.py,58,spider,
scrapy/tests/test_spidermiddleware_output_chain.py,62,engine,
scrapy/tests/test_spidermiddleware_output_chain.py,84,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,85,(2) exceptions from a spider callback (generator),
scrapy/tests/test_spidermiddleware_output_chain.py,103,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,104,"(2.1) exceptions from a spider callback (generator, middleware right after callback)",
scrapy/tests/test_spidermiddleware_output_chain.py,114,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,115,(3) exceptions from a spider callback (not a generator),
scrapy/tests/test_spidermiddleware_output_chain.py,131,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,132,"(3.1) exceptions from a spider callback (not a generator, middleware right after callback)",
scrapy/tests/test_spidermiddleware_output_chain.py,142,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,143,(4) exceptions from a middleware process_spider_output method (generator),
scrapy/tests/test_spidermiddleware_output_chain.py,208,================================================================================,
scrapy/tests/test_spidermiddleware_output_chain.py,209,(5) exceptions from a middleware process_spider_output method (not generator),
scrapy/tests/test_spidermiddleware_output_chain.py,279,================================================================================,
scrapy/tests/test_downloader_handlers.py,46,Default is lazy for backward compatibility,
scrapy/tests/test_downloader_handlers.py,84,force load handlers,
scrapy/tests/test_downloader_handlers.py,95,force load lazy handler,
scrapy/tests/test_downloader_handlers.py,155,Disable terminating chunk on finish.,
scrapy/tests/test_downloader_handlers.py,175,We have to force a disconnection for HTTP/1.1 clients. Otherwise,
scrapy/tests/test_downloader_handlers.py,176,client keeps the connection open waiting for more data.,
scrapy/tests/test_downloader_handlers.py,177,twisted >=16.3.0,
scrapy/tests/test_downloader_handlers.py,209,only used for HTTPS tests,
scrapy/tests/test_downloader_handlers.py,284,client connects but no data is received,
scrapy/tests/test_downloader_handlers.py,293,"client connects, server send headers and some body bytes but hangs",
scrapy/tests/test_downloader_handlers.py,351,PayloadResource requires body length to be 100,
scrapy/tests/test_downloader_handlers.py,397,10 is minimal size for this request and the limit is only counted on,
scrapy/tests/test_downloader_handlers.py,398,response body. (regardless of headers),
scrapy/tests/test_downloader_handlers.py,418,"As the error message is logged in the dataReceived callback, we",
scrapy/tests/test_downloader_handlers.py,419,have to give a bit of time to the reactor to process the queue,
scrapy/tests/test_downloader_handlers.py,420,after closing the connection.,
scrapy/tests/test_downloader_handlers.py,517,"above tests use a server certificate for ""localhost"",",
scrapy/tests/test_downloader_handlers.py,518,"client connection to ""localhost"" too.",
scrapy/tests/test_downloader_handlers.py,519,"here we test that even if the server certificate is for another domain,",
scrapy/tests/test_downloader_handlers.py,520,"""www.example.com"" in this case,",
scrapy/tests/test_downloader_handlers.py,521,the tests still pass,
scrapy/tests/test_downloader_handlers.py,542,noqa: F401,
scrapy/tests/test_downloader_handlers.py,603,"http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid",
scrapy/tests/test_downloader_handlers.py,604,download it,
scrapy/tests/test_downloader_handlers.py,621,PayloadResource requires body length to be 100,
scrapy/tests/test_downloader_handlers.py,626,"download_maxsize < 100, hence the CancelledError",
scrapy/tests/test_downloader_handlers.py,629,See issue https://twistedmatrix.com/trac/ticket/8175,
scrapy/tests/test_downloader_handlers.py,634,download_maxsize = 50 is enough for the gzipped response,
scrapy/tests/test_downloader_handlers.py,648,Note: this is an ugly hack for CONNECT request timeout test.,
scrapy/tests/test_downloader_handlers.py,649,Returning some data here fail SSL/TLS handshake,
scrapy/tests/test_downloader_handlers.py,650,"ToDo: implement proper HTTPS proxy tests, not faking them.",
scrapy/tests/test_downloader_handlers.py,747,"anon=True, # implicit",
scrapy/tests/test_downloader_handlers.py,764,test use same example keys than amazon developer guide,
scrapy/tests/test_downloader_handlers.py,765,http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf,
scrapy/tests/test_downloader_handlers.py,766,and the tests described here are the examples from that manual,
scrapy/tests/test_downloader_handlers.py,788,noqa: F401,
scrapy/tests/test_downloader_handlers.py,792,"We need to mock botocore.auth.formatdate, because otherwise",
scrapy/tests/test_downloader_handlers.py,793,botocore overrides Date header with current date and time,
scrapy/tests/test_downloader_handlers.py,794,and Authorization header is different each time,
scrapy/tests/test_downloader_handlers.py,814,gets an object from the johnsmith bucket.,
scrapy/tests/test_downloader_handlers.py,823,puts an object into the johnsmith bucket.,
scrapy/tests/test_downloader_handlers.py,836,lists the content of the johnsmith bucket.,
scrapy/tests/test_downloader_handlers.py,850,fetches the access control policy sub-resource for the 'johnsmith' bucket.,
scrapy/tests/test_downloader_handlers.py,861,noqa: F401,
scrapy/tests/test_downloader_handlers.py,867,deletes an object from the 'johnsmith' bucket using the,
scrapy/tests/test_downloader_handlers.py,868,path-style and Date alternative.,
scrapy/tests/test_downloader_handlers.py,877,botocore does not override Date with x-amz-date,
scrapy/tests/test_downloader_handlers.py,882,uploads an object to a CNAME style virtual hosted bucket with metadata.,
scrapy/tests/test_downloader_handlers.py,906,ensure that spaces are quoted properly before signing,
scrapy/tests/test_downloader_handlers.py,931,setup dirs and test file,
scrapy/tests/test_downloader_handlers.py,940,setup server,
scrapy/tests/test_downloader_handlers.py,1046,setup dir and test file,
scrapy/tests/test_downloader_handlers.py,1054,setup server for anonymous access,
scrapy/tests/test_http_cookies.py,71,get_all result must be native string,
scrapy/tests/test_downloadermiddleware_cookies.py,116,merge some cookies into jar,
scrapy/tests/test_downloadermiddleware_cookies.py,122,test Cookie header is not seted to request,
scrapy/tests/test_downloadermiddleware_cookies.py,127,check that returned cookies are not merged back to jar,
scrapy/tests/test_downloadermiddleware_cookies.py,131,check that cookies are merged back,
scrapy/tests/test_downloadermiddleware_cookies.py,136,check that cookies are merged when dont_merge_cookies is passed as 0,
scrapy/tests/test_downloadermiddleware_cookies.py,142,merge some cookies into jar,
scrapy/tests/test_downloadermiddleware_cookies.py,151,embed C1 and C3 for scrapytest.org/foo,
scrapy/tests/test_downloadermiddleware_cookies.py,156,embed C2 for scrapytest.org/bar,
scrapy/tests/test_downloadermiddleware_cookies.py,161,embed nothing for scrapytest.org/baz,
scrapy/tests/test_downloadermiddleware_cookies.py,205,cookies from hosts with port,
scrapy/tests/test_downloadermiddleware_cookies.py,221,skip cookie retrieval for not http request,
scrapy/tests/test_linkextractors.py,13,a hack to skip base class tests in pytest,
scrapy/tests/test_linkextractors.py,265,jpg is ignored by default,
scrapy/tests/test_linkextractors.py,273,override denied extensions,
scrapy/tests/test_linkextractors.py,483,Simple text inclusion test,
scrapy/tests/test_linkextractors.py,488,Unique regex test,
scrapy/tests/test_linkextractors.py,493,Multiple regex test,
scrapy/tests/test_http_response.py,1,-*- coding: utf-8 -*-,
scrapy/tests/test_http_response.py,20,Response requires url in the consturctor,
scrapy/tests/test_http_response.py,24,body can be str or None,
scrapy/tests/test_http_response.py,27,test presence of all optional parameters,
scrapy/tests/test_http_response.py,61,make sure flags list is shallow copied,
scrapy/tests/test_http_response.py,65,make sure headers attribute is shallow copied,
scrapy/tests/test_http_response.py,113,Empty attributes (which may fail if not compared properly),
scrapy/tests/test_http_response.py,160,Response.follow,
scrapy/tests/test_http_response.py,191,Response.follow_all,
scrapy/tests/test_http_response.py,314,instantiate with unicode url without encoding (should set default encoding),
scrapy/tests/test_http_response.py,318,make sure urls are converted to str,
scrapy/tests/test_http_response.py,338,check body_as_unicode,
scrapy/tests/test_http_response.py,342,check response.text,
scrapy/tests/test_http_response.py,371,TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies,
scrapy/tests/test_http_response.py,395,w3lib < 1.19.0,
scrapy/tests/test_http_response.py,396,w3lib >= 1.19.0,
scrapy/tests/test_http_response.py,400,"Inferring encoding from body also cache decoded body as sideeffect,",
scrapy/tests/test_http_response.py,401,this test tries to ensure that calling response.encoding and,
scrapy/tests/test_http_response.py,402,response.text in indistint order doesn't affect final,
scrapy/tests/test_http_response.py,403,values for encoding and decoded body.,
scrapy/tests/test_http_response.py,408,Test response without content-type and BOM encoding,
scrapy/tests/test_http_response.py,416,Body caching sideeffect isn't triggered when encoding is declared in,
scrapy/tests/test_http_response.py,417,content-type header but BOM still need to be removed from decoded,
scrapy/tests/test_http_response.py,418,body,
scrapy/tests/test_http_response.py,429,XXX: Policy for replacing invalid chars may suffer minor variations,
scrapy/tests/test_http_response.py,430,but it should always contain the unicode replacement char (u'\ufffd'),
scrapy/tests/test_http_response.py,435,Do not destroy html tags due to encoding bugs,
scrapy/tests/test_http_response.py,440,FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse,
scrapy/tests/test_http_response.py,441,"r = self.response_class(""http://www.example.com"", body=b'PREFIX\xe3\xabSUFFIX')",
scrapy/tests/test_http_response.py,442,"assert u'\ufffd' in r.text, repr(r.text)",
scrapy/tests/test_http_response.py,450,property is cached,
scrapy/tests/test_http_response.py,521,select <a> elements,
scrapy/tests/test_http_response.py,526,select <link> elements,
scrapy/tests/test_http_response.py,533,href attributes should work,
scrapy/tests/test_http_response.py,538,non-a elements are not supported,
scrapy/tests/test_http_response.py,684,for conflicting declarations headers must take precedence,
scrapy/tests/test_http_response.py,691,make sure replace() preserves the encoding of the original response,
scrapy/tests/test_http_response.py,715,make sure replace() preserves the explicit encoding passed in the __init__ method,
scrapy/tests/test_http_response.py,723,make sure replace() keeps the previous encoding unless overridden explicitly,
scrapy/tests/test_http_response.py,739,property is cached,
scrapy/tests/test_spidermiddleware_httperror.py,47,it assumes there is a response attached to failure,
scrapy/tests/test_spidermiddleware_httperror.py,200,HttpError logs ignored responses with level INFO,
scrapy/tests/test_spidermiddleware_httperror.py,212,"with level WARNING, we shouldn't capture anything from HttpError",
scrapy/tests/test_utils_project.py,21,create an empty scrapy.cfg,
scrapy/tests/test_spidermiddleware.py,28,catch deferred result and return the value,
scrapy/tests/test_command_fetch.py,32,required on win32,
scrapy/tests/test_spiderstate.py,36,"state attribute must be present if jobdir is not set, to provide a",
scrapy/tests/test_spiderstate.py,37,consistent interface,
scrapy/tests/test_exporters.py,203,datetime is not marshallable,
scrapy/tests/mocks/dummydbm.py,22,return same instance for same file argument,
scrapy/tests/test_spiderloader/__init__.py,10,ugly hack to avoid cyclic imports of scrapy.spiders when running this test,
scrapy/tests/test_spiderloader/__init__.py,11,alone,
scrapy/tests/test_spiderloader/__init__.py,129,copy 1 spider module so as to have duplicate spider name,
scrapy/tests/test_spiderloader/__init__.py,145,copy 2 spider modules so as to have duplicate spider name,
scrapy/tests/test_spiderloader/__init__.py,146,"This should issue 2 warning, 1 for each duplicate spider name",
scrapy/tests/test_utils_misc/test_return_with_argument_inside_generator.py,37,not recursive,
scrapy/tests/test_utils_misc/__init__.py,112,Check usage of correct constructor using four mocks:,
scrapy/tests/test_utils_misc/__init__.py,113,1. with no alternative constructors,
scrapy/tests/test_utils_misc/__init__.py,114,2. with from_settings() constructor,
scrapy/tests/test_utils_misc/__init__.py,115,3. with from_crawler() constructor,
scrapy/tests/test_utils_misc/__init__.py,116,4. with from_settings() and from_crawler() constructor,
scrapy/tests/test_utils_misc/__init__.py,125,Check adoption of crawler settings,
scrapy/tests/test_settings/__init__.py,49,Insufficient priority,
scrapy/tests/test_settings/__init__.py,74,Note priority 30,
scrapy/tests/test_settings/__init__.py,283,Empty settings should return 'default',
scrapy/tests/test_cmdline/__init__.py,60,XXX: There's gotta be a smarter way to do this...,
scrapy/docs/conf.py,1,-*- coding: utf-8 -*-,
scrapy/docs/conf.py,2,,
scrapy/docs/conf.py,3,"Scrapy documentation build configuration file, created by",
scrapy/docs/conf.py,4,sphinx-quickstart on Mon Nov 24 12:02:52 2008.,
scrapy/docs/conf.py,5,,
scrapy/docs/conf.py,6,This file is execfile()d with the current directory set to its containing dir.,
scrapy/docs/conf.py,7,,
scrapy/docs/conf.py,8,"The contents of this file are pickled, so don't put values in the namespace",
scrapy/docs/conf.py,9,"that aren't pickleable (module imports are okay, they're removed automatically).",
scrapy/docs/conf.py,10,,
scrapy/docs/conf.py,11,All configuration values have a default; values that are commented out,
scrapy/docs/conf.py,12,serve to show the default.,
scrapy/docs/conf.py,18,"If your extensions are in another directory, add it here. If the directory",
scrapy/docs/conf.py,19,"is relative to the documentation root, use os.path.abspath to make it",
scrapy/docs/conf.py,20,"absolute, like shown here.",
scrapy/docs/conf.py,25,General configuration,
scrapy/docs/conf.py,26,---------------------,
scrapy/docs/conf.py,28,"Add any Sphinx extension module names here, as strings. They can be extensions",
scrapy/docs/conf.py,29,coming with Sphinx (named 'sphinx.ext.*') or your custom ones.,
scrapy/docs/conf.py,40,"Add any paths that contain templates here, relative to this directory.",
scrapy/docs/conf.py,43,The suffix of source filenames.,
scrapy/docs/conf.py,46,The encoding of source files.,
scrapy/docs/conf.py,47,source_encoding = 'utf-8',
scrapy/docs/conf.py,49,The master toctree document.,
scrapy/docs/conf.py,52,General information about the project.,
scrapy/docs/conf.py,56,"The version info for the project you're documenting, acts as replacement for",
scrapy/docs/conf.py,57,"|version| and |release|, also used in various other places throughout the",
scrapy/docs/conf.py,58,built documents.,
scrapy/docs/conf.py,59,,
scrapy/docs/conf.py,60,The short X.Y version.,
scrapy/docs/conf.py,69,The language for content autogenerated by Sphinx. Refer to documentation,
scrapy/docs/conf.py,70,for a list of supported languages.,
scrapy/docs/conf.py,73,"There are two options for replacing |today|: either, you set today to some",
scrapy/docs/conf.py,74,"non-false value, then it is used:",
scrapy/docs/conf.py,75,today = '',
scrapy/docs/conf.py,76,"Else, today_fmt is used as the format for a strftime call.",
scrapy/docs/conf.py,77,"today_fmt = '%B %d, %Y'",
scrapy/docs/conf.py,79,List of documents that shouldn't be included in the build.,
scrapy/docs/conf.py,80,unused_docs = [],
scrapy/docs/conf.py,84,"List of directories, relative to source directory, that shouldn't be searched",
scrapy/docs/conf.py,85,for source files.,
scrapy/docs/conf.py,88,The reST default role (used for this markup: `text`) to use for all documents.,
scrapy/docs/conf.py,89,default_role = None,
scrapy/docs/conf.py,91,"If true, '()' will be appended to :func: etc. cross-reference text.",
scrapy/docs/conf.py,92,add_function_parentheses = True,
scrapy/docs/conf.py,94,"If true, the current module name will be prepended to all description",
scrapy/docs/conf.py,95,unit titles (such as .. function::).,
scrapy/docs/conf.py,96,add_module_names = True,
scrapy/docs/conf.py,98,"If true, sectionauthor and moduleauthor directives will be shown in the",
scrapy/docs/conf.py,99,output. They are ignored by default.,
scrapy/docs/conf.py,100,show_authors = False,
scrapy/docs/conf.py,102,The name of the Pygments (syntax highlighting) style to use.,
scrapy/docs/conf.py,106,Options for HTML output,
scrapy/docs/conf.py,107,-----------------------,
scrapy/docs/conf.py,109,The theme to use for HTML and HTML Help pages.  See the documentation for,
scrapy/docs/conf.py,110,a list of builtin themes.,
scrapy/docs/conf.py,113,Theme options are theme-specific and customize the look and feel of a theme,
scrapy/docs/conf.py,114,"further.  For a list of options available for each theme, see the",
scrapy/docs/conf.py,115,documentation.,
scrapy/docs/conf.py,116,html_theme_options = {},
scrapy/docs/conf.py,118,"Add any paths that contain custom themes here, relative to this directory.",
scrapy/docs/conf.py,119,Add path to the RTD explicitly to robustify builds (otherwise might,
scrapy/docs/conf.py,120,fail in a clean Debian build env),
scrapy/docs/conf.py,125,The style sheet to use for HTML and HTML Help pages. A file of that name,
scrapy/docs/conf.py,126,"must exist either in Sphinx' static/ path, or in one of the custom paths",
scrapy/docs/conf.py,127,given in html_static_path.,
scrapy/docs/conf.py,128,html_style = 'scrapydoc.css',
scrapy/docs/conf.py,130,"The name for this set of Sphinx documents.  If None, it defaults to",
scrapy/docs/conf.py,131,"""<project> v<release> documentation"".",
scrapy/docs/conf.py,132,html_title = None,
scrapy/docs/conf.py,134,A shorter title for the navigation bar.  Default is the same as html_title.,
scrapy/docs/conf.py,135,html_short_title = None,
scrapy/docs/conf.py,137,The name of an image file (relative to this directory) to place at the top,
scrapy/docs/conf.py,138,of the sidebar.,
scrapy/docs/conf.py,139,html_logo = None,
scrapy/docs/conf.py,141,The name of an image file (within the static path) to use as favicon of the,
scrapy/docs/conf.py,142,docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32,
scrapy/docs/conf.py,143,pixels large.,
scrapy/docs/conf.py,144,html_favicon = None,
scrapy/docs/conf.py,146,"Add any paths that contain custom static files (such as style sheets) here,",
scrapy/docs/conf.py,147,"relative to this directory. They are copied after the builtin static files,",
scrapy/docs/conf.py,148,"so a file named ""default.css"" will overwrite the builtin ""default.css"".",
scrapy/docs/conf.py,151,"If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",
scrapy/docs/conf.py,152,using the given strftime format.,
scrapy/docs/conf.py,155,"Custom sidebar templates, maps document names to template names.",
scrapy/docs/conf.py,156,html_sidebars = {},
scrapy/docs/conf.py,158,"Additional templates that should be rendered to pages, maps page names to",
scrapy/docs/conf.py,159,template names.,
scrapy/docs/conf.py,160,html_additional_pages = {},
scrapy/docs/conf.py,162,"If false, no module index is generated.",
scrapy/docs/conf.py,163,html_use_modindex = True,
scrapy/docs/conf.py,165,"If false, no index is generated.",
scrapy/docs/conf.py,166,html_use_index = True,
scrapy/docs/conf.py,168,"If true, the index is split into individual pages for each letter.",
scrapy/docs/conf.py,169,html_split_index = False,
scrapy/docs/conf.py,171,"If true, the reST sources are included in the HTML build as _sources/<name>.",
scrapy/docs/conf.py,174,"If true, an OpenSearch description file will be output, and all pages will",
scrapy/docs/conf.py,175,contain a <link> tag referring to it.  The value of this option must be the,
scrapy/docs/conf.py,176,base URL from which the finished HTML is served.,
scrapy/docs/conf.py,177,html_use_opensearch = '',
scrapy/docs/conf.py,179,"If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").",
scrapy/docs/conf.py,180,html_file_suffix = '',
scrapy/docs/conf.py,182,Output file base name for HTML help builder.,
scrapy/docs/conf.py,186,Options for LaTeX output,
scrapy/docs/conf.py,187,------------------------,
scrapy/docs/conf.py,189,The paper size ('letter' or 'a4').,
scrapy/docs/conf.py,190,latex_paper_size = 'letter',
scrapy/docs/conf.py,192,"The font size ('10pt', '11pt' or '12pt').",
scrapy/docs/conf.py,193,latex_font_size = '10pt',
scrapy/docs/conf.py,195,Grouping the document tree into LaTeX files. List of tuples,
scrapy/docs/conf.py,196,"(source start file, target name, title, author, document class [howto/manual]).",
scrapy/docs/conf.py,202,The name of an image file (relative to this directory) to place at the top of,
scrapy/docs/conf.py,203,the title page.,
scrapy/docs/conf.py,204,latex_logo = None,
scrapy/docs/conf.py,206,"For ""manual"" documents, if this is true, then toplevel headings are parts,",
scrapy/docs/conf.py,207,not chapters.,
scrapy/docs/conf.py,208,latex_use_parts = False,
scrapy/docs/conf.py,210,Additional stuff for the LaTeX preamble.,
scrapy/docs/conf.py,211,latex_preamble = '',
scrapy/docs/conf.py,213,Documents to append as an appendix to all manuals.,
scrapy/docs/conf.py,214,latex_appendices = [],
scrapy/docs/conf.py,216,"If false, no module index is generated.",
scrapy/docs/conf.py,217,latex_use_modindex = True,
scrapy/docs/conf.py,220,Options for the linkcheck builder,
scrapy/docs/conf.py,221,---------------------------------,
scrapy/docs/conf.py,223,A list of regular expressions that match URIs that should not be checked when,
scrapy/docs/conf.py,224,doing a linkcheck build.,
scrapy/docs/conf.py,231,Options for the Coverage extension,
scrapy/docs/conf.py,232,----------------------------------,
scrapy/docs/conf.py,234,Contracts add_pre_hook and add_post_hook are not documented because,
scrapy/docs/conf.py,235,"they should be transparent to contract developers, for whom pre_hook and",
scrapy/docs/conf.py,236,post_hook should be the actual concern.,
scrapy/docs/conf.py,239,"ContractsManager is an internal class, developers are not expected to",
scrapy/docs/conf.py,240,interact with it directly in any way.,
scrapy/docs/conf.py,243,For default contracts we only want to document their general purpose in,
scrapy/docs/conf.py,244,"their __init__ method, the methods they reimplement to achieve that purpose",
scrapy/docs/conf.py,245,should be irrelevant to developers using those contracts.,
scrapy/docs/conf.py,248,"Methods of downloader middlewares are not documented, only the classes",
scrapy/docs/conf.py,249,"themselves, since downloader middlewares are controlled through Scrapy",
scrapy/docs/conf.py,250,settings.,
scrapy/docs/conf.py,253,Base classes of downloader middlewares are implementation details that,
scrapy/docs/conf.py,254,are not meant for users.,
scrapy/docs/conf.py,257,Private exception used by the command-line interface implementation.,
scrapy/docs/conf.py,260,Methods of BaseItemExporter subclasses are only documented in,
scrapy/docs/conf.py,261,BaseItemExporter.,
scrapy/docs/conf.py,264,Extension behavior is only modified through settings. Methods of,
scrapy/docs/conf.py,265,"extension classes, as well as helper functions, are implementation",
scrapy/docs/conf.py,266,details that are not documented.,
scrapy/docs/conf.py,267,methods,
scrapy/docs/conf.py,268,helper functions,
scrapy/docs/conf.py,270,"Never documented before, and deprecated now.",
scrapy/docs/conf.py,274,Implementation detail of LxmlLinkExtractor,
scrapy/docs/conf.py,279,Options for the InterSphinx extension,
scrapy/docs/conf.py,280,-------------------------------------,
scrapy/docs/conf.py,294,Options for sphinx-hoverxref options,
scrapy/docs/conf.py,295,------------------------------------,
scrapy/docs/utils/linkfix.py,1,!/usr/bin/python,
scrapy/docs/utils/linkfix.py,17,Used for remembering the file (and its contents),
scrapy/docs/utils/linkfix.py,18,so we don't have to open the same file again.,
scrapy/docs/utils/linkfix.py,22,A regex that matches standard linkcheck output lines,
scrapy/docs/utils/linkfix.py,25,Read lines from the linkcheck output file,
scrapy/docs/utils/linkfix.py,33,"For every line, fix the respective file",
scrapy/docs/utils/linkfix.py,41,Broken links can't be fixed and,
scrapy/docs/utils/linkfix.py,42,I am not sure what do with the local ones.,
scrapy/docs/utils/linkfix.py,46,If this is a new file,
scrapy/docs/utils/linkfix.py,49,Update the previous file,
scrapy/docs/utils/linkfix.py,56,Read the new file to memory,
scrapy/docs/utils/linkfix.py,62,We don't understand what the current line means!,
scrapy/docs/_ext/,19,index entries for setting directives look like:,
scrapy/docs/_ext/,20,"[(u'pair', u'SETTING_NAME; setting', u'std:setting-SETTING_NAME', '')]",
scrapy/docs/_ext/,27,target nodes are placed next to the node in the doc tree,
scrapy/,18,TODO: add `name` attribute to commands and and merge this function with,
scrapy/,19,scrapy.utils.spider.iter_spider_classes,
scrapy/,113,set EDITOR from environment if available,
scrapy/,168,"Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect()",
scrapy/,169,on exit: http://doc.pypy.org/en/latest/cpython_differences.html?highlight=gc.collect#differences-related-to-garbage-collection-strategies,
scrapy/,18,"If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.",
scrapy/,19,Switch to 'allow all' state.,
scrapy/,68,backward compatibility with Scrapy logformatter below 1.4 version,
scrapy/,24,for backward compatibility,
scrapy/,85,Python <= 3.4 raises pickle.PicklingError here while,
scrapy/,86,3.5 <= Python < 3.6 raises AttributeError and,
scrapy/,87,Python >= 3.6 raises TypeError,
scrapy/,9,TODO: cache misses,
scrapy/,38,"in Twisted<=16.6, getHostByName() is always called with",
scrapy/,39,"a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),",
scrapy/,40,so the input argument above is simply overridden,
scrapy/,41,to enforce Scrapy's DNS_TIMEOUT setting's value,
scrapy/,1,-*- coding: utf-8 -*-,
scrapy/,113,Import twisted.mail here because it is not available in python3,
scrapy/,42,disable accidental Ctrl-C key press from shutting down the engine,
scrapy/,73,try all by default,
scrapy/,75,always add standard shell as fallback,
scrapy/,8,Internal,
scrapy/,24,HTTP and crawling,
scrapy/,44,Items,
scrapy/,57,Commands,
scrapy/,17,can return deferred,
scrapy/,20,can return a deferred,
scrapy/,23,log that a request has been filtered,
scrapy/,8,Scrapy version,
scrapy/,15,Check minimum required Python version,
scrapy/,21,Ignore noisy twisted deprecation warnings,
scrapy/,26,Apply monkey patches to fix issues in external libraries,
scrapy/,33,Declare top-level shortcuts,
scrapy/,101,there is a small difference between the behaviour or JsonItemExporter.indent,
scrapy/,102,and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent,
scrapy/,103,the addition of newlines everywhere,
scrapy/,203,Windows needs this https://github.com/scrapy/scrapy/issues/3034,
scrapy/,217,list in value may not contain strings,
scrapy/,242,for dicts try using fields of the first item,
scrapy/,245,use fields declared in Item,
scrapy/,4,Undo what Twisted's perspective broker adds to pickle register,
scrapy/,5,to prevent bugs like Twisted#7989 while serializing requests,
scrapy/,6,NOQA,
scrapy/,7,Remove only entries with twisted serializers for non-twisted types.,
scrapy/,78,avoid creating dict for most common case,
scrapy/,22,as we replace some letters we can get collision for different slots,
scrapy/,23,add we add unique part,
scrapy/,86,this may fail (eg. serialization error),
scrapy/,161,slot -> priority queue,
scrapy/,10,zope >= 5.0 only supports MultipleInvalid,
scrapy/,63,scrapy root handler already installed: update it with new settings,
scrapy/,65,lambda is assigned to Crawler attribute because this way it is not,
scrapy/,66,garbage collected after leaving __init__ scope,
scrapy/,315,Don't start the reactor if the deferreds are already fired,
scrapy/,326,blocking call,
scrapy/,337,raised if already stopped or in shutdown stage,
scrapy/,25,the cookiejar implementation iterates through all domains,
scrapy/,26,instead we restrict to potential matches on the domain,
scrapy/,50,This is still quite inefficient for large number of cookies,
scrapy/,142,python3 uses attributes instead of methods,
scrapy/,189,python3 cookiejars calls get_all,
scrapy/,193,python2 cookiejars calls getheaders,
scrapy/,122,type: (...) -> Request,
scrapy/,161,"type: (...) -> Generator[Request, None, None]",
scrapy/,41,used by encoding detection,
scrapy/,69,access self.encoding before _cached_ubody to make sure,
scrapy/,70,_body_inferred_encoding is called,
scrapy/,125,type: (...) -> Request,
scrapy/,167,"type: (...) -> Generator[Request, None, None]",
scrapy/,231,type: (parsel.Selector) -> str,
scrapy/,233,e.g. ::attr(href) result,
scrapy/,24,spec defines that requests must use POST method,
scrapy/,27,xmlrpc query multiples times over the same url,
scrapy/,30,restore encoding,
scrapy/,95,"Get form element from xpath, if not found, go up",
scrapy/,108,"If we get here, it means that either formname was None",
scrapy/,109,or invalid,
scrapy/,163,Match browser behaviour on simple select tag without options selected,
scrapy/,164,And for select tags wihout options,
scrapy/,168,This is a workround to bug in lxml fixed 2.3.1,
scrapy/,169,fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139,
scrapy/,190,"If we don't have clickdata, we just use the first clickable element",
scrapy/,195,"If clickdata is given, we compare it to the clickable elements to find a",
scrapy/,196,"match. We first look to see if the number is specified in clickdata,",
scrapy/,197,because that uniquely identifies the element,
scrapy/,207,"We didn't find it, so now we build an XPath expression out of the other",
scrapy/,208,"arguments, because they can be used as such",
scrapy/,23,this one has to be set first,
scrapy/,10,contracts,
scrapy/,61,calculate request args,
scrapy/,64,Don't filter requests to allow,
scrapy/,65,testing different callbacks on the same URL.,
scrapy/,74,check if all positional arguments are defined in kwargs,
scrapy/,78,execute pre and post hooks in order,
scrapy/,1,This package will contain the spiders of your Scrapy project,
scrapy/,2,,
scrapy/,3,Please refer to the documentation for information on how to create and manage,
scrapy/,4,your spiders.,
scrapy/,27,"`--compressed` argument is not safe to ignore, but it's included here",
scrapy/,28,because the `HttpCompressionMiddleware` is enabled by default,
scrapy/,63,"curl automatically prepends 'http' if the scheme is missing, but Request",
scrapy/,64,needs the scheme to work,
scrapy/,1,-*- coding: utf-8 -*-,
scrapy/,9,The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.,
scrapy/,10,"Using the binding directly, as this code does, requires cryptography 2.4.",
scrapy/,19,from OpenSSL.crypto.X509Name.__repr__,
scrapy/,27,requires OpenSSL 1.0.2,
scrapy/,30,adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key(),
scrapy/,17,Always use .instace() to ensure _instance propagation to all parents,
scrapy/,18,this is needed for <TAB> completion works well for new imports,
scrapy/,19,and clear the instance to always have the fresh env,
scrapy/,20,on repeated breaks like with inspect_response(),
scrapy/,50,readline module is only available on unix systems,
scrapy/,55,noqa: F401,
scrapy/,76,"list, preference order of shells",
scrapy/,78,available embeddable shells,
scrapy/,83,"function test: run all setup code (imports),",
scrapy/,84,but dont fall into the shell,
scrapy/,101,raised when using exit() in python code.interact,
scrapy/,28,this needs to be imported here until get rid of the spider manager,
scrapy/,29,singleton in scrapy.spider.spiders,
scrapy/,46,"BEGIN Backward compatibility for old (base, custom) call signature",
scrapy/,53,END Backward compatibility,
scrapy/,169,FEEDS setting should take precedence over the -o and -t CLI options,
scrapy/,50,assuming boto=2.2.2,
scrapy/,64,loads acl before it will be deleted,
scrapy/,9,noqa: F401,
scrapy/,9,noqa: F401,
scrapy/,22,urls should be safe (safe_string_url),
scrapy/,89,func has no __self__,
scrapy/,1,-*- coding: utf-8 -*-,
scrapy/,86,Route warnings through python logging,
scrapy/,204,"NOTE: This also handles 'args' being an empty dict, that case doesn't",
scrapy/,205,play well in logger.log calls,
scrapy/,8,noqa: F401,
scrapy/,65,XXX: this implementation is a bit dirty and could be improved,
scrapy/,3,used in global tests code,
scrapy/,4,noqa: F401,
scrapy/,259,not attributes given return False by default,
scrapy/,265,support callables like itemgetter,
scrapy/,271,all attributes equal,
scrapy/,322,Collecting weakreferences can take two collections on PyPy.,
scrapy/,94,named group,
scrapy/,96,full regex or numbered groups,
scrapy/,24,Catch Ctrl-Break in windows,
scrapy/,56,FIXME: Hack to avoid introspecting tracebacks. This to speed up,
scrapy/,57,"processing of IgnoreRequest errors which are, by far, the most common",
scrapy/,58,exception in Scrapy - see #125,
scrapy/,125,workaround for Python before 3.5.3 not having asyncio.isfuture,
scrapy/,137,"wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines",
scrapy/,138,"that use asyncio, e.g. ""await asyncio.sleep(1)""",
scrapy/,141,"wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor",
scrapy/,162,noqa: E722,
scrapy/,11,noqa: F401,
scrapy/,12,scrapy.utils.url was moved to w3lib.url and import * ensures this,
scrapy/,13,move doesn't break old code,
scrapy/,15,noqa: F401,
scrapy/,92,Note: this does not match Windows filepath,
scrapy/,77,see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass,
scrapy/,78,and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks,
scrapy/,79,for implementation details,
scrapy/,86,we should do the magic only if second `issubclass` argument,
scrapy/,87,is the deprecated class itself - subclasses of the,
scrapy/,88,deprecated class should not use custom `__subclasscheck__`,
scrapy/,89,method.,
scrapy/,114,Sometimes inspect.stack() fails (e.g. when the first import of,
scrapy/,115,deprecated class is in jinja2 template). __module__ attribute is not,
scrapy/,116,important enough to raise an exception as users may be unable,
scrapy/,117,to fix inspect.stack() errors.,
scrapy/,103,"key is not weak-referenceable, skip caching",
scrapy/,109,"key is not weak-referenceable, it's not cached",
scrapy/,9,- Python>=3.5 GzipFile's read() has issues returning leftover,
scrapy/,10,uncompressed data when input is corrupted,
scrapy/,11,(regression or bug-fix compared to Python 3.4),
scrapy/,12,"- read1(), which fetches data before raising EOFError on next call",
scrapy/,13,works here but is only available from Python>=3.3,
scrapy/,32,"complete only if there is some data, otherwise re-raise",
scrapy/,33,see issue 87 about catching struct.error,
scrapy/,34,some pages are quite small so output_list is empty and f.extrabuf,
scrapy/,35,contains the whole page content,
scrapy/,11,trial chdirs to temp dir,
scrapy/,122,non serializable request,
scrapy/,28,requests in progress,
scrapy/,97,Will also close spiders and downloader,
scrapy/,100,Will also close downloader,
scrapy/,169,"downloader middleware can return requests (for example, redirects)",
scrapy/,173,response is a Response or Failure,
scrapy/,182,scraper is not idle,
scrapy/,186,downloader has pending requests,
scrapy/,190,not all start requests are handled,
scrapy/,194,scheduler has pending requests,
scrapy/,237,tie request to response received,
scrapy/,128,returns spider's processed output,
scrapy/,74,don't handle _InvalidOutput exception,
scrapy/,83,stop exception handling by handing control over to the,
scrapy/,84,process_spider_output chain if an iterable has been returned,
scrapy/,94,items in this iterable do not need to go through the process_spider_output,
scrapy/,95,"chain, they went through it already from the process_spider_exception method",
scrapy/,103,might fail directly if the output value is not a generator,
scrapy/,140,Delay queue processing if a download_delay is configured,
scrapy/,149,Process enqueued requests if there are free slots to transfer for this slot,
scrapy/,155,prevent burst if inter-request delays were configured,
scrapy/,161,The order is very important for the following deferreds. Do not change!,
scrapy/,163,1. Create the download deferred,
scrapy/,166,2. Notify response_downloaded listeners about the recent download,
scrapy/,167,before querying queue for next request,
scrapy/,176,"3. After response arrives,  remove the request from transferring",
scrapy/,177,state to free up the transferring slot so it can be used by the,
scrapy/,178,following requests (perhaps those which came from the downloader,
scrapy/,179,middleware itself),
scrapy/,38,setting verify=True will require you to provide CAs,
scrapy/,39,to verify against; in other words: it's not that simple,
scrapy/,41,backward-compatible SSL/TLS method:,
scrapy/,42,,
scrapy/,43,* this will respect `method` attribute in often recommended,
scrapy/,44,`ScrapyClientContextFactory` subclass,
scrapy/,45,(https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133),
scrapy/,46,,
scrapy/,47,* getattr() for `_ssl_method` attribute for context factories,
scrapy/,48,"not calling super(..., self).__init__",
scrapy/,55,"kept for old-style HTTP/1.0 downloader context twisted calls,",
scrapy/,56,e.g. connectSSL(),
scrapy/,85,trustRoot set to platformTrust() will use the platform's root CAs.,
scrapy/,86,,
scrapy/,87,This means that a website like https://www.cacert.org will be rejected,
scrapy/,88,"by default, since CAcert.org CA certificate is seldom shipped.",
scrapy/,23,protocol negotiation (recommended),
scrapy/,24,SSL 3 (NOT recommended),
scrapy/,25,TLS 1.0 only,
scrapy/,26,TLS 1.1 only,
scrapy/,27,TLS 1.2 only,
scrapy/,58,requires pyOPenSSL 0.15,
scrapy/,59,requires pyOPenSSL 16.0.0,
scrapy/,15,"Assume parsed is urlparse-d from Request.url,",
scrapy/,16,which was passed via safe_url_string and is ascii-only.,
scrapy/,45,bucket for response headers,
scrapy/,47,Method command,
scrapy/,49,Headers,
scrapy/,54,Body,
scrapy/,87,transport cleanup needed for HTTPS connections,
scrapy/,110,converting to bytes to comply to Twisted interface,
scrapy/,120,Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected,
scrapy/,121,to have _disconnectedDeferred. See Twisted r32329.,
scrapy/,122,As Scrapy implements it's own logic to handle redirects is not,
scrapy/,123,needed to add the callback _waitForDisconnect.,
scrapy/,124,Specifically this avoids the AttributeError exception when,
scrapy/,125,clientConnectionFailed method is called.,
scrapy/,130,set Host header based on url,
scrapy/,133,set Content-Length based len of body,
scrapy/,136,just in case a broken http/1.1 decides to keep connection alive,
scrapy/,138,Content-Length must be specified in POST method even with no body,
scrapy/,43,try method-aware context factory,
scrapy/,52,use context factory defaults,
scrapy/,87,"closeCachedConnections will hang on network or server issues, so",
scrapy/,88,we'll manually timeout the deferred.,
scrapy/,89,,
scrapy/,90,Twisted issue addressing this problem can be found here:,
scrapy/,91,https://twistedmatrix.com/trac/ticket/7738.,
scrapy/,92,,
scrapy/,93,"closeCachedConnections doesn't handle external errbacks, so we'll",
scrapy/,94,issue a callback after `_disconnect_timeout` seconds.,
scrapy/,145,make sure that enough (all) bytes are consumed,
scrapy/,146,and that we've got all HTTP headers (ending with a blank line),
scrapy/,147,from the proxy so that we don't send those bytes to the TLS layer,
scrapy/,148,,
scrapy/,149,see https://github.com/scrapy/scrapy/issues/2491,
scrapy/,155,set proper Server Name Indication extension,
scrapy/,228,proxy host and port are required for HTTP pool `key`,
scrapy/,229,"otherwise, same remote host connection request could reuse",
scrapy/,230,a cached tunneled connection to a different proxy,
scrapy/,258,"Cache *all* connections under the same key, since we are only",
scrapy/,259,"connecting to a single destination, the proxy:",
scrapy/,336,request details,
scrapy/,348,set download latency,
scrapy/,350,response body is ready to be consumed,
scrapy/,353,check download timeout,
scrapy/,362,"needed for HTTPS requests, otherwise _ResponseReader doesn't",
scrapy/,363,receive connectionLost(),
scrapy/,374,deliverBody hangs for responses without body,
scrapy/,398,Abort connection immediately.,
scrapy/,406,save response for timeouts,
scrapy/,459,This maybe called several times after cancel was called with buffered data.,
scrapy/,472,Clear buffer earlier to avoid keeping data in memory for a long time.,
scrapy/,25,noqa: F401,
scrapy/,45,"If no credentials could be found anywhere,",
scrapy/,46,consider this an anonymous connection request by default;,
scrapy/,47,unless 'anon' was set explicitly (True/False).,
scrapy/,21,stores acceptable schemes on instancing,
scrapy/,22,stores instanced handlers for schemes,
scrapy/,23,remembers failed handlers,
scrapy/,46,values from initial item,
scrapy/,440,Do not pass kwarg values here. We don't want to promote user-defined,
scrapy/,441,"dicts, and we want to update, not replace, default dicts with the",
scrapy/,442,values given by the user,
scrapy/,445,Promote default dictionaries to BaseSettings instances for per-key,
scrapy/,446,priorities,
scrapy/,76,3mins,
scrapy/,78,1024m,
scrapy/,79,32m,
scrapy/,88,"Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:",
scrapy/,95,Engine side,
scrapy/,110,Downloader side,
scrapy/,137,a function to extend uri arguments,
scrapy/,215,enable memory debugging,
scrapy/,216,send memory debugging report by mail at engine shutdown,
scrapy/,235,uses Firefox default setting,
scrapy/,242,initial response + 2 retries = 3 requests,
scrapy/,263,Engine side,
scrapy/,269,Spider side,
scrapy/,74,actual gzipped sitemap files are decompressed above ;,
scrapy/,75,if we are here (response body is not gzipped),
scrapy/,76,"and have a response for .xml.gz,",
scrapy/,77,it usually means that it was already gunzipped,
scrapy/,78,"by HttpCompression middleware,",
scrapy/,79,"the HTTP response being sent with ""Content-Encoding: gzip""",
scrapy/,80,"without actually being a .xml.gz file in the first place,",
scrapy/,81,"merely XML gzip-compressed on the fly,",
scrapy/,82,"in other word, here, we have plain XML",
scrapy/,97,"Also consider alternate URLs (xhtml:link rel=""alternate"")",
scrapy/,47,backward compatibility,
scrapy/,103,"When this is None, python's csv module's default delimiter is used",
scrapy/,104,"When this is None, python's csv module's default quotechar is used",
scrapy/,112,Top-level imports,
scrapy/,113,noqa: F401,
scrapy/,114,noqa: F401,
scrapy/,115,noqa: F401,
scrapy/,57,wrapper needed to allow to work directly with text,
scrapy/,71,wrapper needed to allow to work directly with text,
scrapy/,17,from lxml/src/lxml/html/__init__.py,
scrapy/,56,hacky way to get the underlying lxml parsed document,
scrapy/,58,pseudo lxml.html.HtmlElement.make_links_absolute(base_url),
scrapy/,64,skipping bogus links,
scrapy/,70,to fix relative links after process_value,
scrapy/,22,common file extensions that are not followed if they occur in links,
scrapy/,24,archives,
scrapy/,27,images,
scrapy/,31,audio,
scrapy/,34,video,
scrapy/,38,office suites,
scrapy/,42,other,
scrapy/,130,Top-level imports,
scrapy/,131,noqa: F401,
scrapy/,97,BEGIN Backward compatibility for initialising without keys (and,
scrapy/,98,without using from_crawler),
scrapy/,113,END Backward compatibility,
scrapy/,119,"remove first ""/""",
scrapy/,187,feed params,
scrapy/,191,flags,
scrapy/,225,Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings,
scrapy/,232,handle pathlib.Path objects,
scrapy/,235,End: Backward compatibility for FEED_URI and FEED_FORMAT settings,
scrapy/,237,'FEEDS' setting takes precedence over 'FEED_URI',
scrapy/,239,handle pathlib.Path objects,
scrapy/,271,We need to call slot.storage.store nonetheless to get the file,
scrapy/,272,properly closed.,
scrapy/,28,stdlib's resource module is only available on unix platforms.,
scrapy/,50,"on macOS ru_maxrss is in bytes, on Linux it is in KB",
scrapy/,97,warn only once,
scrapy/,44,one year,
scrapy/,67,"obey user-agent directive ""Cache-Control: no-store""",
scrapy/,70,Any other is eligible for caching,
scrapy/,74,What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1,
scrapy/,75,Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4,
scrapy/,76,Status code 206 is not included because cache can not deal with partial contents,
scrapy/,78,"obey directive ""Cache-Control: no-store""",
scrapy/,81,Never cache 304 (Not Modified) responses,
scrapy/,84,Cache unconditionally if configured to do so,
scrapy/,87,Any hint on response expiration is good,
scrapy/,90,Firefox fallbacks this statuses to one year expiration if none is set,
scrapy/,93,Other statuses without expiration requires at least one validator,
scrapy/,96,Any other is probably not eligible for caching,
scrapy/,97,Makes no sense to cache responses that does not contain expiration,
scrapy/,98,info and can not be revalidated,
scrapy/,120,"From RFC2616: ""Indicates that the client is willing to",
scrapy/,121,accept a response that has exceeded its expiration time.,
scrapy/,122,"If max-stale is assigned a value, then the client is",
scrapy/,123,willing to accept a response that has exceeded its,
scrapy/,124,expiration time by no more than the specified number of,
scrapy/,125,"seconds. If no value is assigned to max-stale, then the",
scrapy/,126,"client is willing to accept a stale response of any age.""",
scrapy/,137,"Cached response is stale, try to set validators if any",
scrapy/,142,"Use the cached response if the new response is a server error,",
scrapy/,143,as long as the old response didn't specify must-revalidate.,
scrapy/,149,Use the cached response if the server says it hasn't changed.,
scrapy/,166,Reference nsHttpResponseHead::ComputeFreshnessLifetime,
scrapy/,167,https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706,
scrapy/,173,Parse date header or synthesize it if none exists,
scrapy/,176,Try HTTP/1.0 Expires header,
scrapy/,179,When parsing Expires header fails RFC 2616 section 14.21 says we,
scrapy/,180,should treat this as an expiration time in the past.,
scrapy/,183,Fallback to heuristic using last-modified header,
scrapy/,184,This is not in RFC but on Firefox caching implementation,
scrapy/,189,This request can be cached indefinitely,
scrapy/,193,Insufficient information to compute fresshness lifetime,
scrapy/,197,Reference nsHttpResponseHead::ComputeCurrentAge,
scrapy/,198,https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658,
scrapy/,200,"If Date header is not set we assume it is a fast connection, and",
scrapy/,201,clock is in sync with the server,
scrapy/,236,not cached,
scrapy/,261,not found,
scrapy/,265,expired,
scrapy/,292,not cached,
scrapy/,338,not found,
scrapy/,341,expired,
scrapy/,32,signal to update telnet variables,
scrapy/,33,args: telnet_vars,
scrapy/,96,Note: if you add entries here also update topics/telnetconsole.rst,
scrapy/,71,If a server needs `latency` seconds to respond then,
scrapy/,72,we should send a request each `latency/N` seconds,
scrapy/,73,to have N requests processed in parallel,
scrapy/,76,Adjust the delay to make it closer to target_delay,
scrapy/,79,"If target delay is bigger than old delay, then use it instead of mean.",
scrapy/,80,It works better with problematic sites.,
scrapy/,83,Make sure self.mindelay <= new_delay <= self.max_delay,
scrapy/,86,Dont adjust delay if response status != 200 and new delay is smaller,
scrapy/,87,"than old one, as error pages (and redirections) are usually small and",
scrapy/,88,"so tend to reduce latency, thus provoking a positive feedback by",
scrapy/,89,reducing delay instead of increase.,
scrapy/,28,win32 platforms don't support SIGUSR signals,
scrapy/,60,win32 platforms don't support SIGUSR signals,
scrapy/,4,noqa: F401,
scrapy/,54,"by default, let the framework handle redirects,",
scrapy/,55,i.e. command handles all codes expect 3xx,
scrapy/,58,load contracts,
scrapy/,64,contract requests,
scrapy/,81,start checks,
scrapy/,149,"Request requires callback argument as callable or None, not string",
scrapy/,165,memorize first request,
scrapy/,169,determine real callback,
scrapy/,193,parse items and requests,
scrapy/,211,update request meta if any extra meta was passed through the --meta/-m opts.,
scrapy/,215,update cb_kwargs if any extra values were was passed through the --cbkwargs option.,
scrapy/,254,parse arguments,
scrapy/,260,prepare spidercls,
scrapy/,52,first argument may be a local file,
scrapy/,64,The crawler is created this way since the Shell manually handles the,
scrapy/,65,"crawling engine, so the set up in the crawl method won't work",
scrapy/,67,The Shell class needs a persistent engine in the crawler,
scrapy/,74,if spider already exists and not --force then halt,
scrapy/,17,default settings to be used for this command instead of global defaults,
scrapy/,23,set in scrapy.cmdline,
scrapy/,86,Overriden from settings.FILES_STORE_S3_ACL in,
scrapy/,87,FilesPipeline.from_settings.,
scrapy/,128,disable ssl (is_secure=False) because of this python bug:,
scrapy/,129,https://bugs.python.org/issue5103,
scrapy/,176,This is required while we need to support both boto and botocore.,
scrapy/,222,The bucket's default object ACL will be applied to the object.,
scrapy/,223,Overriden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.,
scrapy/,299,The file doesn't exist,
scrapy/,388,to support win32 paths like: C:\\some\dir,
scrapy/,398,returning None force download,
scrapy/,402,returning None force download,
scrapy/,407,returning None force download,
scrapy/,503,Overridable Interface,
scrapy/,523,Handles empty and wild extensions by trying to guess the,
scrapy/,524,mime type then extension or default to empty string otherwise,
scrapy/,89,Return cached result if request was already seen,
scrapy/,93,"Otherwise, wait for result",
scrapy/,97,Check if request is downloading right now to avoid doing it twice,
scrapy/,101,Download request checking media_to_download hook output first,
scrapy/,109,it must return wad at last,
scrapy/,121,this ugly code was left only to support tests. TODO: remove,
scrapy/,136,minimize cached information for failure,
scrapy/,141,This code fixes a memory leak by avoiding to keep references to,
scrapy/,142,the Request and Response objects on the Media Pipeline cache.,
scrapy/,143,,
scrapy/,144,Twisted inline callbacks pass return values using the function,
scrapy/,145,"twisted.internet.defer.returnValue, which encapsulates the return",
scrapy/,146,value inside a _DefGen_Return base exception.,
scrapy/,147,,
scrapy/,148,What happens when the media_downloaded callback raises another,
scrapy/,149,"exception, for example a FileException('download-error') when",
scrapy/,150,"the Response status code is not 200 OK, is that it stores the",
scrapy/,151,_DefGen_Return exception on the FileException context.,
scrapy/,152,,
scrapy/,153,To avoid keeping references to the Response and therefore Request,
scrapy/,154,"objects on the Media Pipeline cache, we should wipe the context of",
scrapy/,155,the exception encapsulated by the Twisted Failure when its a,
scrapy/,156,_DefGen_Return instance.,
scrapy/,157,,
scrapy/,158,This problem does not occur in Python 2.7 since we don't have,
scrapy/,159,Exception Chaining (https://www.python.org/dev/peps/pep-3134/).,
scrapy/,165,cache result,
scrapy/,169,Overridable Interface,
scrapy/,17,TODO: from scrapy.pipelines.media import MediaPipeline,
scrapy/,36,Uppercase attributes kept for backward compatibility with code that subclasses,
scrapy/,37,ImagesPipeline. They may be overridden by settings.,
scrapy/,47,hostname can be None for wrong urls (like javascript links),
scrapy/,55,allow all by default,
scrapy/,73,Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy,
scrapy/,263,Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string,
scrapy/,301,Note: this hook is a bit of a hack to intercept redirections,
scrapy/,341,"check redirected request to patch ""Referer"" header if necessary",
scrapy/,345,we don't patch the referrer value if there is none,
scrapy/,347,the request's referrer header value acts as a surrogate,
scrapy/,348,for the parent response URL,
scrapy/,349,,
scrapy/,350,"Note: if the 3xx response contained a Referrer-Policy header,",
scrapy/,351,the information is not available using this hook,
scrapy/,32,common case,
scrapy/,52,base case (depth=0),
scrapy/,31,"check if parser dependencies are met, this should throw an error otherwise.",
scrapy/,36,set Cookie header,
scrapy/,45,extract cookies from Set-Cookie and drop invalid/expired cookies,
scrapy/,72,build cookie string,
scrapy/,52,Skip uncacheable requests,
scrapy/,54,flag as uncacheable,
scrapy/,57,Look for cached response and check if expired,
scrapy/,64,first time request,
scrapy/,66,Return cached response only if not expired,
scrapy/,72,Keep a reference to cached response to avoid a second cache lookup on,
scrapy/,73,process_response hook,
scrapy/,80,Skip cached responses and uncacheable requests,
scrapy/,85,"RFC2616 requires origin server to set Date header,",
scrapy/,86,https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18,
scrapy/,90,Do not validate first-hand responses,
scrapy/,30,IOError is raised by the HttpCompression middleware when trying to,
scrapy/,31,decompress an empty response,
scrapy/,1,-*- coding: utf-8 -*-,
scrapy/,24,XXX: Google parses at least first 100k bytes; scrapy's redirect,
scrapy/,25,middleware parses first 4k. 4k turns out to be insufficient,
scrapy/,26,"for this middleware, and parsing 100k could be slow.",
scrapy/,27,We use something in between (32K) by default.,
scrapy/,40,other HTTP methods are either not safe or don't have a body,
scrapy/,43,prevent loops,
scrapy/,49,scrapy already handles #! links properly,
scrapy/,67,XXX: move it to w3lib?,
scrapy/,83,Stripping scripts and comments is slow (about 20x slower than,
scrapy/,84,just checking if a string is in text); this is a quick fail-fast,
scrapy/,85,path that should work for most pages.,
scrapy/,45,force recalculating the encoding until we make sure the,
scrapy/,46,responsetypes guessing is reliable,
scrapy/,62,ugly hack to work with raw deflate content that may,
scrapy/,63,"be sent by microsoft servers. For more information, see:",
scrapy/,64,http://carsten.codimi.de/gzip.yaws/,
scrapy/,65,http://www.port80software.com/200ok/archive/2005/10/31/868.aspx,
scrapy/,66,http://www.gzip.org/zlib/zlib_faq.html#faq38,
scrapy/,43,ignore if proxy is already set,
scrapy/,47,extract credentials if present,
scrapy/,59,'no_proxy' is only supported by http schemes,
